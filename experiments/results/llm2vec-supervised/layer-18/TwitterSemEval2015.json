{"dataset_revision": "70970daeab8776df92f5ea462b6173c0b46fd2d1", "evaluation_time": 197.38273072242737, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"cosine": {"accuracy": 0.8689873040472075, "accuracy_threshold": 0.8660793304443359, "ap": 0.7638479777786743, "f1": 0.6961311811427108, "f1_threshold": 0.8395851850509644, "precision": 0.6765438247011952, "recall": 0.7168865435356201}, "dot": {"accuracy": 0.7748107528163557, "accuracy_threshold": 140.5596160888672, "ap": 0.3036390913039495, "f1": 0.38574267262791856, "f1_threshold": 77.98747253417969, "precision": 0.25223323046938445, "recall": 0.8195250659630607}, "euclidean": {"accuracy": 0.8680336174524647, "accuracy_threshold": 5.491673946380615, "ap": 0.7555721611149344, "f1": 0.696715238330452, "f1_threshold": 6.030850410461426, "precision": 0.6548282265552461, "recall": 0.7443271767810027}, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.7638479777786743, "manhattan": {"accuracy": 0.8701198068784646, "accuracy_threshold": 270.41033935546875, "ap": 0.7600421244647124, "f1": 0.7037223340040241, "f1_threshold": 295.4637756347656, "precision": 0.672272945699183, "recall": 0.7382585751978892}, "max": {"accuracy": 0.8701198068784646, "ap": 0.7638479777786743, "f1": 0.7037223340040241}, "similarity": {"accuracy": 0.8689873040472075, "accuracy_threshold": 0.8660793304443359, "ap": 0.7638479777786743, "f1": 0.6961311811427108, "f1_threshold": 0.8395851850509644, "precision": 0.6765438247011952, "recall": 0.7168865435356201}}]}, "task_name": "TwitterSemEval2015", "avg_layerwise_entropy": 0.41384852087589197, "num_samples": 64}