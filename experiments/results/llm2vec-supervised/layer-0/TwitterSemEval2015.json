{"dataset_revision": "70970daeab8776df92f5ea462b6173c0b46fd2d1", "evaluation_time": 202.0176510810852, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"cosine": {"accuracy": 0.8036001668951541, "accuracy_threshold": 0.7343878746032715, "ap": 0.5072658866528887, "f1": 0.4922462406015038, "f1_threshold": 0.648777961730957, "precision": 0.44366793731469717, "recall": 0.5527704485488126}, "dot": {"accuracy": 0.7858973594802408, "accuracy_threshold": 0.026137158274650574, "ap": 0.42317395984097206, "f1": 0.43953105717811597, "f1_threshold": 0.020815644413232803, "precision": 0.36012807549713516, "recall": 0.5638522427440633}, "euclidean": {"accuracy": 0.7928711927042975, "accuracy_threshold": 0.12412704527378082, "ap": 0.46597837218071236, "f1": 0.4662974187480406, "f1_threshold": 0.1567791849374771, "precision": 0.38605295033742865, "recall": 0.5886543535620052}, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.5072658866528887, "manhattan": {"accuracy": 0.7923347439947547, "accuracy_threshold": 6.1793317794799805, "ap": 0.46192844234150204, "f1": 0.46214883122453354, "f1_threshold": 7.847965240478516, "precision": 0.38927023121387283, "recall": 0.5686015831134564}, "max": {"accuracy": 0.8036001668951541, "ap": 0.5072658866528887, "f1": 0.4922462406015038}, "similarity": {"accuracy": 0.8036001668951541, "accuracy_threshold": 0.7343878746032715, "ap": 0.5072658866528887, "f1": 0.4922462406015038, "f1_threshold": 0.648777961730957, "precision": 0.44366793731469717, "recall": 0.5527704485488126}}]}, "task_name": "TwitterSemEval2015", "avg_layerwise_entropy": 0.8969304978801979, "num_samples": 64}