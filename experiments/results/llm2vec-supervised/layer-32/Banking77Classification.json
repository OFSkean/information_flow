{"dataset_revision": "0fd18e25b25c072e09e0d92ab615fda904d66300", "evaluation_time": 156.63263511657715, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"accuracy": 0.8801623376623375, "f1": 0.8794992598723794, "f1_weighted": 0.8794992598723796, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.8801623376623375, "scores_per_experiment": [{"accuracy": 0.8746753246753247, "f1": 0.8748336145079542, "f1_weighted": 0.8748336145079545}, {"accuracy": 0.8896103896103896, "f1": 0.8895424054952854, "f1_weighted": 0.8895424054952857}, {"accuracy": 0.8873376623376623, "f1": 0.8865570444444776, "f1_weighted": 0.8865570444444777}, {"accuracy": 0.8785714285714286, "f1": 0.8778762630679157, "f1_weighted": 0.8778762630679154}, {"accuracy": 0.8730519480519481, "f1": 0.8728910699217475, "f1_weighted": 0.8728910699217474}, {"accuracy": 0.8769480519480519, "f1": 0.8759766636555237, "f1_weighted": 0.8759766636555238}, {"accuracy": 0.8814935064935064, "f1": 0.8801156725103366, "f1_weighted": 0.8801156725103367}, {"accuracy": 0.8857142857142857, "f1": 0.8857474765317604, "f1_weighted": 0.8857474765317603}, {"accuracy": 0.8769480519480519, "f1": 0.8748608442775954, "f1_weighted": 0.8748608442775955}, {"accuracy": 0.8772727272727273, "f1": 0.876591544311199, "f1_weighted": 0.8765915443111992}]}]}, "task_name": "Banking77Classification", "avg_layerwise_entropy": 0.8884968905518186, "num_samples": 64}