{"dataset_revision": "70970daeab8776df92f5ea462b6173c0b46fd2d1", "evaluation_time": 209.453471660614, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"cosine": {"accuracy": 0.8854980032186922, "accuracy_threshold": 0.8274345397949219, "ap": 0.8116982877041135, "f1": 0.738891023178384, "f1_threshold": 0.801772952079773, "precision": 0.7178402587708386, "recall": 0.7612137203166227}, "dot": {"accuracy": 0.8772128509268642, "accuracy_threshold": 16061.29296875, "ap": 0.7924849777286953, "f1": 0.7192569335426479, "f1_threshold": 15768.001953125, "precision": 0.7132848988064349, "recall": 0.7253298153034301}, "euclidean": {"accuracy": 0.886034451928235, "accuracy_threshold": 82.06777954101562, "ap": 0.8086189845447604, "f1": 0.737550122881904, "f1_threshold": 87.3665771484375, "precision": 0.7234204516620147, "recall": 0.7522427440633246}, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.8116982877041135, "manhattan": {"accuracy": 0.8857960302795493, "accuracy_threshold": 4135.19140625, "ap": 0.8086351709753917, "f1": 0.7374562427071177, "f1_threshold": 4442.33544921875, "precision": 0.724955391282182, "recall": 0.7503957783641161}, "max": {"accuracy": 0.886034451928235, "ap": 0.8116982877041135, "f1": 0.738891023178384}, "similarity": {"accuracy": 0.8854980032186922, "accuracy_threshold": 0.8274345397949219, "ap": 0.8116982877041135, "f1": 0.738891023178384, "f1_threshold": 0.801772952079773, "precision": 0.7178402587708386, "recall": 0.7612137203166227}}]}, "task_name": "TwitterSemEval2015", "avg_layerwise_entropy": 0.8952337088980762, "num_samples": 64}