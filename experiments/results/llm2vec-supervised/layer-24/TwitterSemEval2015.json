{"dataset_revision": "70970daeab8776df92f5ea462b6173c0b46fd2d1", "evaluation_time": 199.8957450389862, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"cosine": {"accuracy": 0.8713715205340645, "accuracy_threshold": 0.9187107086181641, "ap": 0.772361442598814, "f1": 0.7110438729198185, "f1_threshold": 0.9043557047843933, "precision": 0.680830516658619, "recall": 0.7440633245382586}, "dot": {"accuracy": 0.7742146986946414, "accuracy_threshold": 616.37939453125, "ap": 0.2891294811708863, "f1": 0.3763410016637026, "f1_threshold": 400.12225341796875, "precision": 0.24045157979620263, "recall": 0.8654353562005277}, "euclidean": {"accuracy": 0.8707158610001788, "accuracy_threshold": 9.317359924316406, "ap": 0.7687065170247677, "f1": 0.7102030259924997, "f1_threshold": 9.764039993286133, "precision": 0.696424042607152, "recall": 0.7245382585751979}, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.772361442598814, "manhattan": {"accuracy": 0.8715503367705788, "accuracy_threshold": 459.358154296875, "ap": 0.7701295621778975, "f1": 0.7131500065163561, "f1_threshold": 485.06402587890625, "precision": 0.7046098377543136, "recall": 0.7218997361477573}, "max": {"accuracy": 0.8715503367705788, "ap": 0.772361442598814, "f1": 0.7131500065163561}, "similarity": {"accuracy": 0.8713715205340645, "accuracy_threshold": 0.9187107086181641, "ap": 0.772361442598814, "f1": 0.7110438729198185, "f1_threshold": 0.9043557047843933, "precision": 0.680830516658619, "recall": 0.7440633245382586}}]}, "task_name": "TwitterSemEval2015", "avg_layerwise_entropy": 0.5998776409179407, "num_samples": 64}