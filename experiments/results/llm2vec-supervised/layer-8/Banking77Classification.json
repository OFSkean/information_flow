{"dataset_revision": "0fd18e25b25c072e09e0d92ab615fda904d66300", "evaluation_time": 164.95899295806885, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"accuracy": 0.7495779220779221, "f1": 0.7472221752933889, "f1_weighted": 0.747222175293389, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.7495779220779221, "scores_per_experiment": [{"accuracy": 0.750974025974026, "f1": 0.7483473322619986, "f1_weighted": 0.7483473322619988}, {"accuracy": 0.7305194805194806, "f1": 0.7293897038987536, "f1_weighted": 0.7293897038987538}, {"accuracy": 0.7363636363636363, "f1": 0.7336018630990113, "f1_weighted": 0.7336018630990112}, {"accuracy": 0.7542207792207792, "f1": 0.7501780764862553, "f1_weighted": 0.7501780764862553}, {"accuracy": 0.7464285714285714, "f1": 0.743027264118676, "f1_weighted": 0.743027264118676}, {"accuracy": 0.7474025974025974, "f1": 0.745164744590314, "f1_weighted": 0.7451647445903141}, {"accuracy": 0.7532467532467533, "f1": 0.7517037495436071, "f1_weighted": 0.7517037495436072}, {"accuracy": 0.7561688311688312, "f1": 0.754453649503327, "f1_weighted": 0.7544536495033274}, {"accuracy": 0.7577922077922078, "f1": 0.7550939581176731, "f1_weighted": 0.755093958117673}, {"accuracy": 0.7626623376623377, "f1": 0.761261411314273, "f1_weighted": 0.7612614113142732}]}]}, "task_name": "Banking77Classification", "avg_layerwise_entropy": 0.07771952677578321, "num_samples": 64}