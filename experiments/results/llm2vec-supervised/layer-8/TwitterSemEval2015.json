{"dataset_revision": "70970daeab8776df92f5ea462b6173c0b46fd2d1", "evaluation_time": 195.89544367790222, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"cosine": {"accuracy": 0.840317100792752, "accuracy_threshold": 0.8157570958137512, "ap": 0.6714239804600945, "f1": 0.628370751285589, "f1_threshold": 0.7849392890930176, "precision": 0.5988524982070285, "recall": 0.6609498680738787}, "dot": {"accuracy": 0.7928115872921261, "accuracy_threshold": 10.140710830688477, "ap": 0.48612181469813814, "f1": 0.5032912485162404, "f1_threshold": 8.909289360046387, "precision": 0.4257805367902136, "recall": 0.6153034300791557}, "euclidean": {"accuracy": 0.8382309113667521, "accuracy_threshold": 2.067854881286621, "ap": 0.6584834339425941, "f1": 0.6241468580842552, "f1_threshold": 2.2740988731384277, "precision": 0.5632965165675446, "recall": 0.6997361477572559}, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.6714239804600945, "manhattan": {"accuracy": 0.8393634141980092, "accuracy_threshold": 103.39387512207031, "ap": 0.6618459023517899, "f1": 0.62793542074364, "f1_threshold": 111.66041564941406, "precision": 0.5852713178294574, "recall": 0.6773087071240106}, "max": {"accuracy": 0.840317100792752, "ap": 0.6714239804600945, "f1": 0.628370751285589}, "similarity": {"accuracy": 0.840317100792752, "accuracy_threshold": 0.8157570958137512, "ap": 0.6714239804600945, "f1": 0.628370751285589, "f1_threshold": 0.7849392890930176, "precision": 0.5988524982070285, "recall": 0.6609498680738787}}]}, "task_name": "TwitterSemEval2015", "avg_layerwise_entropy": 0.14098559933299487, "num_samples": 64}