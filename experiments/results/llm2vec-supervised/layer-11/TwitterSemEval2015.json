{"dataset_revision": "70970daeab8776df92f5ea462b6173c0b46fd2d1", "evaluation_time": 205.54271864891052, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"cosine": {"accuracy": 0.8525362102878942, "accuracy_threshold": 0.817955732345581, "ap": 0.7086220349612387, "f1": 0.6526782223380685, "f1_threshold": 0.7960323095321655, "precision": 0.6448622199330415, "recall": 0.6606860158311345}, "dot": {"accuracy": 0.7916194790486977, "accuracy_threshold": 14.371452331542969, "ap": 0.4863789666123418, "f1": 0.5167764189401066, "f1_threshold": 12.339454650878906, "precision": 0.42790375627488314, "recall": 0.6522427440633245}, "euclidean": {"accuracy": 0.8468736961316088, "accuracy_threshold": 2.375032424926758, "ap": 0.6905275634887482, "f1": 0.644167450611477, "f1_threshold": 2.6437301635742188, "precision": 0.5810352142554094, "recall": 0.7226912928759894}, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.7086220349612387, "manhattan": {"accuracy": 0.8479465935506943, "accuracy_threshold": 118.50636291503906, "ap": 0.6942048183670887, "f1": 0.646077210460772, "f1_threshold": 128.70428466796875, "precision": 0.6117924528301887, "recall": 0.6844327176781002}, "max": {"accuracy": 0.8525362102878942, "ap": 0.7086220349612387, "f1": 0.6526782223380685}, "similarity": {"accuracy": 0.8525362102878942, "accuracy_threshold": 0.817955732345581, "ap": 0.7086220349612387, "f1": 0.6526782223380685, "f1_threshold": 0.7960323095321655, "precision": 0.6448622199330415, "recall": 0.6606860158311345}}]}, "task_name": "TwitterSemEval2015", "avg_layerwise_entropy": 0.1788370595638913, "num_samples": 64}