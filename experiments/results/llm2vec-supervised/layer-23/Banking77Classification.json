{"dataset_revision": "0fd18e25b25c072e09e0d92ab615fda904d66300", "evaluation_time": 192.1909122467041, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"accuracy": 0.8715909090909092, "f1": 0.871098667603589, "f1_weighted": 0.871098667603589, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.8715909090909092, "scores_per_experiment": [{"accuracy": 0.8711038961038962, "f1": 0.8715266016275861, "f1_weighted": 0.8715266016275862}, {"accuracy": 0.8753246753246753, "f1": 0.874474325710594, "f1_weighted": 0.8744743257105938}, {"accuracy": 0.875, "f1": 0.8745736894559871, "f1_weighted": 0.8745736894559871}, {"accuracy": 0.8633116883116884, "f1": 0.8620889059050009, "f1_weighted": 0.8620889059050011}, {"accuracy": 0.8701298701298701, "f1": 0.8696954967188898, "f1_weighted": 0.8696954967188895}, {"accuracy": 0.8717532467532467, "f1": 0.8717443147868698, "f1_weighted": 0.8717443147868703}, {"accuracy": 0.8694805194805195, "f1": 0.8678157315172917, "f1_weighted": 0.8678157315172916}, {"accuracy": 0.8733766233766234, "f1": 0.8730479234898902, "f1_weighted": 0.8730479234898901}, {"accuracy": 0.8808441558441559, "f1": 0.8804769446699764, "f1_weighted": 0.8804769446699764}, {"accuracy": 0.8655844155844156, "f1": 0.865542742153804, "f1_weighted": 0.8655427421538041}]}]}, "task_name": "Banking77Classification", "avg_layerwise_entropy": 0.36036479713534286, "num_samples": 64}