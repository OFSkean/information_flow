{"dataset_revision": "70970daeab8776df92f5ea462b6173c0b46fd2d1", "evaluation_time": 206.77758955955505, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"cosine": {"accuracy": 0.8714907313584074, "accuracy_threshold": 0.9087188839912415, "ap": 0.7705068920561076, "f1": 0.7098765432098767, "f1_threshold": 0.8944641947746277, "precision": 0.6924234821876568, "recall": 0.7282321899736148}, "dot": {"accuracy": 0.7742146986946414, "accuracy_threshold": 477.52252197265625, "ap": 0.3048799099752356, "f1": 0.38577223088923557, "f1_threshold": 311.7569580078125, "precision": 0.2526358806702084, "recall": 0.8155672823218998}, "euclidean": {"accuracy": 0.8695237527567503, "accuracy_threshold": 8.522253036499023, "ap": 0.7640848905351498, "f1": 0.7083228564244406, "f1_threshold": 9.213197708129883, "precision": 0.6765129682997119, "recall": 0.7432717678100264}, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.7705068920561076, "manhattan": {"accuracy": 0.8698217798176074, "accuracy_threshold": 434.992919921875, "ap": 0.7650343607254859, "f1": 0.709993552546744, "f1_threshold": 453.77935791015625, "precision": 0.6943253467843632, "recall": 0.7263852242744063}, "max": {"accuracy": 0.8714907313584074, "ap": 0.7705068920561076, "f1": 0.709993552546744}, "similarity": {"accuracy": 0.8714907313584074, "accuracy_threshold": 0.9087188839912415, "ap": 0.7705068920561076, "f1": 0.7098765432098767, "f1_threshold": 0.8944641947746277, "precision": 0.6924234821876568, "recall": 0.7282321899736148}}]}, "task_name": "TwitterSemEval2015", "avg_layerwise_entropy": 0.5813955167723498, "num_samples": 64}