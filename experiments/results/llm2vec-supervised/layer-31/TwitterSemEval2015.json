{"dataset_revision": "70970daeab8776df92f5ea462b6173c0b46fd2d1", "evaluation_time": 203.23550391197205, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"cosine": {"accuracy": 0.8800143052989211, "accuracy_threshold": 0.9323352575302124, "ap": 0.7950689852155791, "f1": 0.7256613756613757, "f1_threshold": 0.9254204034805298, "precision": 0.7275862068965517, "recall": 0.7237467018469657}, "dot": {"accuracy": 0.7784466829588127, "accuracy_threshold": 2292.617919921875, "ap": 0.39308499464632396, "f1": 0.44042933810375673, "f1_threshold": 2079.685546875, "precision": 0.3331529093369418, "recall": 0.6496042216358839}, "euclidean": {"accuracy": 0.8820408893127496, "accuracy_threshold": 17.8507137298584, "ap": 0.7952630501746205, "f1": 0.7273898479773254, "f1_threshold": 18.808063507080078, "precision": 0.7107250755287009, "recall": 0.7448548812664908}, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.7955163049445623, "manhattan": {"accuracy": 0.8815640460153782, "accuracy_threshold": 907.851318359375, "ap": 0.7955163049445623, "f1": 0.7286464336022538, "f1_threshold": 957.5784912109375, "precision": 0.7078875342124906, "recall": 0.7506596306068601}, "max": {"accuracy": 0.8820408893127496, "ap": 0.7955163049445623, "f1": 0.7286464336022538}, "similarity": {"accuracy": 0.8800143052989211, "accuracy_threshold": 0.9323352575302124, "ap": 0.7950689852155791, "f1": 0.7256613756613757, "f1_threshold": 0.9254204034805298, "precision": 0.7275862068965517, "recall": 0.7237467018469657}}]}, "task_name": "TwitterSemEval2015", "avg_layerwise_entropy": 0.7087839207689791, "num_samples": 64}