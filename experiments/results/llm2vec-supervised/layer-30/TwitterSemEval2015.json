{"dataset_revision": "70970daeab8776df92f5ea462b6173c0b46fd2d1", "evaluation_time": 202.28857517242432, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"cosine": {"accuracy": 0.8791798295285211, "accuracy_threshold": 0.9313971996307373, "ap": 0.7914888313461442, "f1": 0.7240641711229947, "f1_threshold": 0.9281213283538818, "precision": 0.7338753387533875, "recall": 0.7145118733509235}, "dot": {"accuracy": 0.7742743041068129, "accuracy_threshold": 2080.56591796875, "ap": 0.3206064006192391, "f1": 0.41070850857327723, "f1_threshold": 1663.6075439453125, "precision": 0.2961278283181712, "recall": 0.6699208443271768}, "euclidean": {"accuracy": 0.8800143052989211, "accuracy_threshold": 15.946300506591797, "ap": 0.7915252920458672, "f1": 0.7259142744789618, "f1_threshold": 16.52629280090332, "precision": 0.7212815837457671, "recall": 0.7306068601583113}, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.7921881386895451, "manhattan": {"accuracy": 0.8809679918936639, "accuracy_threshold": 801.8333740234375, "ap": 0.7921881386895451, "f1": 0.726638231872819, "f1_threshold": 842.8657836914062, "precision": 0.7121864707372688, "recall": 0.741688654353562}, "max": {"accuracy": 0.8809679918936639, "ap": 0.7921881386895451, "f1": 0.726638231872819}, "similarity": {"accuracy": 0.8791798295285211, "accuracy_threshold": 0.9313971996307373, "ap": 0.7914888313461442, "f1": 0.7240641711229947, "f1_threshold": 0.9281213283538818, "precision": 0.7338753387533875, "recall": 0.7145118733509235}}]}, "task_name": "TwitterSemEval2015", "avg_layerwise_entropy": 0.6903084736684455, "num_samples": 64}