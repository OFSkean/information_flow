{"dataset_revision": "0fd18e25b25c072e09e0d92ab615fda904d66300", "evaluation_time": 197.18903231620789, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"accuracy": 0.885422077922078, "f1": 0.885273677360402, "f1_weighted": 0.885273677360402, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.885422077922078, "scores_per_experiment": [{"accuracy": 0.8827922077922078, "f1": 0.8828194847387631, "f1_weighted": 0.8828194847387633}, {"accuracy": 0.8909090909090909, "f1": 0.8905802419243849, "f1_weighted": 0.8905802419243848}, {"accuracy": 0.8811688311688312, "f1": 0.8807397424974718, "f1_weighted": 0.8807397424974722}, {"accuracy": 0.8892857142857142, "f1": 0.8888810221191262, "f1_weighted": 0.8888810221191262}, {"accuracy": 0.8860389610389611, "f1": 0.8860011730643881, "f1_weighted": 0.8860011730643883}, {"accuracy": 0.8928571428571429, "f1": 0.89296548994313, "f1_weighted": 0.8929654899431297}, {"accuracy": 0.8805194805194805, "f1": 0.8799528076223013, "f1_weighted": 0.8799528076223013}, {"accuracy": 0.875974025974026, "f1": 0.8761024737878056, "f1_weighted": 0.8761024737878055}, {"accuracy": 0.8912337662337663, "f1": 0.8915266080351543, "f1_weighted": 0.8915266080351543}, {"accuracy": 0.8834415584415585, "f1": 0.8831677298714948, "f1_weighted": 0.8831677298714948}]}]}, "task_name": "Banking77Classification", "avg_layerwise_entropy": 0.46843976985705243, "num_samples": 64}