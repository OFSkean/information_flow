{"dataset_revision": "70970daeab8776df92f5ea462b6173c0b46fd2d1", "evaluation_time": 203.19866275787354, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"cosine": {"accuracy": 0.8636228169517792, "accuracy_threshold": 0.8642497062683105, "ap": 0.7514006350035076, "f1": 0.6844487948121865, "f1_threshold": 0.8410618305206299, "precision": 0.6381473876340407, "recall": 0.7379947229551451}, "dot": {"accuracy": 0.7747511474041843, "accuracy_threshold": 95.89461517333984, "ap": 0.2964846936999325, "f1": 0.38491085249097057, "f1_threshold": 52.527835845947266, "precision": 0.24588002636783124, "recall": 0.8857519788918206}, "euclidean": {"accuracy": 0.8649341360195506, "accuracy_threshold": 4.507636547088623, "ap": 0.7492200337148948, "f1": 0.6844173957144668, "f1_threshold": 4.8689985275268555, "precision": 0.658774713204784, "recall": 0.7121372031662269}, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.7574596226386531, "manhattan": {"accuracy": 0.8683316445133218, "accuracy_threshold": 222.75494384765625, "ap": 0.7574596226386531, "f1": 0.6961666461235055, "f1_threshold": 242.51058959960938, "precision": 0.6532500578302105, "recall": 0.7451187335092349}, "max": {"accuracy": 0.8683316445133218, "ap": 0.7574596226386531, "f1": 0.6961666461235055}, "similarity": {"accuracy": 0.8636228169517792, "accuracy_threshold": 0.8642497062683105, "ap": 0.7514006350035076, "f1": 0.6844487948121865, "f1_threshold": 0.8410618305206299, "precision": 0.6381473876340407, "recall": 0.7379947229551451}}]}, "task_name": "TwitterSemEval2015", "avg_layerwise_entropy": 0.36527949480490307, "num_samples": 64}