{"dataset_revision": "0fd18e25b25c072e09e0d92ab615fda904d66300", "evaluation_time": 159.00790810585022, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"accuracy": 0.7478246753246753, "f1": 0.7459663247512317, "f1_weighted": 0.7459663247512317, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.7478246753246753, "scores_per_experiment": [{"accuracy": 0.7603896103896104, "f1": 0.7591068470633683, "f1_weighted": 0.7591068470633684}, {"accuracy": 0.7597402597402597, "f1": 0.7589598531163209, "f1_weighted": 0.7589598531163207}, {"accuracy": 0.7487012987012988, "f1": 0.7477111313763982, "f1_weighted": 0.7477111313763982}, {"accuracy": 0.7422077922077922, "f1": 0.7409336328617493, "f1_weighted": 0.7409336328617494}, {"accuracy": 0.7503246753246753, "f1": 0.7479814242555569, "f1_weighted": 0.747981424255557}, {"accuracy": 0.7415584415584415, "f1": 0.7396076227712027, "f1_weighted": 0.7396076227712027}, {"accuracy": 0.7581168831168831, "f1": 0.7562823488268946, "f1_weighted": 0.7562823488268945}, {"accuracy": 0.7532467532467533, "f1": 0.7513453671324256, "f1_weighted": 0.7513453671324257}, {"accuracy": 0.7350649350649351, "f1": 0.733082310282761, "f1_weighted": 0.7330823102827612}, {"accuracy": 0.7288961038961039, "f1": 0.7246527098256395, "f1_weighted": 0.7246527098256395}]}]}, "task_name": "Banking77Classification", "avg_layerwise_entropy": 0.08286064036414598, "num_samples": 64}