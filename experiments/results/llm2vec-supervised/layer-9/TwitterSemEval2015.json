{"dataset_revision": "70970daeab8776df92f5ea462b6173c0b46fd2d1", "evaluation_time": 207.02224278450012, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"cosine": {"accuracy": 0.8431781605769804, "accuracy_threshold": 0.8132190704345703, "ap": 0.6826800876292335, "f1": 0.6349546971864569, "f1_threshold": 0.7773746848106384, "precision": 0.5791648542844715, "recall": 0.7026385224274406}, "dot": {"accuracy": 0.794063300947726, "accuracy_threshold": 11.345041275024414, "ap": 0.4865869748441198, "f1": 0.5090470446320869, "f1_threshold": 9.637357711791992, "precision": 0.4355413773691124, "recall": 0.6124010554089709}, "euclidean": {"accuracy": 0.8412707873874948, "accuracy_threshold": 2.151266098022461, "ap": 0.6665637663052464, "f1": 0.6283524904214559, "f1_threshold": 2.370655059814453, "precision": 0.5610615799295045, "recall": 0.7139841688654354}, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.6826800876292335, "manhattan": {"accuracy": 0.8415092090361805, "accuracy_threshold": 105.44963073730469, "ap": 0.6704268998675713, "f1": 0.6327097163548582, "f1_threshold": 116.25604248046875, "precision": 0.5830923248053392, "recall": 0.6915567282321899}, "max": {"accuracy": 0.8431781605769804, "ap": 0.6826800876292335, "f1": 0.6349546971864569}, "similarity": {"accuracy": 0.8431781605769804, "accuracy_threshold": 0.8132190704345703, "ap": 0.6826800876292335, "f1": 0.6349546971864569, "f1_threshold": 0.7773746848106384, "precision": 0.5791648542844715, "recall": 0.7026385224274406}}]}, "task_name": "TwitterSemEval2015", "avg_layerwise_entropy": 0.15109852798673998, "num_samples": 64}