{"dataset_revision": "70970daeab8776df92f5ea462b6173c0b46fd2d1", "evaluation_time": 201.22019290924072, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"cosine": {"accuracy": 0.8204684985396674, "accuracy_threshold": 0.8184939622879028, "ap": 0.5989809644357399, "f1": 0.5683006933463082, "f1_threshold": 0.765496015548706, "precision": 0.5271947641615888, "recall": 0.6163588390501319}, "dot": {"accuracy": 0.7794599749657269, "accuracy_threshold": 2.052523612976074, "ap": 0.37777011746241423, "f1": 0.4218859957776214, "f1_threshold": 1.5953402519226074, "precision": 0.31644233306941144, "recall": 0.6327176781002638}, "euclidean": {"accuracy": 0.8174882279310961, "accuracy_threshold": 0.8991875648498535, "ap": 0.5822550276465702, "f1": 0.5665206363845977, "f1_threshold": 1.0209746360778809, "precision": 0.5030712530712531, "recall": 0.6482849604221635}, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.5989809644357399, "manhattan": {"accuracy": 0.8173690171067532, "accuracy_threshold": 45.177459716796875, "ap": 0.5823692900290303, "f1": 0.5681522638609764, "f1_threshold": 50.19656753540039, "precision": 0.5146712358106661, "recall": 0.6340369393139842}, "max": {"accuracy": 0.8204684985396674, "ap": 0.5989809644357399, "f1": 0.5683006933463082}, "similarity": {"accuracy": 0.8204684985396674, "accuracy_threshold": 0.8184939622879028, "ap": 0.5989809644357399, "f1": 0.5683006933463082, "f1_threshold": 0.765496015548706, "precision": 0.5271947641615888, "recall": 0.6163588390501319}}]}, "task_name": "TwitterSemEval2015", "avg_layerwise_entropy": 0.054192492277265046, "num_samples": 64}