{"dataset_revision": "70970daeab8776df92f5ea462b6173c0b46fd2d1", "evaluation_time": 202.72630548477173, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"cosine": {"accuracy": 0.8500923883888657, "accuracy_threshold": 0.8312474489212036, "ap": 0.7049162704925165, "f1": 0.6488942020322773, "f1_threshold": 0.8001229763031006, "precision": 0.593224043715847, "recall": 0.7160949868073878}, "dot": {"accuracy": 0.7811289265065268, "accuracy_threshold": 17.805419921875, "ap": 0.41962804496999007, "f1": 0.4698202269759968, "f1_threshold": 14.823801040649414, "precision": 0.379276795848873, "recall": 0.6171503957783642}, "euclidean": {"accuracy": 0.8462180365977231, "accuracy_threshold": 2.507584810256958, "ap": 0.6861996916760991, "f1": 0.6416129801956575, "f1_threshold": 2.744405746459961, "precision": 0.5855836236933798, "recall": 0.7094986807387863}, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.7049162704925165, "manhattan": {"accuracy": 0.8466948798950945, "accuracy_threshold": 124.34660339355469, "ap": 0.689522816141797, "f1": 0.6435512897420516, "f1_threshold": 136.07196044921875, "precision": 0.5900990099009901, "recall": 0.7076517150395778}, "max": {"accuracy": 0.8500923883888657, "ap": 0.7049162704925165, "f1": 0.6488942020322773}, "similarity": {"accuracy": 0.8500923883888657, "accuracy_threshold": 0.8312474489212036, "ap": 0.7049162704925165, "f1": 0.6488942020322773, "f1_threshold": 0.8001229763031006, "precision": 0.593224043715847, "recall": 0.7160949868073878}}]}, "task_name": "TwitterSemEval2015", "avg_layerwise_entropy": 0.19093177125606467, "num_samples": 64}