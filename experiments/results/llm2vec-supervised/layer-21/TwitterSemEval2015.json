{"dataset_revision": "70970daeab8776df92f5ea462b6173c0b46fd2d1", "evaluation_time": 202.13298916816711, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"cosine": {"accuracy": 0.8684508553376646, "accuracy_threshold": 0.8943290710449219, "ap": 0.7629296106582641, "f1": 0.700699472327893, "f1_threshold": 0.8754925727844238, "precision": 0.6549667354897912, "recall": 0.7532981530343008}, "dot": {"accuracy": 0.7743339095189843, "accuracy_threshold": 267.019775390625, "ap": 0.28105339239733895, "f1": 0.3754588358678553, "f1_threshold": 167.11334228515625, "precision": 0.24093322365961264, "recall": 0.850131926121372}, "euclidean": {"accuracy": 0.8666030875603504, "accuracy_threshold": 6.737728118896484, "ap": 0.7573445228178138, "f1": 0.7001987083954297, "f1_threshold": 7.472236633300781, "precision": 0.6614265603003285, "recall": 0.7437994722955145}, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.7629296106582641, "manhattan": {"accuracy": 0.8679144066281218, "accuracy_threshold": 338.1212158203125, "ap": 0.7597564070997239, "f1": 0.7035741064733817, "f1_threshold": 369.1107482910156, "precision": 0.6683285849952516, "recall": 0.7427440633245382}, "max": {"accuracy": 0.8684508553376646, "ap": 0.7629296106582641, "f1": 0.7035741064733817}, "similarity": {"accuracy": 0.8684508553376646, "accuracy_threshold": 0.8943290710449219, "ap": 0.7629296106582641, "f1": 0.700699472327893, "f1_threshold": 0.8754925727844238, "precision": 0.6549667354897912, "recall": 0.7532981530343008}}]}, "task_name": "TwitterSemEval2015", "avg_layerwise_entropy": 0.5246023604109262, "num_samples": 64}