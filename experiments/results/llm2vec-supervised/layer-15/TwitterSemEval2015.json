{"dataset_revision": "70970daeab8776df92f5ea462b6173c0b46fd2d1", "evaluation_time": 206.09898567199707, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"cosine": {"accuracy": 0.8618942599988079, "accuracy_threshold": 0.8382927179336548, "ap": 0.7438199704619404, "f1": 0.6795318725099602, "f1_threshold": 0.8118940591812134, "precision": 0.6433286185761433, "recall": 0.7200527704485488}, "dot": {"accuracy": 0.7803540561482982, "accuracy_threshold": 41.16411590576172, "ap": 0.3842574085527311, "f1": 0.4193161367726455, "f1_threshold": 29.666217803955078, "precision": 0.2928975487115022, "recall": 0.7377308707124011}, "euclidean": {"accuracy": 0.8611789950527508, "accuracy_threshold": 3.565290689468384, "ap": 0.732449078397905, "f1": 0.6796526054590571, "f1_threshold": 3.8998641967773438, "precision": 0.6414519906323185, "recall": 0.7226912928759894}, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.7438199704619404, "manhattan": {"accuracy": 0.861357811289265, "accuracy_threshold": 180.3998260498047, "ap": 0.7349410224006186, "f1": 0.6810096153846155, "f1_threshold": 195.95346069335938, "precision": 0.6253863134657837, "recall": 0.7474934036939314}, "max": {"accuracy": 0.8618942599988079, "ap": 0.7438199704619404, "f1": 0.6810096153846155}, "similarity": {"accuracy": 0.8618942599988079, "accuracy_threshold": 0.8382927179336548, "ap": 0.7438199704619404, "f1": 0.6795318725099602, "f1_threshold": 0.8118940591812134, "precision": 0.6433286185761433, "recall": 0.7200527704485488}}]}, "task_name": "TwitterSemEval2015", "avg_layerwise_entropy": 0.27339320266480066, "num_samples": 64}