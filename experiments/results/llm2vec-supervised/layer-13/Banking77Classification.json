{"dataset_revision": "0fd18e25b25c072e09e0d92ab615fda904d66300", "evaluation_time": 184.57850074768066, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"accuracy": 0.7647402597402597, "f1": 0.7630352416336381, "f1_weighted": 0.7630352416336381, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.7647402597402597, "scores_per_experiment": [{"accuracy": 0.7711038961038961, "f1": 0.7685099402594213, "f1_weighted": 0.7685099402594212}, {"accuracy": 0.7623376623376623, "f1": 0.762075556127469, "f1_weighted": 0.7620755561274688}, {"accuracy": 0.7470779220779221, "f1": 0.7458704923897901, "f1_weighted": 0.7458704923897901}, {"accuracy": 0.7568181818181818, "f1": 0.7547688830427164, "f1_weighted": 0.7547688830427165}, {"accuracy": 0.775974025974026, "f1": 0.7737053512836665, "f1_weighted": 0.7737053512836665}, {"accuracy": 0.7574675324675325, "f1": 0.7557223772375574, "f1_weighted": 0.7557223772375575}, {"accuracy": 0.7668831168831168, "f1": 0.7646976537389218, "f1_weighted": 0.7646976537389218}, {"accuracy": 0.7688311688311689, "f1": 0.7656493742089727, "f1_weighted": 0.7656493742089728}, {"accuracy": 0.7701298701298701, "f1": 0.7692592167235565, "f1_weighted": 0.7692592167235566}, {"accuracy": 0.7707792207792208, "f1": 0.7700935713243099, "f1_weighted": 0.7700935713243099}]}]}, "task_name": "Banking77Classification", "avg_layerwise_entropy": 0.1226748231522271, "num_samples": 64}