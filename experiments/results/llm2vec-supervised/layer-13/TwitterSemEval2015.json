{"dataset_revision": "70970daeab8776df92f5ea462b6173c0b46fd2d1", "evaluation_time": 202.38806557655334, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"cosine": {"accuracy": 0.8558741133694939, "accuracy_threshold": 0.853412389755249, "ap": 0.7254990229434279, "f1": 0.6659890738152712, "f1_threshold": 0.8311790227890015, "precision": 0.6422445479049252, "recall": 0.6915567282321899}, "dot": {"accuracy": 0.778625499195327, "accuracy_threshold": 26.85472869873047, "ap": 0.4017795659800165, "f1": 0.45158714314234377, "f1_threshold": 21.493070602416992, "precision": 0.3631984585741811, "recall": 0.5968337730870712}, "euclidean": {"accuracy": 0.8519401561661799, "accuracy_threshold": 2.7874107360839844, "ap": 0.7087087000316338, "f1": 0.6539196940726577, "f1_threshold": 3.049415111541748, "precision": 0.5976408912188729, "recall": 0.7218997361477573}, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.7254990229434279, "manhattan": {"accuracy": 0.8533706860582941, "accuracy_threshold": 135.574462890625, "ap": 0.7130395320172259, "f1": 0.6572571159727123, "f1_threshold": 151.96713256835938, "precision": 0.5929541595925297, "recall": 0.737203166226913}, "max": {"accuracy": 0.8558741133694939, "ap": 0.7254990229434279, "f1": 0.6659890738152712}, "similarity": {"accuracy": 0.8558741133694939, "accuracy_threshold": 0.853412389755249, "ap": 0.7254990229434279, "f1": 0.6659890738152712, "f1_threshold": 0.8311790227890015, "precision": 0.6422445479049252, "recall": 0.6915567282321899}}]}, "task_name": "TwitterSemEval2015", "avg_layerwise_entropy": 0.21093720889285533, "num_samples": 64}