{"dataset_revision": "70970daeab8776df92f5ea462b6173c0b46fd2d1", "evaluation_time": 184.32760000228882, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"cosine": {"accuracy": 0.8109316325922393, "accuracy_threshold": 0.8671565055847168, "ap": 0.5434693154067902, "f1": 0.5316300208819555, "f1_threshold": 0.8268060684204102, "precision": 0.4973569294415077, "recall": 0.570976253298153}, "dot": {"accuracy": 0.7803540561482982, "accuracy_threshold": 0.2503153085708618, "ap": 0.3694342937234554, "f1": 0.41192711209276645, "f1_threshold": 0.18284659087657928, "precision": 0.2937999324856532, "recall": 0.6889182058047494}, "euclidean": {"accuracy": 0.8028252965369256, "accuracy_threshold": 0.24605345726013184, "ap": 0.5142475754018024, "f1": 0.5154763338552331, "f1_threshold": 0.2893733084201813, "precision": 0.47418568579658765, "recall": 0.5646437994722955}, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.5434693154067902, "manhattan": {"accuracy": 0.8025868748882399, "accuracy_threshold": 12.10931396484375, "ap": 0.5159657688592165, "f1": 0.5153417015341701, "f1_threshold": 14.188132286071777, "precision": 0.4605317823016203, "recall": 0.5849604221635883}, "max": {"accuracy": 0.8109316325922393, "ap": 0.5434693154067902, "f1": 0.5316300208819555}, "similarity": {"accuracy": 0.8109316325922393, "accuracy_threshold": 0.8671565055847168, "ap": 0.5434693154067902, "f1": 0.5316300208819555, "f1_threshold": 0.8268060684204102, "precision": 0.4973569294415077, "recall": 0.570976253298153}}]}, "task_name": "TwitterSemEval2015", "avg_layerwise_entropy": 0.8820814208327398, "num_samples": 64}