{"dataset_revision": "70970daeab8776df92f5ea462b6173c0b46fd2d1", "evaluation_time": 200.12627506256104, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"cosine": {"accuracy": 0.8112892650652679, "accuracy_threshold": 0.8279294967651367, "ap": 0.5518635908396826, "f1": 0.5380214541120382, "f1_threshold": 0.7735394239425659, "precision": 0.4906521739130435, "recall": 0.5955145118733509}, "dot": {"accuracy": 0.7765989151814985, "accuracy_threshold": 0.47287750244140625, "ap": 0.35278628236058357, "f1": 0.41727367325702397, "f1_threshold": 0.36681443452835083, "precision": 0.31077241022991475, "recall": 0.6348284960422164}, "euclidean": {"accuracy": 0.8086666269297252, "accuracy_threshold": 0.4185120463371277, "ap": 0.5447778084028047, "f1": 0.539940489814603, "f1_threshold": 0.47820401191711426, "precision": 0.47675828617623284, "recall": 0.6224274406332454}, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.5518635908396826, "manhattan": {"accuracy": 0.8081897836323538, "accuracy_threshold": 19.61022186279297, "ap": 0.5450992394442856, "f1": 0.543494241607449, "f1_threshold": 22.739360809326172, "precision": 0.5073193046660567, "recall": 0.5852242744063324}, "max": {"accuracy": 0.8112892650652679, "ap": 0.5518635908396826, "f1": 0.543494241607449}, "similarity": {"accuracy": 0.8112892650652679, "accuracy_threshold": 0.8279294967651367, "ap": 0.5518635908396826, "f1": 0.5380214541120382, "f1_threshold": 0.7735394239425659, "precision": 0.4906521739130435, "recall": 0.5955145118733509}}]}, "task_name": "TwitterSemEval2015", "avg_layerwise_entropy": 0.018888898877140473, "num_samples": 64}