{"dataset_revision": "70970daeab8776df92f5ea462b6173c0b46fd2d1", "evaluation_time": 204.6746461391449, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"cosine": {"accuracy": 0.8690469094593789, "accuracy_threshold": 0.8930222988128662, "ap": 0.7651256142358152, "f1": 0.7024006096786487, "f1_threshold": 0.8775938749313354, "precision": 0.6771981386235612, "recall": 0.7295514511873351}, "dot": {"accuracy": 0.7742743041068129, "accuracy_threshold": 248.0949249267578, "ap": 0.2747465954742093, "f1": 0.37509802738734394, "f1_threshold": 141.03897094726562, "precision": 0.24313756158598576, "recall": 0.8203166226912929}, "euclidean": {"accuracy": 0.867795195803779, "accuracy_threshold": 6.289525032043457, "ap": 0.7585541786823204, "f1": 0.7024486767252041, "f1_threshold": 6.885618209838867, "precision": 0.6610800744878957, "recall": 0.7493403693931399}, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.7651256142358152, "manhattan": {"accuracy": 0.8691661202837218, "accuracy_threshold": 316.3998107910156, "ap": 0.76237509785005, "f1": 0.7076605089781681, "f1_threshold": 333.1533508300781, "precision": 0.6932422171602126, "recall": 0.7226912928759894}, "max": {"accuracy": 0.8691661202837218, "ap": 0.7651256142358152, "f1": 0.7076605089781681}, "similarity": {"accuracy": 0.8690469094593789, "accuracy_threshold": 0.8930222988128662, "ap": 0.7651256142358152, "f1": 0.7024006096786487, "f1_threshold": 0.8775938749313354, "precision": 0.6771981386235612, "recall": 0.7295514511873351}}]}, "task_name": "TwitterSemEval2015", "avg_layerwise_entropy": 0.485452224890865, "num_samples": 64}