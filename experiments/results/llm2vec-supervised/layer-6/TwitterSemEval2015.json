{"dataset_revision": "70970daeab8776df92f5ea462b6173c0b46fd2d1", "evaluation_time": 195.97961163520813, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"cosine": {"accuracy": 0.8289920724801813, "accuracy_threshold": 0.8160082101821899, "ap": 0.631170833602085, "f1": 0.594692400482509, "f1_threshold": 0.7768194675445557, "precision": 0.5477777777777778, "recall": 0.6503957783641161}, "dot": {"accuracy": 0.7867318352506407, "accuracy_threshold": 4.949559211730957, "ap": 0.44031059798994826, "f1": 0.4665536084416808, "f1_threshold": 4.089565753936768, "precision": 0.36283704572098474, "recall": 0.6532981530343008}, "euclidean": {"accuracy": 0.8220182392561245, "accuracy_threshold": 1.3861488103866577, "ap": 0.6067691482143317, "f1": 0.5841088045812455, "f1_threshold": 1.5578542947769165, "precision": 0.5331010452961672, "recall": 0.645910290237467}, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.631170833602085, "manhattan": {"accuracy": 0.8220778446682959, "accuracy_threshold": 69.06956481933594, "ap": 0.6066754401572626, "f1": 0.5833897540058679, "f1_threshold": 78.47178649902344, "precision": 0.5096608832807571, "recall": 0.6820580474934037}, "max": {"accuracy": 0.8289920724801813, "ap": 0.631170833602085, "f1": 0.594692400482509}, "similarity": {"accuracy": 0.8289920724801813, "accuracy_threshold": 0.8160082101821899, "ap": 0.631170833602085, "f1": 0.594692400482509, "f1_threshold": 0.7768194675445557, "precision": 0.5477777777777778, "recall": 0.6503957783641161}}]}, "task_name": "TwitterSemEval2015", "avg_layerwise_entropy": 0.09679598942959104, "num_samples": 64}