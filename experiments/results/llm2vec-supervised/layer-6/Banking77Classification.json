{"dataset_revision": "0fd18e25b25c072e09e0d92ab615fda904d66300", "evaluation_time": 145.5836148262024, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"accuracy": 0.7262012987012987, "f1": 0.7221527492381968, "f1_weighted": 0.722152749238197, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.7262012987012987, "scores_per_experiment": [{"accuracy": 0.7331168831168832, "f1": 0.7297656452172849, "f1_weighted": 0.7297656452172849}, {"accuracy": 0.7337662337662337, "f1": 0.7291608774943235, "f1_weighted": 0.7291608774943235}, {"accuracy": 0.712012987012987, "f1": 0.7085569853063785, "f1_weighted": 0.7085569853063785}, {"accuracy": 0.7457792207792208, "f1": 0.7428242883185038, "f1_weighted": 0.7428242883185038}, {"accuracy": 0.7301948051948052, "f1": 0.7293949294719683, "f1_weighted": 0.7293949294719685}, {"accuracy": 0.7308441558441559, "f1": 0.7254822989420204, "f1_weighted": 0.7254822989420207}, {"accuracy": 0.711038961038961, "f1": 0.7075854986036738, "f1_weighted": 0.707585498603674}, {"accuracy": 0.7178571428571429, "f1": 0.7127998517356448, "f1_weighted": 0.7127998517356449}, {"accuracy": 0.7259740259740259, "f1": 0.7206727494175597, "f1_weighted": 0.7206727494175599}, {"accuracy": 0.7214285714285714, "f1": 0.7152843678746109, "f1_weighted": 0.715284367874611}]}]}, "task_name": "Banking77Classification", "avg_layerwise_entropy": 0.05370156695277668, "num_samples": 64}