{"dataset_revision": "0fd18e25b25c072e09e0d92ab615fda904d66300", "evaluation_time": 205.5863482952118, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"accuracy": 0.8815584415584414, "f1": 0.8814618537735533, "f1_weighted": 0.8814618537735533, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.8815584415584414, "scores_per_experiment": [{"accuracy": 0.8847402597402597, "f1": 0.8852634262276856, "f1_weighted": 0.8852634262276856}, {"accuracy": 0.8873376623376623, "f1": 0.8881887437857857, "f1_weighted": 0.8881887437857857}, {"accuracy": 0.8818181818181818, "f1": 0.8809848319225992, "f1_weighted": 0.8809848319225991}, {"accuracy": 0.8866883116883116, "f1": 0.8866582679561402, "f1_weighted": 0.8866582679561404}, {"accuracy": 0.8737012987012988, "f1": 0.873713885137184, "f1_weighted": 0.8737138851371838}, {"accuracy": 0.8720779220779221, "f1": 0.8715044343037384, "f1_weighted": 0.8715044343037383}, {"accuracy": 0.887987012987013, "f1": 0.8868460426726724, "f1_weighted": 0.8868460426726726}, {"accuracy": 0.8707792207792208, "f1": 0.870937587968519, "f1_weighted": 0.870937587968519}, {"accuracy": 0.8847402597402597, "f1": 0.8844696262353181, "f1_weighted": 0.8844696262353178}, {"accuracy": 0.8857142857142857, "f1": 0.8860516915258907, "f1_weighted": 0.8860516915258908}]}]}, "task_name": "Banking77Classification", "avg_layerwise_entropy": 0.44808383340647306, "num_samples": 64}