{"dataset_revision": "70970daeab8776df92f5ea462b6173c0b46fd2d1", "evaluation_time": 207.4293065071106, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"cosine": {"accuracy": 0.8803123323597782, "accuracy_threshold": 0.9302642941474915, "ap": 0.7922846157418683, "f1": 0.7245171673819741, "f1_threshold": 0.9273279905319214, "precision": 0.7367703218767049, "recall": 0.7126649076517151}, "dot": {"accuracy": 0.7749299636406985, "accuracy_threshold": 1702.96728515625, "ap": 0.3193310391874891, "f1": 0.39866785801315896, "f1_threshold": 1412.33935546875, "precision": 0.28799436685834995, "recall": 0.6474934036939314}, "euclidean": {"accuracy": 0.8802527269476068, "accuracy_threshold": 14.803255081176758, "ap": 0.7903812476185429, "f1": 0.7264050901378579, "f1_threshold": 15.245010375976562, "precision": 0.7298881193393714, "recall": 0.7229551451187335}, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.7922846157418683, "manhattan": {"accuracy": 0.8804911485962925, "accuracy_threshold": 748.1436767578125, "ap": 0.7905637718534473, "f1": 0.7266771488469601, "f1_threshold": 775.6925048828125, "precision": 0.7217595002602811, "recall": 0.7316622691292876}, "max": {"accuracy": 0.8804911485962925, "ap": 0.7922846157418683, "f1": 0.7266771488469601}, "similarity": {"accuracy": 0.8803123323597782, "accuracy_threshold": 0.9302642941474915, "ap": 0.7922846157418683, "f1": 0.7245171673819741, "f1_threshold": 0.9273279905319214, "precision": 0.7367703218767049, "recall": 0.7126649076517151}}]}, "task_name": "TwitterSemEval2015", "avg_layerwise_entropy": 0.6728058053701769, "num_samples": 64}