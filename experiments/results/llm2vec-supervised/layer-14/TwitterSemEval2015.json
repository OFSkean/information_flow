{"dataset_revision": "70970daeab8776df92f5ea462b6173c0b46fd2d1", "evaluation_time": 194.2630045413971, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"cosine": {"accuracy": 0.8611193896405793, "accuracy_threshold": 0.843894898891449, "ap": 0.7455637734759704, "f1": 0.6808780611597514, "f1_threshold": 0.8186601996421814, "precision": 0.6558298704473234, "recall": 0.7079155672823219}, "dot": {"accuracy": 0.7824402455742981, "accuracy_threshold": 31.901670455932617, "ap": 0.421416352978623, "f1": 0.453069306930693, "f1_threshold": 26.738330841064453, "precision": 0.36259904912836766, "recall": 0.6036939313984169}, "euclidean": {"accuracy": 0.8584371460928653, "accuracy_threshold": 3.2410330772399902, "ap": 0.7259362130430925, "f1": 0.6735224879197127, "f1_threshold": 3.5117344856262207, "precision": 0.6348983882270498, "recall": 0.7171503957783641}, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.7455637734759704, "manhattan": {"accuracy": 0.8583179352685224, "accuracy_threshold": 162.73863220214844, "ap": 0.7268351251339634, "f1": 0.6717689174176028, "f1_threshold": 175.20335388183594, "precision": 0.6311760612386917, "recall": 0.7179419525065963}, "max": {"accuracy": 0.8611193896405793, "ap": 0.7455637734759704, "f1": 0.6808780611597514}, "similarity": {"accuracy": 0.8611193896405793, "accuracy_threshold": 0.843894898891449, "ap": 0.7455637734759704, "f1": 0.6808780611597514, "f1_threshold": 0.8186601996421814, "precision": 0.6558298704473234, "recall": 0.7079155672823219}}]}, "task_name": "TwitterSemEval2015", "avg_layerwise_entropy": 0.2393945042788316, "num_samples": 64}