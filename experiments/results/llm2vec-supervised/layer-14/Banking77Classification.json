{"dataset_revision": "0fd18e25b25c072e09e0d92ab615fda904d66300", "evaluation_time": 185.24405455589294, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"accuracy": 0.7702597402597402, "f1": 0.7689979536570342, "f1_weighted": 0.7689979536570342, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.7702597402597402, "scores_per_experiment": [{"accuracy": 0.7529220779220779, "f1": 0.7525197186469996, "f1_weighted": 0.7525197186469995}, {"accuracy": 0.7646103896103896, "f1": 0.7623366185875182, "f1_weighted": 0.7623366185875182}, {"accuracy": 0.7853896103896104, "f1": 0.785046065727704, "f1_weighted": 0.785046065727704}, {"accuracy": 0.7707792207792208, "f1": 0.7688174748286742, "f1_weighted": 0.7688174748286746}, {"accuracy": 0.7672077922077922, "f1": 0.7670382123521374, "f1_weighted": 0.7670382123521372}, {"accuracy": 0.7756493506493507, "f1": 0.7726440860151824, "f1_weighted": 0.7726440860151826}, {"accuracy": 0.7633116883116883, "f1": 0.7636316073542936, "f1_weighted": 0.7636316073542935}, {"accuracy": 0.775974025974026, "f1": 0.7736243292741521, "f1_weighted": 0.7736243292741524}, {"accuracy": 0.7782467532467533, "f1": 0.7770724941275553, "f1_weighted": 0.7770724941275554}, {"accuracy": 0.7685064935064935, "f1": 0.7672489296561252, "f1_weighted": 0.7672489296561251}]}]}, "task_name": "Banking77Classification", "avg_layerwise_entropy": 0.13637744423238776, "num_samples": 64}