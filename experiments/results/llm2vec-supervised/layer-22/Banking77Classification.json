{"dataset_revision": "0fd18e25b25c072e09e0d92ab615fda904d66300", "evaluation_time": 193.5808985233307, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"accuracy": 0.8594805194805195, "f1": 0.8587224845845656, "f1_weighted": 0.8587224845845658, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.8594805194805195, "scores_per_experiment": [{"accuracy": 0.8766233766233766, "f1": 0.8753394913919605, "f1_weighted": 0.8753394913919607}, {"accuracy": 0.8568181818181818, "f1": 0.8569694071258208, "f1_weighted": 0.8569694071258209}, {"accuracy": 0.8425324675324676, "f1": 0.840567188961481, "f1_weighted": 0.840567188961481}, {"accuracy": 0.8655844155844156, "f1": 0.864394847537453, "f1_weighted": 0.8643948475374532}, {"accuracy": 0.8431818181818181, "f1": 0.8435127687017998, "f1_weighted": 0.8435127687017997}, {"accuracy": 0.8594155844155844, "f1": 0.8597103166843898, "f1_weighted": 0.8597103166843898}, {"accuracy": 0.861038961038961, "f1": 0.8600627932075959, "f1_weighted": 0.8600627932075958}, {"accuracy": 0.8714285714285714, "f1": 0.8716063349082225, "f1_weighted": 0.8716063349082229}, {"accuracy": 0.8607142857142858, "f1": 0.8599930049732389, "f1_weighted": 0.8599930049732393}, {"accuracy": 0.8574675324675325, "f1": 0.855068692353693, "f1_weighted": 0.8550686923536928}]}]}, "task_name": "Banking77Classification", "avg_layerwise_entropy": 0.3442008159756776, "num_samples": 64}