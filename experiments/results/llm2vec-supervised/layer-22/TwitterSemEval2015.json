{"dataset_revision": "70970daeab8776df92f5ea462b6173c0b46fd2d1", "evaluation_time": 198.7992959022522, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"cosine": {"accuracy": 0.8694641473445789, "accuracy_threshold": 0.9020823240280151, "ap": 0.7647357811375195, "f1": 0.7032195594287097, "f1_threshold": 0.8806262016296387, "precision": 0.649597495527728, "recall": 0.7664907651715039}, "dot": {"accuracy": 0.7742743041068129, "accuracy_threshold": 385.4793701171875, "ap": 0.2970717661763838, "f1": 0.3835114864465836, "f1_threshold": 230.1507110595703, "precision": 0.24705166248192956, "recall": 0.8567282321899736}, "euclidean": {"accuracy": 0.8674375633307504, "accuracy_threshold": 7.520047664642334, "ap": 0.7570425230095907, "f1": 0.7004155647903286, "f1_threshold": 8.339569091796875, "precision": 0.6699590460130089, "recall": 0.7337730870712401}, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.7647357811375195, "manhattan": {"accuracy": 0.8676759849794361, "accuracy_threshold": 378.334716796875, "ap": 0.7582971574938107, "f1": 0.7030240372189196, "f1_threshold": 408.25408935546875, "precision": 0.6889564336372846, "recall": 0.7176781002638523}, "max": {"accuracy": 0.8694641473445789, "ap": 0.7647357811375195, "f1": 0.7032195594287097}, "similarity": {"accuracy": 0.8694641473445789, "accuracy_threshold": 0.9020823240280151, "ap": 0.7647357811375195, "f1": 0.7032195594287097, "f1_threshold": 0.8806262016296387, "precision": 0.649597495527728, "recall": 0.7664907651715039}}]}, "task_name": "TwitterSemEval2015", "avg_layerwise_entropy": 0.5607962694907123, "num_samples": 64}