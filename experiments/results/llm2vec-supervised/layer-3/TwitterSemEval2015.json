{"dataset_revision": "70970daeab8776df92f5ea462b6173c0b46fd2d1", "evaluation_time": 204.85173559188843, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"cosine": {"accuracy": 0.8180842820528104, "accuracy_threshold": 0.8170939683914185, "ap": 0.5877473746481163, "f1": 0.5598574821852732, "f1_threshold": 0.7622129917144775, "precision": 0.5090712742980562, "recall": 0.6218997361477573}, "dot": {"accuracy": 0.7780890504857841, "accuracy_threshold": 1.081761121749878, "ap": 0.3645076068641012, "f1": 0.411433521004764, "f1_threshold": 0.8255431652069092, "precision": 0.30625402965828497, "recall": 0.6266490765171504}, "euclidean": {"accuracy": 0.8127794003695535, "accuracy_threshold": 0.630652904510498, "ap": 0.567832990537319, "f1": 0.556098644745612, "f1_threshold": 0.7431628108024597, "precision": 0.48023791250959325, "recall": 0.6604221635883905}, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.5877473746481163, "manhattan": {"accuracy": 0.8127197949573821, "accuracy_threshold": 31.250621795654297, "ap": 0.5674927195726336, "f1": 0.5551349371107582, "f1_threshold": 35.600032806396484, "precision": 0.516708342805183, "recall": 0.5997361477572559}, "max": {"accuracy": 0.8180842820528104, "ap": 0.5877473746481163, "f1": 0.5598574821852732}, "similarity": {"accuracy": 0.8180842820528104, "accuracy_threshold": 0.8170939683914185, "ap": 0.5877473746481163, "f1": 0.5598574821852732, "f1_threshold": 0.7622129917144775, "precision": 0.5090712742980562, "recall": 0.6218997361477573}}]}, "task_name": "TwitterSemEval2015", "avg_layerwise_entropy": 0.03640700318198149, "num_samples": 64}