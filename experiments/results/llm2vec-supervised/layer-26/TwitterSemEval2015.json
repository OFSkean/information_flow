{"dataset_revision": "70970daeab8776df92f5ea462b6173c0b46fd2d1", "evaluation_time": 203.4466540813446, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"cosine": {"accuracy": 0.8742921857304643, "accuracy_threshold": 0.9095709919929504, "ap": 0.7796190449614238, "f1": 0.7165474974463739, "f1_threshold": 0.9018872380256653, "precision": 0.694210786739238, "recall": 0.7403693931398417}, "dot": {"accuracy": 0.7743339095189843, "accuracy_threshold": 811.839111328125, "ap": 0.3073478691155932, "f1": 0.38529209621993127, "f1_threshold": 586.143310546875, "precision": 0.26050185873605947, "recall": 0.7395778364116095}, "euclidean": {"accuracy": 0.8750074506765214, "accuracy_threshold": 11.137334823608398, "ap": 0.7770126993110084, "f1": 0.7168228909718796, "f1_threshold": 11.972395896911621, "precision": 0.6729967577582214, "recall": 0.766754617414248}, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.7796190449614238, "manhattan": {"accuracy": 0.8752458723252071, "accuracy_threshold": 563.1337890625, "ap": 0.7781509082570919, "f1": 0.7190661478599222, "f1_threshold": 584.71875, "precision": 0.7071428571428572, "recall": 0.7313984168865435}, "max": {"accuracy": 0.8752458723252071, "ap": 0.7796190449614238, "f1": 0.7190661478599222}, "similarity": {"accuracy": 0.8742921857304643, "accuracy_threshold": 0.9095709919929504, "ap": 0.7796190449614238, "f1": 0.7165474974463739, "f1_threshold": 0.9018872380256653, "precision": 0.694210786739238, "recall": 0.7403693931398417}}]}, "task_name": "TwitterSemEval2015", "avg_layerwise_entropy": 0.6328423904383464, "num_samples": 64}