{"dataset_revision": "0fd18e25b25c072e09e0d92ab615fda904d66300", "evaluation_time": 194.40846300125122, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"accuracy": 0.8827272727272728, "f1": 0.8819960312276953, "f1_weighted": 0.8819960312276953, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.8827272727272728, "scores_per_experiment": [{"accuracy": 0.8756493506493507, "f1": 0.8749994958099014, "f1_weighted": 0.8749994958099013}, {"accuracy": 0.884090909090909, "f1": 0.8827199075775862, "f1_weighted": 0.8827199075775861}, {"accuracy": 0.8824675324675325, "f1": 0.881738859431683, "f1_weighted": 0.8817388594316831}, {"accuracy": 0.8717532467532467, "f1": 0.8713259016112415, "f1_weighted": 0.8713259016112419}, {"accuracy": 0.8860389610389611, "f1": 0.8848608976051654, "f1_weighted": 0.8848608976051652}, {"accuracy": 0.8837662337662338, "f1": 0.8826157310445281, "f1_weighted": 0.8826157310445278}, {"accuracy": 0.8860389610389611, "f1": 0.8858603833105676, "f1_weighted": 0.8858603833105676}, {"accuracy": 0.8961038961038961, "f1": 0.8959372007686061, "f1_weighted": 0.8959372007686065}, {"accuracy": 0.8746753246753247, "f1": 0.8736383216790794, "f1_weighted": 0.8736383216790795}, {"accuracy": 0.8866883116883116, "f1": 0.8862636134385945, "f1_weighted": 0.8862636134385944}]}]}, "task_name": "Banking77Classification", "avg_layerwise_entropy": 0.43325270457715626, "num_samples": 64}