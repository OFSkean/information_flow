{"dataset_revision": "70970daeab8776df92f5ea462b6173c0b46fd2d1", "evaluation_time": 187.6183648109436, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"cosine": {"accuracy": 0.8766764022173213, "accuracy_threshold": 0.9313353300094604, "ap": 0.784970788125111, "f1": 0.7188489968321012, "f1_threshold": 0.9248538017272949, "precision": 0.719228737453777, "recall": 0.7184696569920844}, "dot": {"accuracy": 0.7740358824581272, "accuracy_threshold": 1489.29931640625, "ap": 0.28197248319439616, "f1": 0.37456001941983247, "f1_threshold": 1010.544921875, "precision": 0.2432219419924338, "recall": 0.8142480211081794}, "euclidean": {"accuracy": 0.8763187697442928, "accuracy_threshold": 12.92120361328125, "ap": 0.7830197900219703, "f1": 0.7228177641653905, "f1_threshold": 13.71923828125, "precision": 0.6999505684626792, "recall": 0.7472295514511873}, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.784970788125111, "manhattan": {"accuracy": 0.8768552184538356, "accuracy_threshold": 654.7884521484375, "ap": 0.7840235897373116, "f1": 0.7237771383730054, "f1_threshold": 683.087890625, "precision": 0.7175829875518672, "recall": 0.7300791556728232}, "max": {"accuracy": 0.8768552184538356, "ap": 0.784970788125111, "f1": 0.7237771383730054}, "similarity": {"accuracy": 0.8766764022173213, "accuracy_threshold": 0.9313353300094604, "ap": 0.784970788125111, "f1": 0.7188489968321012, "f1_threshold": 0.9248538017272949, "precision": 0.719228737453777, "recall": 0.7184696569920844}}]}, "task_name": "TwitterSemEval2015", "avg_layerwise_entropy": 0.6607529971937413, "num_samples": 64}