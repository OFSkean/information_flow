{"dataset_revision": "70970daeab8776df92f5ea462b6173c0b46fd2d1", "evaluation_time": 203.6362657546997, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"cosine": {"accuracy": 0.8724444179531502, "accuracy_threshold": 0.9155936241149902, "ap": 0.7770528329041887, "f1": 0.7145073700543056, "f1_threshold": 0.9080556035041809, "precision": 0.7005578093306288, "recall": 0.729023746701847}, "dot": {"accuracy": 0.7743935149311557, "accuracy_threshold": 723.8517456054688, "ap": 0.2953836051004823, "f1": 0.3792961796521977, "f1_threshold": 501.42962646484375, "precision": 0.25610207100591714, "recall": 0.7308707124010554}, "euclidean": {"accuracy": 0.8725040233653216, "accuracy_threshold": 9.803638458251953, "ap": 0.7732971663586382, "f1": 0.7145045965270683, "f1_threshold": 10.566214561462402, "precision": 0.6922315685304304, "recall": 0.7382585751978892}, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.7770528329041887, "manhattan": {"accuracy": 0.8733981045478929, "accuracy_threshold": 503.18841552734375, "ap": 0.7741232356109259, "f1": 0.7170242478100799, "f1_threshold": 530.3130493164062, "precision": 0.6909713726449719, "recall": 0.7451187335092349}, "max": {"accuracy": 0.8733981045478929, "ap": 0.7770528329041887, "f1": 0.7170242478100799}, "similarity": {"accuracy": 0.8724444179531502, "accuracy_threshold": 0.9155936241149902, "ap": 0.7770528329041887, "f1": 0.7145073700543056, "f1_threshold": 0.9080556035041809, "precision": 0.7005578093306288, "recall": 0.729023746701847}}]}, "task_name": "TwitterSemEval2015", "avg_layerwise_entropy": 0.6161411197302538, "num_samples": 64}