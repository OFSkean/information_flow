{"dataset_revision": "70970daeab8776df92f5ea462b6173c0b46fd2d1", "evaluation_time": 203.9543170928955, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"cosine": {"accuracy": 0.8750074506765214, "accuracy_threshold": 0.9196843504905701, "ap": 0.7827647300871627, "f1": 0.7191533298915849, "f1_threshold": 0.9134787321090698, "precision": 0.7038908539666499, "recall": 0.7350923482849604}, "dot": {"accuracy": 0.7743339095189843, "accuracy_threshold": 1037.285400390625, "ap": 0.3111485998158684, "f1": 0.39652173913043476, "f1_threshold": 803.8817138671875, "precision": 0.2779014308426073, "recall": 0.691820580474934}, "euclidean": {"accuracy": 0.8755438993860643, "accuracy_threshold": 12.223264694213867, "ap": 0.780764277863836, "f1": 0.7202373274861344, "f1_threshold": 12.72862434387207, "precision": 0.7045167802170074, "recall": 0.7366754617414248}, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.7827647300871627, "manhattan": {"accuracy": 0.8764379805686356, "accuracy_threshold": 616.2948608398438, "ap": 0.7814886192993148, "f1": 0.7207397622192866, "f1_threshold": 633.09033203125, "precision": 0.7216931216931217, "recall": 0.7197889182058047}, "max": {"accuracy": 0.8764379805686356, "ap": 0.7827647300871627, "f1": 0.7207397622192866}, "similarity": {"accuracy": 0.8750074506765214, "accuracy_threshold": 0.9196843504905701, "ap": 0.7827647300871627, "f1": 0.7191533298915849, "f1_threshold": 0.9134787321090698, "precision": 0.7038908539666499, "recall": 0.7350923482849604}}]}, "task_name": "TwitterSemEval2015", "avg_layerwise_entropy": 0.6462712871966839, "num_samples": 64}