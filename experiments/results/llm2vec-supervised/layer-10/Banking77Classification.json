{"dataset_revision": "0fd18e25b25c072e09e0d92ab615fda904d66300", "evaluation_time": 158.6503927707672, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"accuracy": 0.7662337662337663, "f1": 0.7646660312691712, "f1_weighted": 0.7646660312691711, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.7662337662337663, "scores_per_experiment": [{"accuracy": 0.7694805194805194, "f1": 0.7678711896331994, "f1_weighted": 0.7678711896331998}, {"accuracy": 0.7818181818181819, "f1": 0.7778692125178539, "f1_weighted": 0.777869212517854}, {"accuracy": 0.772077922077922, "f1": 0.7709606066468367, "f1_weighted": 0.7709606066468369}, {"accuracy": 0.7785714285714286, "f1": 0.7767832015774272, "f1_weighted": 0.7767832015774273}, {"accuracy": 0.7646103896103896, "f1": 0.7628329238196992, "f1_weighted": 0.7628329238196992}, {"accuracy": 0.7538961038961038, "f1": 0.7531039207217244, "f1_weighted": 0.7531039207217244}, {"accuracy": 0.7633116883116883, "f1": 0.7614626130921505, "f1_weighted": 0.7614626130921504}, {"accuracy": 0.7594155844155844, "f1": 0.7594735834283798, "f1_weighted": 0.7594735834283798}, {"accuracy": 0.7571428571428571, "f1": 0.7553265879953698, "f1_weighted": 0.75532658799537}, {"accuracy": 0.762012987012987, "f1": 0.760976473259071, "f1_weighted": 0.7609764732590709}]}]}, "task_name": "Banking77Classification", "avg_layerwise_entropy": 0.09420131102897933, "num_samples": 64}