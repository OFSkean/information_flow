{"dataset_revision": "70970daeab8776df92f5ea462b6173c0b46fd2d1", "evaluation_time": 191.58478474617004, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"cosine": {"accuracy": 0.8475293556654945, "accuracy_threshold": 0.8265095949172974, "ap": 0.6950205025261142, "f1": 0.6428486079579401, "f1_threshold": 0.7963153123855591, "precision": 0.5874645119021621, "recall": 0.7097625329815304}, "dot": {"accuracy": 0.7858377540680693, "accuracy_threshold": 14.61021900177002, "ap": 0.4534306876216798, "f1": 0.488915164055572, "f1_threshold": 11.86334228515625, "precision": 0.39015568485610946, "recall": 0.6546174142480211}, "euclidean": {"accuracy": 0.8435953984621803, "accuracy_threshold": 2.273620367050171, "ap": 0.6757537300039829, "f1": 0.6317504655493482, "f1_threshold": 2.5142500400543213, "precision": 0.5651811745106206, "recall": 0.7160949868073878}, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.6950205025261142, "manhattan": {"accuracy": 0.8449067175299517, "accuracy_threshold": 113.50607299804688, "ap": 0.6793910072770464, "f1": 0.6354069223573433, "f1_threshold": 124.47211456298828, "precision": 0.5705585888282234, "recall": 0.7168865435356201}, "max": {"accuracy": 0.8475293556654945, "ap": 0.6950205025261142, "f1": 0.6428486079579401}, "similarity": {"accuracy": 0.8475293556654945, "accuracy_threshold": 0.8265095949172974, "ap": 0.6950205025261142, "f1": 0.6428486079579401, "f1_threshold": 0.7963153123855591, "precision": 0.5874645119021621, "recall": 0.7097625329815304}}]}, "task_name": "TwitterSemEval2015", "avg_layerwise_entropy": 0.16638842412465368, "num_samples": 64}