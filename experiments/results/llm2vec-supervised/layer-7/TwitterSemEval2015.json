{"dataset_revision": "70970daeab8776df92f5ea462b6173c0b46fd2d1", "evaluation_time": 206.5348265171051, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"cosine": {"accuracy": 0.8298265482505811, "accuracy_threshold": 0.8123877048492432, "ap": 0.6384497995032611, "f1": 0.6042498390212493, "f1_threshold": 0.7865688800811768, "precision": 0.5901886792452831, "recall": 0.6189973614775726}, "dot": {"accuracy": 0.7831555105203553, "accuracy_threshold": 7.671198844909668, "ap": 0.44241125189737607, "f1": 0.48236226714229086, "f1_threshold": 6.302348613739014, "precision": 0.3862265947318312, "recall": 0.6422163588390501}, "euclidean": {"accuracy": 0.8292304941288668, "accuracy_threshold": 1.8014227151870728, "ap": 0.6295147764401191, "f1": 0.6031898309149778, "f1_threshold": 1.9295141696929932, "precision": 0.552868762365355, "recall": 0.6635883905013192}, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.6384497995032611, "manhattan": {"accuracy": 0.829528521189724, "accuracy_threshold": 89.03062438964844, "ap": 0.630651886793641, "f1": 0.6044086276368807, "f1_threshold": 96.14437103271484, "precision": 0.5486230636833046, "recall": 0.6728232189973615}, "max": {"accuracy": 0.8298265482505811, "ap": 0.6384497995032611, "f1": 0.6044086276368807}, "similarity": {"accuracy": 0.8298265482505811, "accuracy_threshold": 0.8123877048492432, "ap": 0.6384497995032611, "f1": 0.6042498390212493, "f1_threshold": 0.7865688800811768, "precision": 0.5901886792452831, "recall": 0.6189973614775726}}]}, "task_name": "TwitterSemEval2015", "avg_layerwise_entropy": 0.12111307740064067, "num_samples": 64}