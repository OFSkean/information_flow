{"dataset_revision": "70970daeab8776df92f5ea462b6173c0b46fd2d1", "evaluation_time": 219.4340214729309, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"cosine": {"accuracy": 0.8688680932228646, "accuracy_threshold": 0.8623208999633789, "ap": 0.7661152616244871, "f1": 0.6995991206517521, "f1_threshold": 0.8513604402542114, "precision": 0.6860258686279482, "recall": 0.7137203166226913}, "dot": {"accuracy": 0.7745127257554986, "accuracy_threshold": 168.93533325195312, "ap": 0.31015717593418124, "f1": 0.39074414935577173, "f1_threshold": 101.96318817138672, "precision": 0.2601996147784976, "recall": 0.7841688654353562}, "euclidean": {"accuracy": 0.8670799308577218, "accuracy_threshold": 5.930927276611328, "ap": 0.7554360495128446, "f1": 0.6966511855292103, "f1_threshold": 6.545598030090332, "precision": 0.6489071038251366, "recall": 0.7519788918205804}, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.7661152616244871, "manhattan": {"accuracy": 0.8686892769863503, "accuracy_threshold": 291.23065185546875, "ap": 0.7592842698391817, "f1": 0.7042110603754439, "f1_threshold": 318.29278564453125, "precision": 0.6780654616511969, "recall": 0.7324538258575198}, "max": {"accuracy": 0.8688680932228646, "ap": 0.7661152616244871, "f1": 0.7042110603754439}, "similarity": {"accuracy": 0.8688680932228646, "accuracy_threshold": 0.8623208999633789, "ap": 0.7661152616244871, "f1": 0.6995991206517521, "f1_threshold": 0.8513604402542114, "precision": 0.6860258686279482, "recall": 0.7137203166226913}}]}, "task_name": "TwitterSemEval2015", "avg_layerwise_entropy": 0.4502362169677404, "num_samples": 64}