{"dataset_revision": "0fd18e25b25c072e09e0d92ab615fda904d66300", "evaluation_time": 172.3122365474701, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"accuracy": 0.8419480519480519, "f1": 0.8414749716230444, "f1_weighted": 0.8414749716230444, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.8419480519480519, "scores_per_experiment": [{"accuracy": 0.8396103896103896, "f1": 0.8399611534011946, "f1_weighted": 0.8399611534011946}, {"accuracy": 0.849025974025974, "f1": 0.8479762858522049, "f1_weighted": 0.8479762858522049}, {"accuracy": 0.839935064935065, "f1": 0.8394135298123543, "f1_weighted": 0.8394135298123544}, {"accuracy": 0.8392857142857143, "f1": 0.8387278566684101, "f1_weighted": 0.83872785666841}, {"accuracy": 0.8435064935064935, "f1": 0.8437280773176001, "f1_weighted": 0.8437280773176001}, {"accuracy": 0.8577922077922078, "f1": 0.8570835388832778, "f1_weighted": 0.8570835388832783}, {"accuracy": 0.837987012987013, "f1": 0.8376084748891524, "f1_weighted": 0.8376084748891527}, {"accuracy": 0.8353896103896103, "f1": 0.8352894431460488, "f1_weighted": 0.835289443146049}, {"accuracy": 0.8366883116883117, "f1": 0.835476586397131, "f1_weighted": 0.8354765863971307}, {"accuracy": 0.8402597402597403, "f1": 0.8394847698630703, "f1_weighted": 0.8394847698630704}]}]}, "task_name": "Banking77Classification", "avg_layerwise_entropy": 0.2696406248981572, "num_samples": 64}