{"dataset_revision": "70970daeab8776df92f5ea462b6173c0b46fd2d1", "evaluation_time": 203.6333384513855, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"cosine": {"accuracy": 0.8261310126959528, "accuracy_threshold": 0.8160834312438965, "ap": 0.6203815160386785, "f1": 0.5842995169082125, "f1_threshold": 0.7662115693092346, "precision": 0.5387527839643652, "recall": 0.6382585751978892}, "dot": {"accuracy": 0.7823806401621267, "accuracy_threshold": 3.3973464965820312, "ap": 0.4202596136556615, "f1": 0.4567031336918228, "f1_threshold": 2.699559211730957, "precision": 0.3520583190394511, "recall": 0.649868073878628}, "euclidean": {"accuracy": 0.8206473147761817, "accuracy_threshold": 1.1736693382263184, "ap": 0.6002149990674852, "f1": 0.5809031044214488, "f1_threshold": 1.3123855590820312, "precision": 0.5239711497666525, "recall": 0.6517150395778364}, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.6203815160386785, "manhattan": {"accuracy": 0.8205281039518388, "accuracy_threshold": 58.27799987792969, "ap": 0.5989935996056441, "f1": 0.5804695474651241, "f1_threshold": 65.5013656616211, "precision": 0.5090511239307738, "recall": 0.675197889182058}, "max": {"accuracy": 0.8261310126959528, "ap": 0.6203815160386785, "f1": 0.5842995169082125}, "similarity": {"accuracy": 0.8261310126959528, "accuracy_threshold": 0.8160834312438965, "ap": 0.6203815160386785, "f1": 0.5842995169082125, "f1_threshold": 0.7662115693092346, "precision": 0.5387527839643652, "recall": 0.6382585751978892}}]}, "task_name": "TwitterSemEval2015", "avg_layerwise_entropy": 0.07452277913697043, "num_samples": 64}