{"dataset_revision": "70970daeab8776df92f5ea462b6173c0b46fd2d1", "evaluation_time": 200.9978575706482, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"cosine": {"accuracy": 0.8631459736544078, "accuracy_threshold": 0.8535608053207397, "ap": 0.7503552312703549, "f1": 0.6835309617918313, "f1_threshold": 0.8323025703430176, "precision": 0.6826315789473684, "recall": 0.6844327176781002}, "dot": {"accuracy": 0.779758002026584, "accuracy_threshold": 57.37628936767578, "ap": 0.38130237062468914, "f1": 0.4209553596815468, "f1_threshold": 40.249454498291016, "precision": 0.28809106830122594, "recall": 0.7812664907651715}, "euclidean": {"accuracy": 0.8646957143708649, "accuracy_threshold": 4.047372341156006, "ap": 0.746364125681966, "f1": 0.6875953228266396, "f1_threshold": 4.3316569328308105, "precision": 0.6633153506620892, "recall": 0.7137203166226913}, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.7510734785090378, "manhattan": {"accuracy": 0.8674971687429218, "accuracy_threshold": 201.2462615966797, "ap": 0.7510734785090378, "f1": 0.6928412187267774, "f1_threshold": 217.3525390625, "precision": 0.6528944911297853, "recall": 0.7379947229551451}, "max": {"accuracy": 0.8674971687429218, "ap": 0.7510734785090378, "f1": 0.6928412187267774}, "similarity": {"accuracy": 0.8631459736544078, "accuracy_threshold": 0.8535608053207397, "ap": 0.7503552312703549, "f1": 0.6835309617918313, "f1_threshold": 0.8323025703430176, "precision": 0.6826315789473684, "recall": 0.6844327176781002}}]}, "task_name": "TwitterSemEval2015", "avg_layerwise_entropy": 0.32195877591332367, "num_samples": 64}