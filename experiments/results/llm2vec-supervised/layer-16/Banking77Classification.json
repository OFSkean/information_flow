{"dataset_revision": "0fd18e25b25c072e09e0d92ab615fda904d66300", "evaluation_time": 181.82077479362488, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"accuracy": 0.8293181818181818, "f1": 0.8283289148227873, "f1_weighted": 0.8283289148227875, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.8293181818181818, "scores_per_experiment": [{"accuracy": 0.8207792207792208, "f1": 0.8199282732361772, "f1_weighted": 0.8199282732361775}, {"accuracy": 0.8383116883116883, "f1": 0.8382792650650939, "f1_weighted": 0.838279265065094}, {"accuracy": 0.8227272727272728, "f1": 0.821382577285355, "f1_weighted": 0.8213825772853551}, {"accuracy": 0.8396103896103896, "f1": 0.8380170177426907, "f1_weighted": 0.8380170177426907}, {"accuracy": 0.837987012987013, "f1": 0.8371148320682249, "f1_weighted": 0.8371148320682252}, {"accuracy": 0.8256493506493506, "f1": 0.8245185099826525, "f1_weighted": 0.8245185099826526}, {"accuracy": 0.836038961038961, "f1": 0.8358007819874345, "f1_weighted": 0.8358007819874343}, {"accuracy": 0.8165584415584416, "f1": 0.8161606277584323, "f1_weighted": 0.8161606277584327}, {"accuracy": 0.825, "f1": 0.8238505415170967, "f1_weighted": 0.8238505415170967}, {"accuracy": 0.8305194805194805, "f1": 0.828236721584716, "f1_weighted": 0.8282367215847158}]}]}, "task_name": "Banking77Classification", "avg_layerwise_entropy": 0.18670823573389875, "num_samples": 64}