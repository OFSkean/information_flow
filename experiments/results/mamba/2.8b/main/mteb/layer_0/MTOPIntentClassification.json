{
  "dataset_revision": "ae001d0e6b1228650b7bd1c2c65fb50ad11a8aba",
  "evaluation_time": 71.48373246192932,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.6504559963520292,
        "f1": 0.46795036098127546,
        "f1_weighted": 0.6926804462038185,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.6504559963520292,
        "scores_per_experiment": [
          {
            "accuracy": 0.6516187870497036,
            "f1": 0.480792176527216,
            "f1_weighted": 0.691619011149172
          },
          {
            "accuracy": 0.6491108071135431,
            "f1": 0.4574050627256765,
            "f1_weighted": 0.6882029536464289
          },
          {
            "accuracy": 0.6470588235294118,
            "f1": 0.4619784489526809,
            "f1_weighted": 0.6907068958873848
          },
          {
            "accuracy": 0.6502507979936161,
            "f1": 0.4655002666360083,
            "f1_weighted": 0.690389418413655
          },
          {
            "accuracy": 0.645234838121295,
            "f1": 0.46255888227617387,
            "f1_weighted": 0.6918937930418979
          },
          {
            "accuracy": 0.6283629730962152,
            "f1": 0.46325026735059,
            "f1_weighted": 0.6748464066381252
          },
          {
            "accuracy": 0.6429548563611491,
            "f1": 0.46332570405884754,
            "f1_weighted": 0.6855008921414452
          },
          {
            "accuracy": 0.6803465572275422,
            "f1": 0.4978257994702319,
            "f1_weighted": 0.7184330717178751
          },
          {
            "accuracy": 0.6518467852257182,
            "f1": 0.46099860465377374,
            "f1_weighted": 0.696086354688911
          },
          {
            "accuracy": 0.6577747378020976,
            "f1": 0.46586839716155576,
            "f1_weighted": 0.6991256647132891
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.6493512304250558,
        "f1": 0.4374449181385178,
        "f1_weighted": 0.6927511808247167,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.6493512304250558,
        "scores_per_experiment": [
          {
            "accuracy": 0.6425055928411633,
            "f1": 0.4334139992137159,
            "f1_weighted": 0.6833385447590964
          },
          {
            "accuracy": 0.6416107382550336,
            "f1": 0.42305179418875266,
            "f1_weighted": 0.6836707241764421
          },
          {
            "accuracy": 0.6366890380313199,
            "f1": 0.42805913388874756,
            "f1_weighted": 0.6823104340657486
          },
          {
            "accuracy": 0.6416107382550336,
            "f1": 0.43313101100554674,
            "f1_weighted": 0.6839021184979246
          },
          {
            "accuracy": 0.6635346756152125,
            "f1": 0.4585166913508189,
            "f1_weighted": 0.70749618170154
          },
          {
            "accuracy": 0.6384787472035794,
            "f1": 0.43915404320528717,
            "f1_weighted": 0.6813246474948885
          },
          {
            "accuracy": 0.6451901565995526,
            "f1": 0.43370428285868595,
            "f1_weighted": 0.6903006537276467
          },
          {
            "accuracy": 0.6796420581655481,
            "f1": 0.4677271261198176,
            "f1_weighted": 0.7227450039586716
          },
          {
            "accuracy": 0.6429530201342282,
            "f1": 0.4244916326959242,
            "f1_weighted": 0.6920317706508158
          },
          {
            "accuracy": 0.6612975391498881,
            "f1": 0.43319946685788174,
            "f1_weighted": 0.7003917292143919
          }
        ]
      }
    ]
  },
  "task_name": "MTOPIntentClassification"
}