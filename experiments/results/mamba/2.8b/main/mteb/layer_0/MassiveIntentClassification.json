{
  "dataset_revision": "4672e20407010da34463acc759c162ca9734bca6",
  "evaluation_time": 42.52699851989746,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.5598184263618023,
        "f1": 0.5413071211317917,
        "f1_weighted": 0.5622636652735135,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5598184263618023,
        "scores_per_experiment": [
          {
            "accuracy": 0.5585070611970411,
            "f1": 0.5422349842789937,
            "f1_weighted": 0.560079240459882
          },
          {
            "accuracy": 0.5739744451916611,
            "f1": 0.5469932402592614,
            "f1_weighted": 0.579766004977182
          },
          {
            "accuracy": 0.5601882985877606,
            "f1": 0.5429305855574049,
            "f1_weighted": 0.5565576390797099
          },
          {
            "accuracy": 0.5783456624075319,
            "f1": 0.553039428071589,
            "f1_weighted": 0.584019982454935
          },
          {
            "accuracy": 0.5675857431069267,
            "f1": 0.5435544431338554,
            "f1_weighted": 0.5645677560833228
          },
          {
            "accuracy": 0.527236045729657,
            "f1": 0.5153456018627591,
            "f1_weighted": 0.5308045338629265
          },
          {
            "accuracy": 0.5527908540685945,
            "f1": 0.542911811107547,
            "f1_weighted": 0.5549138321201281
          },
          {
            "accuracy": 0.5487558843308675,
            "f1": 0.5353590649675933,
            "f1_weighted": 0.5542691920677337
          },
          {
            "accuracy": 0.554808338937458,
            "f1": 0.5344787159237033,
            "f1_weighted": 0.5549055938240802
          },
          {
            "accuracy": 0.5759919300605245,
            "f1": 0.5562233361552088,
            "f1_weighted": 0.5827528778052339
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.5678799803246434,
        "f1": 0.5519792090036603,
        "f1_weighted": 0.5693537799097688,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5678799803246434,
        "scores_per_experiment": [
          {
            "accuracy": 0.5661583866207575,
            "f1": 0.5530171791043783,
            "f1_weighted": 0.5660440353691383
          },
          {
            "accuracy": 0.5833743236596163,
            "f1": 0.5436247261640677,
            "f1_weighted": 0.5844835230106377
          },
          {
            "accuracy": 0.5848499754058042,
            "f1": 0.565819220583712,
            "f1_weighted": 0.5842296267406526
          },
          {
            "accuracy": 0.5750122970978849,
            "f1": 0.5461250280999657,
            "f1_weighted": 0.5741866306820667
          },
          {
            "accuracy": 0.5932120019675357,
            "f1": 0.5764246513930633,
            "f1_weighted": 0.5962049968832241
          },
          {
            "accuracy": 0.5479586817511067,
            "f1": 0.5449849433582398,
            "f1_weighted": 0.551486398255141
          },
          {
            "accuracy": 0.5395966551893753,
            "f1": 0.5471513659828329,
            "f1_weighted": 0.5399227564644292
          },
          {
            "accuracy": 0.5381210034431874,
            "f1": 0.5233988019892153,
            "f1_weighted": 0.5399903472262227
          },
          {
            "accuracy": 0.5691096901131333,
            "f1": 0.5553046930090718,
            "f1_weighted": 0.5726012269907685
          },
          {
            "accuracy": 0.5814067879980325,
            "f1": 0.5639414803520564,
            "f1_weighted": 0.5843882574754078
          }
        ]
      }
    ]
  },
  "task_name": "MassiveIntentClassification"
}