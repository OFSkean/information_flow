{
  "dataset_revision": "4672e20407010da34463acc759c162ca9734bca6",
  "evaluation_time": 72.58093738555908,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.5718897108271689,
        "f1": 0.5515469148022449,
        "f1_weighted": 0.5736637055947446,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5718897108271689,
        "scores_per_experiment": [
          {
            "accuracy": 0.5716207128446537,
            "f1": 0.5551656944121831,
            "f1_weighted": 0.5713714049333208
          },
          {
            "accuracy": 0.5796906523201076,
            "f1": 0.5523148944037147,
            "f1_weighted": 0.5848455463318033
          },
          {
            "accuracy": 0.5722932078009415,
            "f1": 0.551651366683466,
            "f1_weighted": 0.5674449309391101
          },
          {
            "accuracy": 0.5941492938802959,
            "f1": 0.5602771527041465,
            "f1_weighted": 0.5986492341017908
          },
          {
            "accuracy": 0.5796906523201076,
            "f1": 0.5562699538153792,
            "f1_weighted": 0.5748298767452547
          },
          {
            "accuracy": 0.5427034297242771,
            "f1": 0.5302955820582805,
            "f1_weighted": 0.5459897444275598
          },
          {
            "accuracy": 0.5632145258910558,
            "f1": 0.5529071594079471,
            "f1_weighted": 0.5648787690555481
          },
          {
            "accuracy": 0.5628782784129119,
            "f1": 0.543647204728277,
            "f1_weighted": 0.5686936519090732
          },
          {
            "accuracy": 0.5699394754539341,
            "f1": 0.5514237000326138,
            "f1_weighted": 0.569054132705005
          },
          {
            "accuracy": 0.5827168796234028,
            "f1": 0.561516439776442,
            "f1_weighted": 0.5908797647989801
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.5792424987702901,
        "f1": 0.5612450817607493,
        "f1_weighted": 0.5802065836117567,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5792424987702901,
        "scores_per_experiment": [
          {
            "accuracy": 0.5705853418593212,
            "f1": 0.5540204671024004,
            "f1_weighted": 0.5699411544065078
          },
          {
            "accuracy": 0.5927201180521396,
            "f1": 0.5614234296446988,
            "f1_weighted": 0.5938882960427441
          },
          {
            "accuracy": 0.5897688145597639,
            "f1": 0.5695517402677921,
            "f1_weighted": 0.5885204838195257
          },
          {
            "accuracy": 0.5922282341367437,
            "f1": 0.5590500196595116,
            "f1_weighted": 0.591051107458212
          },
          {
            "accuracy": 0.6074766355140186,
            "f1": 0.5854480162924214,
            "f1_weighted": 0.6060502353704811
          },
          {
            "accuracy": 0.5528775209050664,
            "f1": 0.5476177878212888,
            "f1_weighted": 0.5573285394259523
          },
          {
            "accuracy": 0.5558288243974422,
            "f1": 0.5538725550574991,
            "f1_weighted": 0.5547898138205598
          },
          {
            "accuracy": 0.5622233152975897,
            "f1": 0.5416734033483633,
            "f1_weighted": 0.5651589341099968
          },
          {
            "accuracy": 0.5764879488440728,
            "f1": 0.5674319645951219,
            "f1_weighted": 0.5782886695822446
          },
          {
            "accuracy": 0.5922282341367437,
            "f1": 0.5723614338183955,
            "f1_weighted": 0.5970486020813431
          }
        ]
      }
    ]
  },
  "task_name": "MassiveIntentClassification"
}