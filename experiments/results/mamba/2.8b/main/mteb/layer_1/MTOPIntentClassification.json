{
  "dataset_revision": "ae001d0e6b1228650b7bd1c2c65fb50ad11a8aba",
  "evaluation_time": 142.29840660095215,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.6618787049703603,
        "f1": 0.46979643920547254,
        "f1_weighted": 0.703138533815636,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.6618787049703603,
        "scores_per_experiment": [
          {
            "accuracy": 0.6668946648426812,
            "f1": 0.4678311297188783,
            "f1_weighted": 0.7040373750098409
          },
          {
            "accuracy": 0.6527587779297765,
            "f1": 0.4527086809493104,
            "f1_weighted": 0.6908916622166055
          },
          {
            "accuracy": 0.6554947560419516,
            "f1": 0.47205191127688806,
            "f1_weighted": 0.7006550892889802
          },
          {
            "accuracy": 0.6714546283629731,
            "f1": 0.4745789914922855,
            "f1_weighted": 0.709667186883408
          },
          {
            "accuracy": 0.6650706794345645,
            "f1": 0.47683191094741567,
            "f1_weighted": 0.7097525828003103
          },
          {
            "accuracy": 0.6361149110807114,
            "f1": 0.45970695183823473,
            "f1_weighted": 0.6822389074978679
          },
          {
            "accuracy": 0.6513907888736891,
            "f1": 0.47092624682111356,
            "f1_weighted": 0.6943581382272265
          },
          {
            "accuracy": 0.6896944824441404,
            "f1": 0.4969954850882448,
            "f1_weighted": 0.7263439386943741
          },
          {
            "accuracy": 0.6655266757865937,
            "f1": 0.45711180762459,
            "f1_weighted": 0.7090175632511876
          },
          {
            "accuracy": 0.6643866849065208,
            "f1": 0.4692212762977645,
            "f1_weighted": 0.7044228942865588
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.661476510067114,
        "f1": 0.45349995479425387,
        "f1_weighted": 0.7039793614555641,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.661476510067114,
        "scores_per_experiment": [
          {
            "accuracy": 0.647427293064877,
            "f1": 0.4383538981239812,
            "f1_weighted": 0.6878322420793347
          },
          {
            "accuracy": 0.6541387024608502,
            "f1": 0.44261516508453386,
            "f1_weighted": 0.6932216140963566
          },
          {
            "accuracy": 0.6554809843400448,
            "f1": 0.4505757306182144,
            "f1_weighted": 0.7003050527113552
          },
          {
            "accuracy": 0.661744966442953,
            "f1": 0.4485905203541252,
            "f1_weighted": 0.7010007392997953
          },
          {
            "accuracy": 0.6751677852348993,
            "f1": 0.47980468904179524,
            "f1_weighted": 0.7231015000397819
          },
          {
            "accuracy": 0.6465324384787472,
            "f1": 0.44339630245264433,
            "f1_weighted": 0.6907890408381868
          },
          {
            "accuracy": 0.654586129753915,
            "f1": 0.45384137695620874,
            "f1_weighted": 0.6959834194815389
          },
          {
            "accuracy": 0.6921700223713646,
            "f1": 0.48287405408647727,
            "f1_weighted": 0.7323369194768357
          },
          {
            "accuracy": 0.6541387024608502,
            "f1": 0.43182509575639094,
            "f1_weighted": 0.7020880520536991
          },
          {
            "accuracy": 0.6733780760626398,
            "f1": 0.463122715468168,
            "f1_weighted": 0.7131350344787573
          }
        ]
      }
    ]
  },
  "task_name": "MTOPIntentClassification"
}