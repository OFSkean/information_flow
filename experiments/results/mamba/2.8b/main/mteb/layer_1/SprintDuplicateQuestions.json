{
  "dataset_revision": "d66bd1f72af766a5cc4b0ca5e00c162f89e8cc46",
  "evaluation_time": 34.85748362541199,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "cosine_accuracy": 0.9905148514851485,
        "cosine_accuracy_threshold": 0.9516328573226929,
        "cosine_ap": 0.18393747856545667,
        "cosine_f1": 0.26319135410044503,
        "cosine_f1_threshold": 0.9280660152435303,
        "cosine_precision": 0.3612565445026178,
        "cosine_recall": 0.207,
        "dot_accuracy": 0.9900891089108911,
        "dot_accuracy_threshold": 1233.1895751953125,
        "dot_ap": 0.01419499961839818,
        "dot_f1": 0.0354464894342195,
        "dot_f1_threshold": 686.4136352539062,
        "dot_precision": 0.02688728024819028,
        "dot_recall": 0.052,
        "euclidean_accuracy": 0.9911683168316832,
        "euclidean_accuracy_threshold": 8.512057304382324,
        "euclidean_ap": 0.2655134890075884,
        "euclidean_f1": 0.32868405093996367,
        "euclidean_f1_threshold": 9.99191665649414,
        "euclidean_precision": 0.41756548536209553,
        "euclidean_recall": 0.271,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5077897190239966,
        "manhattan_accuracy": 0.9925247524752475,
        "manhattan_accuracy_threshold": 346.3484802246094,
        "manhattan_ap": 0.5077897190239966,
        "manhattan_f1": 0.5203693644758284,
        "manhattan_f1_threshold": 378.5213317871094,
        "manhattan_precision": 0.5695600475624257,
        "manhattan_recall": 0.479,
        "max_accuracy": 0.9925247524752475,
        "max_ap": 0.5077897190239966,
        "max_f1": 0.5203693644758284,
        "max_precision": 0.5695600475624257,
        "max_recall": 0.479,
        "similarity_accuracy": 0.9905148514851485,
        "similarity_accuracy_threshold": 0.9516328573226929,
        "similarity_ap": 0.18393747856545667,
        "similarity_f1": 0.26319135410044503,
        "similarity_f1_threshold": 0.9280660152435303,
        "similarity_precision": 0.3612565445026178,
        "similarity_recall": 0.207
      }
    ],
    "validation": [
      {
        "cosine_accuracy": 0.9906435643564356,
        "cosine_accuracy_threshold": 0.955436110496521,
        "cosine_ap": 0.15655946251755523,
        "cosine_f1": 0.21455938697318006,
        "cosine_f1_threshold": 0.9188679456710815,
        "cosine_precision": 0.20588235294117646,
        "cosine_recall": 0.224,
        "dot_accuracy": 0.9900891089108911,
        "dot_accuracy_threshold": 1106.68115234375,
        "dot_ap": 0.014751173070211787,
        "dot_f1": 0.03622441466647033,
        "dot_f1_threshold": 608.7894287109375,
        "dot_precision": 0.021239854947332066,
        "dot_recall": 0.123,
        "euclidean_accuracy": 0.9909405940594059,
        "euclidean_accuracy_threshold": 8.794004440307617,
        "euclidean_ap": 0.2263316313873234,
        "euclidean_f1": 0.30323005932762026,
        "euclidean_f1_threshold": 9.870121002197266,
        "euclidean_precision": 0.4448742746615087,
        "euclidean_recall": 0.23,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.44997091355752766,
        "manhattan_accuracy": 0.992029702970297,
        "manhattan_accuracy_threshold": 340.558837890625,
        "manhattan_ap": 0.44997091355752766,
        "manhattan_f1": 0.47828604516502604,
        "manhattan_f1_threshold": 377.22625732421875,
        "manhattan_precision": 0.5680880330123796,
        "manhattan_recall": 0.413,
        "max_accuracy": 0.992029702970297,
        "max_ap": 0.44997091355752766,
        "max_f1": 0.47828604516502604,
        "max_precision": 0.5680880330123796,
        "max_recall": 0.413,
        "similarity_accuracy": 0.9906435643564356,
        "similarity_accuracy_threshold": 0.955436110496521,
        "similarity_ap": 0.15655946251755523,
        "similarity_f1": 0.21455938697318006,
        "similarity_f1_threshold": 0.9188679456710815,
        "similarity_precision": 0.20588235294117646,
        "similarity_recall": 0.224
      }
    ]
  },
  "task_name": "SprintDuplicateQuestions"
}