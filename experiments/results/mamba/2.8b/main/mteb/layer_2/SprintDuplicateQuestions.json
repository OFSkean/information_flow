{
  "dataset_revision": "d66bd1f72af766a5cc4b0ca5e00c162f89e8cc46",
  "evaluation_time": 49.16656422615051,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "cosine_accuracy": 0.9904356435643564,
        "cosine_accuracy_threshold": 0.9593771696090698,
        "cosine_ap": 0.15874463286056004,
        "cosine_f1": 0.2301087578706354,
        "cosine_f1_threshold": 0.9327026009559631,
        "cosine_precision": 0.26907630522088355,
        "cosine_recall": 0.201,
        "dot_accuracy": 0.9900891089108911,
        "dot_accuracy_threshold": 1320.3826904296875,
        "dot_ap": 0.012932831306233206,
        "dot_f1": 0.02976190476190476,
        "dot_f1_threshold": 737.08154296875,
        "dot_precision": 0.019299287410926364,
        "dot_recall": 0.065,
        "euclidean_accuracy": 0.9912475247524752,
        "euclidean_accuracy_threshold": 8.989019393920898,
        "euclidean_ap": 0.2671326263679523,
        "euclidean_f1": 0.326072607260726,
        "euclidean_f1_threshold": 9.988054275512695,
        "euclidean_precision": 0.4796116504854369,
        "euclidean_recall": 0.247,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5042473881714158,
        "manhattan_accuracy": 0.9924851485148515,
        "manhattan_accuracy_threshold": 347.5794677734375,
        "manhattan_ap": 0.5042473881714158,
        "manhattan_f1": 0.5155902004454344,
        "manhattan_f1_threshold": 380.120849609375,
        "manhattan_precision": 0.5816582914572864,
        "manhattan_recall": 0.463,
        "max_accuracy": 0.9924851485148515,
        "max_ap": 0.5042473881714158,
        "max_f1": 0.5155902004454344,
        "max_precision": 0.5816582914572864,
        "max_recall": 0.463,
        "similarity_accuracy": 0.9904356435643564,
        "similarity_accuracy_threshold": 0.9593771696090698,
        "similarity_ap": 0.15874463286056004,
        "similarity_f1": 0.2301087578706354,
        "similarity_f1_threshold": 0.9327026009559631,
        "similarity_precision": 0.26907630522088355,
        "similarity_recall": 0.201
      }
    ],
    "validation": [
      {
        "cosine_accuracy": 0.9905148514851485,
        "cosine_accuracy_threshold": 0.9623454809188843,
        "cosine_ap": 0.13624797615827924,
        "cosine_f1": 0.19811842833425566,
        "cosine_f1_threshold": 0.9323519468307495,
        "cosine_precision": 0.2218091697645601,
        "cosine_recall": 0.179,
        "dot_accuracy": 0.9900891089108911,
        "dot_accuracy_threshold": 1245.2982177734375,
        "dot_ap": 0.014209728710253534,
        "dot_f1": 0.035036822137915646,
        "dot_f1_threshold": 662.847412109375,
        "dot_precision": 0.019718663652348655,
        "dot_recall": 0.157,
        "euclidean_accuracy": 0.9907524752475247,
        "euclidean_accuracy_threshold": 8.784175872802734,
        "euclidean_ap": 0.2118067604840733,
        "euclidean_f1": 0.2852233676975945,
        "euclidean_f1_threshold": 10.43130874633789,
        "euclidean_precision": 0.33378016085790885,
        "euclidean_recall": 0.249,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.4394907238949498,
        "manhattan_accuracy": 0.991970297029703,
        "manhattan_accuracy_threshold": 355.93255615234375,
        "manhattan_ap": 0.4394907238949498,
        "manhattan_f1": 0.47116430903155604,
        "manhattan_f1_threshold": 386.8447265625,
        "manhattan_precision": 0.5167064439140812,
        "manhattan_recall": 0.433,
        "max_accuracy": 0.991970297029703,
        "max_ap": 0.4394907238949498,
        "max_f1": 0.47116430903155604,
        "max_precision": 0.5167064439140812,
        "max_recall": 0.433,
        "similarity_accuracy": 0.9905148514851485,
        "similarity_accuracy_threshold": 0.9623454809188843,
        "similarity_ap": 0.13624797615827924,
        "similarity_f1": 0.19811842833425566,
        "similarity_f1_threshold": 0.9323519468307495,
        "similarity_precision": 0.2218091697645601,
        "similarity_recall": 0.179
      }
    ]
  },
  "task_name": "SprintDuplicateQuestions"
}