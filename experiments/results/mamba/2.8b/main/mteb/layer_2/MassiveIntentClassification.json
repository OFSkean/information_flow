{
  "dataset_revision": "4672e20407010da34463acc759c162ca9734bca6",
  "evaluation_time": 84.85383915901184,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.5785137861466039,
        "f1": 0.5552167115894351,
        "f1_weighted": 0.5804842160925548,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5785137861466039,
        "scores_per_experiment": [
          {
            "accuracy": 0.5800268997982515,
            "f1": 0.5578232052420898,
            "f1_weighted": 0.5798483623051569
          },
          {
            "accuracy": 0.5911230665770006,
            "f1": 0.563723874023226,
            "f1_weighted": 0.5966862760663486
          },
          {
            "accuracy": 0.5793544048419637,
            "f1": 0.5521511934976474,
            "f1_weighted": 0.5757143907387491
          },
          {
            "accuracy": 0.5991930060524546,
            "f1": 0.5665335172823721,
            "f1_weighted": 0.6034777593818316
          },
          {
            "accuracy": 0.5880968392737055,
            "f1": 0.5561559919603039,
            "f1_weighted": 0.5841067496950569
          },
          {
            "accuracy": 0.5494283792871554,
            "f1": 0.5396182575175658,
            "f1_weighted": 0.5530017895788345
          },
          {
            "accuracy": 0.5662407531943511,
            "f1": 0.5521521442238418,
            "f1_weighted": 0.5664839456332412
          },
          {
            "accuracy": 0.5699394754539341,
            "f1": 0.5491468260567881,
            "f1_weighted": 0.5748670664723795
          },
          {
            "accuracy": 0.5726294552790854,
            "f1": 0.5580154720467024,
            "f1_weighted": 0.5746197869654598
          },
          {
            "accuracy": 0.5891055817081372,
            "f1": 0.5568466340438142,
            "f1_weighted": 0.5960360340884892
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.5867683226758486,
        "f1": 0.5690262936456101,
        "f1_weighted": 0.587199997950596,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5867683226758486,
        "scores_per_experiment": [
          {
            "accuracy": 0.5794392523364486,
            "f1": 0.5612183225135929,
            "f1_weighted": 0.5785372134298796
          },
          {
            "accuracy": 0.5912444663059518,
            "f1": 0.558509765840122,
            "f1_weighted": 0.5928961424325793
          },
          {
            "accuracy": 0.5927201180521396,
            "f1": 0.5684296578927367,
            "f1_weighted": 0.591057571706834
          },
          {
            "accuracy": 0.5956714215445155,
            "f1": 0.5646921624190152,
            "f1_weighted": 0.5948632271354203
          },
          {
            "accuracy": 0.6109198229217905,
            "f1": 0.5892125485317752,
            "f1_weighted": 0.6088711126593616
          },
          {
            "accuracy": 0.5681259222823414,
            "f1": 0.579092826576858,
            "f1_weighted": 0.5717548364166569
          },
          {
            "accuracy": 0.5646827348745695,
            "f1": 0.5586388499312294,
            "f1_weighted": 0.5641255420394331
          },
          {
            "accuracy": 0.5794392523364486,
            "f1": 0.5590438275532822,
            "f1_weighted": 0.5792087687560117
          },
          {
            "accuracy": 0.5868175110673881,
            "f1": 0.578041221857699,
            "f1_weighted": 0.5886884278326149
          },
          {
            "accuracy": 0.5986227250368913,
            "f1": 0.5733837533397904,
            "f1_weighted": 0.6019971370971685
          }
        ]
      }
    ]
  },
  "task_name": "MassiveIntentClassification"
}