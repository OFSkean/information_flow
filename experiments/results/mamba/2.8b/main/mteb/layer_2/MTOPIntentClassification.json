{
  "dataset_revision": "ae001d0e6b1228650b7bd1c2c65fb50ad11a8aba",
  "evaluation_time": 160.08616495132446,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.673734610123119,
        "f1": 0.4798455918592913,
        "f1_weighted": 0.7138163016154757,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.673734610123119,
        "scores_per_experiment": [
          {
            "accuracy": 0.6785225718194254,
            "f1": 0.4781375691158486,
            "f1_weighted": 0.7148408672169284
          },
          {
            "accuracy": 0.666438668490652,
            "f1": 0.4668353263983885,
            "f1_weighted": 0.7059495559922362
          },
          {
            "accuracy": 0.6627906976744186,
            "f1": 0.48246311092773814,
            "f1_weighted": 0.7065045392624592
          },
          {
            "accuracy": 0.6808025535795713,
            "f1": 0.4820327287052629,
            "f1_weighted": 0.7170769392233667
          },
          {
            "accuracy": 0.6755585955312358,
            "f1": 0.48213355500227845,
            "f1_weighted": 0.7208178508443575
          },
          {
            "accuracy": 0.6554947560419516,
            "f1": 0.4750060495959791,
            "f1_weighted": 0.6974420857480503
          },
          {
            "accuracy": 0.668718650250798,
            "f1": 0.47592224626068025,
            "f1_weighted": 0.7099852160876076
          },
          {
            "accuracy": 0.7017783857729138,
            "f1": 0.5012217122444761,
            "f1_weighted": 0.7363470130557008
          },
          {
            "accuracy": 0.6771545827633378,
            "f1": 0.4811554004699784,
            "f1_weighted": 0.719110521320033
          },
          {
            "accuracy": 0.6700866393068855,
            "f1": 0.4735482198722834,
            "f1_weighted": 0.7100884274040177
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.6733780760626398,
        "f1": 0.4584224315185571,
        "f1_weighted": 0.7153198490271996,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.6733780760626398,
        "scores_per_experiment": [
          {
            "accuracy": 0.6612975391498881,
            "f1": 0.4317957189968424,
            "f1_weighted": 0.7023395336362778
          },
          {
            "accuracy": 0.6612975391498881,
            "f1": 0.444971372739613,
            "f1_weighted": 0.7042231042174283
          },
          {
            "accuracy": 0.6639821029082774,
            "f1": 0.441332833799865,
            "f1_weighted": 0.7068697224839068
          },
          {
            "accuracy": 0.680089485458613,
            "f1": 0.47993386640314367,
            "f1_weighted": 0.7210092076968853
          },
          {
            "accuracy": 0.6850111856823267,
            "f1": 0.48262554347594594,
            "f1_weighted": 0.7318316456343789
          },
          {
            "accuracy": 0.6657718120805369,
            "f1": 0.4644035838404346,
            "f1_weighted": 0.7081809440809617
          },
          {
            "accuracy": 0.6680089485458613,
            "f1": 0.4664831855944922,
            "f1_weighted": 0.7093989760823952
          },
          {
            "accuracy": 0.6939597315436241,
            "f1": 0.4762903634323263,
            "f1_weighted": 0.7318300296827392
          },
          {
            "accuracy": 0.6639821029082774,
            "f1": 0.4398487640008357,
            "f1_weighted": 0.7090726843945537
          },
          {
            "accuracy": 0.6903803131991052,
            "f1": 0.45653908290207246,
            "f1_weighted": 0.7284426423624698
          }
        ]
      }
    ]
  },
  "task_name": "MTOPIntentClassification"
}