{
  "dataset_revision": "1399c76144fd37290681b995c656ef9b2e06e26d",
  "evaluation_time": 282.00551533699036,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.30910000000000004,
        "f1": 0.30901700512944863,
        "f1_weighted": 0.30901700512944863,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.30910000000000004,
        "scores_per_experiment": [
          {
            "accuracy": 0.3428,
            "f1": 0.33905551093279385,
            "f1_weighted": 0.33905551093279385
          },
          {
            "accuracy": 0.3242,
            "f1": 0.32797045526348845,
            "f1_weighted": 0.32797045526348845
          },
          {
            "accuracy": 0.2862,
            "f1": 0.28898427991507264,
            "f1_weighted": 0.2889842799150726
          },
          {
            "accuracy": 0.292,
            "f1": 0.29627077553544623,
            "f1_weighted": 0.29627077553544623
          },
          {
            "accuracy": 0.34,
            "f1": 0.3381836414485655,
            "f1_weighted": 0.3381836414485655
          },
          {
            "accuracy": 0.293,
            "f1": 0.2910137313738499,
            "f1_weighted": 0.29101373137384984
          },
          {
            "accuracy": 0.254,
            "f1": 0.2550814343334039,
            "f1_weighted": 0.2550814343334039
          },
          {
            "accuracy": 0.3368,
            "f1": 0.33281782333359305,
            "f1_weighted": 0.33281782333359305
          },
          {
            "accuracy": 0.293,
            "f1": 0.2907097610344105,
            "f1_weighted": 0.2907097610344105
          },
          {
            "accuracy": 0.329,
            "f1": 0.33008263812386235,
            "f1_weighted": 0.3300826381238623
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.3042600000000001,
        "f1": 0.3038847189326,
        "f1_weighted": 0.3038847189326,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.3042600000000001,
        "scores_per_experiment": [
          {
            "accuracy": 0.3358,
            "f1": 0.3342214323617329,
            "f1_weighted": 0.3342214323617329
          },
          {
            "accuracy": 0.3266,
            "f1": 0.3299561243975201,
            "f1_weighted": 0.32995612439752003
          },
          {
            "accuracy": 0.2882,
            "f1": 0.2914266195167123,
            "f1_weighted": 0.29142661951671234
          },
          {
            "accuracy": 0.2772,
            "f1": 0.28007687315182106,
            "f1_weighted": 0.280076873151821
          },
          {
            "accuracy": 0.3408,
            "f1": 0.33496717451524755,
            "f1_weighted": 0.3349671745152475
          },
          {
            "accuracy": 0.2932,
            "f1": 0.2907244907065573,
            "f1_weighted": 0.2907244907065573
          },
          {
            "accuracy": 0.252,
            "f1": 0.25170660601385997,
            "f1_weighted": 0.25170660601385997
          },
          {
            "accuracy": 0.314,
            "f1": 0.3112418931610822,
            "f1_weighted": 0.31124189316108225
          },
          {
            "accuracy": 0.285,
            "f1": 0.28295605544968916,
            "f1_weighted": 0.28295605544968916
          },
          {
            "accuracy": 0.3298,
            "f1": 0.3315699200517773,
            "f1_weighted": 0.33156992005177727
          }
        ]
      }
    ]
  },
  "task_name": "AmazonReviewsClassification"
}