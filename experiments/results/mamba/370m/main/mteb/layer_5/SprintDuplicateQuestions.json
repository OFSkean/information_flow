{
  "dataset_revision": "d66bd1f72af766a5cc4b0ca5e00c162f89e8cc46",
  "evaluation_time": 36.41637849807739,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "cosine_accuracy": 0.9914455445544554,
        "cosine_accuracy_threshold": 0.9604485630989075,
        "cosine_ap": 0.3710750978699072,
        "cosine_f1": 0.4072798819478603,
        "cosine_f1_threshold": 0.945443868637085,
        "cosine_precision": 0.40077444336882867,
        "cosine_recall": 0.414,
        "dot_accuracy": 0.9900891089108911,
        "dot_accuracy_threshold": 1014.7509155273438,
        "dot_ap": 0.02627771670698488,
        "dot_f1": 0.0687637161667886,
        "dot_f1_threshold": 852.0556640625,
        "dot_precision": 0.05420991926182238,
        "dot_recall": 0.094,
        "euclidean_accuracy": 0.9917326732673267,
        "euclidean_accuracy_threshold": 8.325722694396973,
        "euclidean_ap": 0.4068624666380021,
        "euclidean_f1": 0.4289923394225103,
        "euclidean_f1_threshold": 9.160287857055664,
        "euclidean_precision": 0.5222381635581061,
        "euclidean_recall": 0.364,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5257244265996414,
        "manhattan_accuracy": 0.9925544554455445,
        "manhattan_accuracy_threshold": 195.006591796875,
        "manhattan_ap": 0.5257244265996414,
        "manhattan_f1": 0.5239977413890458,
        "manhattan_f1_threshold": 209.1023712158203,
        "manhattan_precision": 0.6018158236057068,
        "manhattan_recall": 0.464,
        "max_accuracy": 0.9925544554455445,
        "max_ap": 0.5257244265996414,
        "max_f1": 0.5239977413890458,
        "max_precision": 0.6018158236057068,
        "max_recall": 0.464,
        "similarity_accuracy": 0.9914455445544554,
        "similarity_accuracy_threshold": 0.9604485630989075,
        "similarity_ap": 0.3710750978699072,
        "similarity_f1": 0.4072798819478603,
        "similarity_f1_threshold": 0.945443868637085,
        "similarity_precision": 0.40077444336882867,
        "similarity_recall": 0.414
      }
    ],
    "validation": [
      {
        "cosine_accuracy": 0.9908910891089109,
        "cosine_accuracy_threshold": 0.9607962369918823,
        "cosine_ap": 0.28962678424417226,
        "cosine_f1": 0.3436213991769547,
        "cosine_f1_threshold": 0.9460556507110596,
        "cosine_precision": 0.3538135593220339,
        "cosine_recall": 0.334,
        "dot_accuracy": 0.9901089108910891,
        "dot_accuracy_threshold": 1023.39306640625,
        "dot_ap": 0.02832986672450962,
        "dot_f1": 0.07028360049321826,
        "dot_f1_threshold": 847.3604736328125,
        "dot_precision": 0.05080213903743316,
        "dot_recall": 0.114,
        "euclidean_accuracy": 0.9911980198019802,
        "euclidean_accuracy_threshold": 8.281511306762695,
        "euclidean_ap": 0.3298664080860675,
        "euclidean_f1": 0.38662952646239557,
        "euclidean_f1_threshold": 9.444648742675781,
        "euclidean_precision": 0.43647798742138366,
        "euclidean_recall": 0.347,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.45059550274945837,
        "manhattan_accuracy": 0.9918910891089109,
        "manhattan_accuracy_threshold": 194.95565795898438,
        "manhattan_ap": 0.45059550274945837,
        "manhattan_f1": 0.4753261486103233,
        "manhattan_f1_threshold": 211.78475952148438,
        "manhattan_precision": 0.5491480996068152,
        "manhattan_recall": 0.419,
        "max_accuracy": 0.9918910891089109,
        "max_ap": 0.45059550274945837,
        "max_f1": 0.4753261486103233,
        "max_precision": 0.5491480996068152,
        "max_recall": 0.419,
        "similarity_accuracy": 0.9908910891089109,
        "similarity_accuracy_threshold": 0.9607962369918823,
        "similarity_ap": 0.28962678424417226,
        "similarity_f1": 0.3436213991769547,
        "similarity_f1_threshold": 0.9460556507110596,
        "similarity_precision": 0.3538135593220339,
        "similarity_recall": 0.334
      }
    ]
  },
  "task_name": "SprintDuplicateQuestions"
}