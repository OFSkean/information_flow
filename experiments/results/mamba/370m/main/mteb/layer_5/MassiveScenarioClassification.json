{
  "dataset_revision": "fad2c6e8459f9e1c45d9315f4953d921437d70f8",
  "evaluation_time": 39.09492015838623,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.5954942837928716,
        "f1": 0.5825084649684772,
        "f1_weighted": 0.5957705060078938,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5954942837928716,
        "scores_per_experiment": [
          {
            "accuracy": 0.6190316072629455,
            "f1": 0.6055093929854534,
            "f1_weighted": 0.6242812142615899
          },
          {
            "accuracy": 0.6126429051782112,
            "f1": 0.5986750798824292,
            "f1_weighted": 0.6129754103385007
          },
          {
            "accuracy": 0.5817081371889711,
            "f1": 0.5735128526480289,
            "f1_weighted": 0.5807600898093983
          },
          {
            "accuracy": 0.5901143241425689,
            "f1": 0.574910730878405,
            "f1_weighted": 0.5924226627498832
          },
          {
            "accuracy": 0.5917955615332885,
            "f1": 0.5772988056084363,
            "f1_weighted": 0.5897413038981781
          },
          {
            "accuracy": 0.5931405514458642,
            "f1": 0.5754206129678454,
            "f1_weighted": 0.5920399908134028
          },
          {
            "accuracy": 0.5901143241425689,
            "f1": 0.578976058211911,
            "f1_weighted": 0.5939032318328502
          },
          {
            "accuracy": 0.6075991930060525,
            "f1": 0.5866315891671743,
            "f1_weighted": 0.6022770984104087
          },
          {
            "accuracy": 0.5857431069266981,
            "f1": 0.5773522496250074,
            "f1_weighted": 0.5853044757014515
          },
          {
            "accuracy": 0.5830531271015468,
            "f1": 0.5767972777100803,
            "f1_weighted": 0.5839995822632749
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.5787014264633548,
        "f1": 0.5736911414402488,
        "f1_weighted": 0.5779353025656552,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5787014264633548,
        "scores_per_experiment": [
          {
            "accuracy": 0.5996064928676832,
            "f1": 0.5933351888519889,
            "f1_weighted": 0.6047931920888511
          },
          {
            "accuracy": 0.5799311362518446,
            "f1": 0.5841998272703692,
            "f1_weighted": 0.5784857843286917
          },
          {
            "accuracy": 0.5696015740285293,
            "f1": 0.5615059074120761,
            "f1_weighted": 0.5692474369948872
          },
          {
            "accuracy": 0.5553369404820462,
            "f1": 0.5450802013135365,
            "f1_weighted": 0.5516779238278716
          },
          {
            "accuracy": 0.5907525823905558,
            "f1": 0.5841566295499937,
            "f1_weighted": 0.587364426647147
          },
          {
            "accuracy": 0.5740285292670929,
            "f1": 0.5634126488827232,
            "f1_weighted": 0.5714817852517751
          },
          {
            "accuracy": 0.5627151992129857,
            "f1": 0.5654824151043332,
            "f1_weighted": 0.567046503832771
          },
          {
            "accuracy": 0.5981308411214953,
            "f1": 0.5839877734826435,
            "f1_weighted": 0.5923381356883192
          },
          {
            "accuracy": 0.5873093949827841,
            "f1": 0.5940947898672415,
            "f1_weighted": 0.5835478185590475
          },
          {
            "accuracy": 0.5696015740285293,
            "f1": 0.5616560326675808,
            "f1_weighted": 0.5733700184371914
          }
        ]
      }
    ]
  },
  "task_name": "MassiveScenarioClassification"
}