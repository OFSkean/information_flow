{
  "dataset_revision": "ae001d0e6b1228650b7bd1c2c65fb50ad11a8aba",
  "evaluation_time": 96.41886305809021,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.6613543091655266,
        "f1": 0.4758874988878553,
        "f1_weighted": 0.7028032720608319,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.6613543091655266,
        "scores_per_experiment": [
          {
            "accuracy": 0.6491108071135431,
            "f1": 0.4808497165244031,
            "f1_weighted": 0.6936592972952522
          },
          {
            "accuracy": 0.6598267213862289,
            "f1": 0.46240310969480003,
            "f1_weighted": 0.7023682160979935
          },
          {
            "accuracy": 0.6393068855449157,
            "f1": 0.4503982585157236,
            "f1_weighted": 0.6810704794343333
          },
          {
            "accuracy": 0.6785225718194254,
            "f1": 0.5068157866522681,
            "f1_weighted": 0.7179884272908668
          },
          {
            "accuracy": 0.647514819881441,
            "f1": 0.4730141683613606,
            "f1_weighted": 0.6876847532001141
          },
          {
            "accuracy": 0.6616507067943457,
            "f1": 0.47453572083039086,
            "f1_weighted": 0.7041936275182138
          },
          {
            "accuracy": 0.6420428636570907,
            "f1": 0.45959092521096107,
            "f1_weighted": 0.6872761531728836
          },
          {
            "accuracy": 0.6953944368445052,
            "f1": 0.5026016176946689,
            "f1_weighted": 0.7305581741161572
          },
          {
            "accuracy": 0.6739626082991336,
            "f1": 0.4820828440370393,
            "f1_weighted": 0.7161310278533793
          },
          {
            "accuracy": 0.6662106703146374,
            "f1": 0.4665828413569366,
            "f1_weighted": 0.7071025646291262
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.6631319910514542,
        "f1": 0.44985877530568563,
        "f1_weighted": 0.7060624108288062,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.6631319910514542,
        "scores_per_experiment": [
          {
            "accuracy": 0.6295302013422819,
            "f1": 0.41828367354974366,
            "f1_weighted": 0.6751828682987996
          },
          {
            "accuracy": 0.6572706935123043,
            "f1": 0.4327183321129346,
            "f1_weighted": 0.699971198473088
          },
          {
            "accuracy": 0.6662192393736018,
            "f1": 0.4558735828829335,
            "f1_weighted": 0.7083439213973534
          },
          {
            "accuracy": 0.6671140939597315,
            "f1": 0.45334993350136,
            "f1_weighted": 0.7114866956797533
          },
          {
            "accuracy": 0.6514541387024608,
            "f1": 0.44281785670872154,
            "f1_weighted": 0.6965641062380756
          },
          {
            "accuracy": 0.6756152125279642,
            "f1": 0.46584848289214487,
            "f1_weighted": 0.7194099604801207
          },
          {
            "accuracy": 0.6451901565995526,
            "f1": 0.44093815782284884,
            "f1_weighted": 0.6902652983651072
          },
          {
            "accuracy": 0.6903803131991052,
            "f1": 0.467242398478875,
            "f1_weighted": 0.7264975986802102
          },
          {
            "accuracy": 0.668903803131991,
            "f1": 0.449154730424896,
            "f1_weighted": 0.714668764102484
          },
          {
            "accuracy": 0.6796420581655481,
            "f1": 0.4723606046823986,
            "f1_weighted": 0.7182336965730693
          }
        ]
      }
    ]
  },
  "task_name": "MTOPIntentClassification"
}