{
  "dataset_revision": "fad2c6e8459f9e1c45d9315f4953d921437d70f8",
  "evaluation_time": 32.781277894973755,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.593207800941493,
        "f1": 0.5802587802238571,
        "f1_weighted": 0.5928392106168461,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.593207800941493,
        "scores_per_experiment": [
          {
            "accuracy": 0.6284465366509752,
            "f1": 0.6131434052634389,
            "f1_weighted": 0.6306909435412119
          },
          {
            "accuracy": 0.6153328850033625,
            "f1": 0.6012177927299004,
            "f1_weighted": 0.6176334061128184
          },
          {
            "accuracy": 0.5857431069266981,
            "f1": 0.5766012827374914,
            "f1_weighted": 0.5826240195733207
          },
          {
            "accuracy": 0.5679219905850706,
            "f1": 0.5610064682512422,
            "f1_weighted": 0.5695796540013838
          },
          {
            "accuracy": 0.5901143241425689,
            "f1": 0.5704238272027999,
            "f1_weighted": 0.5878467956358052
          },
          {
            "accuracy": 0.5833893745796906,
            "f1": 0.560915189845942,
            "f1_weighted": 0.5841992470145595
          },
          {
            "accuracy": 0.5864156018829859,
            "f1": 0.5770005018592096,
            "f1_weighted": 0.5883339578321404
          },
          {
            "accuracy": 0.6163416274377942,
            "f1": 0.5937963026845443,
            "f1_weighted": 0.6124921849819669
          },
          {
            "accuracy": 0.5813718897108272,
            "f1": 0.5773960340107376,
            "f1_weighted": 0.5781367252505273
          },
          {
            "accuracy": 0.5770006724949562,
            "f1": 0.5710869976532655,
            "f1_weighted": 0.5768551722247265
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.5821938022626659,
        "f1": 0.5767960094845834,
        "f1_weighted": 0.5805747937632836,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5821938022626659,
        "scores_per_experiment": [
          {
            "accuracy": 0.6089522872602066,
            "f1": 0.5992252823563586,
            "f1_weighted": 0.6104831855955767
          },
          {
            "accuracy": 0.5912444663059518,
            "f1": 0.5957119547770922,
            "f1_weighted": 0.5911078417177132
          },
          {
            "accuracy": 0.5745204131824889,
            "f1": 0.5646407916174581,
            "f1_weighted": 0.5739623566128076
          },
          {
            "accuracy": 0.5509099852434826,
            "f1": 0.5453737997290536,
            "f1_weighted": 0.5502104024817099
          },
          {
            "accuracy": 0.5956714215445155,
            "f1": 0.5909957082549782,
            "f1_weighted": 0.5908298892290934
          },
          {
            "accuracy": 0.5809149040826365,
            "f1": 0.5747758536016021,
            "f1_weighted": 0.5762672211472569
          },
          {
            "accuracy": 0.5710772257747172,
            "f1": 0.573686519566528,
            "f1_weighted": 0.573596499295449
          },
          {
            "accuracy": 0.6000983767830792,
            "f1": 0.5792624966192621,
            "f1_weighted": 0.5928417415719884
          },
          {
            "accuracy": 0.5843580914904083,
            "f1": 0.5900567142062042,
            "f1_weighted": 0.580765699808999
          },
          {
            "accuracy": 0.5641908509591737,
            "f1": 0.5542309741172964,
            "f1_weighted": 0.5656831001722413
          }
        ]
      }
    ]
  },
  "task_name": "MassiveScenarioClassification"
}