{
  "dataset_revision": "4672e20407010da34463acc759c162ca9734bca6",
  "evaluation_time": 45.69812989234924,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.5721587088096839,
        "f1": 0.5481283516391396,
        "f1_weighted": 0.575204717220675,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5721587088096839,
        "scores_per_experiment": [
          {
            "accuracy": 0.5766644250168124,
            "f1": 0.5536364480086128,
            "f1_weighted": 0.5764088113756733
          },
          {
            "accuracy": 0.5793544048419637,
            "f1": 0.5546140036059948,
            "f1_weighted": 0.5848087479507682
          },
          {
            "accuracy": 0.5766644250168124,
            "f1": 0.53977795977666,
            "f1_weighted": 0.5747829333065345
          },
          {
            "accuracy": 0.5911230665770006,
            "f1": 0.5551202282149839,
            "f1_weighted": 0.5971862189556576
          },
          {
            "accuracy": 0.5800268997982515,
            "f1": 0.5475307993805238,
            "f1_weighted": 0.5787526659240523
          },
          {
            "accuracy": 0.5487558843308675,
            "f1": 0.5310142953677102,
            "f1_weighted": 0.5553165476914359
          },
          {
            "accuracy": 0.5591795561533288,
            "f1": 0.5500309858707965,
            "f1_weighted": 0.5617739408212432
          },
          {
            "accuracy": 0.5585070611970411,
            "f1": 0.5386481368608343,
            "f1_weighted": 0.5642527547172678
          },
          {
            "accuracy": 0.5642232683254875,
            "f1": 0.5466608238907767,
            "f1_weighted": 0.5663096596865715
          },
          {
            "accuracy": 0.5870880968392737,
            "f1": 0.5642498354145036,
            "f1_weighted": 0.5924548917775463
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.5840629611411707,
        "f1": 0.5634943841407265,
        "f1_weighted": 0.5856508913859894,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5840629611411707,
        "scores_per_experiment": [
          {
            "accuracy": 0.5700934579439252,
            "f1": 0.5553337561158502,
            "f1_weighted": 0.5661333213833537
          },
          {
            "accuracy": 0.6000983767830792,
            "f1": 0.5763217072704835,
            "f1_weighted": 0.6029527955145463
          },
          {
            "accuracy": 0.6064928676832267,
            "f1": 0.5803618952413518,
            "f1_weighted": 0.606157314683728
          },
          {
            "accuracy": 0.5902606984751598,
            "f1": 0.5600661276070515,
            "f1_weighted": 0.5894012805577726
          },
          {
            "accuracy": 0.602557796360059,
            "f1": 0.576221105457832,
            "f1_weighted": 0.6022293713995927
          },
          {
            "accuracy": 0.5696015740285293,
            "f1": 0.5571070722138035,
            "f1_weighted": 0.575847802003951
          },
          {
            "accuracy": 0.5538612887358584,
            "f1": 0.5487612962409888,
            "f1_weighted": 0.5518814502051658
          },
          {
            "accuracy": 0.5676340383669454,
            "f1": 0.5407215463141292,
            "f1_weighted": 0.5708438275620883
          },
          {
            "accuracy": 0.5789473684210527,
            "f1": 0.5623563433621276,
            "f1_weighted": 0.5868245707969281
          },
          {
            "accuracy": 0.6010821446138711,
            "f1": 0.5776929915836464,
            "f1_weighted": 0.6042371797527679
          }
        ]
      }
    ]
  },
  "task_name": "MassiveIntentClassification"
}