{
  "dataset_revision": "ae001d0e6b1228650b7bd1c2c65fb50ad11a8aba",
  "evaluation_time": 79.91293835639954,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.660282717738258,
        "f1": 0.4723094862784486,
        "f1_weighted": 0.7021294816718247,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.660282717738258,
        "scores_per_experiment": [
          {
            "accuracy": 0.6422708618331053,
            "f1": 0.46518834563763306,
            "f1_weighted": 0.6852731875565221
          },
          {
            "accuracy": 0.6536707706338349,
            "f1": 0.4544107901713434,
            "f1_weighted": 0.6940712713780521
          },
          {
            "accuracy": 0.6486548107615139,
            "f1": 0.46759607597854447,
            "f1_weighted": 0.6911661564113886
          },
          {
            "accuracy": 0.6753305973552212,
            "f1": 0.5003260804429188,
            "f1_weighted": 0.7175335954079652
          },
          {
            "accuracy": 0.6468308253533972,
            "f1": 0.4685078690482839,
            "f1_weighted": 0.6886666316820823
          },
          {
            "accuracy": 0.6504787961696307,
            "f1": 0.4633701290561472,
            "f1_weighted": 0.6957549928958845
          },
          {
            "accuracy": 0.6459188326493388,
            "f1": 0.46321268228585594,
            "f1_weighted": 0.6907137706968497
          },
          {
            "accuracy": 0.6969904240766074,
            "f1": 0.5089770498398024,
            "f1_weighted": 0.7331876253131989
          },
          {
            "accuracy": 0.6794345645234838,
            "f1": 0.466556930489446,
            "f1_weighted": 0.7214426267676235
          },
          {
            "accuracy": 0.6632466940264478,
            "f1": 0.4649489098345107,
            "f1_weighted": 0.7034849586086801
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.6643847874720358,
        "f1": 0.46087934138758974,
        "f1_weighted": 0.7067081650964886,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.6643847874720358,
        "scores_per_experiment": [
          {
            "accuracy": 0.6335570469798658,
            "f1": 0.4384209957921901,
            "f1_weighted": 0.6789578057700412
          },
          {
            "accuracy": 0.6527964205816554,
            "f1": 0.4498237522788934,
            "f1_weighted": 0.695142719127011
          },
          {
            "accuracy": 0.6604026845637584,
            "f1": 0.45771859340098753,
            "f1_weighted": 0.7029358804028143
          },
          {
            "accuracy": 0.668903803131991,
            "f1": 0.4765332269853603,
            "f1_weighted": 0.7122552145816923
          },
          {
            "accuracy": 0.6577181208053692,
            "f1": 0.4617507766330827,
            "f1_weighted": 0.6994625842740364
          },
          {
            "accuracy": 0.6621923937360179,
            "f1": 0.4617496106012316,
            "f1_weighted": 0.7031354374211761
          },
          {
            "accuracy": 0.643847874720358,
            "f1": 0.45071779862932687,
            "f1_weighted": 0.6884985166487352
          },
          {
            "accuracy": 0.7033557046979866,
            "f1": 0.4793194863406259,
            "f1_weighted": 0.7415209192188679
          },
          {
            "accuracy": 0.676510067114094,
            "f1": 0.4638831230745385,
            "f1_weighted": 0.7218163781310426
          },
          {
            "accuracy": 0.6845637583892618,
            "f1": 0.46887605013966055,
            "f1_weighted": 0.723356195389468
          }
        ]
      }
    ]
  },
  "task_name": "MTOPIntentClassification"
}