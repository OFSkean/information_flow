{
  "dataset_revision": "1399c76144fd37290681b995c656ef9b2e06e26d",
  "evaluation_time": 577.5663795471191,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.30379999999999996,
        "f1": 0.3023067458799523,
        "f1_weighted": 0.3023067458799523,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.30379999999999996,
        "scores_per_experiment": [
          {
            "accuracy": 0.346,
            "f1": 0.3395921724285874,
            "f1_weighted": 0.3395921724285874
          },
          {
            "accuracy": 0.3248,
            "f1": 0.32686966621791314,
            "f1_weighted": 0.3268696662179132
          },
          {
            "accuracy": 0.2748,
            "f1": 0.2748809955890173,
            "f1_weighted": 0.2748809955890173
          },
          {
            "accuracy": 0.2974,
            "f1": 0.2998919512619632,
            "f1_weighted": 0.2998919512619632
          },
          {
            "accuracy": 0.3296,
            "f1": 0.32447445391568863,
            "f1_weighted": 0.3244744539156886
          },
          {
            "accuracy": 0.2846,
            "f1": 0.28310770135796914,
            "f1_weighted": 0.2831077013579692
          },
          {
            "accuracy": 0.2554,
            "f1": 0.25604179952602735,
            "f1_weighted": 0.2560417995260273
          },
          {
            "accuracy": 0.307,
            "f1": 0.30419857561539787,
            "f1_weighted": 0.30419857561539787
          },
          {
            "accuracy": 0.3112,
            "f1": 0.3039694688145039,
            "f1_weighted": 0.3039694688145039
          },
          {
            "accuracy": 0.3072,
            "f1": 0.3100406740724544,
            "f1_weighted": 0.3100406740724544
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.30052,
        "f1": 0.29744121914562993,
        "f1_weighted": 0.29744121914562993,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.30052,
        "scores_per_experiment": [
          {
            "accuracy": 0.3322,
            "f1": 0.32893858545901283,
            "f1_weighted": 0.3289385854590128
          },
          {
            "accuracy": 0.3174,
            "f1": 0.32138653510695675,
            "f1_weighted": 0.32138653510695675
          },
          {
            "accuracy": 0.2744,
            "f1": 0.2735529064074575,
            "f1_weighted": 0.2735529064074575
          },
          {
            "accuracy": 0.2796,
            "f1": 0.2773984829740391,
            "f1_weighted": 0.2773984829740391
          },
          {
            "accuracy": 0.3298,
            "f1": 0.3225124049037069,
            "f1_weighted": 0.3225124049037069
          },
          {
            "accuracy": 0.295,
            "f1": 0.2935054767577064,
            "f1_weighted": 0.2935054767577064
          },
          {
            "accuracy": 0.2452,
            "f1": 0.24259410694963898,
            "f1_weighted": 0.24259410694963898
          },
          {
            "accuracy": 0.2914,
            "f1": 0.28725085021934826,
            "f1_weighted": 0.28725085021934826
          },
          {
            "accuracy": 0.3234,
            "f1": 0.3087630154229307,
            "f1_weighted": 0.3087630154229307
          },
          {
            "accuracy": 0.3168,
            "f1": 0.31850982725550214,
            "f1_weighted": 0.3185098272555021
          }
        ]
      }
    ]
  },
  "task_name": "AmazonReviewsClassification"
}