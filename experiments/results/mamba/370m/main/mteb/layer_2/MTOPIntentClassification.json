{
  "dataset_revision": "ae001d0e6b1228650b7bd1c2c65fb50ad11a8aba",
  "evaluation_time": 73.10184359550476,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.6626766985864113,
        "f1": 0.48028901724205914,
        "f1_weighted": 0.7027401918128715,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.6626766985864113,
        "scores_per_experiment": [
          {
            "accuracy": 0.6424988600091199,
            "f1": 0.4762730701273544,
            "f1_weighted": 0.6822786021830322
          },
          {
            "accuracy": 0.6595987232102143,
            "f1": 0.4662683940897823,
            "f1_weighted": 0.6994191242943653
          },
          {
            "accuracy": 0.647514819881441,
            "f1": 0.47367133157479685,
            "f1_weighted": 0.6897849028035079
          },
          {
            "accuracy": 0.6837665298677611,
            "f1": 0.5087640510634727,
            "f1_weighted": 0.7238630768980092
          },
          {
            "accuracy": 0.655266757865937,
            "f1": 0.47505532023004865,
            "f1_weighted": 0.6953003586425048
          },
          {
            "accuracy": 0.6484268125854993,
            "f1": 0.4635660725117665,
            "f1_weighted": 0.6896377362153561
          },
          {
            "accuracy": 0.6434108527131783,
            "f1": 0.4716477256579584,
            "f1_weighted": 0.6853873889375117
          },
          {
            "accuracy": 0.7029183766529867,
            "f1": 0.5097939494760738,
            "f1_weighted": 0.7363097009005748
          },
          {
            "accuracy": 0.6757865937072504,
            "f1": 0.4868162867818919,
            "f1_weighted": 0.7174282450406766
          },
          {
            "accuracy": 0.667578659370725,
            "f1": 0.4710339709074457,
            "f1_weighted": 0.7079927822131757
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.6624608501118568,
        "f1": 0.45112099974379866,
        "f1_weighted": 0.7037177847548904,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.6624608501118568,
        "scores_per_experiment": [
          {
            "accuracy": 0.629082774049217,
            "f1": 0.43247260443607843,
            "f1_weighted": 0.6712536128816556
          },
          {
            "accuracy": 0.6483221476510067,
            "f1": 0.442040242425822,
            "f1_weighted": 0.6897127951486889
          },
          {
            "accuracy": 0.6501118568232662,
            "f1": 0.4376950981843438,
            "f1_weighted": 0.6921207812379648
          },
          {
            "accuracy": 0.6671140939597315,
            "f1": 0.4604801231518626,
            "f1_weighted": 0.7094131430356952
          },
          {
            "accuracy": 0.6621923937360179,
            "f1": 0.46223588350206724,
            "f1_weighted": 0.706498461471113
          },
          {
            "accuracy": 0.6595078299776287,
            "f1": 0.44931907040324,
            "f1_weighted": 0.7004168895802814
          },
          {
            "accuracy": 0.6429530201342282,
            "f1": 0.44592631444743974,
            "f1_weighted": 0.6846370069596049
          },
          {
            "accuracy": 0.7082774049217002,
            "f1": 0.46990436937954194,
            "f1_weighted": 0.7438787311457941
          },
          {
            "accuracy": 0.6706935123042506,
            "f1": 0.4449610554596076,
            "f1_weighted": 0.715433804029555
          },
          {
            "accuracy": 0.6863534675615213,
            "f1": 0.46617523604798383,
            "f1_weighted": 0.7238126220585522
          }
        ]
      }
    ]
  },
  "task_name": "MTOPIntentClassification"
}