{
  "dataset_revision": "4672e20407010da34463acc759c162ca9734bca6",
  "evaluation_time": 42.49542045593262,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.57320107599193,
        "f1": 0.5500275945231797,
        "f1_weighted": 0.5761612208997242,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.57320107599193,
        "scores_per_experiment": [
          {
            "accuracy": 0.5759919300605245,
            "f1": 0.5556750744498876,
            "f1_weighted": 0.5769798649499668
          },
          {
            "accuracy": 0.5854068594485541,
            "f1": 0.5624129356929413,
            "f1_weighted": 0.5899216917086068
          },
          {
            "accuracy": 0.5746469401479489,
            "f1": 0.5361758474158822,
            "f1_weighted": 0.5707857198757913
          },
          {
            "accuracy": 0.5981842636180229,
            "f1": 0.5613451067488135,
            "f1_weighted": 0.6047923681788056
          },
          {
            "accuracy": 0.5823806321452589,
            "f1": 0.5518682103253975,
            "f1_weighted": 0.5815038753587791
          },
          {
            "accuracy": 0.5537995965030262,
            "f1": 0.5369307288170361,
            "f1_weighted": 0.558059185953424
          },
          {
            "accuracy": 0.5611970410221924,
            "f1": 0.5501843470502833,
            "f1_weighted": 0.5651120185248447
          },
          {
            "accuracy": 0.5571620712844654,
            "f1": 0.5404845589214047,
            "f1_weighted": 0.5633565668570759
          },
          {
            "accuracy": 0.5605245460659045,
            "f1": 0.5458006088670889,
            "f1_weighted": 0.5639876209486453
          },
          {
            "accuracy": 0.5827168796234028,
            "f1": 0.559398526943061,
            "f1_weighted": 0.5871132966413032
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.5825873093949828,
        "f1": 0.5633357749293249,
        "f1_weighted": 0.5842049440691751,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5825873093949828,
        "scores_per_experiment": [
          {
            "accuracy": 0.5700934579439252,
            "f1": 0.5550075535168442,
            "f1_weighted": 0.5687014902607324
          },
          {
            "accuracy": 0.5986227250368913,
            "f1": 0.5639316117789388,
            "f1_weighted": 0.6019559974751841
          },
          {
            "accuracy": 0.5946876537137236,
            "f1": 0.5699278265294766,
            "f1_weighted": 0.5938996712658169
          },
          {
            "accuracy": 0.5966551893753075,
            "f1": 0.5628113549218627,
            "f1_weighted": 0.5942940579612622
          },
          {
            "accuracy": 0.6000983767830792,
            "f1": 0.5809798488196629,
            "f1_weighted": 0.6004933579225582
          },
          {
            "accuracy": 0.5622233152975897,
            "f1": 0.5529449832920847,
            "f1_weighted": 0.5677014326260136
          },
          {
            "accuracy": 0.559272011805214,
            "f1": 0.5584200400496238,
            "f1_weighted": 0.5602166067871777
          },
          {
            "accuracy": 0.5691096901131333,
            "f1": 0.5435992467471348,
            "f1_weighted": 0.5701859047084639
          },
          {
            "accuracy": 0.573536645351697,
            "f1": 0.5662399459290385,
            "f1_weighted": 0.5797192720707525
          },
          {
            "accuracy": 0.6015740285292671,
            "f1": 0.5794953377085822,
            "f1_weighted": 0.6048816496137895
          }
        ]
      }
    ]
  },
  "task_name": "MassiveIntentClassification"
}