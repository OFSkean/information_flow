{
  "dataset_revision": "fad2c6e8459f9e1c45d9315f4953d921437d70f8",
  "evaluation_time": 36.61742043495178,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.6003698722259583,
        "f1": 0.5873792451271701,
        "f1_weighted": 0.5997100839094343,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.6003698722259583,
        "scores_per_experiment": [
          {
            "accuracy": 0.6385339609952926,
            "f1": 0.6229506749169822,
            "f1_weighted": 0.6422353707975449
          },
          {
            "accuracy": 0.6153328850033625,
            "f1": 0.6045906528162484,
            "f1_weighted": 0.6161838312683615
          },
          {
            "accuracy": 0.5887693342299932,
            "f1": 0.5796741118565911,
            "f1_weighted": 0.5865683251558329
          },
          {
            "accuracy": 0.5790181573638198,
            "f1": 0.5694158964134974,
            "f1_weighted": 0.5774800518129163
          },
          {
            "accuracy": 0.5948217888365838,
            "f1": 0.5748814781972394,
            "f1_weighted": 0.5923460851216805
          },
          {
            "accuracy": 0.5968392737054472,
            "f1": 0.5754890591714844,
            "f1_weighted": 0.5947147031823968
          },
          {
            "accuracy": 0.5961667787491594,
            "f1": 0.5854855973877942,
            "f1_weighted": 0.599317605170561
          },
          {
            "accuracy": 0.6200403496973773,
            "f1": 0.6006322760262934,
            "f1_weighted": 0.6156632239783248
          },
          {
            "accuracy": 0.5907868190988568,
            "f1": 0.5831873093207026,
            "f1_weighted": 0.5894376724552007
          },
          {
            "accuracy": 0.5833893745796906,
            "f1": 0.5774853951648669,
            "f1_weighted": 0.5831539701515229
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.5868666994589277,
        "f1": 0.581176847757592,
        "f1_weighted": 0.5854507244356479,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5868666994589277,
        "scores_per_experiment": [
          {
            "accuracy": 0.6123954746679784,
            "f1": 0.6054652147824494,
            "f1_weighted": 0.6177624202030031
          },
          {
            "accuracy": 0.5892769306443679,
            "f1": 0.5938873422998894,
            "f1_weighted": 0.5867927690328252
          },
          {
            "accuracy": 0.5745204131824889,
            "f1": 0.5648783334065505,
            "f1_weighted": 0.5744353523612098
          },
          {
            "accuracy": 0.5553369404820462,
            "f1": 0.5487920500479084,
            "f1_weighted": 0.5517159798922249
          },
          {
            "accuracy": 0.6060009837678307,
            "f1": 0.5964765734187792,
            "f1_weighted": 0.6014976628851018
          },
          {
            "accuracy": 0.5848499754058042,
            "f1": 0.5788856938663408,
            "f1_weighted": 0.5815570824856544
          },
          {
            "accuracy": 0.5745204131824889,
            "f1": 0.573891251500062,
            "f1_weighted": 0.5767620659923832
          },
          {
            "accuracy": 0.5986227250368913,
            "f1": 0.5800670325438815,
            "f1_weighted": 0.5909863971240429
          },
          {
            "accuracy": 0.5986227250368913,
            "f1": 0.6037954836548158,
            "f1_weighted": 0.596076448504126
          },
          {
            "accuracy": 0.5745204131824889,
            "f1": 0.5656295020552452,
            "f1_weighted": 0.5769210658759069
          }
        ]
      }
    ]
  },
  "task_name": "MassiveScenarioClassification"
}