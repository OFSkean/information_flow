{
  "dataset_revision": "ae001d0e6b1228650b7bd1c2c65fb50ad11a8aba",
  "evaluation_time": 88.04756212234497,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.6652986776105791,
        "f1": 0.4781977288025166,
        "f1_weighted": 0.7070235624811166,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.6652986776105791,
        "scores_per_experiment": [
          {
            "accuracy": 0.6532147742818057,
            "f1": 0.4784865802602298,
            "f1_weighted": 0.6972458067243332
          },
          {
            "accuracy": 0.6605107159142727,
            "f1": 0.4668042413030726,
            "f1_weighted": 0.7032955606639395
          },
          {
            "accuracy": 0.6497948016415869,
            "f1": 0.45577524923660984,
            "f1_weighted": 0.6933506017609934
          },
          {
            "accuracy": 0.6801185590515276,
            "f1": 0.5132387514670026,
            "f1_weighted": 0.7203682076882666
          },
          {
            "accuracy": 0.6534427724578203,
            "f1": 0.4705565269775678,
            "f1_weighted": 0.6942473092962596
          },
          {
            "accuracy": 0.6627906976744186,
            "f1": 0.46745526595566994,
            "f1_weighted": 0.705181704594539
          },
          {
            "accuracy": 0.6488828089375285,
            "f1": 0.48322871600458356,
            "f1_weighted": 0.6921801032875364
          },
          {
            "accuracy": 0.6951664386684907,
            "f1": 0.5023979477985573,
            "f1_weighted": 0.7323205284052542
          },
          {
            "accuracy": 0.6794345645234838,
            "f1": 0.4781923423696019,
            "f1_weighted": 0.7206495446823548
          },
          {
            "accuracy": 0.6696306429548564,
            "f1": 0.4658416666522707,
            "f1_weighted": 0.7113962577076892
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.6652796420581656,
        "f1": 0.4590283622762802,
        "f1_weighted": 0.7081358336861745,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.6652796420581656,
        "scores_per_experiment": [
          {
            "accuracy": 0.6313199105145414,
            "f1": 0.4294848978076371,
            "f1_weighted": 0.6771279957471676
          },
          {
            "accuracy": 0.6541387024608502,
            "f1": 0.4322990706223,
            "f1_weighted": 0.6988936071428191
          },
          {
            "accuracy": 0.6577181208053692,
            "f1": 0.43940611574743493,
            "f1_weighted": 0.7011190291478429
          },
          {
            "accuracy": 0.672930648769575,
            "f1": 0.47087769992314343,
            "f1_weighted": 0.7150686559965845
          },
          {
            "accuracy": 0.654586129753915,
            "f1": 0.4671564330175301,
            "f1_weighted": 0.6977006307211112
          },
          {
            "accuracy": 0.6774049217002237,
            "f1": 0.480503185199358,
            "f1_weighted": 0.7206241253432688
          },
          {
            "accuracy": 0.647427293064877,
            "f1": 0.44811565053280333,
            "f1_weighted": 0.6927063636526241
          },
          {
            "accuracy": 0.7002237136465325,
            "f1": 0.48214600271728686,
            "f1_weighted": 0.7364743904110098
          },
          {
            "accuracy": 0.672930648769575,
            "f1": 0.4485049402506877,
            "f1_weighted": 0.7194367455351399
          },
          {
            "accuracy": 0.6841163310961969,
            "f1": 0.49178962694462036,
            "f1_weighted": 0.7222067931641764
          }
        ]
      }
    ]
  },
  "task_name": "MTOPIntentClassification"
}