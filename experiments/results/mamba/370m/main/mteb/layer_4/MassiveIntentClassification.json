{
  "dataset_revision": "4672e20407010da34463acc759c162ca9734bca6",
  "evaluation_time": 51.30549335479736,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.5734028244788164,
        "f1": 0.5478091921519552,
        "f1_weighted": 0.5769431880717482,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5734028244788164,
        "scores_per_experiment": [
          {
            "accuracy": 0.5766644250168124,
            "f1": 0.5610996484402868,
            "f1_weighted": 0.5786561295928495
          },
          {
            "accuracy": 0.5796906523201076,
            "f1": 0.5528790951524981,
            "f1_weighted": 0.5843551278177479
          },
          {
            "accuracy": 0.5746469401479489,
            "f1": 0.5364972985924136,
            "f1_weighted": 0.5728969208387058
          },
          {
            "accuracy": 0.5941492938802959,
            "f1": 0.5528874782582118,
            "f1_weighted": 0.5992397489235143
          },
          {
            "accuracy": 0.5766644250168124,
            "f1": 0.5416104346439314,
            "f1_weighted": 0.5774497768367581
          },
          {
            "accuracy": 0.5494283792871554,
            "f1": 0.5355313163508179,
            "f1_weighted": 0.5550149805047713
          },
          {
            "accuracy": 0.5601882985877606,
            "f1": 0.5463767160201367,
            "f1_weighted": 0.5637699574948426
          },
          {
            "accuracy": 0.5655682582380632,
            "f1": 0.5421524219712774,
            "f1_weighted": 0.5736740395804957
          },
          {
            "accuracy": 0.5669132481506388,
            "f1": 0.5474326488377845,
            "f1_weighted": 0.5685239413192875
          },
          {
            "accuracy": 0.5901143241425689,
            "f1": 0.5616248632521944,
            "f1_weighted": 0.5958512578085097
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.586227250368913,
        "f1": 0.5653575762102648,
        "f1_weighted": 0.5890226493180852,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.586227250368913,
        "scores_per_experiment": [
          {
            "accuracy": 0.5700934579439252,
            "f1": 0.5612961816817232,
            "f1_weighted": 0.569155662687742
          },
          {
            "accuracy": 0.5996064928676832,
            "f1": 0.5740555425210413,
            "f1_weighted": 0.6020756607081312
          },
          {
            "accuracy": 0.6109198229217905,
            "f1": 0.5772618072181498,
            "f1_weighted": 0.612434875061301
          },
          {
            "accuracy": 0.5917363502213477,
            "f1": 0.5656857261905778,
            "f1_weighted": 0.589243451210008
          },
          {
            "accuracy": 0.6060009837678307,
            "f1": 0.5785233473453832,
            "f1_weighted": 0.609057311797684
          },
          {
            "accuracy": 0.5700934579439252,
            "f1": 0.5625202549771066,
            "f1_weighted": 0.5773325356068031
          },
          {
            "accuracy": 0.5597638957206099,
            "f1": 0.547716143319827,
            "f1_weighted": 0.5628434800882971
          },
          {
            "accuracy": 0.5705853418593212,
            "f1": 0.5406513932538232,
            "f1_weighted": 0.5746156211697041
          },
          {
            "accuracy": 0.5804230201672406,
            "f1": 0.5655978210171936,
            "f1_weighted": 0.5870351968127167
          },
          {
            "accuracy": 0.603049680275455,
            "f1": 0.5802675445778238,
            "f1_weighted": 0.6064326980384642
          }
        ]
      }
    ]
  },
  "task_name": "MassiveIntentClassification"
}