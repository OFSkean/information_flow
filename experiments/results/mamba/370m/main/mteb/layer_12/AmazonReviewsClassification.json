{
  "dataset_revision": "1399c76144fd37290681b995c656ef9b2e06e26d",
  "evaluation_time": 625.5100269317627,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.31257999999999997,
        "f1": 0.3123693788981573,
        "f1_weighted": 0.3123693788981573,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.31257999999999997,
        "scores_per_experiment": [
          {
            "accuracy": 0.356,
            "f1": 0.35087957487589144,
            "f1_weighted": 0.3508795748758915
          },
          {
            "accuracy": 0.3348,
            "f1": 0.33745159489156573,
            "f1_weighted": 0.3374515948915658
          },
          {
            "accuracy": 0.285,
            "f1": 0.2847338406092853,
            "f1_weighted": 0.28473384060928536
          },
          {
            "accuracy": 0.3022,
            "f1": 0.3058267019466504,
            "f1_weighted": 0.3058267019466504
          },
          {
            "accuracy": 0.3388,
            "f1": 0.3346012996645837,
            "f1_weighted": 0.3346012996645837
          },
          {
            "accuracy": 0.2906,
            "f1": 0.29189479546663333,
            "f1_weighted": 0.29189479546663333
          },
          {
            "accuracy": 0.2764,
            "f1": 0.2803090813603103,
            "f1_weighted": 0.2803090813603103
          },
          {
            "accuracy": 0.3086,
            "f1": 0.307845785021318,
            "f1_weighted": 0.30784578502131804
          },
          {
            "accuracy": 0.3188,
            "f1": 0.3117462452494844,
            "f1_weighted": 0.31174624524948447
          },
          {
            "accuracy": 0.3146,
            "f1": 0.31840486989585043,
            "f1_weighted": 0.31840486989585043
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.30826,
        "f1": 0.307126683636216,
        "f1_weighted": 0.307126683636216,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.30826,
        "scores_per_experiment": [
          {
            "accuracy": 0.3428,
            "f1": 0.33903730237467444,
            "f1_weighted": 0.33903730237467444
          },
          {
            "accuracy": 0.3216,
            "f1": 0.32664415914653155,
            "f1_weighted": 0.32664415914653155
          },
          {
            "accuracy": 0.28,
            "f1": 0.2786781415105757,
            "f1_weighted": 0.2786781415105757
          },
          {
            "accuracy": 0.29,
            "f1": 0.28996445772428914,
            "f1_weighted": 0.28996445772428914
          },
          {
            "accuracy": 0.3322,
            "f1": 0.3268658859131174,
            "f1_weighted": 0.3268658859131174
          },
          {
            "accuracy": 0.3096,
            "f1": 0.3101950577495565,
            "f1_weighted": 0.3101950577495565
          },
          {
            "accuracy": 0.2706,
            "f1": 0.2734825070712017,
            "f1_weighted": 0.2734825070712017
          },
          {
            "accuracy": 0.2912,
            "f1": 0.29049879100792186,
            "f1_weighted": 0.2904987910079218
          },
          {
            "accuracy": 0.3272,
            "f1": 0.31490609588300966,
            "f1_weighted": 0.31490609588300966
          },
          {
            "accuracy": 0.3174,
            "f1": 0.3209944379812818,
            "f1_weighted": 0.32099443798128185
          }
        ]
      }
    ]
  },
  "task_name": "AmazonReviewsClassification"
}