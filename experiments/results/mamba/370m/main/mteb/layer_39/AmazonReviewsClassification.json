{
  "dataset_revision": "1399c76144fd37290681b995c656ef9b2e06e26d",
  "evaluation_time": 135.99644422531128,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.20",
  "scores": {
    "test": [
      {
        "accuracy": 0.34886,
        "f1": 0.3477358737353526,
        "f1_weighted": 0.34773587373535253,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.34886,
        "scores_per_experiment": [
          {
            "accuracy": 0.3608,
            "f1": 0.3652118889073918,
            "f1_weighted": 0.36521188890739187
          },
          {
            "accuracy": 0.3558,
            "f1": 0.3583768051294489,
            "f1_weighted": 0.3583768051294489
          },
          {
            "accuracy": 0.3208,
            "f1": 0.3157792242490421,
            "f1_weighted": 0.3157792242490421
          },
          {
            "accuracy": 0.3356,
            "f1": 0.33857678728350776,
            "f1_weighted": 0.3385767872835077
          },
          {
            "accuracy": 0.3746,
            "f1": 0.36433484635178076,
            "f1_weighted": 0.36433484635178076
          },
          {
            "accuracy": 0.347,
            "f1": 0.3424531975195361,
            "f1_weighted": 0.3424531975195361
          },
          {
            "accuracy": 0.3366,
            "f1": 0.33321379916830207,
            "f1_weighted": 0.33321379916830207
          },
          {
            "accuracy": 0.3586,
            "f1": 0.35949063402352555,
            "f1_weighted": 0.3594906340235256
          },
          {
            "accuracy": 0.3468,
            "f1": 0.3497137843628305,
            "f1_weighted": 0.34971378436283057
          },
          {
            "accuracy": 0.352,
            "f1": 0.3502077703581599,
            "f1_weighted": 0.3502077703581599
          }
        ]
      }
    ]
  },
  "task_name": "AmazonReviewsClassification"
}