{
  "dataset_revision": "ae001d0e6b1228650b7bd1c2c65fb50ad11a8aba",
  "evaluation_time": 110.68639731407166,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.6629730962152303,
        "f1": 0.4729317684112515,
        "f1_weighted": 0.7043051619265108,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.6629730962152303,
        "scores_per_experiment": [
          {
            "accuracy": 0.6491108071135431,
            "f1": 0.4705805032408176,
            "f1_weighted": 0.6914854904683307
          },
          {
            "accuracy": 0.6591427268581851,
            "f1": 0.4715754011876382,
            "f1_weighted": 0.7024308705768696
          },
          {
            "accuracy": 0.6470588235294118,
            "f1": 0.47279554477136315,
            "f1_weighted": 0.6888679596873245
          },
          {
            "accuracy": 0.6814865481076151,
            "f1": 0.4853744348065357,
            "f1_weighted": 0.7201518495468513
          },
          {
            "accuracy": 0.6509347925216599,
            "f1": 0.4630049166292836,
            "f1_weighted": 0.6929965750106134
          },
          {
            "accuracy": 0.6671226630186958,
            "f1": 0.46611213273682417,
            "f1_weighted": 0.7088885057897342
          },
          {
            "accuracy": 0.6372549019607843,
            "f1": 0.47010343756805933,
            "f1_weighted": 0.6822912120010888
          },
          {
            "accuracy": 0.6919744642042863,
            "f1": 0.49994827772655176,
            "f1_weighted": 0.727976556911181
          },
          {
            "accuracy": 0.67875056999544,
            "f1": 0.47035111334124513,
            "f1_weighted": 0.7201644607418327
          },
          {
            "accuracy": 0.6668946648426812,
            "f1": 0.45947192210419635,
            "f1_weighted": 0.707798138531283
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.6655480984340045,
        "f1": 0.4496924491947202,
        "f1_weighted": 0.7077950621944885,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.6655480984340045,
        "scores_per_experiment": [
          {
            "accuracy": 0.6348993288590604,
            "f1": 0.4223093534148813,
            "f1_weighted": 0.6794874131172863
          },
          {
            "accuracy": 0.6532438478747203,
            "f1": 0.4377504534250663,
            "f1_weighted": 0.6965427080741876
          },
          {
            "accuracy": 0.6563758389261745,
            "f1": 0.42792505662791547,
            "f1_weighted": 0.698991559132358
          },
          {
            "accuracy": 0.6841163310961969,
            "f1": 0.4682880710758337,
            "f1_weighted": 0.7254689286663648
          },
          {
            "accuracy": 0.6608501118568233,
            "f1": 0.4404689583513537,
            "f1_weighted": 0.7029529954520507
          },
          {
            "accuracy": 0.6724832214765101,
            "f1": 0.4554991977911038,
            "f1_weighted": 0.7170673748143064
          },
          {
            "accuracy": 0.6380313199105145,
            "f1": 0.435741757774375,
            "f1_weighted": 0.6848954554181415
          },
          {
            "accuracy": 0.6899328859060403,
            "f1": 0.47994202279189685,
            "f1_weighted": 0.7271233712754552
          },
          {
            "accuracy": 0.6818791946308724,
            "f1": 0.46425310134745384,
            "f1_weighted": 0.7235813113288709
          },
          {
            "accuracy": 0.6836689038031319,
            "f1": 0.46474651934732136,
            "f1_weighted": 0.7218395046658631
          }
        ]
      }
    ]
  },
  "task_name": "MTOPIntentClassification"
}