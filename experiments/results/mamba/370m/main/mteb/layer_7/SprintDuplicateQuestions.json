{
  "dataset_revision": "d66bd1f72af766a5cc4b0ca5e00c162f89e8cc46",
  "evaluation_time": 45.46929574012756,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "cosine_accuracy": 0.9915346534653465,
        "cosine_accuracy_threshold": 0.9637120962142944,
        "cosine_ap": 0.37214069594426985,
        "cosine_f1": 0.3957115009746589,
        "cosine_f1_threshold": 0.9495553970336914,
        "cosine_precision": 0.38593155893536124,
        "cosine_recall": 0.406,
        "dot_accuracy": 0.9901188118811881,
        "dot_accuracy_threshold": 1098.938720703125,
        "dot_ap": 0.03047067627493548,
        "dot_f1": 0.06598407281001137,
        "dot_f1_threshold": 939.1296997070312,
        "dot_precision": 0.046104928457869634,
        "dot_recall": 0.116,
        "euclidean_accuracy": 0.9917326732673267,
        "euclidean_accuracy_threshold": 8.147180557250977,
        "euclidean_ap": 0.41043623083060965,
        "euclidean_f1": 0.4265098877605559,
        "euclidean_f1_threshold": 9.590812683105469,
        "euclidean_precision": 0.4580941446613088,
        "euclidean_recall": 0.399,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5319894506736883,
        "manhattan_accuracy": 0.9925643564356436,
        "manhattan_accuracy_threshold": 194.81430053710938,
        "manhattan_ap": 0.5319894506736883,
        "manhattan_f1": 0.5219373219373219,
        "manhattan_f1_threshold": 211.08135986328125,
        "manhattan_precision": 0.6066225165562914,
        "manhattan_recall": 0.458,
        "max_accuracy": 0.9925643564356436,
        "max_ap": 0.5319894506736883,
        "max_f1": 0.5219373219373219,
        "max_precision": 0.6066225165562914,
        "max_recall": 0.458,
        "similarity_accuracy": 0.9915346534653465,
        "similarity_accuracy_threshold": 0.9637120962142944,
        "similarity_ap": 0.37214069594426985,
        "similarity_f1": 0.3957115009746589,
        "similarity_f1_threshold": 0.9495553970336914,
        "similarity_precision": 0.38593155893536124,
        "similarity_recall": 0.406
      }
    ],
    "validation": [
      {
        "cosine_accuracy": 0.9909207920792079,
        "cosine_accuracy_threshold": 0.9652798771858215,
        "cosine_ap": 0.29177046286058117,
        "cosine_f1": 0.3441762854144806,
        "cosine_f1_threshold": 0.9501252174377441,
        "cosine_precision": 0.3620309050772627,
        "cosine_recall": 0.328,
        "dot_accuracy": 0.9901089108910891,
        "dot_accuracy_threshold": 1122.7271728515625,
        "dot_ap": 0.02896335513426831,
        "dot_f1": 0.07505518763796909,
        "dot_f1_threshold": 949.4256591796875,
        "dot_precision": 0.059371362048894066,
        "dot_recall": 0.102,
        "euclidean_accuracy": 0.9912079207920792,
        "euclidean_accuracy_threshold": 8.457634925842285,
        "euclidean_ap": 0.3371327130734852,
        "euclidean_f1": 0.38526315789473686,
        "euclidean_f1_threshold": 9.757770538330078,
        "euclidean_precision": 0.4066666666666667,
        "euclidean_recall": 0.366,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.4672349186835987,
        "manhattan_accuracy": 0.9921584158415842,
        "manhattan_accuracy_threshold": 202.539306640625,
        "manhattan_ap": 0.4672349186835987,
        "manhattan_f1": 0.48571428571428577,
        "manhattan_f1_threshold": 216.39732360839844,
        "manhattan_precision": 0.5390243902439025,
        "manhattan_recall": 0.442,
        "max_accuracy": 0.9921584158415842,
        "max_ap": 0.4672349186835987,
        "max_f1": 0.48571428571428577,
        "max_precision": 0.5390243902439025,
        "max_recall": 0.442,
        "similarity_accuracy": 0.9909207920792079,
        "similarity_accuracy_threshold": 0.9652798771858215,
        "similarity_ap": 0.29177046286058117,
        "similarity_f1": 0.3441762854144806,
        "similarity_f1_threshold": 0.9501252174377441,
        "similarity_precision": 0.3620309050772627,
        "similarity_recall": 0.328
      }
    ]
  },
  "task_name": "SprintDuplicateQuestions"
}