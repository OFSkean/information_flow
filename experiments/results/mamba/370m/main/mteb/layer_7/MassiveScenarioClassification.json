{
  "dataset_revision": "fad2c6e8459f9e1c45d9315f4953d921437d70f8",
  "evaluation_time": 46.803375482559204,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.5937457969065232,
        "f1": 0.5819657757434031,
        "f1_weighted": 0.5944055729845366,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5937457969065232,
        "scores_per_experiment": [
          {
            "accuracy": 0.6156691324815063,
            "f1": 0.6079094735192421,
            "f1_weighted": 0.6196766221478922
          },
          {
            "accuracy": 0.6149966375252186,
            "f1": 0.6063460601575975,
            "f1_weighted": 0.6157645719615268
          },
          {
            "accuracy": 0.5907868190988568,
            "f1": 0.5802994984743144,
            "f1_weighted": 0.5925316254352223
          },
          {
            "accuracy": 0.597511768661735,
            "f1": 0.5832883345917473,
            "f1_weighted": 0.5974397485997223
          },
          {
            "accuracy": 0.5907868190988568,
            "f1": 0.5785977829684091,
            "f1_weighted": 0.5914662726156605
          },
          {
            "accuracy": 0.5692669804976462,
            "f1": 0.5499765046840362,
            "f1_weighted": 0.5691917053225739
          },
          {
            "accuracy": 0.5877605917955615,
            "f1": 0.575656512457181,
            "f1_weighted": 0.5931636463457503
          },
          {
            "accuracy": 0.6146603900470746,
            "f1": 0.5928403983318234,
            "f1_weighted": 0.61096254441158
          },
          {
            "accuracy": 0.5749831876260928,
            "f1": 0.5699953204168309,
            "f1_weighted": 0.5742725299712951
          },
          {
            "accuracy": 0.5810356422326832,
            "f1": 0.5747478718328499,
            "f1_weighted": 0.5795864630341422
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.582193802262666,
        "f1": 0.5783242462375418,
        "f1_weighted": 0.5814746702014817,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.582193802262666,
        "scores_per_experiment": [
          {
            "accuracy": 0.5976389572060994,
            "f1": 0.5907073797339045,
            "f1_weighted": 0.6013732388955942
          },
          {
            "accuracy": 0.5863256271519921,
            "f1": 0.5939569960793091,
            "f1_weighted": 0.5817919278035373
          },
          {
            "accuracy": 0.5681259222823414,
            "f1": 0.5618993344066061,
            "f1_weighted": 0.5682233686322634
          },
          {
            "accuracy": 0.5681259222823414,
            "f1": 0.5641654010703357,
            "f1_weighted": 0.5659093769670706
          },
          {
            "accuracy": 0.5892769306443679,
            "f1": 0.5880533564161838,
            "f1_weighted": 0.5892113127111819
          },
          {
            "accuracy": 0.5676340383669454,
            "f1": 0.5623355828303512,
            "f1_weighted": 0.5627755618386262
          },
          {
            "accuracy": 0.5823905558288244,
            "f1": 0.5813574094226632,
            "f1_weighted": 0.5900747509563157
          },
          {
            "accuracy": 0.6094441711756026,
            "f1": 0.589562612925362,
            "f1_weighted": 0.6068096640577194
          },
          {
            "accuracy": 0.5833743236596163,
            "f1": 0.5875981286742903,
            "f1_weighted": 0.5799086660609827
          },
          {
            "accuracy": 0.5696015740285293,
            "f1": 0.5636062608164101,
            "f1_weighted": 0.5686688340915262
          }
        ]
      }
    ]
  },
  "task_name": "MassiveScenarioClassification"
}