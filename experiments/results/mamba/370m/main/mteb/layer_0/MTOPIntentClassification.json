{
  "dataset_revision": "ae001d0e6b1228650b7bd1c2c65fb50ad11a8aba",
  "evaluation_time": 59.05065989494324,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.6303465572275422,
        "f1": 0.4504538784186449,
        "f1_weighted": 0.6741675467998988,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.6303465572275422,
        "scores_per_experiment": [
          {
            "accuracy": 0.6192430460556315,
            "f1": 0.4558440758630989,
            "f1_weighted": 0.6601362575244708
          },
          {
            "accuracy": 0.6388508891928865,
            "f1": 0.4406279971066293,
            "f1_weighted": 0.6789479246343328
          },
          {
            "accuracy": 0.6151390788873689,
            "f1": 0.43620166285213113,
            "f1_weighted": 0.6628249045023847
          },
          {
            "accuracy": 0.6235750113999088,
            "f1": 0.4523230760784575,
            "f1_weighted": 0.6682384925914268
          },
          {
            "accuracy": 0.6324669402644779,
            "f1": 0.4535686720399758,
            "f1_weighted": 0.6771520114816748
          },
          {
            "accuracy": 0.6112631098951209,
            "f1": 0.448592383046262,
            "f1_weighted": 0.6565760278384414
          },
          {
            "accuracy": 0.6288189694482444,
            "f1": 0.44832385641103073,
            "f1_weighted": 0.6729522507038876
          },
          {
            "accuracy": 0.6646146830825354,
            "f1": 0.4731171829963708,
            "f1_weighted": 0.7041352121487877
          },
          {
            "accuracy": 0.6377108983128135,
            "f1": 0.44773657864678773,
            "f1_weighted": 0.683513241376694
          },
          {
            "accuracy": 0.6317829457364341,
            "f1": 0.4482032991457048,
            "f1_weighted": 0.6771991451968871
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.6307829977628636,
        "f1": 0.42913588067316155,
        "f1_weighted": 0.6751663928287865,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.6307829977628636,
        "scores_per_experiment": [
          {
            "accuracy": 0.6040268456375839,
            "f1": 0.417819160929559,
            "f1_weighted": 0.6471198612040419
          },
          {
            "accuracy": 0.6250559284116332,
            "f1": 0.40283387025541484,
            "f1_weighted": 0.6677943696217563
          },
          {
            "accuracy": 0.6205816554809843,
            "f1": 0.4147952422036033,
            "f1_weighted": 0.6655672885660835
          },
          {
            "accuracy": 0.6192393736017897,
            "f1": 0.44144701187413776,
            "f1_weighted": 0.6628420943903013
          },
          {
            "accuracy": 0.6429530201342282,
            "f1": 0.44059467191649726,
            "f1_weighted": 0.6896000320500175
          },
          {
            "accuracy": 0.6125279642058166,
            "f1": 0.40978708873251807,
            "f1_weighted": 0.6575144068505312
          },
          {
            "accuracy": 0.639821029082774,
            "f1": 0.43828784762653933,
            "f1_weighted": 0.6832635332785546
          },
          {
            "accuracy": 0.6639821029082774,
            "f1": 0.4623355069191611,
            "f1_weighted": 0.7064419871456852
          },
          {
            "accuracy": 0.6353467561521253,
            "f1": 0.42123744861146295,
            "f1_weighted": 0.6849254811911771
          },
          {
            "accuracy": 0.6442953020134228,
            "f1": 0.4422209576627219,
            "f1_weighted": 0.6865948739897163
          }
        ]
      }
    ]
  },
  "task_name": "MTOPIntentClassification"
}