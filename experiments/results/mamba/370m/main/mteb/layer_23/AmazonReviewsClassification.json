{
  "dataset_revision": "1399c76144fd37290681b995c656ef9b2e06e26d",
  "evaluation_time": 89.04470467567444,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.20",
  "scores": {
    "test": [
      {
        "accuracy": 0.34866,
        "f1": 0.3481453037791768,
        "f1_weighted": 0.3481453037791768,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.34866,
        "scores_per_experiment": [
          {
            "accuracy": 0.3696,
            "f1": 0.3708683514356102,
            "f1_weighted": 0.37086835143561014
          },
          {
            "accuracy": 0.3472,
            "f1": 0.3529106659270875,
            "f1_weighted": 0.3529106659270875
          },
          {
            "accuracy": 0.3302,
            "f1": 0.3249964265132882,
            "f1_weighted": 0.32499642651328825
          },
          {
            "accuracy": 0.346,
            "f1": 0.3483232859782202,
            "f1_weighted": 0.3483232859782202
          },
          {
            "accuracy": 0.378,
            "f1": 0.3694086819477055,
            "f1_weighted": 0.3694086819477055
          },
          {
            "accuracy": 0.3232,
            "f1": 0.32409696813077427,
            "f1_weighted": 0.32409696813077427
          },
          {
            "accuracy": 0.3364,
            "f1": 0.3386422458906564,
            "f1_weighted": 0.33864224589065633
          },
          {
            "accuracy": 0.348,
            "f1": 0.3451056078803479,
            "f1_weighted": 0.3451056078803479
          },
          {
            "accuracy": 0.365,
            "f1": 0.36393283804316867,
            "f1_weighted": 0.3639328380431686
          },
          {
            "accuracy": 0.343,
            "f1": 0.34316796604490934,
            "f1_weighted": 0.3431679660449093
          }
        ]
      }
    ]
  },
  "task_name": "AmazonReviewsClassification"
}