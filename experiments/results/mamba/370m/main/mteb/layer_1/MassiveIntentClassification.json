{
  "dataset_revision": "4672e20407010da34463acc759c162ca9734bca6",
  "evaluation_time": 39.52123236656189,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.5707464694014794,
        "f1": 0.5504414653308655,
        "f1_weighted": 0.5728791068546817,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5707464694014794,
        "scores_per_experiment": [
          {
            "accuracy": 0.5759919300605245,
            "f1": 0.558462433292921,
            "f1_weighted": 0.575941046955123
          },
          {
            "accuracy": 0.5823806321452589,
            "f1": 0.5589682862914336,
            "f1_weighted": 0.5870405583973511
          },
          {
            "accuracy": 0.5780094149293881,
            "f1": 0.5441656140704318,
            "f1_weighted": 0.5749142368296486
          },
          {
            "accuracy": 0.5928043039677202,
            "f1": 0.5629983179837323,
            "f1_weighted": 0.5964292520739726
          },
          {
            "accuracy": 0.5759919300605245,
            "f1": 0.5481919388328386,
            "f1_weighted": 0.5749341540059705
          },
          {
            "accuracy": 0.5487558843308675,
            "f1": 0.5384984208194726,
            "f1_weighted": 0.552306811304324
          },
          {
            "accuracy": 0.5608607935440484,
            "f1": 0.5521863290204804,
            "f1_weighted": 0.5644717821380155
          },
          {
            "accuracy": 0.5574983187626092,
            "f1": 0.5383559224703607,
            "f1_weighted": 0.5615769021692845
          },
          {
            "accuracy": 0.5585070611970411,
            "f1": 0.5473726398903935,
            "f1_weighted": 0.558946098089862
          },
          {
            "accuracy": 0.5766644250168124,
            "f1": 0.5552147506365902,
            "f1_weighted": 0.5822302265832643
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.5819970486965077,
        "f1": 0.5648376008727956,
        "f1_weighted": 0.5835559912344321,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5819970486965077,
        "scores_per_experiment": [
          {
            "accuracy": 0.5661583866207575,
            "f1": 0.5600968118350684,
            "f1_weighted": 0.5641775829255777
          },
          {
            "accuracy": 0.5951795376291196,
            "f1": 0.5556296072429919,
            "f1_weighted": 0.597518288418954
          },
          {
            "accuracy": 0.5971470732907034,
            "f1": 0.5717753174699344,
            "f1_weighted": 0.5963970597761313
          },
          {
            "accuracy": 0.5971470732907034,
            "f1": 0.5684628328562098,
            "f1_weighted": 0.5970920765355554
          },
          {
            "accuracy": 0.5966551893753075,
            "f1": 0.5771382121798785,
            "f1_weighted": 0.5960767309546786
          },
          {
            "accuracy": 0.5725528775209051,
            "f1": 0.5651583225490239,
            "f1_weighted": 0.575521566615962
          },
          {
            "accuracy": 0.5538612887358584,
            "f1": 0.5557651921500628,
            "f1_weighted": 0.5570578870518373
          },
          {
            "accuracy": 0.5671421544515495,
            "f1": 0.5502402182303059,
            "f1_weighted": 0.5687679332857731
          },
          {
            "accuracy": 0.5759960649286768,
            "f1": 0.5664140364165108,
            "f1_weighted": 0.5813459761029445
          },
          {
            "accuracy": 0.5981308411214953,
            "f1": 0.577695457797969,
            "f1_weighted": 0.6016048106769081
          }
        ]
      }
    ]
  },
  "task_name": "MassiveIntentClassification"
}