{
  "dataset_revision": "1399c76144fd37290681b995c656ef9b2e06e26d",
  "evaluation_time": 92.56169724464417,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.29694000000000004,
        "f1": 0.29344693314546155,
        "f1_weighted": 0.2934469331454615,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.29694000000000004,
        "scores_per_experiment": [
          {
            "accuracy": 0.3308,
            "f1": 0.3305724498317257,
            "f1_weighted": 0.3305724498317258
          },
          {
            "accuracy": 0.3076,
            "f1": 0.3086007174094341,
            "f1_weighted": 0.308600717409434
          },
          {
            "accuracy": 0.286,
            "f1": 0.28685228045434397,
            "f1_weighted": 0.28685228045434386
          },
          {
            "accuracy": 0.2998,
            "f1": 0.29837779424056365,
            "f1_weighted": 0.2983777942405637
          },
          {
            "accuracy": 0.339,
            "f1": 0.32647875315643415,
            "f1_weighted": 0.3264787531564342
          },
          {
            "accuracy": 0.2506,
            "f1": 0.24942849918066484,
            "f1_weighted": 0.24942849918066481
          },
          {
            "accuracy": 0.2466,
            "f1": 0.24278988910346913,
            "f1_weighted": 0.24278988910346916
          },
          {
            "accuracy": 0.3122,
            "f1": 0.30392894091696643,
            "f1_weighted": 0.3039289409169664
          },
          {
            "accuracy": 0.2944,
            "f1": 0.2884915245752965,
            "f1_weighted": 0.2884915245752964
          },
          {
            "accuracy": 0.3024,
            "f1": 0.29894848258571677,
            "f1_weighted": 0.2989484825857167
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.29162,
        "f1": 0.2876876971613552,
        "f1_weighted": 0.2876876971613552,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.29162,
        "scores_per_experiment": [
          {
            "accuracy": 0.321,
            "f1": 0.32021200280439843,
            "f1_weighted": 0.3202120028043985
          },
          {
            "accuracy": 0.2968,
            "f1": 0.30015282601622467,
            "f1_weighted": 0.3001528260162247
          },
          {
            "accuracy": 0.2796,
            "f1": 0.28085233482730165,
            "f1_weighted": 0.2808523348273017
          },
          {
            "accuracy": 0.284,
            "f1": 0.28208920576363583,
            "f1_weighted": 0.2820892057636358
          },
          {
            "accuracy": 0.3258,
            "f1": 0.31203656722128087,
            "f1_weighted": 0.31203656722128087
          },
          {
            "accuracy": 0.2554,
            "f1": 0.25424618747250394,
            "f1_weighted": 0.25424618747250394
          },
          {
            "accuracy": 0.2528,
            "f1": 0.24724389259224994,
            "f1_weighted": 0.24724389259225
          },
          {
            "accuracy": 0.294,
            "f1": 0.2854250989834262,
            "f1_weighted": 0.2854250989834262
          },
          {
            "accuracy": 0.2876,
            "f1": 0.28148743752752364,
            "f1_weighted": 0.28148743752752364
          },
          {
            "accuracy": 0.3192,
            "f1": 0.31313141840500647,
            "f1_weighted": 0.3131314184050064
          }
        ]
      }
    ]
  },
  "task_name": "AmazonReviewsClassification"
}