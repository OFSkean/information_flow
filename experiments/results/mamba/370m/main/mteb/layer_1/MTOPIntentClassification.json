{
  "dataset_revision": "ae001d0e6b1228650b7bd1c2c65fb50ad11a8aba",
  "evaluation_time": 66.91325831413269,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.6547651618787049,
        "f1": 0.4733189983552072,
        "f1_weighted": 0.6962697200465856,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.6547651618787049,
        "scores_per_experiment": [
          {
            "accuracy": 0.647514819881441,
            "f1": 0.4781765108079597,
            "f1_weighted": 0.6864775189027258
          },
          {
            "accuracy": 0.6479708162334701,
            "f1": 0.46376655267037564,
            "f1_weighted": 0.688724600453707
          },
          {
            "accuracy": 0.635202918376653,
            "f1": 0.45283760788435107,
            "f1_weighted": 0.6804216558262568
          },
          {
            "accuracy": 0.658686730506156,
            "f1": 0.4877526766436396,
            "f1_weighted": 0.7006394473926673
          },
          {
            "accuracy": 0.6486548107615139,
            "f1": 0.4814035495371929,
            "f1_weighted": 0.6890567609747678
          },
          {
            "accuracy": 0.6434108527131783,
            "f1": 0.4732851604243344,
            "f1_weighted": 0.6860948327722491
          },
          {
            "accuracy": 0.636342909256726,
            "f1": 0.46575449272338615,
            "f1_weighted": 0.6793223280996918
          },
          {
            "accuracy": 0.6958504331965344,
            "f1": 0.49252967234981854,
            "f1_weighted": 0.7322087376591105
          },
          {
            "accuracy": 0.666438668490652,
            "f1": 0.4627011887625423,
            "f1_weighted": 0.7105654423027066
          },
          {
            "accuracy": 0.667578659370725,
            "f1": 0.47498257174847125,
            "f1_weighted": 0.7091858760819734
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.6547203579418346,
        "f1": 0.4469857493136205,
        "f1_weighted": 0.6965213626258859,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.6547203579418346,
        "scores_per_experiment": [
          {
            "accuracy": 0.6286353467561522,
            "f1": 0.43354332695739534,
            "f1_weighted": 0.667356855823109
          },
          {
            "accuracy": 0.6447427293064877,
            "f1": 0.4417682601981331,
            "f1_weighted": 0.6878398481135121
          },
          {
            "accuracy": 0.643847874720358,
            "f1": 0.4269862236314222,
            "f1_weighted": 0.6872981661902061
          },
          {
            "accuracy": 0.6639821029082774,
            "f1": 0.45246545135654753,
            "f1_weighted": 0.705675340824931
          },
          {
            "accuracy": 0.6563758389261745,
            "f1": 0.4563155568683962,
            "f1_weighted": 0.6990312789112545
          },
          {
            "accuracy": 0.6442953020134228,
            "f1": 0.44042655844400935,
            "f1_weighted": 0.6843643053840276
          },
          {
            "accuracy": 0.6456375838926175,
            "f1": 0.4380693062522289,
            "f1_weighted": 0.6880329916907226
          },
          {
            "accuracy": 0.6894854586129754,
            "f1": 0.46372453527592444,
            "f1_weighted": 0.7280490952649967
          },
          {
            "accuracy": 0.654586129753915,
            "f1": 0.44329189790462414,
            "f1_weighted": 0.7035140425325451
          },
          {
            "accuracy": 0.6756152125279642,
            "f1": 0.4732663762475232,
            "f1_weighted": 0.7140517015235546
          }
        ]
      }
    ]
  },
  "task_name": "MTOPIntentClassification"
}