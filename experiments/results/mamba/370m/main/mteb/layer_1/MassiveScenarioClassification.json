{
  "dataset_revision": "fad2c6e8459f9e1c45d9315f4953d921437d70f8",
  "evaluation_time": 28.236867904663086,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.5940484196368527,
        "f1": 0.5806004232154145,
        "f1_weighted": 0.594942369316172,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5940484196368527,
        "scores_per_experiment": [
          {
            "accuracy": 0.6375252185608608,
            "f1": 0.6210933413543377,
            "f1_weighted": 0.6395126452623205
          },
          {
            "accuracy": 0.5985205110961668,
            "f1": 0.5839710008074938,
            "f1_weighted": 0.6010627072864093
          },
          {
            "accuracy": 0.5780094149293881,
            "f1": 0.5679384704877022,
            "f1_weighted": 0.5753418065582796
          },
          {
            "accuracy": 0.5813718897108272,
            "f1": 0.571724837691784,
            "f1_weighted": 0.584316717897731
          },
          {
            "accuracy": 0.5954942837928715,
            "f1": 0.5780920783940445,
            "f1_weighted": 0.5943289071248529
          },
          {
            "accuracy": 0.5854068594485541,
            "f1": 0.5639836483225296,
            "f1_weighted": 0.5905914069073783
          },
          {
            "accuracy": 0.5880968392737055,
            "f1": 0.5796011426815315,
            "f1_weighted": 0.5903111105233608
          },
          {
            "accuracy": 0.6183591123066577,
            "f1": 0.5991376333723798,
            "f1_weighted": 0.6150817038336256
          },
          {
            "accuracy": 0.5827168796234028,
            "f1": 0.5705142200225788,
            "f1_weighted": 0.582921913675663
          },
          {
            "accuracy": 0.5749831876260928,
            "f1": 0.5699478590197634,
            "f1_weighted": 0.5759547740920978
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.5860796851942942,
        "f1": 0.5816120063495613,
        "f1_weighted": 0.5840635398370849,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5860796851942942,
        "scores_per_experiment": [
          {
            "accuracy": 0.6276438760452533,
            "f1": 0.6156067854640999,
            "f1_weighted": 0.628945354989686
          },
          {
            "accuracy": 0.5873093949827841,
            "f1": 0.5891095127594153,
            "f1_weighted": 0.5862658863831055
          },
          {
            "accuracy": 0.5755041810132808,
            "f1": 0.570678881054998,
            "f1_weighted": 0.5726757699903876
          },
          {
            "accuracy": 0.5656665027053616,
            "f1": 0.559004714442774,
            "f1_weighted": 0.5619391285157656
          },
          {
            "accuracy": 0.5917363502213477,
            "f1": 0.5843804244147874,
            "f1_weighted": 0.5867120031668531
          },
          {
            "accuracy": 0.5779636005902608,
            "f1": 0.5768782999880294,
            "f1_weighted": 0.5752590100815197
          },
          {
            "accuracy": 0.5710772257747172,
            "f1": 0.5727371146071625,
            "f1_weighted": 0.5740834453327075
          },
          {
            "accuracy": 0.5941957697983276,
            "f1": 0.5775908237355648,
            "f1_weighted": 0.5863156892781429
          },
          {
            "accuracy": 0.5922282341367437,
            "f1": 0.5940707624877273,
            "f1_weighted": 0.589205791921746
          },
          {
            "accuracy": 0.5774717166748647,
            "f1": 0.5760627445410548,
            "f1_weighted": 0.5792333187109352
          }
        ]
      }
    ]
  },
  "task_name": "MassiveScenarioClassification"
}