{
  "dataset_revision": "fad2c6e8459f9e1c45d9315f4953d921437d70f8",
  "evaluation_time": 42.291417360305786,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.5925016812373907,
        "f1": 0.5796676640209666,
        "f1_weighted": 0.5929729320781996,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5925016812373907,
        "scores_per_experiment": [
          {
            "accuracy": 0.6223940820443846,
            "f1": 0.6144612383448305,
            "f1_weighted": 0.6270650975231863
          },
          {
            "accuracy": 0.6086079354404842,
            "f1": 0.5982847246341335,
            "f1_weighted": 0.609241836931401
          },
          {
            "accuracy": 0.5860793544048419,
            "f1": 0.5736582916130422,
            "f1_weighted": 0.5847752902627978
          },
          {
            "accuracy": 0.5860793544048419,
            "f1": 0.5703038052705914,
            "f1_weighted": 0.5877522640513546
          },
          {
            "accuracy": 0.5891055817081372,
            "f1": 0.5764586548769407,
            "f1_weighted": 0.5876769095114629
          },
          {
            "accuracy": 0.5699394754539341,
            "f1": 0.5492314597369439,
            "f1_weighted": 0.568941048423751
          },
          {
            "accuracy": 0.5891055817081372,
            "f1": 0.5787789635875262,
            "f1_weighted": 0.5956379106021062
          },
          {
            "accuracy": 0.609280430396772,
            "f1": 0.584838219306209,
            "f1_weighted": 0.6042278257184026
          },
          {
            "accuracy": 0.5874243443174176,
            "f1": 0.5785926416821229,
            "f1_weighted": 0.5877376120265386
          },
          {
            "accuracy": 0.5770006724949562,
            "f1": 0.5720686411573253,
            "f1_weighted": 0.5766735257309945
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.5769798327594688,
        "f1": 0.5726834020280109,
        "f1_weighted": 0.5765945428282707,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5769798327594688,
        "scores_per_experiment": [
          {
            "accuracy": 0.5946876537137236,
            "f1": 0.5878594710899514,
            "f1_weighted": 0.5991356439233932
          },
          {
            "accuracy": 0.5700934579439252,
            "f1": 0.5760687299629429,
            "f1_weighted": 0.5683323367113213
          },
          {
            "accuracy": 0.5681259222823414,
            "f1": 0.5587945079698384,
            "f1_weighted": 0.5665126735941604
          },
          {
            "accuracy": 0.5573044761436301,
            "f1": 0.552872664741391,
            "f1_weighted": 0.5555921363752248
          },
          {
            "accuracy": 0.58780127889818,
            "f1": 0.5855521961108161,
            "f1_weighted": 0.5859145448695849
          },
          {
            "accuracy": 0.5636989670437776,
            "f1": 0.5581619409847345,
            "f1_weighted": 0.5591995517980086
          },
          {
            "accuracy": 0.5696015740285293,
            "f1": 0.5677511541710992,
            "f1_weighted": 0.5782593545053261
          },
          {
            "accuracy": 0.602557796360059,
            "f1": 0.5847084410279807,
            "f1_weighted": 0.5987587966824475
          },
          {
            "accuracy": 0.5922282341367437,
            "f1": 0.5953959871645224,
            "f1_weighted": 0.588729737724979
          },
          {
            "accuracy": 0.5636989670437776,
            "f1": 0.5596689270568325,
            "f1_weighted": 0.5655106520982609
          }
        ]
      }
    ]
  },
  "task_name": "MassiveScenarioClassification"
}