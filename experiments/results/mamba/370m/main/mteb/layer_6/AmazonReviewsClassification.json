{
  "dataset_revision": "1399c76144fd37290681b995c656ef9b2e06e26d",
  "evaluation_time": 326.66381788253784,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.30476000000000003,
        "f1": 0.30277127102123735,
        "f1_weighted": 0.30277127102123735,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.30476000000000003,
        "scores_per_experiment": [
          {
            "accuracy": 0.346,
            "f1": 0.33946021180624764,
            "f1_weighted": 0.33946021180624764
          },
          {
            "accuracy": 0.3154,
            "f1": 0.3154373983688071,
            "f1_weighted": 0.315437398368807
          },
          {
            "accuracy": 0.28,
            "f1": 0.2830389028671684,
            "f1_weighted": 0.28303890286716843
          },
          {
            "accuracy": 0.2888,
            "f1": 0.2928420308855326,
            "f1_weighted": 0.2928420308855326
          },
          {
            "accuracy": 0.3396,
            "f1": 0.33088411219660524,
            "f1_weighted": 0.33088411219660524
          },
          {
            "accuracy": 0.2788,
            "f1": 0.27103260193273565,
            "f1_weighted": 0.27103260193273565
          },
          {
            "accuracy": 0.2466,
            "f1": 0.2475408345896623,
            "f1_weighted": 0.24754083458966228
          },
          {
            "accuracy": 0.3376,
            "f1": 0.3320879856126152,
            "f1_weighted": 0.3320879856126152
          },
          {
            "accuracy": 0.2908,
            "f1": 0.290618700364758,
            "f1_weighted": 0.290618700364758
          },
          {
            "accuracy": 0.324,
            "f1": 0.32476993158824147,
            "f1_weighted": 0.3247699315882415
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.2992,
        "f1": 0.294621281196368,
        "f1_weighted": 0.29462128119636805,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.2992,
        "scores_per_experiment": [
          {
            "accuracy": 0.3374,
            "f1": 0.3314058036043498,
            "f1_weighted": 0.33140580360434985
          },
          {
            "accuracy": 0.3178,
            "f1": 0.31157822454202727,
            "f1_weighted": 0.31157822454202727
          },
          {
            "accuracy": 0.2768,
            "f1": 0.27835179505654906,
            "f1_weighted": 0.27835179505654906
          },
          {
            "accuracy": 0.2742,
            "f1": 0.27525245414955096,
            "f1_weighted": 0.27525245414955096
          },
          {
            "accuracy": 0.3386,
            "f1": 0.32271557152247476,
            "f1_weighted": 0.32271557152247476
          },
          {
            "accuracy": 0.2732,
            "f1": 0.2618844453579387,
            "f1_weighted": 0.2618844453579387
          },
          {
            "accuracy": 0.2432,
            "f1": 0.23952771310077262,
            "f1_weighted": 0.2395277131007726
          },
          {
            "accuracy": 0.3134,
            "f1": 0.31040358749867714,
            "f1_weighted": 0.3104035874986772
          },
          {
            "accuracy": 0.2922,
            "f1": 0.2881272715360268,
            "f1_weighted": 0.2881272715360268
          },
          {
            "accuracy": 0.3252,
            "f1": 0.3269659455953132,
            "f1_weighted": 0.3269659455953132
          }
        ]
      }
    ]
  },
  "task_name": "AmazonReviewsClassification"
}