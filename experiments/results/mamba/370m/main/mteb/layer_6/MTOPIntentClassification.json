{
  "dataset_revision": "ae001d0e6b1228650b7bd1c2c65fb50ad11a8aba",
  "evaluation_time": 105.73302388191223,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.6607387140902873,
        "f1": 0.47968038346338665,
        "f1_weighted": 0.7017837240565319,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.6607387140902873,
        "scores_per_experiment": [
          {
            "accuracy": 0.6472868217054264,
            "f1": 0.4790910441839672,
            "f1_weighted": 0.6909478798201477
          },
          {
            "accuracy": 0.6550387596899225,
            "f1": 0.4701239561408796,
            "f1_weighted": 0.6989801599042493
          },
          {
            "accuracy": 0.6399908800729595,
            "f1": 0.4727762292977372,
            "f1_weighted": 0.6814763796487471
          },
          {
            "accuracy": 0.6773825809393524,
            "f1": 0.49428751589049397,
            "f1_weighted": 0.7168817403688861
          },
          {
            "accuracy": 0.6507067943456453,
            "f1": 0.4707964295155543,
            "f1_weighted": 0.6917839982060995
          },
          {
            "accuracy": 0.66484268125855,
            "f1": 0.4696625440845069,
            "f1_weighted": 0.7068452420232141
          },
          {
            "accuracy": 0.6377108983128135,
            "f1": 0.47561118278258624,
            "f1_weighted": 0.6820447269251251
          },
          {
            "accuracy": 0.6919744642042863,
            "f1": 0.501447164047984,
            "f1_weighted": 0.7266685909551291
          },
          {
            "accuracy": 0.6744186046511628,
            "f1": 0.4848970241695674,
            "f1_weighted": 0.7151492491101646
          },
          {
            "accuracy": 0.6680346557227542,
            "f1": 0.4781107445205891,
            "f1_weighted": 0.7070592736035559
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.6630872483221475,
        "f1": 0.45669104420183243,
        "f1_weighted": 0.7052463285880666,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.6630872483221475,
        "scores_per_experiment": [
          {
            "accuracy": 0.6295302013422819,
            "f1": 0.4169021809305341,
            "f1_weighted": 0.6748120389425553
          },
          {
            "accuracy": 0.6519015659955257,
            "f1": 0.4499533608670258,
            "f1_weighted": 0.6950823764994573
          },
          {
            "accuracy": 0.6586129753914989,
            "f1": 0.4572279485667428,
            "f1_weighted": 0.700269681073086
          },
          {
            "accuracy": 0.6782997762863535,
            "f1": 0.4793934430792723,
            "f1_weighted": 0.7206430085120028
          },
          {
            "accuracy": 0.6612975391498881,
            "f1": 0.4470190885126473,
            "f1_weighted": 0.704572094633866
          },
          {
            "accuracy": 0.676510067114094,
            "f1": 0.47105156184530167,
            "f1_weighted": 0.7191573583072137
          },
          {
            "accuracy": 0.6322147651006711,
            "f1": 0.44772594242689917,
            "f1_weighted": 0.6780467315384857
          },
          {
            "accuracy": 0.6903803131991052,
            "f1": 0.4798061280667515,
            "f1_weighted": 0.726013086985749
          },
          {
            "accuracy": 0.6666666666666666,
            "f1": 0.44636358733976284,
            "f1_weighted": 0.7105299941278013
          },
          {
            "accuracy": 0.6854586129753915,
            "f1": 0.4714672003833867,
            "f1_weighted": 0.7233369152604484
          }
        ]
      }
    ]
  },
  "task_name": "MTOPIntentClassification"
}