{
  "dataset_revision": "1399c76144fd37290681b995c656ef9b2e06e26d",
  "evaluation_time": 673.64546251297,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.31498000000000004,
        "f1": 0.31491311962500773,
        "f1_weighted": 0.31491311962500773,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.31498000000000004,
        "scores_per_experiment": [
          {
            "accuracy": 0.3496,
            "f1": 0.34444606432161606,
            "f1_weighted": 0.34444606432161606
          },
          {
            "accuracy": 0.329,
            "f1": 0.3340267929566488,
            "f1_weighted": 0.3340267929566488
          },
          {
            "accuracy": 0.2902,
            "f1": 0.2913576144678252,
            "f1_weighted": 0.2913576144678252
          },
          {
            "accuracy": 0.3056,
            "f1": 0.3102249924168986,
            "f1_weighted": 0.31022499241689866
          },
          {
            "accuracy": 0.3456,
            "f1": 0.3381562693748042,
            "f1_weighted": 0.3381562693748042
          },
          {
            "accuracy": 0.2932,
            "f1": 0.2960541165579745,
            "f1_weighted": 0.2960541165579745
          },
          {
            "accuracy": 0.2776,
            "f1": 0.2805597111358254,
            "f1_weighted": 0.28055971113582534
          },
          {
            "accuracy": 0.3164,
            "f1": 0.3148864366704863,
            "f1_weighted": 0.3148864366704864
          },
          {
            "accuracy": 0.3256,
            "f1": 0.319584290614209,
            "f1_weighted": 0.3195842906142091
          },
          {
            "accuracy": 0.317,
            "f1": 0.31983490773378886,
            "f1_weighted": 0.3198349077337888
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.30986,
        "f1": 0.3095327133966307,
        "f1_weighted": 0.3095327133966307,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.30986,
        "scores_per_experiment": [
          {
            "accuracy": 0.3356,
            "f1": 0.33370946118420197,
            "f1_weighted": 0.33370946118420197
          },
          {
            "accuracy": 0.3166,
            "f1": 0.32337344772746013,
            "f1_weighted": 0.32337344772746013
          },
          {
            "accuracy": 0.2922,
            "f1": 0.29095977148802976,
            "f1_weighted": 0.2909597714880298
          },
          {
            "accuracy": 0.2914,
            "f1": 0.29350629421066304,
            "f1_weighted": 0.29350629421066304
          },
          {
            "accuracy": 0.3432,
            "f1": 0.3349604546179986,
            "f1_weighted": 0.33496045461799867
          },
          {
            "accuracy": 0.3024,
            "f1": 0.3052330058418494,
            "f1_weighted": 0.3052330058418494
          },
          {
            "accuracy": 0.2746,
            "f1": 0.2765150175446605,
            "f1_weighted": 0.2765150175446605
          },
          {
            "accuracy": 0.2922,
            "f1": 0.29301625980406626,
            "f1_weighted": 0.2930162598040662
          },
          {
            "accuracy": 0.3298,
            "f1": 0.32016911565884293,
            "f1_weighted": 0.32016911565884293
          },
          {
            "accuracy": 0.3206,
            "f1": 0.3238843058885346,
            "f1_weighted": 0.32388430588853456
          }
        ]
      }
    ]
  },
  "task_name": "AmazonReviewsClassification"
}