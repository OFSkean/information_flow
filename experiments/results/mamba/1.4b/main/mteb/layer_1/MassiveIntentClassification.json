{
  "dataset_revision": "4672e20407010da34463acc759c162ca9734bca6",
  "evaluation_time": 43.40504336357117,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.57320107599193,
        "f1": 0.5519285550832322,
        "f1_weighted": 0.5742788760740682,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.57320107599193,
        "scores_per_experiment": [
          {
            "accuracy": 0.5739744451916611,
            "f1": 0.5597163311515327,
            "f1_weighted": 0.5739902516435772
          },
          {
            "accuracy": 0.5823806321452589,
            "f1": 0.5550572897207996,
            "f1_weighted": 0.5859424935219061
          },
          {
            "accuracy": 0.5830531271015468,
            "f1": 0.560479572090332,
            "f1_weighted": 0.5791893234324222
          },
          {
            "accuracy": 0.5921318090114324,
            "f1": 0.5620554644586432,
            "f1_weighted": 0.59425186515694
          },
          {
            "accuracy": 0.5773369199731002,
            "f1": 0.5522219344656732,
            "f1_weighted": 0.5724013569092404
          },
          {
            "accuracy": 0.5376597175521184,
            "f1": 0.5280513510173881,
            "f1_weighted": 0.540213508371182
          },
          {
            "accuracy": 0.5659045057162071,
            "f1": 0.5524815804466796,
            "f1_weighted": 0.5659127651999424
          },
          {
            "accuracy": 0.5679219905850706,
            "f1": 0.541082854137813,
            "f1_weighted": 0.5718050069501157
          },
          {
            "accuracy": 0.5655682582380632,
            "f1": 0.5460512153821147,
            "f1_weighted": 0.5674797086487365
          },
          {
            "accuracy": 0.5860793544048419,
            "f1": 0.5620879579613466,
            "f1_weighted": 0.5916024809066193
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.5827348745696016,
        "f1": 0.5654536131685688,
        "f1_weighted": 0.5835832121348086,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5827348745696016,
        "scores_per_experiment": [
          {
            "accuracy": 0.5725528775209051,
            "f1": 0.558985908485779,
            "f1_weighted": 0.5699801602665396
          },
          {
            "accuracy": 0.5961633054599115,
            "f1": 0.5634062506946488,
            "f1_weighted": 0.5981811380616792
          },
          {
            "accuracy": 0.5897688145597639,
            "f1": 0.5712484250111758,
            "f1_weighted": 0.5895239636078838
          },
          {
            "accuracy": 0.5922282341367437,
            "f1": 0.5665511153011707,
            "f1_weighted": 0.5913274551318032
          },
          {
            "accuracy": 0.602557796360059,
            "f1": 0.5830022244009262,
            "f1_weighted": 0.6022076707112611
          },
          {
            "accuracy": 0.559272011805214,
            "f1": 0.5514487377202499,
            "f1_weighted": 0.5626377456764619
          },
          {
            "accuracy": 0.5568125922282341,
            "f1": 0.5534911015583224,
            "f1_weighted": 0.5557715479027012
          },
          {
            "accuracy": 0.573044761436301,
            "f1": 0.5498221299024456,
            "f1_weighted": 0.5737542988778808
          },
          {
            "accuracy": 0.5784554845056566,
            "f1": 0.5725870217138692,
            "f1_weighted": 0.5811233485128657
          },
          {
            "accuracy": 0.6064928676832267,
            "f1": 0.5839932168970986,
            "f1_weighted": 0.6113247925990096
          }
        ]
      }
    ]
  },
  "task_name": "MassiveIntentClassification"
}