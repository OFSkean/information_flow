{
  "dataset_revision": "ae001d0e6b1228650b7bd1c2c65fb50ad11a8aba",
  "evaluation_time": 80.80435037612915,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.6630642954856362,
        "f1": 0.476896768151418,
        "f1_weighted": 0.7042025982877649,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.6630642954856362,
        "scores_per_experiment": [
          {
            "accuracy": 0.6582307341541268,
            "f1": 0.4862088519439096,
            "f1_weighted": 0.6942397112469193
          },
          {
            "accuracy": 0.6545827633378933,
            "f1": 0.4584881134119294,
            "f1_weighted": 0.6941162553089624
          },
          {
            "accuracy": 0.658686730506156,
            "f1": 0.47221233120924183,
            "f1_weighted": 0.702704620064952
          },
          {
            "accuracy": 0.6657546739626083,
            "f1": 0.47127108550151536,
            "f1_weighted": 0.7051280555014287
          },
          {
            "accuracy": 0.6641586867305062,
            "f1": 0.4805834748767896,
            "f1_weighted": 0.707760387397623
          },
          {
            "accuracy": 0.6434108527131783,
            "f1": 0.4693657493768131,
            "f1_weighted": 0.6877767172390207
          },
          {
            "accuracy": 0.6600547195622435,
            "f1": 0.4825887620863986,
            "f1_weighted": 0.7019899739431091
          },
          {
            "accuracy": 0.6919744642042863,
            "f1": 0.502388225176392,
            "f1_weighted": 0.7287332491778789
          },
          {
            "accuracy": 0.66484268125855,
            "f1": 0.4807538741919045,
            "f1_weighted": 0.7101681570311869
          },
          {
            "accuracy": 0.6689466484268126,
            "f1": 0.46510721373928515,
            "f1_weighted": 0.7094088559665682
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.6622818791946309,
        "f1": 0.4534673160763972,
        "f1_weighted": 0.7038493183371453,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.6622818791946309,
        "scores_per_experiment": [
          {
            "accuracy": 0.6527964205816554,
            "f1": 0.4443068205082931,
            "f1_weighted": 0.6897080447049667
          },
          {
            "accuracy": 0.6492170022371365,
            "f1": 0.4401777810258408,
            "f1_weighted": 0.6879829154374074
          },
          {
            "accuracy": 0.6559284116331097,
            "f1": 0.4483294630856109,
            "f1_weighted": 0.6980133174461655
          },
          {
            "accuracy": 0.6554809843400448,
            "f1": 0.4441761095961299,
            "f1_weighted": 0.6972893239486299
          },
          {
            "accuracy": 0.6774049217002237,
            "f1": 0.47852291866139957,
            "f1_weighted": 0.7215052588020673
          },
          {
            "accuracy": 0.6550335570469799,
            "f1": 0.4443543064558222,
            "f1_weighted": 0.6989247641512096
          },
          {
            "accuracy": 0.6519015659955257,
            "f1": 0.4625024871367758,
            "f1_weighted": 0.6964682329426188
          },
          {
            "accuracy": 0.687248322147651,
            "f1": 0.46778746951168615,
            "f1_weighted": 0.7249826573300612
          },
          {
            "accuracy": 0.6604026845637584,
            "f1": 0.45399903446259904,
            "f1_weighted": 0.7072648001325778
          },
          {
            "accuracy": 0.6774049217002237,
            "f1": 0.45051677031981474,
            "f1_weighted": 0.7163538684757488
          }
        ]
      }
    ]
  },
  "task_name": "MTOPIntentClassification"
}