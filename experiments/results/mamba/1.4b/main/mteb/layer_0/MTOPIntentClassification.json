{
  "dataset_revision": "ae001d0e6b1228650b7bd1c2c65fb50ad11a8aba",
  "evaluation_time": 60.59888553619385,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.6451208390332879,
        "f1": 0.4636335163892354,
        "f1_weighted": 0.6879405142599974,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.6451208390332879,
        "scores_per_experiment": [
          {
            "accuracy": 0.6418148654810761,
            "f1": 0.4604113866444822,
            "f1_weighted": 0.6805155130846533
          },
          {
            "accuracy": 0.6486548107615139,
            "f1": 0.4544116695145894,
            "f1_weighted": 0.68956601565501
          },
          {
            "accuracy": 0.6395348837209303,
            "f1": 0.45844096370894627,
            "f1_weighted": 0.6849446549609669
          },
          {
            "accuracy": 0.6450068399452804,
            "f1": 0.4630341211600936,
            "f1_weighted": 0.6855502827462293
          },
          {
            "accuracy": 0.6427268581851345,
            "f1": 0.46633438143847394,
            "f1_weighted": 0.6887408439577483
          },
          {
            "accuracy": 0.6185590515275878,
            "f1": 0.4541095480295688,
            "f1_weighted": 0.6639991941619053
          },
          {
            "accuracy": 0.6404468764249887,
            "f1": 0.47750443366585915,
            "f1_weighted": 0.6834270272090784
          },
          {
            "accuracy": 0.6764705882352942,
            "f1": 0.4884492492611782,
            "f1_weighted": 0.7168651522596456
          },
          {
            "accuracy": 0.6470588235294118,
            "f1": 0.4586897795028673,
            "f1_weighted": 0.6929591893758279
          },
          {
            "accuracy": 0.6509347925216599,
            "f1": 0.45494963096629565,
            "f1_weighted": 0.6928372691889082
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.645145413870246,
        "f1": 0.44209342956885733,
        "f1_weighted": 0.6882523487913456,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.645145413870246,
        "scores_per_experiment": [
          {
            "accuracy": 0.636241610738255,
            "f1": 0.4324046983415084,
            "f1_weighted": 0.674273688108427
          },
          {
            "accuracy": 0.640268456375839,
            "f1": 0.42983782114787666,
            "f1_weighted": 0.68323160752942
          },
          {
            "accuracy": 0.6335570469798658,
            "f1": 0.4282201018100583,
            "f1_weighted": 0.677059086309978
          },
          {
            "accuracy": 0.6331096196868009,
            "f1": 0.4404445021879684,
            "f1_weighted": 0.6762130610012138
          },
          {
            "accuracy": 0.6635346756152125,
            "f1": 0.4711788071990027,
            "f1_weighted": 0.7088387317562809
          },
          {
            "accuracy": 0.6344519015659955,
            "f1": 0.4431618187870913,
            "f1_weighted": 0.6768876116011155
          },
          {
            "accuracy": 0.6434004474272931,
            "f1": 0.44869926912878555,
            "f1_weighted": 0.6867231653937678
          },
          {
            "accuracy": 0.6671140939597315,
            "f1": 0.46360668568712443,
            "f1_weighted": 0.7102513333505048
          },
          {
            "accuracy": 0.6416107382550336,
            "f1": 0.4284610657885546,
            "f1_weighted": 0.6909235175217524
          },
          {
            "accuracy": 0.658165548098434,
            "f1": 0.4349195256106028,
            "f1_weighted": 0.6981216853409947
          }
        ]
      }
    ]
  },
  "task_name": "MTOPIntentClassification"
}