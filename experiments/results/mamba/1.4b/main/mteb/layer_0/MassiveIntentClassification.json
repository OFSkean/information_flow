{
  "dataset_revision": "4672e20407010da34463acc759c162ca9734bca6",
  "evaluation_time": 38.481754779815674,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.5606926698049766,
        "f1": 0.5422012774125735,
        "f1_weighted": 0.5632759970708902,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5606926698049766,
        "scores_per_experiment": [
          {
            "accuracy": 0.5558170813718897,
            "f1": 0.542172870397146,
            "f1_weighted": 0.5582866907606733
          },
          {
            "accuracy": 0.5733019502353732,
            "f1": 0.5491379540621167,
            "f1_weighted": 0.5787575818928725
          },
          {
            "accuracy": 0.5652320107599192,
            "f1": 0.5456974124591067,
            "f1_weighted": 0.5631524096676443
          },
          {
            "accuracy": 0.5790181573638198,
            "f1": 0.5541972559047129,
            "f1_weighted": 0.583015132959511
          },
          {
            "accuracy": 0.5679219905850706,
            "f1": 0.5475053698952255,
            "f1_weighted": 0.5657995754735603
          },
          {
            "accuracy": 0.5336247478143914,
            "f1": 0.5188648163699549,
            "f1_weighted": 0.5365122686759761
          },
          {
            "accuracy": 0.5517821116341628,
            "f1": 0.543311072565233,
            "f1_weighted": 0.5523403649364563
          },
          {
            "accuracy": 0.5460659045057162,
            "f1": 0.5286339630997943,
            "f1_weighted": 0.5509354935422883
          },
          {
            "accuracy": 0.5615332885003362,
            "f1": 0.5396144410078201,
            "f1_weighted": 0.5629632293036285
          },
          {
            "accuracy": 0.5726294552790854,
            "f1": 0.552877618364625,
            "f1_weighted": 0.5809972234962921
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.5674864731923266,
        "f1": 0.5514039039484888,
        "f1_weighted": 0.5693319855370106,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5674864731923266,
        "scores_per_experiment": [
          {
            "accuracy": 0.5597638957206099,
            "f1": 0.5540490533675833,
            "f1_weighted": 0.559103157166383
          },
          {
            "accuracy": 0.5769798327594687,
            "f1": 0.543120153819186,
            "f1_weighted": 0.5771415710497528
          },
          {
            "accuracy": 0.5863256271519921,
            "f1": 0.5637337043008086,
            "f1_weighted": 0.58568140605394
          },
          {
            "accuracy": 0.5789473684210527,
            "f1": 0.5490484964409134,
            "f1_weighted": 0.5810351457526589
          },
          {
            "accuracy": 0.5843580914904083,
            "f1": 0.5678711508534181,
            "f1_weighted": 0.588527362084918
          },
          {
            "accuracy": 0.5484505656665027,
            "f1": 0.5410439781531713,
            "f1_weighted": 0.5533143431757456
          },
          {
            "accuracy": 0.5395966551893753,
            "f1": 0.5409086097956414,
            "f1_weighted": 0.5405444859803054
          },
          {
            "accuracy": 0.5474667978357107,
            "f1": 0.5323988776110918,
            "f1_weighted": 0.5501602301778156
          },
          {
            "accuracy": 0.5720609936055091,
            "f1": 0.5610394514041401,
            "f1_weighted": 0.5746142091289287
          },
          {
            "accuracy": 0.5809149040826365,
            "f1": 0.5608255637389351,
            "f1_weighted": 0.583197944799658
          }
        ]
      }
    ]
  },
  "task_name": "MassiveIntentClassification"
}