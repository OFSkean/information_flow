{
  "dataset_revision": "fad2c6e8459f9e1c45d9315f4953d921437d70f8",
  "evaluation_time": 38.235732316970825,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.6061533288500337,
        "f1": 0.5904887228833587,
        "f1_weighted": 0.6076239709058864,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.6061533288500337,
        "scores_per_experiment": [
          {
            "accuracy": 0.6429051782111634,
            "f1": 0.629097754578512,
            "f1_weighted": 0.6469279661377635
          },
          {
            "accuracy": 0.6190316072629455,
            "f1": 0.5989342940579863,
            "f1_weighted": 0.62183888462068
          },
          {
            "accuracy": 0.589778076664425,
            "f1": 0.5809391632651114,
            "f1_weighted": 0.5903509554180362
          },
          {
            "accuracy": 0.5961667787491594,
            "f1": 0.5799547546936397,
            "f1_weighted": 0.5978482484318183
          },
          {
            "accuracy": 0.5931405514458642,
            "f1": 0.5739137708181848,
            "f1_weighted": 0.5926057231366697
          },
          {
            "accuracy": 0.597511768661735,
            "f1": 0.572688218273934,
            "f1_weighted": 0.6009404898602273
          },
          {
            "accuracy": 0.6059179556153329,
            "f1": 0.5923849261443166,
            "f1_weighted": 0.608366861712973
          },
          {
            "accuracy": 0.6294552790854069,
            "f1": 0.6068358147740616,
            "f1_weighted": 0.626449095462243
          },
          {
            "accuracy": 0.5958305312710155,
            "f1": 0.5903427765635518,
            "f1_weighted": 0.5980171998415161
          },
          {
            "accuracy": 0.5917955615332885,
            "f1": 0.5797957556642888,
            "f1_weighted": 0.5928942844369368
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.600245941957698,
        "f1": 0.5935844816556558,
        "f1_weighted": 0.5990865945625046,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.600245941957698,
        "scores_per_experiment": [
          {
            "accuracy": 0.6335464830300049,
            "f1": 0.6202686504907182,
            "f1_weighted": 0.638813803361114
          },
          {
            "accuracy": 0.6143630103295622,
            "f1": 0.6131717307404542,
            "f1_weighted": 0.6131265857753339
          },
          {
            "accuracy": 0.5809149040826365,
            "f1": 0.5759169980668435,
            "f1_weighted": 0.5816839910261019
          },
          {
            "accuracy": 0.5838662075750123,
            "f1": 0.5739516668441453,
            "f1_weighted": 0.5811366840976996
          },
          {
            "accuracy": 0.5971470732907034,
            "f1": 0.5890679880491908,
            "f1_weighted": 0.5926269347023727
          },
          {
            "accuracy": 0.5976389572060994,
            "f1": 0.5909063559221962,
            "f1_weighted": 0.5974194463747691
          },
          {
            "accuracy": 0.5873093949827841,
            "f1": 0.5839062521694072,
            "f1_weighted": 0.5889733770298065
          },
          {
            "accuracy": 0.616822429906542,
            "f1": 0.5960257958966001,
            "f1_weighted": 0.6085748504619726
          },
          {
            "accuracy": 0.5981308411214953,
            "f1": 0.6055513856767604,
            "f1_weighted": 0.5950404060128398
          },
          {
            "accuracy": 0.5927201180521396,
            "f1": 0.5870779927002429,
            "f1_weighted": 0.5934698667830359
          }
        ]
      }
    ]
  },
  "task_name": "MassiveScenarioClassification"
}