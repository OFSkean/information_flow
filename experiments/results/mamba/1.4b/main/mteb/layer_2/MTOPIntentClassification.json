{
  "dataset_revision": "ae001d0e6b1228650b7bd1c2c65fb50ad11a8aba",
  "evaluation_time": 99.86786317825317,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.6694938440492476,
        "f1": 0.4828643324936426,
        "f1_weighted": 0.7094085604821735,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.6694938440492476,
        "scores_per_experiment": [
          {
            "accuracy": 0.6716826265389877,
            "f1": 0.4857966227462233,
            "f1_weighted": 0.7076430270680982
          },
          {
            "accuracy": 0.6623347013223895,
            "f1": 0.4646260188527295,
            "f1_weighted": 0.7002478831585208
          },
          {
            "accuracy": 0.6568627450980392,
            "f1": 0.4677804132948476,
            "f1_weighted": 0.7012265518957628
          },
          {
            "accuracy": 0.6744186046511628,
            "f1": 0.4916269650016534,
            "f1_weighted": 0.7110390921117077
          },
          {
            "accuracy": 0.6689466484268126,
            "f1": 0.49810596399251844,
            "f1_weighted": 0.7121083703819326
          },
          {
            "accuracy": 0.6518467852257182,
            "f1": 0.484178587810316,
            "f1_weighted": 0.6952654612931167
          },
          {
            "accuracy": 0.6580027359781122,
            "f1": 0.47205802568218086,
            "f1_weighted": 0.7001837404139878
          },
          {
            "accuracy": 0.7029183766529867,
            "f1": 0.5124867146108436,
            "f1_weighted": 0.7382966644114293
          },
          {
            "accuracy": 0.6721386228910169,
            "f1": 0.4752694244433905,
            "f1_weighted": 0.7145757742075244
          },
          {
            "accuracy": 0.6757865937072504,
            "f1": 0.4767145885017221,
            "f1_weighted": 0.7134990398796538
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.6718568232662192,
        "f1": 0.4688897092269707,
        "f1_weighted": 0.7127449296723082,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.6718568232662192,
        "scores_per_experiment": [
          {
            "accuracy": 0.6662192393736018,
            "f1": 0.4506996868831354,
            "f1_weighted": 0.7034652943131721
          },
          {
            "accuracy": 0.6612975391498881,
            "f1": 0.46222903325365183,
            "f1_weighted": 0.7001713008727802
          },
          {
            "accuracy": 0.6590604026845638,
            "f1": 0.45383407290344985,
            "f1_weighted": 0.7014187601855235
          },
          {
            "accuracy": 0.6657718120805369,
            "f1": 0.46076955022935195,
            "f1_weighted": 0.7049357048459783
          },
          {
            "accuracy": 0.6782997762863535,
            "f1": 0.49096549071391604,
            "f1_weighted": 0.7235129256518688
          },
          {
            "accuracy": 0.6671140939597315,
            "f1": 0.46470974363467304,
            "f1_weighted": 0.7103822877211691
          },
          {
            "accuracy": 0.658165548098434,
            "f1": 0.4667591248761475,
            "f1_weighted": 0.7000173335096842
          },
          {
            "accuracy": 0.702013422818792,
            "f1": 0.48460723870493383,
            "f1_weighted": 0.7403200440619985
          },
          {
            "accuracy": 0.6671140939597315,
            "f1": 0.4740602197546623,
            "f1_weighted": 0.7129565446250763
          },
          {
            "accuracy": 0.6935123042505593,
            "f1": 0.48026293131578485,
            "f1_weighted": 0.7302691009358302
          }
        ]
      }
    ]
  },
  "task_name": "MTOPIntentClassification"
}