{
  "dataset_revision": "ae001d0e6b1228650b7bd1c2c65fb50ad11a8aba",
  "evaluation_time": 118.54978632926941,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.6764021887824897,
        "f1": 0.4845040219609871,
        "f1_weighted": 0.7158304750731499,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.6764021887824897,
        "scores_per_experiment": [
          {
            "accuracy": 0.6751025991792066,
            "f1": 0.4830311978865014,
            "f1_weighted": 0.7115212286586639
          },
          {
            "accuracy": 0.6712266301869585,
            "f1": 0.46483542651735615,
            "f1_weighted": 0.7093960805098668
          },
          {
            "accuracy": 0.6605107159142727,
            "f1": 0.4764487911840593,
            "f1_weighted": 0.7044058641651826
          },
          {
            "accuracy": 0.6842225262197903,
            "f1": 0.48703613834538734,
            "f1_weighted": 0.720343420164848
          },
          {
            "accuracy": 0.6741906064751482,
            "f1": 0.49403806417638163,
            "f1_weighted": 0.7179177314420463
          },
          {
            "accuracy": 0.6589147286821705,
            "f1": 0.4787954775184981,
            "f1_weighted": 0.700595888629513
          },
          {
            "accuracy": 0.6668946648426812,
            "f1": 0.48996286195251837,
            "f1_weighted": 0.7082332036807527
          },
          {
            "accuracy": 0.7074783401732786,
            "f1": 0.5249304920656309,
            "f1_weighted": 0.7426334814771686
          },
          {
            "accuracy": 0.6828545371637027,
            "f1": 0.469804780434205,
            "f1_weighted": 0.7230444048823289
          },
          {
            "accuracy": 0.6826265389876881,
            "f1": 0.47615698952933294,
            "f1_weighted": 0.7202134471211288
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.6784340044742729,
        "f1": 0.4787862045158259,
        "f1_weighted": 0.7182756659472832,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.6784340044742729,
        "scores_per_experiment": [
          {
            "accuracy": 0.6751677852348993,
            "f1": 0.4533160046626124,
            "f1_weighted": 0.7109563176376875
          },
          {
            "accuracy": 0.6626398210290828,
            "f1": 0.46289569675229736,
            "f1_weighted": 0.7010781372671666
          },
          {
            "accuracy": 0.6662192393736018,
            "f1": 0.47229846586338015,
            "f1_weighted": 0.7073658788638398
          },
          {
            "accuracy": 0.6832214765100671,
            "f1": 0.49748010781824886,
            "f1_weighted": 0.7216760561033427
          },
          {
            "accuracy": 0.6894854586129754,
            "f1": 0.5091683410074425,
            "f1_weighted": 0.7332254519376432
          },
          {
            "accuracy": 0.6697986577181209,
            "f1": 0.47671882686057127,
            "f1_weighted": 0.7127525206207694
          },
          {
            "accuracy": 0.6621923937360179,
            "f1": 0.46765705410927416,
            "f1_weighted": 0.702062355962832
          },
          {
            "accuracy": 0.702013422818792,
            "f1": 0.4885392581816524,
            "f1_weighted": 0.739054543283722
          },
          {
            "accuracy": 0.6715883668903803,
            "f1": 0.472787036936467,
            "f1_weighted": 0.7161623843556453
          },
          {
            "accuracy": 0.702013422818792,
            "f1": 0.4870012529663131,
            "f1_weighted": 0.7384230134401838
          }
        ]
      }
    ]
  },
  "task_name": "MTOPIntentClassification"
}