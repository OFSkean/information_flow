{
  "dataset_revision": "4672e20407010da34463acc759c162ca9734bca6",
  "evaluation_time": 64.93245816230774,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.581439139206456,
        "f1": 0.5585272890672683,
        "f1_weighted": 0.5823063213606121,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.581439139206456,
        "scores_per_experiment": [
          {
            "accuracy": 0.5850706119704102,
            "f1": 0.5689588977071536,
            "f1_weighted": 0.5866663004262164
          },
          {
            "accuracy": 0.5870880968392737,
            "f1": 0.5627583870511592,
            "f1_weighted": 0.5889488305933946
          },
          {
            "accuracy": 0.5860793544048419,
            "f1": 0.5589001910191289,
            "f1_weighted": 0.5797355336391126
          },
          {
            "accuracy": 0.6042367182246133,
            "f1": 0.5714447640486918,
            "f1_weighted": 0.6064088385068537
          },
          {
            "accuracy": 0.5800268997982515,
            "f1": 0.5522748336942718,
            "f1_weighted": 0.5752746670892986
          },
          {
            "accuracy": 0.5527908540685945,
            "f1": 0.5390952593642837,
            "f1_weighted": 0.5576748101520526
          },
          {
            "accuracy": 0.5648957632817754,
            "f1": 0.5554398043340055,
            "f1_weighted": 0.5660371434329758
          },
          {
            "accuracy": 0.574310692669805,
            "f1": 0.5471335248163706,
            "f1_weighted": 0.5768486511264967
          },
          {
            "accuracy": 0.5837256220578345,
            "f1": 0.5624049990925497,
            "f1_weighted": 0.5853205486979417
          },
          {
            "accuracy": 0.5961667787491594,
            "f1": 0.5668622295450677,
            "f1_weighted": 0.6001478899417788
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.5927693064436793,
        "f1": 0.5764272761834331,
        "f1_weighted": 0.5922561941316183,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5927693064436793,
        "scores_per_experiment": [
          {
            "accuracy": 0.5799311362518446,
            "f1": 0.5609653257067136,
            "f1_weighted": 0.5791597513904505
          },
          {
            "accuracy": 0.6050172159370388,
            "f1": 0.5762344422262425,
            "f1_weighted": 0.6037355523061801
          },
          {
            "accuracy": 0.5981308411214953,
            "f1": 0.5792379581437879,
            "f1_weighted": 0.5928476979727498
          },
          {
            "accuracy": 0.602065912444663,
            "f1": 0.5712528332856438,
            "f1_weighted": 0.6000753411093516
          },
          {
            "accuracy": 0.6094441711756026,
            "f1": 0.5932989319714862,
            "f1_weighted": 0.6065876934071431
          },
          {
            "accuracy": 0.5799311362518446,
            "f1": 0.5752625170027763,
            "f1_weighted": 0.5825682634706529
          },
          {
            "accuracy": 0.5651746187899656,
            "f1": 0.5664594749495769,
            "f1_weighted": 0.564182408995847
          },
          {
            "accuracy": 0.5818986719134285,
            "f1": 0.5620291303301983,
            "f1_weighted": 0.5806828223807657
          },
          {
            "accuracy": 0.5932120019675357,
            "f1": 0.5853172824776433,
            "f1_weighted": 0.596999144998215
          },
          {
            "accuracy": 0.6128873585833743,
            "f1": 0.5942148657402638,
            "f1_weighted": 0.6157232652848267
          }
        ]
      }
    ]
  },
  "task_name": "MassiveIntentClassification"
}