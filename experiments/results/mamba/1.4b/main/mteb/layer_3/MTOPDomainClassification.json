{
  "dataset_revision": "d80d48c1eb48d3562165c59d59d0034df9fff0bf",
  "evaluation_time": 41.5303053855896,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.816393068855449,
        "f1": 0.8104546983133802,
        "f1_weighted": 0.818870936340692,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.816393068855449,
        "scores_per_experiment": [
          {
            "accuracy": 0.7861377108983129,
            "f1": 0.7776605100051781,
            "f1_weighted": 0.7880557872232848
          },
          {
            "accuracy": 0.8052895576835385,
            "f1": 0.804635444491204,
            "f1_weighted": 0.8097683184638148
          },
          {
            "accuracy": 0.8141814865481076,
            "f1": 0.8042891290583424,
            "f1_weighted": 0.8147236885314397
          },
          {
            "accuracy": 0.8303693570451436,
            "f1": 0.8244515965166103,
            "f1_weighted": 0.8347576259330984
          },
          {
            "accuracy": 0.8419972640218878,
            "f1": 0.8359260126126096,
            "f1_weighted": 0.8455257544606309
          },
          {
            "accuracy": 0.8191974464204287,
            "f1": 0.8161891045770567,
            "f1_weighted": 0.8210695386612666
          },
          {
            "accuracy": 0.8116735066119471,
            "f1": 0.8056459128375745,
            "f1_weighted": 0.8116923935426253
          },
          {
            "accuracy": 0.7895576835385317,
            "f1": 0.7838768932793004,
            "f1_weighted": 0.7948234015443798
          },
          {
            "accuracy": 0.831281349749202,
            "f1": 0.8287096724290208,
            "f1_weighted": 0.8338244600574739
          },
          {
            "accuracy": 0.8342453260373917,
            "f1": 0.823162707326907,
            "f1_weighted": 0.834468394988905
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.8116778523489933,
        "f1": 0.8090592441084853,
        "f1_weighted": 0.8131279171342556,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.8116778523489933,
        "scores_per_experiment": [
          {
            "accuracy": 0.7807606263982103,
            "f1": 0.7748283722153183,
            "f1_weighted": 0.7798479170389933
          },
          {
            "accuracy": 0.7991051454138702,
            "f1": 0.8020100202925488,
            "f1_weighted": 0.8024179529479987
          },
          {
            "accuracy": 0.8076062639821029,
            "f1": 0.8022557259189017,
            "f1_weighted": 0.8079235236655884
          },
          {
            "accuracy": 0.8237136465324385,
            "f1": 0.8246795254413624,
            "f1_weighted": 0.8283950316528081
          },
          {
            "accuracy": 0.8393736017897092,
            "f1": 0.8357351862812358,
            "f1_weighted": 0.8414433936626573
          },
          {
            "accuracy": 0.8147651006711409,
            "f1": 0.8150879852943868,
            "f1_weighted": 0.8162870871195541
          },
          {
            "accuracy": 0.8044742729306488,
            "f1": 0.7994809089839446,
            "f1_weighted": 0.8035171285106074
          },
          {
            "accuracy": 0.7865771812080536,
            "f1": 0.785684427953591,
            "f1_weighted": 0.7901975988995898
          },
          {
            "accuracy": 0.8241610738255034,
            "f1": 0.8229622937768272,
            "f1_weighted": 0.8253795314590096
          },
          {
            "accuracy": 0.836241610738255,
            "f1": 0.8278679949267365,
            "f1_weighted": 0.8358700063857488
          }
        ]
      }
    ]
  },
  "task_name": "MTOPDomainClassification"
}