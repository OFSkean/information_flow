{
  "dataset_revision": "fad2c6e8459f9e1c45d9315f4953d921437d70f8",
  "evaluation_time": 47.91096329689026,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.6080026899798251,
        "f1": 0.5932822481468284,
        "f1_weighted": 0.6098216041102362,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.6080026899798251,
        "scores_per_experiment": [
          {
            "accuracy": 0.6459314055144586,
            "f1": 0.6315183703745073,
            "f1_weighted": 0.6502777786312771
          },
          {
            "accuracy": 0.6146603900470746,
            "f1": 0.5969838039817862,
            "f1_weighted": 0.6184604272497416
          },
          {
            "accuracy": 0.5948217888365838,
            "f1": 0.5865106274743094,
            "f1_weighted": 0.5950492738388202
          },
          {
            "accuracy": 0.6025554808338938,
            "f1": 0.5884173654391797,
            "f1_weighted": 0.6035143108515457
          },
          {
            "accuracy": 0.5934767989240081,
            "f1": 0.5758703507599203,
            "f1_weighted": 0.5933476529621454
          },
          {
            "accuracy": 0.6032279757901816,
            "f1": 0.5798927640367869,
            "f1_weighted": 0.608237481212363
          },
          {
            "accuracy": 0.6096166778749159,
            "f1": 0.5965296399963349,
            "f1_weighted": 0.612641349275835
          },
          {
            "accuracy": 0.628782784129119,
            "f1": 0.6052683467489614,
            "f1_weighted": 0.6266164919595558
          },
          {
            "accuracy": 0.6005379959650302,
            "f1": 0.5948961679488376,
            "f1_weighted": 0.6029590126498758
          },
          {
            "accuracy": 0.5864156018829859,
            "f1": 0.57693504470766,
            "f1_weighted": 0.5871122624712022
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.6030988686669947,
        "f1": 0.5959525753571134,
        "f1_weighted": 0.602190497743424,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.6030988686669947,
        "scores_per_experiment": [
          {
            "accuracy": 0.6335464830300049,
            "f1": 0.621683745164722,
            "f1_weighted": 0.6380415783975562
          },
          {
            "accuracy": 0.6143630103295622,
            "f1": 0.6119826458531805,
            "f1_weighted": 0.6137931383885878
          },
          {
            "accuracy": 0.5823905558288244,
            "f1": 0.5769373791459949,
            "f1_weighted": 0.583298592656826
          },
          {
            "accuracy": 0.5873093949827841,
            "f1": 0.576719233181265,
            "f1_weighted": 0.5841952822882646
          },
          {
            "accuracy": 0.6005902606984752,
            "f1": 0.5927544558921741,
            "f1_weighted": 0.596464889357894
          },
          {
            "accuracy": 0.5976389572060994,
            "f1": 0.5910797987792937,
            "f1_weighted": 0.5971346470860752
          },
          {
            "accuracy": 0.5971470732907034,
            "f1": 0.593411737387937,
            "f1_weighted": 0.5999695701098793
          },
          {
            "accuracy": 0.6217412690605018,
            "f1": 0.599842627030918,
            "f1_weighted": 0.6145050519033942
          },
          {
            "accuracy": 0.6050172159370388,
            "f1": 0.6090488897360238,
            "f1_weighted": 0.6016404889118027
          },
          {
            "accuracy": 0.5912444663059518,
            "f1": 0.5860652413996249,
            "f1_weighted": 0.5928617383339614
          }
        ]
      }
    ]
  },
  "task_name": "MassiveScenarioClassification"
}