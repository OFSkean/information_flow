{
  "dataset_revision": "1399c76144fd37290681b995c656ef9b2e06e26d",
  "evaluation_time": 134.1749608516693,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.20",
  "scores": {
    "test": [
      {
        "accuracy": 0.32682,
        "f1": 0.32750474077911296,
        "f1_weighted": 0.327504740779113,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.32682,
        "scores_per_experiment": [
          {
            "accuracy": 0.3558,
            "f1": 0.35185006597902885,
            "f1_weighted": 0.35185006597902885
          },
          {
            "accuracy": 0.3302,
            "f1": 0.3273972482955244,
            "f1_weighted": 0.3273972482955244
          },
          {
            "accuracy": 0.3066,
            "f1": 0.30628826104486534,
            "f1_weighted": 0.30628826104486534
          },
          {
            "accuracy": 0.3166,
            "f1": 0.31876381776009255,
            "f1_weighted": 0.31876381776009255
          },
          {
            "accuracy": 0.3686,
            "f1": 0.36616696111875024,
            "f1_weighted": 0.3661669611187503
          },
          {
            "accuracy": 0.2916,
            "f1": 0.29551725407199764,
            "f1_weighted": 0.29551725407199764
          },
          {
            "accuracy": 0.313,
            "f1": 0.3161829845758478,
            "f1_weighted": 0.31618298457584776
          },
          {
            "accuracy": 0.3232,
            "f1": 0.32225293799363736,
            "f1_weighted": 0.32225293799363736
          },
          {
            "accuracy": 0.3356,
            "f1": 0.3358162739015235,
            "f1_weighted": 0.3358162739015236
          },
          {
            "accuracy": 0.327,
            "f1": 0.33481160304986224,
            "f1_weighted": 0.33481160304986224
          }
        ]
      }
    ]
  },
  "task_name": "AmazonReviewsClassification"
}