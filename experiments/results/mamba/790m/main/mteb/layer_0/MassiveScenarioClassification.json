{
  "dataset_revision": "fad2c6e8459f9e1c45d9315f4953d921437d70f8",
  "evaluation_time": 26.567228317260742,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.5631136516476126,
        "f1": 0.5475820654159123,
        "f1_weighted": 0.5679311684750639,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5631136516476126,
        "scores_per_experiment": [
          {
            "accuracy": 0.5995292535305985,
            "f1": 0.5857515373951495,
            "f1_weighted": 0.6039719687601319
          },
          {
            "accuracy": 0.5796906523201076,
            "f1": 0.5630059102796888,
            "f1_weighted": 0.5887332981207218
          },
          {
            "accuracy": 0.5608607935440484,
            "f1": 0.5464782443566121,
            "f1_weighted": 0.5613686761175427
          },
          {
            "accuracy": 0.5490921318090114,
            "f1": 0.5365922011255856,
            "f1_weighted": 0.5533478049947822
          },
          {
            "accuracy": 0.5427034297242771,
            "f1": 0.5187278422616884,
            "f1_weighted": 0.5461115875818794
          },
          {
            "accuracy": 0.5480833893745797,
            "f1": 0.534802576797542,
            "f1_weighted": 0.5592938360289239
          },
          {
            "accuracy": 0.5662407531943511,
            "f1": 0.5497711992653396,
            "f1_weighted": 0.5706024350128258
          },
          {
            "accuracy": 0.5753194351042367,
            "f1": 0.5564016124475635,
            "f1_weighted": 0.5743522005946666
          },
          {
            "accuracy": 0.562542030934768,
            "f1": 0.5502549814755826,
            "f1_weighted": 0.5684125185771244
          },
          {
            "accuracy": 0.547074646940148,
            "f1": 0.53403454875437,
            "f1_weighted": 0.5531173589620387
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.5522872602065912,
        "f1": 0.5430182646265825,
        "f1_weighted": 0.5528168250467653,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5522872602065912,
        "scores_per_experiment": [
          {
            "accuracy": 0.573044761436301,
            "f1": 0.5606958730053017,
            "f1_weighted": 0.5770617450021981
          },
          {
            "accuracy": 0.5666502705361535,
            "f1": 0.5607184499518376,
            "f1_weighted": 0.5709559339371817
          },
          {
            "accuracy": 0.5538612887358584,
            "f1": 0.542109001549815,
            "f1_weighted": 0.5520731314325469
          },
          {
            "accuracy": 0.529758976881456,
            "f1": 0.518737963087334,
            "f1_weighted": 0.5271507424806646
          },
          {
            "accuracy": 0.5469749139203148,
            "f1": 0.533089747850936,
            "f1_weighted": 0.5455462771313476
          },
          {
            "accuracy": 0.5400885391047713,
            "f1": 0.542626004060616,
            "f1_weighted": 0.5428244947982269
          },
          {
            "accuracy": 0.5494343334972946,
            "f1": 0.5394293279046691,
            "f1_weighted": 0.5511020190793362
          },
          {
            "accuracy": 0.5607476635514018,
            "f1": 0.5451744492603151,
            "f1_weighted": 0.5537444877948291
          },
          {
            "accuracy": 0.5646827348745695,
            "f1": 0.5582838888746227,
            "f1_weighted": 0.5660653377015069
          },
          {
            "accuracy": 0.5376291195277915,
            "f1": 0.5293179407203772,
            "f1_weighted": 0.5416440811098148
          }
        ]
      }
    ]
  },
  "task_name": "MassiveScenarioClassification"
}