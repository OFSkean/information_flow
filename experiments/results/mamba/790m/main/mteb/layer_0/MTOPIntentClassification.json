{
  "dataset_revision": "ae001d0e6b1228650b7bd1c2c65fb50ad11a8aba",
  "evaluation_time": 76.04692387580872,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.6308937528499772,
        "f1": 0.45092244731068903,
        "f1_weighted": 0.6759803728012269,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.6308937528499772,
        "scores_per_experiment": [
          {
            "accuracy": 0.6295029639762882,
            "f1": 0.4442895983384147,
            "f1_weighted": 0.6703715839488605
          },
          {
            "accuracy": 0.6347469220246238,
            "f1": 0.4402607016749732,
            "f1_weighted": 0.6796742484408446
          },
          {
            "accuracy": 0.615595075239398,
            "f1": 0.43161152273164666,
            "f1_weighted": 0.6635201926002424
          },
          {
            "accuracy": 0.6274509803921569,
            "f1": 0.4558608904071218,
            "f1_weighted": 0.6710408082953699
          },
          {
            "accuracy": 0.6233470132238942,
            "f1": 0.4537391377801039,
            "f1_weighted": 0.6699966142399942
          },
          {
            "accuracy": 0.6114911080711354,
            "f1": 0.46007773721615974,
            "f1_weighted": 0.6570825305862186
          },
          {
            "accuracy": 0.6306429548563611,
            "f1": 0.4456180161176287,
            "f1_weighted": 0.6751643335689406
          },
          {
            "accuracy": 0.6639306885544916,
            "f1": 0.4637189732094037,
            "f1_weighted": 0.7047879451004213
          },
          {
            "accuracy": 0.6315549475604195,
            "f1": 0.45003451552618295,
            "f1_weighted": 0.683191171556438
          },
          {
            "accuracy": 0.6406748746010031,
            "f1": 0.4640133801052549,
            "f1_weighted": 0.6849742996749388
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.6252796420581656,
        "f1": 0.4250083255216251,
        "f1_weighted": 0.6718048434136702,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.6252796420581656,
        "scores_per_experiment": [
          {
            "accuracy": 0.6058165548098434,
            "f1": 0.3998397219839629,
            "f1_weighted": 0.6512921703784781
          },
          {
            "accuracy": 0.625503355704698,
            "f1": 0.40848446289329166,
            "f1_weighted": 0.6735102834037001
          },
          {
            "accuracy": 0.6089485458612975,
            "f1": 0.42199844318337176,
            "f1_weighted": 0.6566502927109038
          },
          {
            "accuracy": 0.6161073825503356,
            "f1": 0.42228645924638436,
            "f1_weighted": 0.6646363681566175
          },
          {
            "accuracy": 0.6134228187919463,
            "f1": 0.4255655689850031,
            "f1_weighted": 0.6634469880861616
          },
          {
            "accuracy": 0.6080536912751678,
            "f1": 0.42037839436600277,
            "f1_weighted": 0.653963716217428
          },
          {
            "accuracy": 0.6451901565995526,
            "f1": 0.43621034310567774,
            "f1_weighted": 0.6882797977384664
          },
          {
            "accuracy": 0.6550335570469799,
            "f1": 0.4398492778598014,
            "f1_weighted": 0.6980356741088213
          },
          {
            "accuracy": 0.6228187919463087,
            "f1": 0.417209935907576,
            "f1_weighted": 0.6750183902365876
          },
          {
            "accuracy": 0.6519015659955257,
            "f1": 0.4582606476851797,
            "f1_weighted": 0.6932147530995378
          }
        ]
      }
    ]
  },
  "task_name": "MTOPIntentClassification"
}