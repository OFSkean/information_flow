{
  "dataset_revision": "4672e20407010da34463acc759c162ca9734bca6",
  "evaluation_time": 49.34903264045715,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.5249831876260929,
        "f1": 0.5087843910764548,
        "f1_weighted": 0.528952402207232,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5249831876260929,
        "scores_per_experiment": [
          {
            "accuracy": 0.5084061869535978,
            "f1": 0.504429869505331,
            "f1_weighted": 0.5150400595541635
          },
          {
            "accuracy": 0.5501008742434432,
            "f1": 0.5243864926534101,
            "f1_weighted": 0.5540050535552843
          },
          {
            "accuracy": 0.5282447881640888,
            "f1": 0.5049671361583747,
            "f1_weighted": 0.5280059383313627
          },
          {
            "accuracy": 0.539340954942838,
            "f1": 0.5132589462226828,
            "f1_weighted": 0.5367280317349786
          },
          {
            "accuracy": 0.5245460659045057,
            "f1": 0.5016165611215252,
            "f1_weighted": 0.5253675207554439
          },
          {
            "accuracy": 0.5100874243443174,
            "f1": 0.5084870632469009,
            "f1_weighted": 0.515748711574094
          },
          {
            "accuracy": 0.5319435104236718,
            "f1": 0.5181372914546011,
            "f1_weighted": 0.5378588092483847
          },
          {
            "accuracy": 0.5100874243443174,
            "f1": 0.4945436423100489,
            "f1_weighted": 0.5224135687134396
          },
          {
            "accuracy": 0.4989912575655683,
            "f1": 0.499212259648977,
            "f1_weighted": 0.49698887795373453
          },
          {
            "accuracy": 0.5480833893745797,
            "f1": 0.5188046484426957,
            "f1_weighted": 0.5573674506514334
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.5472700442695524,
        "f1": 0.5288779012260079,
        "f1_weighted": 0.5498010066968027,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5472700442695524,
        "scores_per_experiment": [
          {
            "accuracy": 0.528775209050664,
            "f1": 0.5122159853884793,
            "f1_weighted": 0.5334037885319137
          },
          {
            "accuracy": 0.5636989670437776,
            "f1": 0.5349126638797027,
            "f1_weighted": 0.566092308282321
          },
          {
            "accuracy": 0.5533694048204624,
            "f1": 0.5325471785763266,
            "f1_weighted": 0.5527667953850972
          },
          {
            "accuracy": 0.5612395474667978,
            "f1": 0.5384879653329792,
            "f1_weighted": 0.5567379212581682
          },
          {
            "accuracy": 0.5577963600590261,
            "f1": 0.5293408701685718,
            "f1_weighted": 0.5618297361456267
          },
          {
            "accuracy": 0.5322183964584358,
            "f1": 0.5219244969379582,
            "f1_weighted": 0.5334786965735129
          },
          {
            "accuracy": 0.5336940482046237,
            "f1": 0.5115278375445892,
            "f1_weighted": 0.5358791328780949
          },
          {
            "accuracy": 0.5371372356123955,
            "f1": 0.5270377303726392,
            "f1_weighted": 0.5462026042460872
          },
          {
            "accuracy": 0.528775209050664,
            "f1": 0.5323872613881743,
            "f1_weighted": 0.5286182375630714
          },
          {
            "accuracy": 0.5759960649286768,
            "f1": 0.5483970226706576,
            "f1_weighted": 0.5830008461041336
          }
        ]
      }
    ]
  },
  "task_name": "MassiveIntentClassification"
}