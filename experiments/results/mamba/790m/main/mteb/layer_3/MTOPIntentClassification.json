{
  "dataset_revision": "ae001d0e6b1228650b7bd1c2c65fb50ad11a8aba",
  "evaluation_time": 86.53300333023071,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.6256953944368445,
        "f1": 0.42358390735025253,
        "f1_weighted": 0.6715921207126422,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.6256953944368445,
        "scores_per_experiment": [
          {
            "accuracy": 0.616735066119471,
            "f1": 0.41856954084223047,
            "f1_weighted": 0.660421922189416
          },
          {
            "accuracy": 0.6356589147286822,
            "f1": 0.4169967891760279,
            "f1_weighted": 0.6801931046202364
          },
          {
            "accuracy": 0.6000911992704059,
            "f1": 0.4098884489168071,
            "f1_weighted": 0.6476139641380899
          },
          {
            "accuracy": 0.6349749202006384,
            "f1": 0.4290477680206359,
            "f1_weighted": 0.6796280651827284
          },
          {
            "accuracy": 0.6342909256725946,
            "f1": 0.41693616572159364,
            "f1_weighted": 0.6809992625102306
          },
          {
            "accuracy": 0.606703146374829,
            "f1": 0.4138783561277713,
            "f1_weighted": 0.6580732531189185
          },
          {
            "accuracy": 0.6333789329685362,
            "f1": 0.440121156281756,
            "f1_weighted": 0.6802551318798402
          },
          {
            "accuracy": 0.647514819881441,
            "f1": 0.45452959495364786,
            "f1_weighted": 0.6890742613500402
          },
          {
            "accuracy": 0.6112631098951209,
            "f1": 0.4066936496552513,
            "f1_weighted": 0.6572160103693963
          },
          {
            "accuracy": 0.636342909256726,
            "f1": 0.4291776038068038,
            "f1_weighted": 0.6824462317675256
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.6253691275167785,
        "f1": 0.4177421875225007,
        "f1_weighted": 0.6726918496418397,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.6253691275167785,
        "scores_per_experiment": [
          {
            "accuracy": 0.6049217002237136,
            "f1": 0.3941078349406447,
            "f1_weighted": 0.6522836057041428
          },
          {
            "accuracy": 0.6353467561521253,
            "f1": 0.4230948810165818,
            "f1_weighted": 0.6801227302200372
          },
          {
            "accuracy": 0.5959731543624162,
            "f1": 0.3993229727325604,
            "f1_weighted": 0.6446486015230487
          },
          {
            "accuracy": 0.6375838926174496,
            "f1": 0.4193486991205853,
            "f1_weighted": 0.6874023761276796
          },
          {
            "accuracy": 0.6344519015659955,
            "f1": 0.42276033962543885,
            "f1_weighted": 0.6836527412824481
          },
          {
            "accuracy": 0.6,
            "f1": 0.3995011403291658,
            "f1_weighted": 0.6533975053429207
          },
          {
            "accuracy": 0.6317673378076063,
            "f1": 0.4261704612051498,
            "f1_weighted": 0.6800852100880584
          },
          {
            "accuracy": 0.6510067114093959,
            "f1": 0.44191200932569685,
            "f1_weighted": 0.6933994784921421
          },
          {
            "accuracy": 0.6138702460850112,
            "f1": 0.42106225233878014,
            "f1_weighted": 0.6595597806280926
          },
          {
            "accuracy": 0.6487695749440716,
            "f1": 0.4301412845904037,
            "f1_weighted": 0.6923664670098261
          }
        ]
      }
    ]
  },
  "task_name": "MTOPIntentClassification"
}