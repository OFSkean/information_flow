{
  "dataset_revision": "fad2c6e8459f9e1c45d9315f4953d921437d70f8",
  "evaluation_time": 37.19934368133545,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.5397444519166107,
        "f1": 0.5236277748936914,
        "f1_weighted": 0.5425383974235112,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5397444519166107,
        "scores_per_experiment": [
          {
            "accuracy": 0.582044384667115,
            "f1": 0.5642256582063248,
            "f1_weighted": 0.5821959491051387
          },
          {
            "accuracy": 0.5571620712844654,
            "f1": 0.5358899518397194,
            "f1_weighted": 0.5550787304563591
          },
          {
            "accuracy": 0.5087424344317417,
            "f1": 0.5075212974415836,
            "f1_weighted": 0.519784139336146
          },
          {
            "accuracy": 0.5416946872898454,
            "f1": 0.5238285374252313,
            "f1_weighted": 0.5478818648663484
          },
          {
            "accuracy": 0.5191661062542031,
            "f1": 0.4896698352184043,
            "f1_weighted": 0.5132600890439779
          },
          {
            "accuracy": 0.5386684599865501,
            "f1": 0.5076701364928484,
            "f1_weighted": 0.5453878024633468
          },
          {
            "accuracy": 0.5672494956287828,
            "f1": 0.5421218736241685,
            "f1_weighted": 0.5819091440043558
          },
          {
            "accuracy": 0.5309347679892401,
            "f1": 0.5210809719313991,
            "f1_weighted": 0.5299053621524764
          },
          {
            "accuracy": 0.5208473436449227,
            "f1": 0.5135260951939806,
            "f1_weighted": 0.5168889846763988
          },
          {
            "accuracy": 0.5309347679892401,
            "f1": 0.5307433915632536,
            "f1_weighted": 0.5330919081305631
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.5380226266601082,
        "f1": 0.5295922988884848,
        "f1_weighted": 0.5384143371674727,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5380226266601082,
        "scores_per_experiment": [
          {
            "accuracy": 0.5833743236596163,
            "f1": 0.5655834734903682,
            "f1_weighted": 0.5819730654071456
          },
          {
            "accuracy": 0.5523856369896705,
            "f1": 0.5532463167421507,
            "f1_weighted": 0.5480138054908776
          },
          {
            "accuracy": 0.5199212985735366,
            "f1": 0.5208744289170414,
            "f1_weighted": 0.5272479842685809
          },
          {
            "accuracy": 0.500245941957698,
            "f1": 0.48888630937037836,
            "f1_weighted": 0.5048000889496829
          },
          {
            "accuracy": 0.5332021642892277,
            "f1": 0.5199062659087865,
            "f1_weighted": 0.5277619774125616
          },
          {
            "accuracy": 0.5272995573044762,
            "f1": 0.5139965507491221,
            "f1_weighted": 0.528217541767454
          },
          {
            "accuracy": 0.5509099852434826,
            "f1": 0.5366145280804684,
            "f1_weighted": 0.5615075834682155
          },
          {
            "accuracy": 0.5381210034431874,
            "f1": 0.5292244997660513,
            "f1_weighted": 0.5356552348969106
          },
          {
            "accuracy": 0.5395966551893753,
            "f1": 0.5356525239065455,
            "f1_weighted": 0.5349359730804735
          },
          {
            "accuracy": 0.5351696999508117,
            "f1": 0.5319380919539352,
            "f1_weighted": 0.5340301169328239
          }
        ]
      }
    ]
  },
  "task_name": "MassiveScenarioClassification"
}