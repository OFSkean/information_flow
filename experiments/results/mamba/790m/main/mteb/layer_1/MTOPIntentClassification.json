{
  "dataset_revision": "ae001d0e6b1228650b7bd1c2c65fb50ad11a8aba",
  "evaluation_time": 80.38553071022034,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.6516187870497036,
        "f1": 0.4665548374513494,
        "f1_weighted": 0.6941521347178685,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.6516187870497036,
        "scores_per_experiment": [
          {
            "accuracy": 0.6484268125854993,
            "f1": 0.456583629435989,
            "f1_weighted": 0.6884069543917036
          },
          {
            "accuracy": 0.6507067943456453,
            "f1": 0.46205814966778813,
            "f1_weighted": 0.6915513189123356
          },
          {
            "accuracy": 0.6372549019607843,
            "f1": 0.45497088068607283,
            "f1_weighted": 0.6832996484259488
          },
          {
            "accuracy": 0.6632466940264478,
            "f1": 0.4720635361700594,
            "f1_weighted": 0.7044551476400261
          },
          {
            "accuracy": 0.6536707706338349,
            "f1": 0.4800769268346445,
            "f1_weighted": 0.6998476019678015
          },
          {
            "accuracy": 0.63406292749658,
            "f1": 0.4719078539426776,
            "f1_weighted": 0.6774877586780033
          },
          {
            "accuracy": 0.6481988144094847,
            "f1": 0.46934821229266865,
            "f1_weighted": 0.6918216371019833
          },
          {
            "accuracy": 0.6725946192430461,
            "f1": 0.48324234936425736,
            "f1_weighted": 0.7088083217697901
          },
          {
            "accuracy": 0.6484268125854993,
            "f1": 0.4415551864311293,
            "f1_weighted": 0.6945256635031676
          },
          {
            "accuracy": 0.6595987232102143,
            "f1": 0.47374164968820753,
            "f1_weighted": 0.7013172947879239
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.6522147651006712,
        "f1": 0.44856076627776015,
        "f1_weighted": 0.6957400093474864,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.6522147651006712,
        "scores_per_experiment": [
          {
            "accuracy": 0.6340044742729306,
            "f1": 0.43547714346856303,
            "f1_weighted": 0.6772319666637223
          },
          {
            "accuracy": 0.6407158836689038,
            "f1": 0.43208753734241157,
            "f1_weighted": 0.6845706509023434
          },
          {
            "accuracy": 0.640268456375839,
            "f1": 0.4347464055236005,
            "f1_weighted": 0.6867654351072969
          },
          {
            "accuracy": 0.6429530201342282,
            "f1": 0.4432618336037432,
            "f1_weighted": 0.6874059323281464
          },
          {
            "accuracy": 0.6680089485458613,
            "f1": 0.47010378189652013,
            "f1_weighted": 0.7144459700474911
          },
          {
            "accuracy": 0.643847874720358,
            "f1": 0.4441548801841328,
            "f1_weighted": 0.6838820863466811
          },
          {
            "accuracy": 0.6568232662192394,
            "f1": 0.4619174935458922,
            "f1_weighted": 0.7029353209927923
          },
          {
            "accuracy": 0.6774049217002237,
            "f1": 0.4654630855093953,
            "f1_weighted": 0.7143699665287468
          },
          {
            "accuracy": 0.6492170022371365,
            "f1": 0.4332463367841276,
            "f1_weighted": 0.6977389870967561
          },
          {
            "accuracy": 0.668903803131991,
            "f1": 0.46514916491921515,
            "f1_weighted": 0.708053777460887
          }
        ]
      }
    ]
  },
  "task_name": "MTOPIntentClassification"
}