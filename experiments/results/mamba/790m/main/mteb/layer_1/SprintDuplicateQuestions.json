{
  "dataset_revision": "d66bd1f72af766a5cc4b0ca5e00c162f89e8cc46",
  "evaluation_time": 24.75856113433838,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "cosine_accuracy": 0.9931188118811881,
        "cosine_accuracy_threshold": 0.9144241809844971,
        "cosine_ap": 0.5680237927566831,
        "cosine_f1": 0.5648484848484848,
        "cosine_f1_threshold": 0.9103249311447144,
        "cosine_precision": 0.7169230769230769,
        "cosine_recall": 0.466,
        "dot_accuracy": 0.9901089108910891,
        "dot_accuracy_threshold": 759.0838623046875,
        "dot_ap": 0.03704860522664195,
        "dot_f1": 0.08348134991119005,
        "dot_f1_threshold": 442.25848388671875,
        "dot_precision": 0.059293523969722456,
        "dot_recall": 0.141,
        "euclidean_accuracy": 0.9917425742574257,
        "euclidean_accuracy_threshold": 7.3384809494018555,
        "euclidean_ap": 0.38934355863432624,
        "euclidean_f1": 0.4233206590621039,
        "euclidean_f1_threshold": 8.030288696289062,
        "euclidean_precision": 0.5778546712802768,
        "euclidean_recall": 0.334,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5680237927566831,
        "manhattan_accuracy": 0.9925148514851485,
        "manhattan_accuracy_threshold": 223.92352294921875,
        "manhattan_ap": 0.5051395048378272,
        "manhattan_f1": 0.5171811298776936,
        "manhattan_f1_threshold": 241.43606567382812,
        "manhattan_precision": 0.6192468619246861,
        "manhattan_recall": 0.444,
        "max_accuracy": 0.9931188118811881,
        "max_ap": 0.5680237927566831,
        "max_f1": 0.5648484848484848,
        "max_precision": 0.7169230769230769,
        "max_recall": 0.466,
        "similarity_accuracy": 0.9931188118811881,
        "similarity_accuracy_threshold": 0.9144241809844971,
        "similarity_ap": 0.5680237927566831,
        "similarity_f1": 0.5648484848484848,
        "similarity_f1_threshold": 0.9103249311447144,
        "similarity_precision": 0.7169230769230769,
        "similarity_recall": 0.466
      }
    ],
    "validation": [
      {
        "cosine_accuracy": 0.9925445544554455,
        "cosine_accuracy_threshold": 0.9167243242263794,
        "cosine_ap": 0.5123690159991958,
        "cosine_f1": 0.5241935483870969,
        "cosine_f1_threshold": 0.9052399396896362,
        "cosine_precision": 0.6182065217391305,
        "cosine_recall": 0.455,
        "dot_accuracy": 0.9901188118811881,
        "dot_accuracy_threshold": 753.7911987304688,
        "dot_ap": 0.03462315660428458,
        "dot_f1": 0.0700140028005601,
        "dot_f1_threshold": 429.26885986328125,
        "dot_precision": 0.04376094023505876,
        "dot_recall": 0.175,
        "euclidean_accuracy": 0.9910792079207921,
        "euclidean_accuracy_threshold": 7.050540924072266,
        "euclidean_ap": 0.3092441064806574,
        "euclidean_f1": 0.3743257820927724,
        "euclidean_f1_threshold": 8.430953979492188,
        "euclidean_precision": 0.4063231850117096,
        "euclidean_recall": 0.347,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5123690159991958,
        "manhattan_accuracy": 0.9919009900990099,
        "manhattan_accuracy_threshold": 226.6715545654297,
        "manhattan_ap": 0.4319187449148666,
        "manhattan_f1": 0.47274749721913245,
        "manhattan_f1_threshold": 246.3031005859375,
        "manhattan_precision": 0.5325814536340853,
        "manhattan_recall": 0.425,
        "max_accuracy": 0.9925445544554455,
        "max_ap": 0.5123690159991958,
        "max_f1": 0.5241935483870969,
        "max_precision": 0.6182065217391305,
        "max_recall": 0.455,
        "similarity_accuracy": 0.9925445544554455,
        "similarity_accuracy_threshold": 0.9167243242263794,
        "similarity_ap": 0.5123690159991958,
        "similarity_f1": 0.5241935483870969,
        "similarity_f1_threshold": 0.9052399396896362,
        "similarity_precision": 0.6182065217391305,
        "similarity_recall": 0.455
      }
    ]
  },
  "task_name": "SprintDuplicateQuestions"
}