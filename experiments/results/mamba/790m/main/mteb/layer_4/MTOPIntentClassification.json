{
  "dataset_revision": "ae001d0e6b1228650b7bd1c2c65fb50ad11a8aba",
  "evaluation_time": 96.04038143157959,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.622047423620611,
        "f1": 0.4238719373458012,
        "f1_weighted": 0.6681095053018627,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.622047423620611,
        "scores_per_experiment": [
          {
            "accuracy": 0.6151390788873689,
            "f1": 0.41912093135311107,
            "f1_weighted": 0.6573056967953262
          },
          {
            "accuracy": 0.6349749202006384,
            "f1": 0.4229365253877854,
            "f1_weighted": 0.6789889041466975
          },
          {
            "accuracy": 0.5918832649338805,
            "f1": 0.39628365154904244,
            "f1_weighted": 0.6409813282832963
          },
          {
            "accuracy": 0.6308709530323757,
            "f1": 0.4340144158787434,
            "f1_weighted": 0.6762297996449038
          },
          {
            "accuracy": 0.6304149566803465,
            "f1": 0.4425165325476367,
            "f1_weighted": 0.6773913538315842
          },
          {
            "accuracy": 0.6062471500227998,
            "f1": 0.416842566381496,
            "f1_weighted": 0.6556312816094577
          },
          {
            "accuracy": 0.6310989512083903,
            "f1": 0.42644424867207137,
            "f1_weighted": 0.676645713675183
          },
          {
            "accuracy": 0.6354309165526676,
            "f1": 0.4440871147682428,
            "f1_weighted": 0.6792641941644122
          },
          {
            "accuracy": 0.613999088007296,
            "f1": 0.4190239541770089,
            "f1_weighted": 0.6612266723536019
          },
          {
            "accuracy": 0.6304149566803465,
            "f1": 0.41744943274287427,
            "f1_weighted": 0.677430108514165
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.6211633109619688,
        "f1": 0.4151858502158648,
        "f1_weighted": 0.6694697429334536,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.6211633109619688,
        "scores_per_experiment": [
          {
            "accuracy": 0.5973154362416108,
            "f1": 0.39685097054115176,
            "f1_weighted": 0.6437702028162334
          },
          {
            "accuracy": 0.625503355704698,
            "f1": 0.4243975550634218,
            "f1_weighted": 0.6744857854844353
          },
          {
            "accuracy": 0.5928411633109619,
            "f1": 0.3937823808108725,
            "f1_weighted": 0.6419146128651443
          },
          {
            "accuracy": 0.6322147651006711,
            "f1": 0.42195991327489335,
            "f1_weighted": 0.6837133958162113
          },
          {
            "accuracy": 0.6366890380313199,
            "f1": 0.4269417457382767,
            "f1_weighted": 0.6854637430643012
          },
          {
            "accuracy": 0.5968680089485459,
            "f1": 0.4003980896194661,
            "f1_weighted": 0.6492478122519715
          },
          {
            "accuracy": 0.6420581655480985,
            "f1": 0.42617872175447746,
            "f1_weighted": 0.6870762394392816
          },
          {
            "accuracy": 0.6353467561521253,
            "f1": 0.42842572896796527,
            "f1_weighted": 0.6813416818785322
          },
          {
            "accuracy": 0.614765100671141,
            "f1": 0.4049595109015822,
            "f1_weighted": 0.6618867789798248
          },
          {
            "accuracy": 0.6380313199105145,
            "f1": 0.42796388548654085,
            "f1_weighted": 0.6857971767386013
          }
        ]
      }
    ]
  },
  "task_name": "MTOPIntentClassification"
}