{
  "dataset_revision": "fad2c6e8459f9e1c45d9315f4953d921437d70f8",
  "evaluation_time": 42.55595684051514,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.5271687962340282,
        "f1": 0.5088953601159041,
        "f1_weighted": 0.5287966407206979,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5271687962340282,
        "scores_per_experiment": [
          {
            "accuracy": 0.5733019502353732,
            "f1": 0.5529545646975409,
            "f1_weighted": 0.5742885338998333
          },
          {
            "accuracy": 0.5406859448554135,
            "f1": 0.5208199917070953,
            "f1_weighted": 0.5415533538370565
          },
          {
            "accuracy": 0.5026899798251513,
            "f1": 0.495278182994803,
            "f1_weighted": 0.5089172456528793
          },
          {
            "accuracy": 0.51546738399462,
            "f1": 0.5006184144303463,
            "f1_weighted": 0.5225815821983032
          },
          {
            "accuracy": 0.5147948890383323,
            "f1": 0.48122110332861795,
            "f1_weighted": 0.5084100929506158
          },
          {
            "accuracy": 0.5211835911230666,
            "f1": 0.49964261863463943,
            "f1_weighted": 0.5247612809984621
          },
          {
            "accuracy": 0.5480833893745797,
            "f1": 0.526116543681267,
            "f1_weighted": 0.5600428826666398
          },
          {
            "accuracy": 0.5268997982515131,
            "f1": 0.5062173097248013,
            "f1_weighted": 0.5195675956717666
          },
          {
            "accuracy": 0.5144586415601883,
            "f1": 0.5035758026690966,
            "f1_weighted": 0.5129584259188136
          },
          {
            "accuracy": 0.5141223940820444,
            "f1": 0.5025090692908325,
            "f1_weighted": 0.5148854134126087
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.5254303984259716,
        "f1": 0.5156733275670901,
        "f1_weighted": 0.5252803795499917,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5254303984259716,
        "scores_per_experiment": [
          {
            "accuracy": 0.5646827348745695,
            "f1": 0.5450185924722092,
            "f1_weighted": 0.5642336425611223
          },
          {
            "accuracy": 0.52926709296606,
            "f1": 0.5286250996283047,
            "f1_weighted": 0.526536698693453
          },
          {
            "accuracy": 0.5095917363502214,
            "f1": 0.5122342815429162,
            "f1_weighted": 0.5164327607932148
          },
          {
            "accuracy": 0.4869650762420069,
            "f1": 0.47875209292632487,
            "f1_weighted": 0.49219790297847665
          },
          {
            "accuracy": 0.5228726020659125,
            "f1": 0.5039076567626332,
            "f1_weighted": 0.5182994423867961
          },
          {
            "accuracy": 0.5272995573044762,
            "f1": 0.5224352184091912,
            "f1_weighted": 0.5250466990693561
          },
          {
            "accuracy": 0.5341859321200196,
            "f1": 0.5210359171218117,
            "f1_weighted": 0.5444987515972314
          },
          {
            "accuracy": 0.5351696999508117,
            "f1": 0.5129711553043023,
            "f1_weighted": 0.5272853585672698
          },
          {
            "accuracy": 0.5272995573044762,
            "f1": 0.524744495431195,
            "f1_weighted": 0.5222988305983629
          },
          {
            "accuracy": 0.5169699950811608,
            "f1": 0.5070087660720122,
            "f1_weighted": 0.5159737082546353
          }
        ]
      }
    ]
  },
  "task_name": "MassiveScenarioClassification"
}