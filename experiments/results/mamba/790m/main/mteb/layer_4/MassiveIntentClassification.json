{
  "dataset_revision": "4672e20407010da34463acc759c162ca9734bca6",
  "evaluation_time": 55.5248019695282,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.5162743779421655,
        "f1": 0.4970237089685434,
        "f1_weighted": 0.5185717472405174,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5162743779421655,
        "scores_per_experiment": [
          {
            "accuracy": 0.5110961667787491,
            "f1": 0.4886283450530203,
            "f1_weighted": 0.5149026636963293
          },
          {
            "accuracy": 0.543039677202421,
            "f1": 0.5206400271519106,
            "f1_weighted": 0.5455241686905652
          },
          {
            "accuracy": 0.508069939475454,
            "f1": 0.4839708300083434,
            "f1_weighted": 0.505788066198454
          },
          {
            "accuracy": 0.5295897780766644,
            "f1": 0.5007734854343057,
            "f1_weighted": 0.5283365586378538
          },
          {
            "accuracy": 0.5211835911230666,
            "f1": 0.49583053833877366,
            "f1_weighted": 0.5231402445992922
          },
          {
            "accuracy": 0.5047074646940148,
            "f1": 0.4953816151986814,
            "f1_weighted": 0.5088224283991395
          },
          {
            "accuracy": 0.5094149293880296,
            "f1": 0.4937943235539744,
            "f1_weighted": 0.5136377783374126
          },
          {
            "accuracy": 0.5100874243443174,
            "f1": 0.48873371536167637,
            "f1_weighted": 0.5164676958248449
          },
          {
            "accuracy": 0.4932750504371217,
            "f1": 0.4955816015094872,
            "f1_weighted": 0.4886772738750834
          },
          {
            "accuracy": 0.5322797579018157,
            "f1": 0.5069026080752617,
            "f1_weighted": 0.5404205941461985
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.5366453516969995,
        "f1": 0.5161142701557212,
        "f1_weighted": 0.5386142098612707,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5366453516969995,
        "scores_per_experiment": [
          {
            "accuracy": 0.5204131824889326,
            "f1": 0.5025621316087134,
            "f1_weighted": 0.5255604196593171
          },
          {
            "accuracy": 0.5499262174126907,
            "f1": 0.520888517026719,
            "f1_weighted": 0.5528780004752712
          },
          {
            "accuracy": 0.5528775209050664,
            "f1": 0.5329304908101902,
            "f1_weighted": 0.5513886399036606
          },
          {
            "accuracy": 0.5632070831283817,
            "f1": 0.5320757704025834,
            "f1_weighted": 0.5581677196550339
          },
          {
            "accuracy": 0.5499262174126907,
            "f1": 0.5113047520244556,
            "f1_weighted": 0.5548544892776112
          },
          {
            "accuracy": 0.5302508607968519,
            "f1": 0.5180337299431851,
            "f1_weighted": 0.5331977354707061
          },
          {
            "accuracy": 0.5056566650270536,
            "f1": 0.4903096880660456,
            "f1_weighted": 0.5038712836096243
          },
          {
            "accuracy": 0.5218888342351206,
            "f1": 0.49407083931005197,
            "f1_weighted": 0.5301574295439799
          },
          {
            "accuracy": 0.5248401377274963,
            "f1": 0.5350615259449184,
            "f1_weighted": 0.5236083376473997
          },
          {
            "accuracy": 0.5474667978357107,
            "f1": 0.5239052564203491,
            "f1_weighted": 0.552458043370104
          }
        ]
      }
    ]
  },
  "task_name": "MassiveIntentClassification"
}