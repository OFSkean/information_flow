{
  "dataset_revision": "ae001d0e6b1228650b7bd1c2c65fb50ad11a8aba",
  "evaluation_time": 80.3280303478241,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.658093935248518,
        "f1": 0.4684663394364449,
        "f1_weighted": 0.7007181714399643,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.658093935248518,
        "scores_per_experiment": [
          {
            "accuracy": 0.6534427724578203,
            "f1": 0.4648243019752411,
            "f1_weighted": 0.6935689985561402
          },
          {
            "accuracy": 0.6536707706338349,
            "f1": 0.46858882032219135,
            "f1_weighted": 0.6954269858784812
          },
          {
            "accuracy": 0.6354309165526676,
            "f1": 0.4530358169514604,
            "f1_weighted": 0.6802327837362073
          },
          {
            "accuracy": 0.6668946648426812,
            "f1": 0.4796294290166013,
            "f1_weighted": 0.7077525568079839
          },
          {
            "accuracy": 0.6602827177382581,
            "f1": 0.46894500202534695,
            "f1_weighted": 0.7031141480765535
          },
          {
            "accuracy": 0.6472868217054264,
            "f1": 0.47097773329639137,
            "f1_weighted": 0.6914641672254032
          },
          {
            "accuracy": 0.6602827177382581,
            "f1": 0.48847501831873635,
            "f1_weighted": 0.7039119169678332
          },
          {
            "accuracy": 0.6817145462836297,
            "f1": 0.4797553679026271,
            "f1_weighted": 0.7210820138962192
          },
          {
            "accuracy": 0.6507067943456453,
            "f1": 0.4540243040655261,
            "f1_weighted": 0.6959088515459761
          },
          {
            "accuracy": 0.6712266301869585,
            "f1": 0.4564076004903272,
            "f1_weighted": 0.7147192917088451
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.6576733780760626,
        "f1": 0.45173078273919903,
        "f1_weighted": 0.7011910034992386,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.6576733780760626,
        "scores_per_experiment": [
          {
            "accuracy": 0.6375838926174496,
            "f1": 0.4370265303562146,
            "f1_weighted": 0.6817341122275111
          },
          {
            "accuracy": 0.6416107382550336,
            "f1": 0.4374926276897118,
            "f1_weighted": 0.6869194525632971
          },
          {
            "accuracy": 0.640268456375839,
            "f1": 0.4409981196209742,
            "f1_weighted": 0.6852953792392409
          },
          {
            "accuracy": 0.658165548098434,
            "f1": 0.458990178933607,
            "f1_weighted": 0.7049064872340625
          },
          {
            "accuracy": 0.6738255033557047,
            "f1": 0.4613357301692607,
            "f1_weighted": 0.7201360306419876
          },
          {
            "accuracy": 0.6501118568232662,
            "f1": 0.4650460323495499,
            "f1_weighted": 0.6933473219965327
          },
          {
            "accuracy": 0.6586129753914989,
            "f1": 0.44988238146139736,
            "f1_weighted": 0.7025432776613582
          },
          {
            "accuracy": 0.6814317673378076,
            "f1": 0.4594991556197211,
            "f1_weighted": 0.7208447676805165
          },
          {
            "accuracy": 0.6496644295302013,
            "f1": 0.4419715363906134,
            "f1_weighted": 0.6915546334275385
          },
          {
            "accuracy": 0.6854586129753915,
            "f1": 0.4650655348009408,
            "f1_weighted": 0.7246285723203403
          }
        ]
      }
    ]
  },
  "task_name": "MTOPIntentClassification"
}