{
  "dataset_revision": "4672e20407010da34463acc759c162ca9734bca6",
  "evaluation_time": 48.86781024932861,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.5729320780094149,
        "f1": 0.55247567333682,
        "f1_weighted": 0.5752191721703955,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5729320780094149,
        "scores_per_experiment": [
          {
            "accuracy": 0.5823806321452589,
            "f1": 0.5654805725676302,
            "f1_weighted": 0.585081200541064
          },
          {
            "accuracy": 0.5911230665770006,
            "f1": 0.5635787287026839,
            "f1_weighted": 0.5926906777392706
          },
          {
            "accuracy": 0.5635507733691997,
            "f1": 0.5438264655142038,
            "f1_weighted": 0.559813570960072
          },
          {
            "accuracy": 0.5837256220578345,
            "f1": 0.5503117892488507,
            "f1_weighted": 0.5847361957272508
          },
          {
            "accuracy": 0.5790181573638198,
            "f1": 0.5518521065668133,
            "f1_weighted": 0.5795643844280196
          },
          {
            "accuracy": 0.5544720914593141,
            "f1": 0.5408379536886034,
            "f1_weighted": 0.5622024644157755
          },
          {
            "accuracy": 0.5699394754539341,
            "f1": 0.5547159290216496,
            "f1_weighted": 0.573442262609623
          },
          {
            "accuracy": 0.5598520511096167,
            "f1": 0.543607291719471,
            "f1_weighted": 0.5653474424339527
          },
          {
            "accuracy": 0.562542030934768,
            "f1": 0.5518579001904208,
            "f1_weighted": 0.5615059445642404
          },
          {
            "accuracy": 0.5827168796234028,
            "f1": 0.5586879961478737,
            "f1_weighted": 0.5878075782846861
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.5843089030988686,
        "f1": 0.5654098600822538,
        "f1_weighted": 0.585102662786455,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5843089030988686,
        "scores_per_experiment": [
          {
            "accuracy": 0.5814067879980325,
            "f1": 0.5778574580707242,
            "f1_weighted": 0.5797348329982477
          },
          {
            "accuracy": 0.5961633054599115,
            "f1": 0.5577830591956777,
            "f1_weighted": 0.597273764207154
          },
          {
            "accuracy": 0.5902606984751598,
            "f1": 0.5710232122242842,
            "f1_weighted": 0.5857954407522423
          },
          {
            "accuracy": 0.5897688145597639,
            "f1": 0.5624820653084541,
            "f1_weighted": 0.5896762201515497
          },
          {
            "accuracy": 0.6045253320216429,
            "f1": 0.5884480948204059,
            "f1_weighted": 0.6059026642913797
          },
          {
            "accuracy": 0.5745204131824889,
            "f1": 0.5649915039505474,
            "f1_weighted": 0.5813006066788352
          },
          {
            "accuracy": 0.5627151992129857,
            "f1": 0.5482366830704188,
            "f1_weighted": 0.5634495072395801
          },
          {
            "accuracy": 0.5622233152975897,
            "f1": 0.5401909762415844,
            "f1_weighted": 0.5619082811407994
          },
          {
            "accuracy": 0.5774717166748647,
            "f1": 0.5648472021116226,
            "f1_weighted": 0.5791605623130195
          },
          {
            "accuracy": 0.6040334481062469,
            "f1": 0.5782383458288175,
            "f1_weighted": 0.6068247480917425
          }
        ]
      }
    ]
  },
  "task_name": "MassiveIntentClassification"
}