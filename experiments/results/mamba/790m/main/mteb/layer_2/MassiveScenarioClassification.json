{
  "dataset_revision": "fad2c6e8459f9e1c45d9315f4953d921437d70f8",
  "evaluation_time": 34.31961226463318,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.5885339609952924,
        "f1": 0.5716022988127216,
        "f1_weighted": 0.5907886771923889,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5885339609952924,
        "scores_per_experiment": [
          {
            "accuracy": 0.625084061869536,
            "f1": 0.6071090187433329,
            "f1_weighted": 0.6286185213257975
          },
          {
            "accuracy": 0.5971755211835911,
            "f1": 0.5795746235566593,
            "f1_weighted": 0.5989307647638523
          },
          {
            "accuracy": 0.5806993947545394,
            "f1": 0.5682172092149572,
            "f1_weighted": 0.5816778315148367
          },
          {
            "accuracy": 0.5706119704102219,
            "f1": 0.5591034896249836,
            "f1_weighted": 0.5746511491547406
          },
          {
            "accuracy": 0.5766644250168124,
            "f1": 0.555957633711948,
            "f1_weighted": 0.5765156728146958
          },
          {
            "accuracy": 0.5733019502353732,
            "f1": 0.553553159086592,
            "f1_weighted": 0.5817172167125245
          },
          {
            "accuracy": 0.6002017484868863,
            "f1": 0.5785712838756168,
            "f1_weighted": 0.6043307245115881
          },
          {
            "accuracy": 0.6045729657027572,
            "f1": 0.5764580462658496,
            "f1_weighted": 0.6027826087810072
          },
          {
            "accuracy": 0.5796906523201076,
            "f1": 0.5715047050076646,
            "f1_weighted": 0.579240533903097
          },
          {
            "accuracy": 0.5773369199731002,
            "f1": 0.565973819039612,
            "f1_weighted": 0.5794217484417499
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.580865715691097,
        "f1": 0.5706580474495725,
        "f1_weighted": 0.5809515353858605,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.580865715691097,
        "scores_per_experiment": [
          {
            "accuracy": 0.6163305459911461,
            "f1": 0.5990249645147683,
            "f1_weighted": 0.6194172509035933
          },
          {
            "accuracy": 0.5863256271519921,
            "f1": 0.5834372638468633,
            "f1_weighted": 0.5843640687170789
          },
          {
            "accuracy": 0.5627151992129857,
            "f1": 0.5555326923123921,
            "f1_weighted": 0.5648822115518479
          },
          {
            "accuracy": 0.5538612887358584,
            "f1": 0.5433698419570789,
            "f1_weighted": 0.555715989065355
          },
          {
            "accuracy": 0.5745204131824889,
            "f1": 0.5600300041841388,
            "f1_weighted": 0.5731294419365769
          },
          {
            "accuracy": 0.5607476635514018,
            "f1": 0.5576598941329671,
            "f1_weighted": 0.5633673505414268
          },
          {
            "accuracy": 0.5858337432365962,
            "f1": 0.5728400422226402,
            "f1_weighted": 0.5874604980731669
          },
          {
            "accuracy": 0.6114117068371864,
            "f1": 0.5883900256044744,
            "f1_weighted": 0.6065951693400182
          },
          {
            "accuracy": 0.5833743236596163,
            "f1": 0.5815857175313699,
            "f1_weighted": 0.5804796714638254
          },
          {
            "accuracy": 0.573536645351697,
            "f1": 0.5647100281890315,
            "f1_weighted": 0.5741037022657158
          }
        ]
      }
    ]
  },
  "task_name": "MassiveScenarioClassification"
}