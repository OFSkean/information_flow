{
  "dataset_revision": "ae001d0e6b1228650b7bd1c2c65fb50ad11a8aba",
  "evaluation_time": 79.85045075416565,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.681326949384405,
        "f1": 0.475371091778372,
        "f1_weighted": 0.7201566484121488,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.681326949384405,
        "scores_per_experiment": [
          {
            "accuracy": 0.6509347925216599,
            "f1": 0.460413386005069,
            "f1_weighted": 0.6853428835023927
          },
          {
            "accuracy": 0.6867305061559508,
            "f1": 0.4666184832500716,
            "f1_weighted": 0.7260591536066139
          },
          {
            "accuracy": 0.6609667122663019,
            "f1": 0.4675796330787049,
            "f1_weighted": 0.7012897528397561
          },
          {
            "accuracy": 0.7079343365253078,
            "f1": 0.4900253609343121,
            "f1_weighted": 0.7439225394801933
          },
          {
            "accuracy": 0.6657546739626083,
            "f1": 0.4838991630144483,
            "f1_weighted": 0.7109659492212776
          },
          {
            "accuracy": 0.6684906520747834,
            "f1": 0.454024095622199,
            "f1_weighted": 0.7094728290557638
          },
          {
            "accuracy": 0.6691746466028272,
            "f1": 0.47705854663022057,
            "f1_weighted": 0.7107441446143522
          },
          {
            "accuracy": 0.7136342909256725,
            "f1": 0.49963003563202163,
            "f1_weighted": 0.7493443032858906
          },
          {
            "accuracy": 0.6937984496124031,
            "f1": 0.4819884405220644,
            "f1_weighted": 0.7315939995940591
          },
          {
            "accuracy": 0.6958504331965344,
            "f1": 0.47247377309460864,
            "f1_weighted": 0.7328309289211885
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.6838031319910515,
        "f1": 0.4627144104077293,
        "f1_weighted": 0.7248749979726778,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.6838031319910515,
        "scores_per_experiment": [
          {
            "accuracy": 0.6604026845637584,
            "f1": 0.44012677530065775,
            "f1_weighted": 0.6965295922367943
          },
          {
            "accuracy": 0.6823266219239373,
            "f1": 0.45906132623375323,
            "f1_weighted": 0.7281953412763204
          },
          {
            "accuracy": 0.665324384787472,
            "f1": 0.4475938311142509,
            "f1_weighted": 0.7082116895208866
          },
          {
            "accuracy": 0.7046979865771812,
            "f1": 0.47490526687761875,
            "f1_weighted": 0.7423172673004909
          },
          {
            "accuracy": 0.6836689038031319,
            "f1": 0.4804353944000637,
            "f1_weighted": 0.7284762751142666
          },
          {
            "accuracy": 0.6635346756152125,
            "f1": 0.44995332476295774,
            "f1_weighted": 0.7072869482277812
          },
          {
            "accuracy": 0.6706935123042506,
            "f1": 0.46936930743553223,
            "f1_weighted": 0.7147874445559955
          },
          {
            "accuracy": 0.702013422818792,
            "f1": 0.4716969510648883,
            "f1_weighted": 0.7428720762650203
          },
          {
            "accuracy": 0.7038031319910515,
            "f1": 0.4624638440568648,
            "f1_weighted": 0.7396444470288567
          },
          {
            "accuracy": 0.7015659955257271,
            "f1": 0.47153808283070586,
            "f1_weighted": 0.7404288982003665
          }
        ]
      }
    ]
  },
  "task_name": "MTOPIntentClassification"
}