{
  "dataset_revision": "fad2c6e8459f9e1c45d9315f4953d921437d70f8",
  "evaluation_time": 38.77519631385803,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.5889038332212507,
        "f1": 0.5676044577341179,
        "f1_weighted": 0.5907957913610591,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5889038332212507,
        "scores_per_experiment": [
          {
            "accuracy": 0.5971755211835911,
            "f1": 0.5789723008452329,
            "f1_weighted": 0.600053905876368
          },
          {
            "accuracy": 0.6096166778749159,
            "f1": 0.5862874316130453,
            "f1_weighted": 0.613773961495185
          },
          {
            "accuracy": 0.5736381977135171,
            "f1": 0.5594150992569537,
            "f1_weighted": 0.5767171365860828
          },
          {
            "accuracy": 0.582044384667115,
            "f1": 0.559672574367984,
            "f1_weighted": 0.5870487011497192
          },
          {
            "accuracy": 0.5968392737054472,
            "f1": 0.5666445557738409,
            "f1_weighted": 0.5944713986761919
          },
          {
            "accuracy": 0.5628782784129119,
            "f1": 0.5413455807914627,
            "f1_weighted": 0.5685138302386106
          },
          {
            "accuracy": 0.5817081371889711,
            "f1": 0.5628242829816417,
            "f1_weighted": 0.5866565156528536
          },
          {
            "accuracy": 0.6153328850033625,
            "f1": 0.5845870791758115,
            "f1_weighted": 0.6130667442564094
          },
          {
            "accuracy": 0.5796906523201076,
            "f1": 0.5631046408881021,
            "f1_weighted": 0.579045883612424
          },
          {
            "accuracy": 0.5901143241425689,
            "f1": 0.5731910316471049,
            "f1_weighted": 0.5886098360667462
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.5813575996064929,
        "f1": 0.5645743849795145,
        "f1_weighted": 0.5828183607652354,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5813575996064929,
        "scores_per_experiment": [
          {
            "accuracy": 0.5823905558288244,
            "f1": 0.5651489412622996,
            "f1_weighted": 0.5872297714488047
          },
          {
            "accuracy": 0.5892769306443679,
            "f1": 0.5784387944719886,
            "f1_weighted": 0.5924904693052266
          },
          {
            "accuracy": 0.5612395474667978,
            "f1": 0.5557969371274062,
            "f1_weighted": 0.5635125220645014
          },
          {
            "accuracy": 0.5489424495818986,
            "f1": 0.5281612789253898,
            "f1_weighted": 0.5534031165130475
          },
          {
            "accuracy": 0.6064928676832267,
            "f1": 0.5821769919581935,
            "f1_weighted": 0.6064846625622846
          },
          {
            "accuracy": 0.5577963600590261,
            "f1": 0.5522996040308602,
            "f1_weighted": 0.5573748680450112
          },
          {
            "accuracy": 0.5759960649286768,
            "f1": 0.5636595478760866,
            "f1_weighted": 0.5836084637227578
          },
          {
            "accuracy": 0.6094441711756026,
            "f1": 0.5699406289207567,
            "f1_weighted": 0.6065698727944046
          },
          {
            "accuracy": 0.5853418593212002,
            "f1": 0.5691613251678559,
            "f1_weighted": 0.5822491245379556
          },
          {
            "accuracy": 0.5966551893753075,
            "f1": 0.5809598000543073,
            "f1_weighted": 0.5952607366583595
          }
        ]
      }
    ]
  },
  "task_name": "MassiveScenarioClassification"
}