{
  "dataset_revision": "1399c76144fd37290681b995c656ef9b2e06e26d",
  "evaluation_time": 333.40599274635315,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.3268400000000001,
        "f1": 0.32686659417397956,
        "f1_weighted": 0.32686659417397956,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.3268400000000001,
        "scores_per_experiment": [
          {
            "accuracy": 0.3446,
            "f1": 0.34573352446960387,
            "f1_weighted": 0.34573352446960387
          },
          {
            "accuracy": 0.3538,
            "f1": 0.355134907384998,
            "f1_weighted": 0.355134907384998
          },
          {
            "accuracy": 0.3132,
            "f1": 0.3160416089966726,
            "f1_weighted": 0.3160416089966726
          },
          {
            "accuracy": 0.3138,
            "f1": 0.31886032029424943,
            "f1_weighted": 0.31886032029424943
          },
          {
            "accuracy": 0.3696,
            "f1": 0.36502717094039416,
            "f1_weighted": 0.36502717094039416
          },
          {
            "accuracy": 0.3,
            "f1": 0.30004380514449486,
            "f1_weighted": 0.30004380514449486
          },
          {
            "accuracy": 0.3012,
            "f1": 0.30426683970810153,
            "f1_weighted": 0.3042668397081015
          },
          {
            "accuracy": 0.3294,
            "f1": 0.32250412979426824,
            "f1_weighted": 0.3225041297942682
          },
          {
            "accuracy": 0.3092,
            "f1": 0.3099317560476346,
            "f1_weighted": 0.3099317560476346
          },
          {
            "accuracy": 0.3336,
            "f1": 0.3311218789593789,
            "f1_weighted": 0.3311218789593789
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.32272,
        "f1": 0.3226173609217276,
        "f1_weighted": 0.3226173609217276,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.32272,
        "scores_per_experiment": [
          {
            "accuracy": 0.3444,
            "f1": 0.34440975171921234,
            "f1_weighted": 0.34440975171921234
          },
          {
            "accuracy": 0.3422,
            "f1": 0.34647667927258496,
            "f1_weighted": 0.34647667927258496
          },
          {
            "accuracy": 0.3062,
            "f1": 0.30972959206388906,
            "f1_weighted": 0.309729592063889
          },
          {
            "accuracy": 0.2988,
            "f1": 0.3052721928955352,
            "f1_weighted": 0.30527219289553514
          },
          {
            "accuracy": 0.362,
            "f1": 0.35473593606120424,
            "f1_weighted": 0.35473593606120424
          },
          {
            "accuracy": 0.3052,
            "f1": 0.30596733254981057,
            "f1_weighted": 0.30596733254981057
          },
          {
            "accuracy": 0.2964,
            "f1": 0.29738619733775745,
            "f1_weighted": 0.29738619733775745
          },
          {
            "accuracy": 0.326,
            "f1": 0.3212330753751157,
            "f1_weighted": 0.32123307537511564
          },
          {
            "accuracy": 0.3144,
            "f1": 0.31300555581875517,
            "f1_weighted": 0.31300555581875517
          },
          {
            "accuracy": 0.3316,
            "f1": 0.3279572961234114,
            "f1_weighted": 0.3279572961234114
          }
        ]
      }
    ]
  },
  "task_name": "AmazonReviewsClassification"
}