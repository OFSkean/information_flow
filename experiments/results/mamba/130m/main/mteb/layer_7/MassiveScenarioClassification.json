{
  "dataset_revision": "fad2c6e8459f9e1c45d9315f4953d921437d70f8",
  "evaluation_time": 36.640132665634155,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.5856422326832548,
        "f1": 0.5652630591498633,
        "f1_weighted": 0.5876079761261407,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5856422326832548,
        "scores_per_experiment": [
          {
            "accuracy": 0.6022192333557498,
            "f1": 0.5861115161649909,
            "f1_weighted": 0.6034669206283777
          },
          {
            "accuracy": 0.6123066577000672,
            "f1": 0.5880534889738076,
            "f1_weighted": 0.6180731838121506
          },
          {
            "accuracy": 0.5675857431069267,
            "f1": 0.55623964553602,
            "f1_weighted": 0.5705970863097555
          },
          {
            "accuracy": 0.5749831876260928,
            "f1": 0.5521217945793614,
            "f1_weighted": 0.5783614937816277
          },
          {
            "accuracy": 0.5830531271015468,
            "f1": 0.5557178958060048,
            "f1_weighted": 0.5832437522405349
          },
          {
            "accuracy": 0.5595158036314728,
            "f1": 0.5381765093437563,
            "f1_weighted": 0.5643856123012593
          },
          {
            "accuracy": 0.589778076664425,
            "f1": 0.5746370902618096,
            "f1_weighted": 0.5944218918775671
          },
          {
            "accuracy": 0.597511768661735,
            "f1": 0.5660729635588291,
            "f1_weighted": 0.5952128445876876
          },
          {
            "accuracy": 0.5790181573638198,
            "f1": 0.5617792664159654,
            "f1_weighted": 0.578355075847847
          },
          {
            "accuracy": 0.5904505716207128,
            "f1": 0.5737204208580885,
            "f1_weighted": 0.5899618998745989
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.5830791933103787,
        "f1": 0.5671920361224183,
        "f1_weighted": 0.5851488803819364,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5830791933103787,
        "scores_per_experiment": [
          {
            "accuracy": 0.5868175110673881,
            "f1": 0.5712564355548848,
            "f1_weighted": 0.5905435081167097
          },
          {
            "accuracy": 0.5927201180521396,
            "f1": 0.5799492197386027,
            "f1_weighted": 0.5978007525010702
          },
          {
            "accuracy": 0.5784554845056566,
            "f1": 0.5730052807026602,
            "f1_weighted": 0.5812907147019567
          },
          {
            "accuracy": 0.5489424495818986,
            "f1": 0.5275848039139249,
            "f1_weighted": 0.5554103001765609
          },
          {
            "accuracy": 0.5996064928676832,
            "f1": 0.5772561209856054,
            "f1_weighted": 0.599871344973582
          },
          {
            "accuracy": 0.5622233152975897,
            "f1": 0.5530800015278048,
            "f1_weighted": 0.5626138489042092
          },
          {
            "accuracy": 0.5745204131824889,
            "f1": 0.568316351938401,
            "f1_weighted": 0.5831545778572736
          },
          {
            "accuracy": 0.6040334481062469,
            "f1": 0.5682192189486446,
            "f1_weighted": 0.5996518982141775
          },
          {
            "accuracy": 0.5868175110673881,
            "f1": 0.5690037212426989,
            "f1_weighted": 0.5846401670737869
          },
          {
            "accuracy": 0.5966551893753075,
            "f1": 0.5842492066709556,
            "f1_weighted": 0.5965116913000369
          }
        ]
      }
    ]
  },
  "task_name": "MassiveScenarioClassification"
}