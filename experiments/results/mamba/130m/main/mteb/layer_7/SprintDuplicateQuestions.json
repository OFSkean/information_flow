{
  "dataset_revision": "d66bd1f72af766a5cc4b0ca5e00c162f89e8cc46",
  "evaluation_time": 36.460495948791504,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "cosine_accuracy": 0.9915346534653465,
        "cosine_accuracy_threshold": 0.9277347326278687,
        "cosine_ap": 0.3678233337522207,
        "cosine_f1": 0.4,
        "cosine_f1_threshold": 0.9023947715759277,
        "cosine_precision": 0.4631578947368421,
        "cosine_recall": 0.352,
        "dot_accuracy": 0.9903663366336634,
        "dot_accuracy_threshold": 258.0934753417969,
        "dot_ap": 0.17088155027831536,
        "dot_f1": 0.24280838615309605,
        "dot_f1_threshold": 240.86322021484375,
        "dot_precision": 0.2369172216936251,
        "dot_recall": 0.249,
        "euclidean_accuracy": 0.9912772277227723,
        "euclidean_accuracy_threshold": 5.950585842132568,
        "euclidean_ap": 0.3311395495208488,
        "euclidean_f1": 0.3722584469472436,
        "euclidean_f1_threshold": 6.916337966918945,
        "euclidean_precision": 0.4570596797671033,
        "euclidean_recall": 0.314,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.4611923573618496,
        "manhattan_accuracy": 0.9919405940594059,
        "manhattan_accuracy_threshold": 122.95109558105469,
        "manhattan_ap": 0.4611923573618496,
        "manhattan_f1": 0.4628916987355689,
        "manhattan_f1_threshold": 136.50633239746094,
        "manhattan_precision": 0.514041514041514,
        "manhattan_recall": 0.421,
        "max_accuracy": 0.9919405940594059,
        "max_ap": 0.4611923573618496,
        "max_f1": 0.4628916987355689,
        "max_precision": 0.514041514041514,
        "max_recall": 0.421,
        "similarity_accuracy": 0.9915346534653465,
        "similarity_accuracy_threshold": 0.9277347326278687,
        "similarity_ap": 0.3678233337522207,
        "similarity_f1": 0.4,
        "similarity_f1_threshold": 0.9023947715759277,
        "similarity_precision": 0.4631578947368421,
        "similarity_recall": 0.352
      }
    ],
    "validation": [
      {
        "cosine_accuracy": 0.991029702970297,
        "cosine_accuracy_threshold": 0.9177981615066528,
        "cosine_ap": 0.2827783754550516,
        "cosine_f1": 0.33789411064842356,
        "cosine_f1_threshold": 0.8991340398788452,
        "cosine_precision": 0.4170337738619677,
        "cosine_recall": 0.284,
        "dot_accuracy": 0.9901980198019802,
        "dot_accuracy_threshold": 266.6142578125,
        "dot_ap": 0.12232203819576742,
        "dot_f1": 0.1975609756097561,
        "dot_f1_threshold": 236.13172912597656,
        "dot_precision": 0.16643835616438357,
        "dot_recall": 0.243,
        "euclidean_accuracy": 0.9909009900990099,
        "euclidean_accuracy_threshold": 6.2531514167785645,
        "euclidean_ap": 0.25031185130242284,
        "euclidean_f1": 0.30966364121729845,
        "euclidean_f1_threshold": 7.221502304077148,
        "euclidean_precision": 0.3321878579610538,
        "euclidean_recall": 0.29,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.3876445193178595,
        "manhattan_accuracy": 0.9916336633663366,
        "manhattan_accuracy_threshold": 125.79380798339844,
        "manhattan_ap": 0.3876445193178595,
        "manhattan_f1": 0.41805813234384664,
        "manhattan_f1_threshold": 136.42007446289062,
        "manhattan_precision": 0.5478119935170178,
        "manhattan_recall": 0.338,
        "max_accuracy": 0.9916336633663366,
        "max_ap": 0.3876445193178595,
        "max_f1": 0.41805813234384664,
        "max_precision": 0.5478119935170178,
        "max_recall": 0.338,
        "similarity_accuracy": 0.991029702970297,
        "similarity_accuracy_threshold": 0.9177981615066528,
        "similarity_ap": 0.2827783754550516,
        "similarity_f1": 0.33789411064842356,
        "similarity_f1_threshold": 0.8991340398788452,
        "similarity_precision": 0.4170337738619677,
        "similarity_recall": 0.284
      }
    ]
  },
  "task_name": "SprintDuplicateQuestions"
}