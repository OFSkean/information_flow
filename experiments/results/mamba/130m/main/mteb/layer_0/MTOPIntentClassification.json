{
  "dataset_revision": "ae001d0e6b1228650b7bd1c2c65fb50ad11a8aba",
  "evaluation_time": 49.542224407196045,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.6118331053351572,
        "f1": 0.43399999896484937,
        "f1_weighted": 0.6580625289881166,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.6118331053351572,
        "scores_per_experiment": [
          {
            "accuracy": 0.5991792065663475,
            "f1": 0.4316239940297716,
            "f1_weighted": 0.6434131096178531
          },
          {
            "accuracy": 0.6062471500227998,
            "f1": 0.41621671770694685,
            "f1_weighted": 0.6498810548940894
          },
          {
            "accuracy": 0.5996352029183767,
            "f1": 0.41814651469493436,
            "f1_weighted": 0.6452943354609233
          },
          {
            "accuracy": 0.6041951664386684,
            "f1": 0.4363364478368445,
            "f1_weighted": 0.6527133513049311
          },
          {
            "accuracy": 0.6121751025991792,
            "f1": 0.4362821087993208,
            "f1_weighted": 0.6601510972085982
          },
          {
            "accuracy": 0.6053351573187414,
            "f1": 0.434129780576415,
            "f1_weighted": 0.6510377823751732
          },
          {
            "accuracy": 0.6151390788873689,
            "f1": 0.43820867829845467,
            "f1_weighted": 0.6603611709793535
          },
          {
            "accuracy": 0.6415868673050615,
            "f1": 0.4576694597199046,
            "f1_weighted": 0.683237771317925
          },
          {
            "accuracy": 0.6235750113999088,
            "f1": 0.4427525610020797,
            "f1_weighted": 0.6747635091411766
          },
          {
            "accuracy": 0.6112631098951209,
            "f1": 0.42863372698382146,
            "f1_weighted": 0.6597721075811417
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.6133780760626398,
        "f1": 0.40644277665764905,
        "f1_weighted": 0.6603896505835672,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.6133780760626398,
        "scores_per_experiment": [
          {
            "accuracy": 0.5852348993288591,
            "f1": 0.3971354607938178,
            "f1_weighted": 0.633021023225388
          },
          {
            "accuracy": 0.6116331096196868,
            "f1": 0.40956921296009485,
            "f1_weighted": 0.6580758951067414
          },
          {
            "accuracy": 0.6026845637583893,
            "f1": 0.40887671973932693,
            "f1_weighted": 0.650596013111003
          },
          {
            "accuracy": 0.5986577181208054,
            "f1": 0.391806774937696,
            "f1_weighted": 0.6478842001216626
          },
          {
            "accuracy": 0.6277404921700224,
            "f1": 0.4202537968984203,
            "f1_weighted": 0.6769430417115687
          },
          {
            "accuracy": 0.6093959731543624,
            "f1": 0.3977500135733308,
            "f1_weighted": 0.654519857355004
          },
          {
            "accuracy": 0.6272930648769575,
            "f1": 0.42424749050327165,
            "f1_weighted": 0.6716770615742476
          },
          {
            "accuracy": 0.6340044742729306,
            "f1": 0.42320987838690427,
            "f1_weighted": 0.6777101542162638
          },
          {
            "accuracy": 0.614765100671141,
            "f1": 0.3942221495156797,
            "f1_weighted": 0.6669766700261579
          },
          {
            "accuracy": 0.6223713646532438,
            "f1": 0.39735626926794787,
            "f1_weighted": 0.6664925893876359
          }
        ]
      }
    ]
  },
  "task_name": "MTOPIntentClassification"
}