{
  "dataset_revision": "d66bd1f72af766a5cc4b0ca5e00c162f89e8cc46",
  "evaluation_time": 13.468166589736938,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "cosine_accuracy": 0.9911287128712871,
        "cosine_accuracy_threshold": 0.9573098421096802,
        "cosine_ap": 0.2796036629857156,
        "cosine_f1": 0.3331476323119777,
        "cosine_f1_threshold": 0.9404386281967163,
        "cosine_precision": 0.37610062893081764,
        "cosine_recall": 0.299,
        "dot_accuracy": 0.9900891089108911,
        "dot_accuracy_threshold": 429.04754638671875,
        "dot_ap": 0.018417330352875058,
        "dot_f1": 0.04420731707317072,
        "dot_f1_threshold": 310.59912109375,
        "dot_precision": 0.03571428571428571,
        "dot_recall": 0.058,
        "euclidean_accuracy": 0.9914158415841584,
        "euclidean_accuracy_threshold": 5.134075164794922,
        "euclidean_ap": 0.3171837556276851,
        "euclidean_f1": 0.37651563497128276,
        "euclidean_f1_threshold": 5.776250839233398,
        "euclidean_precision": 0.5202821869488536,
        "euclidean_recall": 0.295,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.4157061195873384,
        "manhattan_accuracy": 0.991960396039604,
        "manhattan_accuracy_threshold": 115.06883239746094,
        "manhattan_ap": 0.4157061195873384,
        "manhattan_f1": 0.45002992220227406,
        "manhattan_f1_threshold": 125.30530548095703,
        "manhattan_precision": 0.5603576751117735,
        "manhattan_recall": 0.376,
        "max_accuracy": 0.991960396039604,
        "max_ap": 0.4157061195873384,
        "max_f1": 0.45002992220227406,
        "max_precision": 0.5603576751117735,
        "max_recall": 0.376,
        "similarity_accuracy": 0.9911287128712871,
        "similarity_accuracy_threshold": 0.9573098421096802,
        "similarity_ap": 0.2796036629857156,
        "similarity_f1": 0.3331476323119777,
        "similarity_f1_threshold": 0.9404386281967163,
        "similarity_precision": 0.37610062893081764,
        "similarity_recall": 0.299
      }
    ],
    "validation": [
      {
        "cosine_accuracy": 0.9908415841584158,
        "cosine_accuracy_threshold": 0.9593085050582886,
        "cosine_ap": 0.2215367535986087,
        "cosine_f1": 0.2933333333333333,
        "cosine_f1_threshold": 0.9393177032470703,
        "cosine_precision": 0.33,
        "cosine_recall": 0.264,
        "dot_accuracy": 0.9900891089108911,
        "dot_accuracy_threshold": 414.79217529296875,
        "dot_ap": 0.018804463114093107,
        "dot_f1": 0.04701002419633599,
        "dot_f1_threshold": 293.5093078613281,
        "dot_precision": 0.028416213957375678,
        "dot_recall": 0.136,
        "euclidean_accuracy": 0.990970297029703,
        "euclidean_accuracy_threshold": 5.214024066925049,
        "euclidean_ap": 0.25025201593231994,
        "euclidean_f1": 0.3109540636042403,
        "euclidean_f1_threshold": 5.986973762512207,
        "euclidean_precision": 0.37822349570200575,
        "euclidean_recall": 0.264,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.3451432783158943,
        "manhattan_accuracy": 0.9913861386138614,
        "manhattan_accuracy_threshold": 117.37274169921875,
        "manhattan_ap": 0.3451432783158943,
        "manhattan_f1": 0.4011299435028248,
        "manhattan_f1_threshold": 128.71295166015625,
        "manhattan_precision": 0.461038961038961,
        "manhattan_recall": 0.355,
        "max_accuracy": 0.9913861386138614,
        "max_ap": 0.3451432783158943,
        "max_f1": 0.4011299435028248,
        "max_precision": 0.461038961038961,
        "max_recall": 0.355,
        "similarity_accuracy": 0.9908415841584158,
        "similarity_accuracy_threshold": 0.9593085050582886,
        "similarity_ap": 0.2215367535986087,
        "similarity_f1": 0.2933333333333333,
        "similarity_f1_threshold": 0.9393177032470703,
        "similarity_precision": 0.33,
        "similarity_recall": 0.264
      }
    ]
  },
  "task_name": "SprintDuplicateQuestions"
}