{
  "dataset_revision": "4672e20407010da34463acc759c162ca9734bca6",
  "evaluation_time": 32.35080432891846,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.5291862811028917,
        "f1": 0.5110186438232579,
        "f1_weighted": 0.5322190356045631,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5291862811028917,
        "scores_per_experiment": [
          {
            "accuracy": 0.5289172831203766,
            "f1": 0.5165476273654563,
            "f1_weighted": 0.5323415701730547
          },
          {
            "accuracy": 0.5517821116341628,
            "f1": 0.5250448842992631,
            "f1_weighted": 0.5558199093732155
          },
          {
            "accuracy": 0.5332885003362475,
            "f1": 0.5100432433878191,
            "f1_weighted": 0.5322150265525634
          },
          {
            "accuracy": 0.5494283792871554,
            "f1": 0.5220049227274852,
            "f1_weighted": 0.5550433225468376
          },
          {
            "accuracy": 0.5437121721587088,
            "f1": 0.5167942300044158,
            "f1_weighted": 0.5441725229159738
          },
          {
            "accuracy": 0.503698722259583,
            "f1": 0.49089527493504154,
            "f1_weighted": 0.5052484618223114
          },
          {
            "accuracy": 0.5137861466039004,
            "f1": 0.5138463883984091,
            "f1_weighted": 0.5173771265287903
          },
          {
            "accuracy": 0.5141223940820444,
            "f1": 0.49589754376213485,
            "f1_weighted": 0.5217107098974191
          },
          {
            "accuracy": 0.5211835911230666,
            "f1": 0.5065390411936372,
            "f1_weighted": 0.5189639533969912
          },
          {
            "accuracy": 0.5319435104236718,
            "f1": 0.5125732821589174,
            "f1_weighted": 0.5392977528384741
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.5376291195277915,
        "f1": 0.5240830575010955,
        "f1_weighted": 0.5395633574311887,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5376291195277915,
        "scores_per_experiment": [
          {
            "accuracy": 0.5263157894736842,
            "f1": 0.5270745023240978,
            "f1_weighted": 0.5283102681858981
          },
          {
            "accuracy": 0.559272011805214,
            "f1": 0.5326392758767357,
            "f1_weighted": 0.5588859802791455
          },
          {
            "accuracy": 0.5597638957206099,
            "f1": 0.5350597383539862,
            "f1_weighted": 0.5618846709959777
          },
          {
            "accuracy": 0.5454992621741269,
            "f1": 0.5196544307891663,
            "f1_weighted": 0.5468416285826773
          },
          {
            "accuracy": 0.5548450565666503,
            "f1": 0.5390864605346987,
            "f1_weighted": 0.558957985709096
          },
          {
            "accuracy": 0.5248401377274963,
            "f1": 0.5218393536074758,
            "f1_weighted": 0.5285778185060573
          },
          {
            "accuracy": 0.5130349237579931,
            "f1": 0.5112897284257704,
            "f1_weighted": 0.5130826226364893
          },
          {
            "accuracy": 0.5189375307427447,
            "f1": 0.503395058126724,
            "f1_weighted": 0.5203002340225192
          },
          {
            "accuracy": 0.5336940482046237,
            "f1": 0.523085352570232,
            "f1_weighted": 0.5356257553734192
          },
          {
            "accuracy": 0.5400885391047713,
            "f1": 0.5277066744020689,
            "f1_weighted": 0.5431666100206064
          }
        ]
      }
    ]
  },
  "task_name": "MassiveIntentClassification"
}