{
  "dataset_revision": "fad2c6e8459f9e1c45d9315f4953d921437d70f8",
  "evaluation_time": 23.085479736328125,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.5406186953597847,
        "f1": 0.5263300244358969,
        "f1_weighted": 0.5450192086286888,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5406186953597847,
        "scores_per_experiment": [
          {
            "accuracy": 0.5699394754539341,
            "f1": 0.5559995893234455,
            "f1_weighted": 0.5761851394423192
          },
          {
            "accuracy": 0.554808338937458,
            "f1": 0.5333014631743073,
            "f1_weighted": 0.5626520909311533
          },
          {
            "accuracy": 0.5322797579018157,
            "f1": 0.5159189797577959,
            "f1_weighted": 0.5310346778242967
          },
          {
            "accuracy": 0.5228648285137861,
            "f1": 0.5111008363335902,
            "f1_weighted": 0.5264002907919698
          },
          {
            "accuracy": 0.5299260255548084,
            "f1": 0.5124561716301178,
            "f1_weighted": 0.5324696850975499
          },
          {
            "accuracy": 0.5437121721587088,
            "f1": 0.5261282236757049,
            "f1_weighted": 0.550988619421871
          },
          {
            "accuracy": 0.5326160053799597,
            "f1": 0.5203756250446214,
            "f1_weighted": 0.5394876770999121
          },
          {
            "accuracy": 0.5645595158036315,
            "f1": 0.5462067494759835,
            "f1_weighted": 0.5635322889128515
          },
          {
            "accuracy": 0.5228648285137861,
            "f1": 0.517027216502875,
            "f1_weighted": 0.529253482273514
          },
          {
            "accuracy": 0.5326160053799597,
            "f1": 0.5247853894405287,
            "f1_weighted": 0.5381881344914512
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.5315297589768815,
        "f1": 0.5264481463154912,
        "f1_weighted": 0.5340346964305975,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5315297589768815,
        "scores_per_experiment": [
          {
            "accuracy": 0.5612395474667978,
            "f1": 0.550993296716872,
            "f1_weighted": 0.5680016539691746
          },
          {
            "accuracy": 0.5489424495818986,
            "f1": 0.540426431376049,
            "f1_weighted": 0.5535030715302964
          },
          {
            "accuracy": 0.5327102803738317,
            "f1": 0.5266778403764628,
            "f1_weighted": 0.5338318411181379
          },
          {
            "accuracy": 0.5017215937038859,
            "f1": 0.4970898956893304,
            "f1_weighted": 0.5005138426865768
          },
          {
            "accuracy": 0.5277914412198721,
            "f1": 0.5245091462947414,
            "f1_weighted": 0.5288798705152146
          },
          {
            "accuracy": 0.52926709296606,
            "f1": 0.5288456437646321,
            "f1_weighted": 0.5334760633234674
          },
          {
            "accuracy": 0.5179537629119528,
            "f1": 0.5117831806841714,
            "f1_weighted": 0.5242036809170593
          },
          {
            "accuracy": 0.5400885391047713,
            "f1": 0.5299535964121163,
            "f1_weighted": 0.5358952217552699
          },
          {
            "accuracy": 0.5238563698967044,
            "f1": 0.5247732229299992,
            "f1_weighted": 0.5259897880429664
          },
          {
            "accuracy": 0.5317265125430398,
            "f1": 0.5294292089105369,
            "f1_weighted": 0.5360519304478124
          }
        ]
      }
    ]
  },
  "task_name": "MassiveScenarioClassification"
}