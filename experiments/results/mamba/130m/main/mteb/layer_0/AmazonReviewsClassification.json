{
  "dataset_revision": "1399c76144fd37290681b995c656ef9b2e06e26d",
  "evaluation_time": 28.979610443115234,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.27914,
        "f1": 0.27609741679389277,
        "f1_weighted": 0.27609741679389277,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.27914,
        "scores_per_experiment": [
          {
            "accuracy": 0.3054,
            "f1": 0.30366134909918335,
            "f1_weighted": 0.3036613490991833
          },
          {
            "accuracy": 0.2962,
            "f1": 0.2958065448038355,
            "f1_weighted": 0.29580654480383556
          },
          {
            "accuracy": 0.276,
            "f1": 0.27246078678816527,
            "f1_weighted": 0.2724607867881652
          },
          {
            "accuracy": 0.2678,
            "f1": 0.27043607063433966,
            "f1_weighted": 0.27043607063433966
          },
          {
            "accuracy": 0.3034,
            "f1": 0.29353007320880364,
            "f1_weighted": 0.2935300732088037
          },
          {
            "accuracy": 0.2502,
            "f1": 0.2493051435958996,
            "f1_weighted": 0.24930514359589961
          },
          {
            "accuracy": 0.2402,
            "f1": 0.23695640392581718,
            "f1_weighted": 0.23695640392581718
          },
          {
            "accuracy": 0.294,
            "f1": 0.28822536209981664,
            "f1_weighted": 0.28822536209981664
          },
          {
            "accuracy": 0.275,
            "f1": 0.2724091740681713,
            "f1_weighted": 0.2724091740681713
          },
          {
            "accuracy": 0.2832,
            "f1": 0.27818325971489566,
            "f1_weighted": 0.27818325971489566
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.2787800000000001,
        "f1": 0.2753568968438421,
        "f1_weighted": 0.2753568968438421,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.2787800000000001,
        "scores_per_experiment": [
          {
            "accuracy": 0.3018,
            "f1": 0.29976270917998127,
            "f1_weighted": 0.29976270917998127
          },
          {
            "accuracy": 0.2962,
            "f1": 0.29708295719825706,
            "f1_weighted": 0.297082957198257
          },
          {
            "accuracy": 0.2692,
            "f1": 0.26581823681377525,
            "f1_weighted": 0.26581823681377525
          },
          {
            "accuracy": 0.267,
            "f1": 0.2688272321000239,
            "f1_weighted": 0.2688272321000239
          },
          {
            "accuracy": 0.3066,
            "f1": 0.29525202716081145,
            "f1_weighted": 0.2952520271608115
          },
          {
            "accuracy": 0.2536,
            "f1": 0.25286037372868997,
            "f1_weighted": 0.25286037372868997
          },
          {
            "accuracy": 0.247,
            "f1": 0.24221093210722416,
            "f1_weighted": 0.2422109321072242
          },
          {
            "accuracy": 0.2882,
            "f1": 0.2821412352850379,
            "f1_weighted": 0.28214123528503793
          },
          {
            "accuracy": 0.2686,
            "f1": 0.26599320922355074,
            "f1_weighted": 0.2659932092235508
          },
          {
            "accuracy": 0.2896,
            "f1": 0.2836200556410695,
            "f1_weighted": 0.28362005564106946
          }
        ]
      }
    ]
  },
  "task_name": "AmazonReviewsClassification"
}