{
  "dataset_revision": "fad2c6e8459f9e1c45d9315f4953d921437d70f8",
  "evaluation_time": 33.44073176383972,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.5959314055144587,
        "f1": 0.5767844462891126,
        "f1_weighted": 0.5975482054518503,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5959314055144587,
        "scores_per_experiment": [
          {
            "accuracy": 0.6139878950907868,
            "f1": 0.5944849791624535,
            "f1_weighted": 0.6137410187531582
          },
          {
            "accuracy": 0.6217215870880969,
            "f1": 0.6005281435091169,
            "f1_weighted": 0.6256997471459904
          },
          {
            "accuracy": 0.5901143241425689,
            "f1": 0.582623678314393,
            "f1_weighted": 0.5913968439533525
          },
          {
            "accuracy": 0.5813718897108272,
            "f1": 0.5597321369601153,
            "f1_weighted": 0.5849469813441648
          },
          {
            "accuracy": 0.5874243443174176,
            "f1": 0.5651207054667922,
            "f1_weighted": 0.5879564370772897
          },
          {
            "accuracy": 0.5608607935440484,
            "f1": 0.5424267918292661,
            "f1_weighted": 0.5638148105603032
          },
          {
            "accuracy": 0.6039004707464694,
            "f1": 0.5842982479856961,
            "f1_weighted": 0.60680985561909
          },
          {
            "accuracy": 0.6096166778749159,
            "f1": 0.5788082791984321,
            "f1_weighted": 0.6089465586688222
          },
          {
            "accuracy": 0.5911230665770006,
            "f1": 0.5776327853199124,
            "f1_weighted": 0.5921916990916861
          },
          {
            "accuracy": 0.5991930060524546,
            "f1": 0.5821887151449493,
            "f1_weighted": 0.5999781023046462
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.5925233644859812,
        "f1": 0.5791116356502987,
        "f1_weighted": 0.593370536425323,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5925233644859812,
        "scores_per_experiment": [
          {
            "accuracy": 0.5996064928676832,
            "f1": 0.5830968667036566,
            "f1_weighted": 0.6017315327772826
          },
          {
            "accuracy": 0.6094441711756026,
            "f1": 0.5965901283132343,
            "f1_weighted": 0.6126306122569222
          },
          {
            "accuracy": 0.5892769306443679,
            "f1": 0.5870000816715562,
            "f1_weighted": 0.5899073158825616
          },
          {
            "accuracy": 0.5597638957206099,
            "f1": 0.5422282727920587,
            "f1_weighted": 0.5620137289884068
          },
          {
            "accuracy": 0.6045253320216429,
            "f1": 0.5902332419830889,
            "f1_weighted": 0.6037714246940995
          },
          {
            "accuracy": 0.5671421544515495,
            "f1": 0.5620017520445811,
            "f1_weighted": 0.564529103015512
          },
          {
            "accuracy": 0.5833743236596163,
            "f1": 0.5724008120139338,
            "f1_weighted": 0.5887076083754013
          },
          {
            "accuracy": 0.6148548942449582,
            "f1": 0.5825575968636678,
            "f1_weighted": 0.6116874810266787
          },
          {
            "accuracy": 0.5897688145597639,
            "f1": 0.5795890116517832,
            "f1_weighted": 0.590947973445483
          },
          {
            "accuracy": 0.6074766355140186,
            "f1": 0.5954185924654265,
            "f1_weighted": 0.6077785837908825
          }
        ]
      }
    ]
  },
  "task_name": "MassiveScenarioClassification"
}