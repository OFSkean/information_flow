{
  "dataset_revision": "ae001d0e6b1228650b7bd1c2c65fb50ad11a8aba",
  "evaluation_time": 71.58283019065857,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.6699498404012768,
        "f1": 0.4655816556113064,
        "f1_weighted": 0.7112617154956682,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.6699498404012768,
        "scores_per_experiment": [
          {
            "accuracy": 0.6488828089375285,
            "f1": 0.46260564124352194,
            "f1_weighted": 0.6873244881140392
          },
          {
            "accuracy": 0.6739626082991336,
            "f1": 0.4549755731008563,
            "f1_weighted": 0.7137150986239359
          },
          {
            "accuracy": 0.6470588235294118,
            "f1": 0.44177831610975626,
            "f1_weighted": 0.6882035078310661
          },
          {
            "accuracy": 0.6933424532603739,
            "f1": 0.47709117120743944,
            "f1_weighted": 0.7349002786229212
          },
          {
            "accuracy": 0.6595987232102143,
            "f1": 0.4691527005847175,
            "f1_weighted": 0.7031760695972511
          },
          {
            "accuracy": 0.6584587323301414,
            "f1": 0.45553822503647073,
            "f1_weighted": 0.7046463284254431
          },
          {
            "accuracy": 0.6650706794345645,
            "f1": 0.47220564180276936,
            "f1_weighted": 0.7079150637435467
          },
          {
            "accuracy": 0.6874145006839946,
            "f1": 0.4751955694795147,
            "f1_weighted": 0.7252180102305252
          },
          {
            "accuracy": 0.6874145006839946,
            "f1": 0.4770604958246004,
            "f1_weighted": 0.7268088523403966
          },
          {
            "accuracy": 0.6782945736434108,
            "f1": 0.4702132217234166,
            "f1_weighted": 0.7207094574275567
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.6760178970917227,
        "f1": 0.4602082035255175,
        "f1_weighted": 0.7191974022749783,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.6760178970917227,
        "scores_per_experiment": [
          {
            "accuracy": 0.6434004474272931,
            "f1": 0.4463787832983876,
            "f1_weighted": 0.6828351651581782
          },
          {
            "accuracy": 0.6760626398210291,
            "f1": 0.4468255478623875,
            "f1_weighted": 0.7185795454351322
          },
          {
            "accuracy": 0.661744966442953,
            "f1": 0.4315091353026368,
            "f1_weighted": 0.7034675720036175
          },
          {
            "accuracy": 0.6903803131991052,
            "f1": 0.46206080889625556,
            "f1_weighted": 0.7318681402679593
          },
          {
            "accuracy": 0.668903803131991,
            "f1": 0.4788253783062173,
            "f1_weighted": 0.7160968481649627
          },
          {
            "accuracy": 0.6644295302013423,
            "f1": 0.45384866195363605,
            "f1_weighted": 0.7150042861646392
          },
          {
            "accuracy": 0.669351230425056,
            "f1": 0.45468726991604586,
            "f1_weighted": 0.7141974166614654
          },
          {
            "accuracy": 0.698434004474273,
            "f1": 0.47863254147762807,
            "f1_weighted": 0.7396215774819166
          },
          {
            "accuracy": 0.6903803131991052,
            "f1": 0.4696645980819927,
            "f1_weighted": 0.7325774074574445
          },
          {
            "accuracy": 0.6970917225950783,
            "f1": 0.4796493101599875,
            "f1_weighted": 0.7377260639544668
          }
        ]
      }
    ]
  },
  "task_name": "MTOPIntentClassification"
}