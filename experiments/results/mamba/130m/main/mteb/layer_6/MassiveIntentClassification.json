{
  "dataset_revision": "4672e20407010da34463acc759c162ca9734bca6",
  "evaluation_time": 42.97225022315979,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.5612642905178211,
        "f1": 0.5363862417647663,
        "f1_weighted": 0.563304851035432,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5612642905178211,
        "scores_per_experiment": [
          {
            "accuracy": 0.558843308675185,
            "f1": 0.5552407588966607,
            "f1_weighted": 0.5602589282932251
          },
          {
            "accuracy": 0.5679219905850706,
            "f1": 0.542046890398895,
            "f1_weighted": 0.5716921223837425
          },
          {
            "accuracy": 0.5591795561533288,
            "f1": 0.5279657584974953,
            "f1_weighted": 0.5573689915070387
          },
          {
            "accuracy": 0.5803631472763954,
            "f1": 0.5416853815624366,
            "f1_weighted": 0.5792648124474717
          },
          {
            "accuracy": 0.5726294552790854,
            "f1": 0.5395822543309107,
            "f1_weighted": 0.5723635412139121
          },
          {
            "accuracy": 0.5379959650302623,
            "f1": 0.5251972919629884,
            "f1_weighted": 0.5433967754479551
          },
          {
            "accuracy": 0.5477471418964358,
            "f1": 0.5331057294487203,
            "f1_weighted": 0.5470801791801643
          },
          {
            "accuracy": 0.5517821116341628,
            "f1": 0.5185673938667036,
            "f1_weighted": 0.560793857447429
          },
          {
            "accuracy": 0.566577000672495,
            "f1": 0.5390902347772267,
            "f1_weighted": 0.5689956417176953
          },
          {
            "accuracy": 0.5696032279757902,
            "f1": 0.541380723905625,
            "f1_weighted": 0.5718336607156873
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.571224790949336,
        "f1": 0.5482450227635869,
        "f1_weighted": 0.5724834700319199,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.571224790949336,
        "scores_per_experiment": [
          {
            "accuracy": 0.5543531726512543,
            "f1": 0.5375628400557801,
            "f1_weighted": 0.5529491596762919
          },
          {
            "accuracy": 0.5897688145597639,
            "f1": 0.560830172135399,
            "f1_weighted": 0.5915056760188072
          },
          {
            "accuracy": 0.5937038858829317,
            "f1": 0.5648545048209951,
            "f1_weighted": 0.592921556667838
          },
          {
            "accuracy": 0.573044761436301,
            "f1": 0.5357156788343449,
            "f1_weighted": 0.572851052706841
          },
          {
            "accuracy": 0.5863256271519921,
            "f1": 0.5658312422308343,
            "f1_weighted": 0.5894051945175551
          },
          {
            "accuracy": 0.5528775209050664,
            "f1": 0.5380073986165077,
            "f1_weighted": 0.5553975770670935
          },
          {
            "accuracy": 0.544023610427939,
            "f1": 0.5392976067285624,
            "f1_weighted": 0.5427275557778376
          },
          {
            "accuracy": 0.5622233152975897,
            "f1": 0.5251017766473406,
            "f1_weighted": 0.5648012483498864
          },
          {
            "accuracy": 0.5715691096901131,
            "f1": 0.5492037236964875,
            "f1_weighted": 0.5750445094241481
          },
          {
            "accuracy": 0.5843580914904083,
            "f1": 0.5660452838696169,
            "f1_weighted": 0.5872311701128995
          }
        ]
      }
    ]
  },
  "task_name": "MassiveIntentClassification"
}