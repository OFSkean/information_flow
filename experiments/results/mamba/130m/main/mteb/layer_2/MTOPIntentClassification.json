{
  "dataset_revision": "ae001d0e6b1228650b7bd1c2c65fb50ad11a8aba",
  "evaluation_time": 57.55660343170166,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.6545599635202917,
        "f1": 0.4694581970941125,
        "f1_weighted": 0.6966183288561542,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.6545599635202917,
        "scores_per_experiment": [
          {
            "accuracy": 0.6413588691290469,
            "f1": 0.4605426384968395,
            "f1_weighted": 0.6817007751927192
          },
          {
            "accuracy": 0.658686730506156,
            "f1": 0.4821423250447571,
            "f1_weighted": 0.6994279406531781
          },
          {
            "accuracy": 0.6431828545371637,
            "f1": 0.45728027787466524,
            "f1_weighted": 0.6878207926118035
          },
          {
            "accuracy": 0.658686730506156,
            "f1": 0.4793501775902972,
            "f1_weighted": 0.7031527165207054
          },
          {
            "accuracy": 0.6491108071135431,
            "f1": 0.4696812006429153,
            "f1_weighted": 0.6915387584493502
          },
          {
            "accuracy": 0.6495668034655723,
            "f1": 0.45774406603600676,
            "f1_weighted": 0.6914147791332869
          },
          {
            "accuracy": 0.6511627906976745,
            "f1": 0.47585967193246026,
            "f1_weighted": 0.6908487141243865
          },
          {
            "accuracy": 0.6903784769721842,
            "f1": 0.49732864703311336,
            "f1_weighted": 0.7273025405049002
          },
          {
            "accuracy": 0.6507067943456453,
            "f1": 0.44695784998209837,
            "f1_weighted": 0.6979552332706273
          },
          {
            "accuracy": 0.6527587779297765,
            "f1": 0.4676951163079725,
            "f1_weighted": 0.6950210381005844
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.6521252796420582,
        "f1": 0.4455106352799597,
        "f1_weighted": 0.6952642347361183,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.6521252796420582,
        "scores_per_experiment": [
          {
            "accuracy": 0.6353467561521253,
            "f1": 0.4324996485905961,
            "f1_weighted": 0.6787383207908285
          },
          {
            "accuracy": 0.6411633109619687,
            "f1": 0.43514761346936487,
            "f1_weighted": 0.6828857898639181
          },
          {
            "accuracy": 0.6478747203579418,
            "f1": 0.43483502792596784,
            "f1_weighted": 0.6926580713004659
          },
          {
            "accuracy": 0.6496644295302013,
            "f1": 0.44850387925420954,
            "f1_weighted": 0.6936338181804083
          },
          {
            "accuracy": 0.6604026845637584,
            "f1": 0.45413409093511653,
            "f1_weighted": 0.7053088198576324
          },
          {
            "accuracy": 0.6469798657718121,
            "f1": 0.4433807709654331,
            "f1_weighted": 0.690672389171028
          },
          {
            "accuracy": 0.6483221476510067,
            "f1": 0.45387287146185384,
            "f1_weighted": 0.6891613204802282
          },
          {
            "accuracy": 0.6814317673378076,
            "f1": 0.46777281716938196,
            "f1_weighted": 0.7202149711336941
          },
          {
            "accuracy": 0.6411633109619687,
            "f1": 0.42209190439939526,
            "f1_weighted": 0.6887448314607567
          },
          {
            "accuracy": 0.668903803131991,
            "f1": 0.4628677286282782,
            "f1_weighted": 0.7106240151222226
          }
        ]
      }
    ]
  },
  "task_name": "MTOPIntentClassification"
}