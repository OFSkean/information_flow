{
  "dataset_revision": "1399c76144fd37290681b995c656ef9b2e06e26d",
  "evaluation_time": 183.35623931884766,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.31300000000000006,
        "f1": 0.3100014371353982,
        "f1_weighted": 0.3100014371353982,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.31300000000000006,
        "scores_per_experiment": [
          {
            "accuracy": 0.3476,
            "f1": 0.345611402095945,
            "f1_weighted": 0.345611402095945
          },
          {
            "accuracy": 0.3416,
            "f1": 0.33718278396665646,
            "f1_weighted": 0.33718278396665646
          },
          {
            "accuracy": 0.3012,
            "f1": 0.29972977139627993,
            "f1_weighted": 0.29972977139627993
          },
          {
            "accuracy": 0.2942,
            "f1": 0.29403589339613745,
            "f1_weighted": 0.29403589339613745
          },
          {
            "accuracy": 0.3502,
            "f1": 0.33812994436439336,
            "f1_weighted": 0.33812994436439336
          },
          {
            "accuracy": 0.29,
            "f1": 0.2856298124008064,
            "f1_weighted": 0.28562981240080637
          },
          {
            "accuracy": 0.2692,
            "f1": 0.266022029968752,
            "f1_weighted": 0.266022029968752
          },
          {
            "accuracy": 0.3272,
            "f1": 0.3264520463100088,
            "f1_weighted": 0.32645204631000885
          },
          {
            "accuracy": 0.2892,
            "f1": 0.291758052270526,
            "f1_weighted": 0.291758052270526
          },
          {
            "accuracy": 0.3196,
            "f1": 0.31546263518447687,
            "f1_weighted": 0.3154626351844769
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.31252,
        "f1": 0.30746578047634887,
        "f1_weighted": 0.30746578047634887,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.31252,
        "scores_per_experiment": [
          {
            "accuracy": 0.3328,
            "f1": 0.3271385576371283,
            "f1_weighted": 0.3271385576371283
          },
          {
            "accuracy": 0.3374,
            "f1": 0.33196152721229705,
            "f1_weighted": 0.331961527212297
          },
          {
            "accuracy": 0.2992,
            "f1": 0.2967633545975,
            "f1_weighted": 0.2967633545974999
          },
          {
            "accuracy": 0.2898,
            "f1": 0.2866951088731902,
            "f1_weighted": 0.2866951088731902
          },
          {
            "accuracy": 0.3588,
            "f1": 0.3401153501666808,
            "f1_weighted": 0.34011535016668076
          },
          {
            "accuracy": 0.3038,
            "f1": 0.2984349289183351,
            "f1_weighted": 0.2984349289183351
          },
          {
            "accuracy": 0.2754,
            "f1": 0.27126810935321394,
            "f1_weighted": 0.27126810935321394
          },
          {
            "accuracy": 0.3064,
            "f1": 0.3056769604854381,
            "f1_weighted": 0.3056769604854381
          },
          {
            "accuracy": 0.2942,
            "f1": 0.294820937630747,
            "f1_weighted": 0.29482093763074707
          },
          {
            "accuracy": 0.3274,
            "f1": 0.32178296988895877,
            "f1_weighted": 0.3217829698889588
          }
        ]
      }
    ]
  },
  "task_name": "AmazonReviewsClassification"
}