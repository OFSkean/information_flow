{
  "dataset_revision": "ae001d0e6b1228650b7bd1c2c65fb50ad11a8aba",
  "evaluation_time": 59.98034310340881,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.6659142726858185,
        "f1": 0.4658232596860573,
        "f1_weighted": 0.707728135364724,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.6659142726858185,
        "scores_per_experiment": [
          {
            "accuracy": 0.6511627906976745,
            "f1": 0.4719828281665263,
            "f1_weighted": 0.691052399378327
          },
          {
            "accuracy": 0.6762425900592796,
            "f1": 0.46949455384503297,
            "f1_weighted": 0.7136168715577798
          },
          {
            "accuracy": 0.6557227542179662,
            "f1": 0.45421487192914894,
            "f1_weighted": 0.6987511542462088
          },
          {
            "accuracy": 0.6762425900592796,
            "f1": 0.4738084467502276,
            "f1_weighted": 0.7187306615906514
          },
          {
            "accuracy": 0.6582307341541268,
            "f1": 0.46278814721370215,
            "f1_weighted": 0.7030715457377763
          },
          {
            "accuracy": 0.6554947560419516,
            "f1": 0.44345663712606076,
            "f1_weighted": 0.7012438195123597
          },
          {
            "accuracy": 0.6641586867305062,
            "f1": 0.47131272044506367,
            "f1_weighted": 0.704492748832585
          },
          {
            "accuracy": 0.6922024623803009,
            "f1": 0.4924783488220736,
            "f1_weighted": 0.7308835664660775
          },
          {
            "accuracy": 0.6716826265389877,
            "f1": 0.4488344284024081,
            "f1_weighted": 0.7132847570414069
          },
          {
            "accuracy": 0.6580027359781122,
            "f1": 0.46986161416032823,
            "f1_weighted": 0.7021538292840676
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.6651901565995526,
        "f1": 0.4552032984911721,
        "f1_weighted": 0.7087059065717107,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.6651901565995526,
        "scores_per_experiment": [
          {
            "accuracy": 0.6456375838926175,
            "f1": 0.44608522176493487,
            "f1_weighted": 0.6867917989232745
          },
          {
            "accuracy": 0.6639821029082774,
            "f1": 0.442425363696995,
            "f1_weighted": 0.7025982231920639
          },
          {
            "accuracy": 0.6554809843400448,
            "f1": 0.4408377157090639,
            "f1_weighted": 0.6984162389375271
          },
          {
            "accuracy": 0.6697986577181209,
            "f1": 0.45652192026091293,
            "f1_weighted": 0.7146961717077378
          },
          {
            "accuracy": 0.6648769574944071,
            "f1": 0.47359173625575685,
            "f1_weighted": 0.713365248894487
          },
          {
            "accuracy": 0.6635346756152125,
            "f1": 0.4469450296016937,
            "f1_weighted": 0.7115536878358124
          },
          {
            "accuracy": 0.6595078299776287,
            "f1": 0.46557625767902344,
            "f1_weighted": 0.7030661744968608
          },
          {
            "accuracy": 0.6885906040268457,
            "f1": 0.475467173401489,
            "f1_weighted": 0.7277055466612807
          },
          {
            "accuracy": 0.665324384787472,
            "f1": 0.43746744217764805,
            "f1_weighted": 0.7113905061893264
          },
          {
            "accuracy": 0.6751677852348993,
            "f1": 0.46711512436420294,
            "f1_weighted": 0.7174754688787358
          }
        ]
      }
    ]
  },
  "task_name": "MTOPIntentClassification"
}