{
  "dataset_revision": "4672e20407010da34463acc759c162ca9734bca6",
  "evaluation_time": 39.61275506019592,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.565803631472764,
        "f1": 0.542400255038112,
        "f1_weighted": 0.5670282661643418,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.565803631472764,
        "scores_per_experiment": [
          {
            "accuracy": 0.558843308675185,
            "f1": 0.5462498279026845,
            "f1_weighted": 0.5610981281453878
          },
          {
            "accuracy": 0.5749831876260928,
            "f1": 0.5563007122576411,
            "f1_weighted": 0.5790624006831108
          },
          {
            "accuracy": 0.5672494956287828,
            "f1": 0.5344877319367981,
            "f1_weighted": 0.5626137149113399
          },
          {
            "accuracy": 0.5830531271015468,
            "f1": 0.5436117827414865,
            "f1_weighted": 0.582049992177312
          },
          {
            "accuracy": 0.5830531271015468,
            "f1": 0.5504694920532661,
            "f1_weighted": 0.5827596277811538
          },
          {
            "accuracy": 0.551109616677875,
            "f1": 0.5312397042727001,
            "f1_weighted": 0.5566230203883309
          },
          {
            "accuracy": 0.5544720914593141,
            "f1": 0.5435783690351506,
            "f1_weighted": 0.5570451107550721
          },
          {
            "accuracy": 0.551109616677875,
            "f1": 0.5302589376008321,
            "f1_weighted": 0.5545441845996771
          },
          {
            "accuracy": 0.5635507733691997,
            "f1": 0.538797391361049,
            "f1_weighted": 0.5639022536442634
          },
          {
            "accuracy": 0.5706119704102219,
            "f1": 0.5490086012195117,
            "f1_weighted": 0.5705842285577699
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.5740285292670931,
        "f1": 0.5511120873961566,
        "f1_weighted": 0.5756042509891187,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5740285292670931,
        "scores_per_experiment": [
          {
            "accuracy": 0.5612395474667978,
            "f1": 0.5484230983887788,
            "f1_weighted": 0.5584668998778385
          },
          {
            "accuracy": 0.5828824397442204,
            "f1": 0.5543294210783029,
            "f1_weighted": 0.5871597400364045
          },
          {
            "accuracy": 0.5966551893753075,
            "f1": 0.5676806921206328,
            "f1_weighted": 0.5985509594580457
          },
          {
            "accuracy": 0.5784554845056566,
            "f1": 0.5412473808474271,
            "f1_weighted": 0.5785410217611298
          },
          {
            "accuracy": 0.5951795376291196,
            "f1": 0.5755244591875095,
            "f1_weighted": 0.5963688522919637
          },
          {
            "accuracy": 0.5676340383669454,
            "f1": 0.5571117655894271,
            "f1_weighted": 0.5732812809047239
          },
          {
            "accuracy": 0.5474667978357107,
            "f1": 0.5390243938711876,
            "f1_weighted": 0.5497945673010436
          },
          {
            "accuracy": 0.5632070831283817,
            "f1": 0.5282921309402321,
            "f1_weighted": 0.5626009520405755
          },
          {
            "accuracy": 0.5656665027053616,
            "f1": 0.5471378492643252,
            "f1_weighted": 0.5687557342965807
          },
          {
            "accuracy": 0.5818986719134285,
            "f1": 0.5523496826737433,
            "f1_weighted": 0.5825225019228816
          }
        ]
      }
    ]
  },
  "task_name": "MassiveIntentClassification"
}