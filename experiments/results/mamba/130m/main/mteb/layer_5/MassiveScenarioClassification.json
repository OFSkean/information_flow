{
  "dataset_revision": "fad2c6e8459f9e1c45d9315f4953d921437d70f8",
  "evaluation_time": 31.26899552345276,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.5959986550100874,
        "f1": 0.5810844922939774,
        "f1_weighted": 0.5971409290923854,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5959986550100874,
        "scores_per_experiment": [
          {
            "accuracy": 0.6176866173503699,
            "f1": 0.6010121175860743,
            "f1_weighted": 0.6175176430978265
          },
          {
            "accuracy": 0.5971755211835911,
            "f1": 0.5778092998820517,
            "f1_weighted": 0.601275075601405
          },
          {
            "accuracy": 0.5847343644922663,
            "f1": 0.5807595308368287,
            "f1_weighted": 0.5834962808844456
          },
          {
            "accuracy": 0.5978480161398789,
            "f1": 0.5823279866908848,
            "f1_weighted": 0.6015057592241876
          },
          {
            "accuracy": 0.5780094149293881,
            "f1": 0.565517066401379,
            "f1_weighted": 0.5780427820803946
          },
          {
            "accuracy": 0.5749831876260928,
            "f1": 0.5568671928431838,
            "f1_weighted": 0.5798768581627087
          },
          {
            "accuracy": 0.6018829858776059,
            "f1": 0.5882772863294505,
            "f1_weighted": 0.6050544162324146
          },
          {
            "accuracy": 0.6240753194351042,
            "f1": 0.5977702715527908,
            "f1_weighted": 0.6232337718561579
          },
          {
            "accuracy": 0.5840618695359785,
            "f1": 0.5754948928099698,
            "f1_weighted": 0.5824164856831914
          },
          {
            "accuracy": 0.5995292535305985,
            "f1": 0.5850092780071611,
            "f1_weighted": 0.5989902181011207
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.5910477127397934,
        "f1": 0.5809339041920282,
        "f1_weighted": 0.5915646889718217,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5910477127397934,
        "scores_per_experiment": [
          {
            "accuracy": 0.6153467781603541,
            "f1": 0.5961909288251904,
            "f1_weighted": 0.6175271587093293
          },
          {
            "accuracy": 0.5956714215445155,
            "f1": 0.5879592073820876,
            "f1_weighted": 0.5970047035827902
          },
          {
            "accuracy": 0.5843580914904083,
            "f1": 0.5825175958190574,
            "f1_weighted": 0.5811788024203743
          },
          {
            "accuracy": 0.5671421544515495,
            "f1": 0.5554104006772936,
            "f1_weighted": 0.5727347607246709
          },
          {
            "accuracy": 0.5941957697983276,
            "f1": 0.5837357009598027,
            "f1_weighted": 0.5921415845667974
          },
          {
            "accuracy": 0.5784554845056566,
            "f1": 0.5719640455725395,
            "f1_weighted": 0.5782574709126641
          },
          {
            "accuracy": 0.5750122970978849,
            "f1": 0.5680409910316735,
            "f1_weighted": 0.5819732961313631
          },
          {
            "accuracy": 0.617314313821938,
            "f1": 0.5878855288153046,
            "f1_weighted": 0.6119268579755122
          },
          {
            "accuracy": 0.5833743236596163,
            "f1": 0.5864048745537159,
            "f1_weighted": 0.5828510272817645
          },
          {
            "accuracy": 0.5996064928676832,
            "f1": 0.589229768283617,
            "f1_weighted": 0.6000512274129511
          }
        ]
      }
    ]
  },
  "task_name": "MassiveScenarioClassification"
}