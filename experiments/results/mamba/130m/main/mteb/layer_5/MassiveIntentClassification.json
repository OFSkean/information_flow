{
  "dataset_revision": "4672e20407010da34463acc759c162ca9734bca6",
  "evaluation_time": 39.98289442062378,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.5651311365164762,
        "f1": 0.5420031532238936,
        "f1_weighted": 0.5671516875825262,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5651311365164762,
        "scores_per_experiment": [
          {
            "accuracy": 0.5601882985877606,
            "f1": 0.5518937458067541,
            "f1_weighted": 0.5628388287762529
          },
          {
            "accuracy": 0.5722932078009415,
            "f1": 0.5500718370021889,
            "f1_weighted": 0.5758940839866273
          },
          {
            "accuracy": 0.5709482178883658,
            "f1": 0.5374642162156601,
            "f1_weighted": 0.567512621321867
          },
          {
            "accuracy": 0.5860793544048419,
            "f1": 0.5494538029972623,
            "f1_weighted": 0.5856379022414685
          },
          {
            "accuracy": 0.5669132481506388,
            "f1": 0.5411184107365933,
            "f1_weighted": 0.5673971890424406
          },
          {
            "accuracy": 0.5450571620712845,
            "f1": 0.528286080438212,
            "f1_weighted": 0.5509871528171503
          },
          {
            "accuracy": 0.558843308675185,
            "f1": 0.5511256762741511,
            "f1_weighted": 0.5593202560594709
          },
          {
            "accuracy": 0.550773369199731,
            "f1": 0.5300099083803308,
            "f1_weighted": 0.5577458382848907
          },
          {
            "accuracy": 0.566577000672495,
            "f1": 0.5354005221878301,
            "f1_weighted": 0.5678409159896629
          },
          {
            "accuracy": 0.5736381977135171,
            "f1": 0.5452073321999525,
            "f1_weighted": 0.576342087305432
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.5768814559763895,
        "f1": 0.5528859148078639,
        "f1_weighted": 0.5779685682959232,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5768814559763895,
        "scores_per_experiment": [
          {
            "accuracy": 0.5681259222823414,
            "f1": 0.5523349026933823,
            "f1_weighted": 0.5654643356624076
          },
          {
            "accuracy": 0.5838662075750123,
            "f1": 0.5643584705920225,
            "f1_weighted": 0.5869574401681749
          },
          {
            "accuracy": 0.5956714215445155,
            "f1": 0.5701769834330683,
            "f1_weighted": 0.595652513171217
          },
          {
            "accuracy": 0.5843580914904083,
            "f1": 0.5468235128524124,
            "f1_weighted": 0.5843932317592713
          },
          {
            "accuracy": 0.588293162813576,
            "f1": 0.5731856878987198,
            "f1_weighted": 0.5916235498349649
          },
          {
            "accuracy": 0.559272011805214,
            "f1": 0.53996720372538,
            "f1_weighted": 0.5648467381625929
          },
          {
            "accuracy": 0.5494343334972946,
            "f1": 0.5430714507656128,
            "f1_weighted": 0.5472533222920691
          },
          {
            "accuracy": 0.5651746187899656,
            "f1": 0.5262817766574582,
            "f1_weighted": 0.5655143373906519
          },
          {
            "accuracy": 0.5755041810132808,
            "f1": 0.5521975926915128,
            "f1_weighted": 0.5766843220517311
          },
          {
            "accuracy": 0.5991146089522873,
            "f1": 0.5604615667690699,
            "f1_weighted": 0.601295892466151
          }
        ]
      }
    ]
  },
  "task_name": "MassiveIntentClassification"
}