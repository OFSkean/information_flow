{
  "dataset_revision": "1399c76144fd37290681b995c656ef9b2e06e26d",
  "evaluation_time": 369.3828372955322,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.33022,
        "f1": 0.3307308963623838,
        "f1_weighted": 0.33073089636238384,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.33022,
        "scores_per_experiment": [
          {
            "accuracy": 0.3446,
            "f1": 0.3447920680556654,
            "f1_weighted": 0.3447920680556655
          },
          {
            "accuracy": 0.3448,
            "f1": 0.34741732640903666,
            "f1_weighted": 0.3474173264090367
          },
          {
            "accuracy": 0.3136,
            "f1": 0.31593253856328196,
            "f1_weighted": 0.31593253856328196
          },
          {
            "accuracy": 0.3274,
            "f1": 0.3324824905760909,
            "f1_weighted": 0.332482490576091
          },
          {
            "accuracy": 0.3762,
            "f1": 0.3699342125308431,
            "f1_weighted": 0.3699342125308431
          },
          {
            "accuracy": 0.3008,
            "f1": 0.30077661915775555,
            "f1_weighted": 0.3007766191577555
          },
          {
            "accuracy": 0.311,
            "f1": 0.3140704514654943,
            "f1_weighted": 0.31407045146549417
          },
          {
            "accuracy": 0.3314,
            "f1": 0.32808196201046275,
            "f1_weighted": 0.32808196201046275
          },
          {
            "accuracy": 0.3134,
            "f1": 0.31527064941717353,
            "f1_weighted": 0.3152706494171736
          },
          {
            "accuracy": 0.339,
            "f1": 0.338550645438034,
            "f1_weighted": 0.338550645438034
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.32646000000000003,
        "f1": 0.3265981805494146,
        "f1_weighted": 0.3265981805494146,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.32646000000000003,
        "scores_per_experiment": [
          {
            "accuracy": 0.3444,
            "f1": 0.3434820661175174,
            "f1_weighted": 0.3434820661175174
          },
          {
            "accuracy": 0.3356,
            "f1": 0.3409573266920939,
            "f1_weighted": 0.3409573266920939
          },
          {
            "accuracy": 0.3228,
            "f1": 0.3247805778800094,
            "f1_weighted": 0.3247805778800094
          },
          {
            "accuracy": 0.3082,
            "f1": 0.31335714220794947,
            "f1_weighted": 0.3133571422079495
          },
          {
            "accuracy": 0.3764,
            "f1": 0.36594883095686176,
            "f1_weighted": 0.3659488309568617
          },
          {
            "accuracy": 0.3056,
            "f1": 0.306129199626202,
            "f1_weighted": 0.306129199626202
          },
          {
            "accuracy": 0.2982,
            "f1": 0.29999473692681594,
            "f1_weighted": 0.29999473692681594
          },
          {
            "accuracy": 0.3178,
            "f1": 0.31578229926611245,
            "f1_weighted": 0.3157822992661125
          },
          {
            "accuracy": 0.3218,
            "f1": 0.32239902902394646,
            "f1_weighted": 0.32239902902394646
          },
          {
            "accuracy": 0.3338,
            "f1": 0.33315059679663633,
            "f1_weighted": 0.3331505967966364
          }
        ]
      }
    ]
  },
  "task_name": "AmazonReviewsClassification"
}