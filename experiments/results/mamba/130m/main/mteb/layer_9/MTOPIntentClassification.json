{
  "dataset_revision": "ae001d0e6b1228650b7bd1c2c65fb50ad11a8aba",
  "evaluation_time": 85.98585271835327,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.6846785225718194,
        "f1": 0.4828971887742538,
        "f1_weighted": 0.7231211328632684,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.6846785225718194,
        "scores_per_experiment": [
          {
            "accuracy": 0.6527587779297765,
            "f1": 0.4657596436455144,
            "f1_weighted": 0.6875167062309983
          },
          {
            "accuracy": 0.6890104879160966,
            "f1": 0.4717911772373574,
            "f1_weighted": 0.7287391839841721
          },
          {
            "accuracy": 0.6671226630186958,
            "f1": 0.46038172070580513,
            "f1_weighted": 0.7068850858105447
          },
          {
            "accuracy": 0.7097583219334246,
            "f1": 0.503984448815804,
            "f1_weighted": 0.7462571591981184
          },
          {
            "accuracy": 0.6721386228910169,
            "f1": 0.47482657890560687,
            "f1_weighted": 0.7157056089903382
          },
          {
            "accuracy": 0.6753305973552212,
            "f1": 0.4785495260585517,
            "f1_weighted": 0.7130748423591683
          },
          {
            "accuracy": 0.6757865937072504,
            "f1": 0.49200407058416584,
            "f1_weighted": 0.7155241961544969
          },
          {
            "accuracy": 0.7143182854537163,
            "f1": 0.5066833900567093,
            "f1_weighted": 0.7501856859403839
          },
          {
            "accuracy": 0.6979024167806658,
            "f1": 0.4953868093542737,
            "f1_weighted": 0.7355655031019243
          },
          {
            "accuracy": 0.6926584587323301,
            "f1": 0.4796045223787504,
            "f1_weighted": 0.7317573568625392
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.6859507829977629,
        "f1": 0.4659737682127261,
        "f1_weighted": 0.7262384647379838,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.6859507829977629,
        "scores_per_experiment": [
          {
            "accuracy": 0.6492170022371365,
            "f1": 0.43220027968235375,
            "f1_weighted": 0.6879100308056214
          },
          {
            "accuracy": 0.6890380313199105,
            "f1": 0.4667692913387069,
            "f1_weighted": 0.7328069813356989
          },
          {
            "accuracy": 0.6680089485458613,
            "f1": 0.4579122977125175,
            "f1_weighted": 0.7088783484686834
          },
          {
            "accuracy": 0.7149888143176734,
            "f1": 0.4923691424503272,
            "f1_weighted": 0.7513620046786545
          },
          {
            "accuracy": 0.6881431767337808,
            "f1": 0.4748698107646557,
            "f1_weighted": 0.7298117677302896
          },
          {
            "accuracy": 0.665324384787472,
            "f1": 0.45011470585224617,
            "f1_weighted": 0.7093872574588307
          },
          {
            "accuracy": 0.6791946308724832,
            "f1": 0.48198130842805526,
            "f1_weighted": 0.7224157712028805
          },
          {
            "accuracy": 0.7096196868008948,
            "f1": 0.47512395506462124,
            "f1_weighted": 0.7476060588409262
          },
          {
            "accuracy": 0.697986577181208,
            "f1": 0.464458749559118,
            "f1_weighted": 0.7336939639727901
          },
          {
            "accuracy": 0.697986577181208,
            "f1": 0.46393814127465893,
            "f1_weighted": 0.7385124628854641
          }
        ]
      }
    ]
  },
  "task_name": "MTOPIntentClassification"
}