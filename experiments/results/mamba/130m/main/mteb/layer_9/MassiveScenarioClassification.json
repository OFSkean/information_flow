{
  "dataset_revision": "fad2c6e8459f9e1c45d9315f4953d921437d70f8",
  "evaluation_time": 39.71968865394592,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.5817417619367855,
        "f1": 0.5602034302259964,
        "f1_weighted": 0.5831062607227113,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5817417619367855,
        "scores_per_experiment": [
          {
            "accuracy": 0.5928043039677202,
            "f1": 0.5725288055906492,
            "f1_weighted": 0.5954762799245706
          },
          {
            "accuracy": 0.6126429051782112,
            "f1": 0.5908634565740498,
            "f1_weighted": 0.6169137478181088
          },
          {
            "accuracy": 0.5558170813718897,
            "f1": 0.5374873627192318,
            "f1_weighted": 0.5574520180183129
          },
          {
            "accuracy": 0.5729657027572294,
            "f1": 0.5510870584527301,
            "f1_weighted": 0.576209312934483
          },
          {
            "accuracy": 0.5914593140551446,
            "f1": 0.5613918052079442,
            "f1_weighted": 0.5885534089623604
          },
          {
            "accuracy": 0.5564895763281775,
            "f1": 0.5371300559259868,
            "f1_weighted": 0.5595713226301966
          },
          {
            "accuracy": 0.582044384667115,
            "f1": 0.5646070982814949,
            "f1_weighted": 0.5869714606065175
          },
          {
            "accuracy": 0.5995292535305985,
            "f1": 0.5672203296215393,
            "f1_weighted": 0.5967205237493626
          },
          {
            "accuracy": 0.570275722932078,
            "f1": 0.5507871671292099,
            "f1_weighted": 0.5719056680230157
          },
          {
            "accuracy": 0.5833893745796906,
            "f1": 0.5689311627571275,
            "f1_weighted": 0.5812888645601847
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.5698967043777668,
        "f1": 0.5555565946350487,
        "f1_weighted": 0.571942128727619,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5698967043777668,
        "scores_per_experiment": [
          {
            "accuracy": 0.5740285292670929,
            "f1": 0.5576765889884925,
            "f1_weighted": 0.5779336038702796
          },
          {
            "accuracy": 0.5917363502213477,
            "f1": 0.5814915161831523,
            "f1_weighted": 0.5947836069395996
          },
          {
            "accuracy": 0.5489424495818986,
            "f1": 0.5427955092047488,
            "f1_weighted": 0.5512569658869577
          },
          {
            "accuracy": 0.5361534677816036,
            "f1": 0.517232062943385,
            "f1_weighted": 0.5402741528442506
          },
          {
            "accuracy": 0.5912444663059518,
            "f1": 0.5728996721985737,
            "f1_weighted": 0.5902972128053967
          },
          {
            "accuracy": 0.5391047712739794,
            "f1": 0.5346244786520091,
            "f1_weighted": 0.537364926902046
          },
          {
            "accuracy": 0.5656665027053616,
            "f1": 0.5551742368468893,
            "f1_weighted": 0.5763261849846293
          },
          {
            "accuracy": 0.5892769306443679,
            "f1": 0.5531564277502485,
            "f1_weighted": 0.587339679104975
          },
          {
            "accuracy": 0.5715691096901131,
            "f1": 0.5597791489539481,
            "f1_weighted": 0.5726717682442242
          },
          {
            "accuracy": 0.5912444663059518,
            "f1": 0.580736304629039,
            "f1_weighted": 0.5911731856938305
          }
        ]
      }
    ]
  },
  "task_name": "MassiveScenarioClassification"
}