{
  "dataset_revision": "ae001d0e6b1228650b7bd1c2c65fb50ad11a8aba",
  "evaluation_time": 60.7297637462616,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.6621295029639762,
        "f1": 0.4692038307657838,
        "f1_weighted": 0.7033469987578397,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.6621295029639762,
        "scores_per_experiment": [
          {
            "accuracy": 0.6418148654810761,
            "f1": 0.4524967388223974,
            "f1_weighted": 0.6824208480667371
          },
          {
            "accuracy": 0.6741906064751482,
            "f1": 0.4763968873695509,
            "f1_weighted": 0.7124027364852304
          },
          {
            "accuracy": 0.645234838121295,
            "f1": 0.4565611437260263,
            "f1_weighted": 0.6870663340292681
          },
          {
            "accuracy": 0.674874601003192,
            "f1": 0.4797971072338898,
            "f1_weighted": 0.7183064416577619
          },
          {
            "accuracy": 0.6566347469220246,
            "f1": 0.47599817315796944,
            "f1_weighted": 0.6975040949455503
          },
          {
            "accuracy": 0.6507067943456453,
            "f1": 0.45555809915181883,
            "f1_weighted": 0.6948152894206793
          },
          {
            "accuracy": 0.6580027359781122,
            "f1": 0.476555465406103,
            "f1_weighted": 0.6981973733074553
          },
          {
            "accuracy": 0.6944824441404469,
            "f1": 0.48855269065446105,
            "f1_weighted": 0.7329424817704875
          },
          {
            "accuracy": 0.6593707250341997,
            "f1": 0.45627495053065287,
            "f1_weighted": 0.7030677740682858
          },
          {
            "accuracy": 0.6659826721386229,
            "f1": 0.47384705160496876,
            "f1_weighted": 0.7067466138269408
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.6614317673378076,
        "f1": 0.4497708751112207,
        "f1_weighted": 0.7039560175984894,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.6614317673378076,
        "scores_per_experiment": [
          {
            "accuracy": 0.6295302013422819,
            "f1": 0.43082252518377523,
            "f1_weighted": 0.6747350503968937
          },
          {
            "accuracy": 0.6492170022371365,
            "f1": 0.43956827668011966,
            "f1_weighted": 0.6882289542184903
          },
          {
            "accuracy": 0.6612975391498881,
            "f1": 0.4458585898565701,
            "f1_weighted": 0.7031104558040865
          },
          {
            "accuracy": 0.6612975391498881,
            "f1": 0.4504522675427398,
            "f1_weighted": 0.7046440309307493
          },
          {
            "accuracy": 0.6612975391498881,
            "f1": 0.461085201789716,
            "f1_weighted": 0.7074251473334926
          },
          {
            "accuracy": 0.6626398210290828,
            "f1": 0.4479798808623971,
            "f1_weighted": 0.708249588385652
          },
          {
            "accuracy": 0.6572706935123043,
            "f1": 0.45492581619573075,
            "f1_weighted": 0.6988525017931193
          },
          {
            "accuracy": 0.6903803131991052,
            "f1": 0.4632126279146577,
            "f1_weighted": 0.7296790354153244
          },
          {
            "accuracy": 0.6604026845637584,
            "f1": 0.4382067392517882,
            "f1_weighted": 0.7051539896339521
          },
          {
            "accuracy": 0.6809843400447427,
            "f1": 0.4655968258347134,
            "f1_weighted": 0.7194814220731341
          }
        ]
      }
    ]
  },
  "task_name": "MTOPIntentClassification"
}