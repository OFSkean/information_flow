{
  "dataset_revision": "fad2c6e8459f9e1c45d9315f4953d921437d70f8",
  "evaluation_time": 27.87423348426819,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.5956624075319434,
        "f1": 0.5792265195757708,
        "f1_weighted": 0.5967084145095981,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5956624075319434,
        "scores_per_experiment": [
          {
            "accuracy": 0.640551445864156,
            "f1": 0.6189442542378862,
            "f1_weighted": 0.6423382572972
          },
          {
            "accuracy": 0.6002017484868863,
            "f1": 0.5844681405588497,
            "f1_weighted": 0.6048487282597241
          },
          {
            "accuracy": 0.5847343644922663,
            "f1": 0.5746732159938812,
            "f1_weighted": 0.5833594180159389
          },
          {
            "accuracy": 0.5870880968392737,
            "f1": 0.5733346076776581,
            "f1_weighted": 0.5911566391797072
          },
          {
            "accuracy": 0.5780094149293881,
            "f1": 0.5534917396307226,
            "f1_weighted": 0.5773037352454525
          },
          {
            "accuracy": 0.5904505716207128,
            "f1": 0.5681393388680199,
            "f1_weighted": 0.5935684148582461
          },
          {
            "accuracy": 0.5874243443174176,
            "f1": 0.5772115332536445,
            "f1_weighted": 0.5898195405004251
          },
          {
            "accuracy": 0.6220578345662408,
            "f1": 0.594152949233969,
            "f1_weighted": 0.6212751275746782
          },
          {
            "accuracy": 0.5800268997982515,
            "f1": 0.5695473188479964,
            "f1_weighted": 0.5776900492539094
          },
          {
            "accuracy": 0.5860793544048419,
            "f1": 0.5783020974550805,
            "f1_weighted": 0.5857242349106997
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.5850959173635022,
        "f1": 0.5747639375575894,
        "f1_weighted": 0.5843976584312415,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5850959173635022,
        "scores_per_experiment": [
          {
            "accuracy": 0.6222331529758977,
            "f1": 0.6055528194059752,
            "f1_weighted": 0.6238747494068739
          },
          {
            "accuracy": 0.5818986719134285,
            "f1": 0.5740799936566684,
            "f1_weighted": 0.5833618160191021
          },
          {
            "accuracy": 0.5573044761436301,
            "f1": 0.5509132548189872,
            "f1_weighted": 0.5551949762412036
          },
          {
            "accuracy": 0.5705853418593212,
            "f1": 0.5590246777592958,
            "f1_weighted": 0.5735434693143443
          },
          {
            "accuracy": 0.5833743236596163,
            "f1": 0.5680179840978145,
            "f1_weighted": 0.5784822351939014
          },
          {
            "accuracy": 0.5769798327594687,
            "f1": 0.5697564571045739,
            "f1_weighted": 0.5774324110164115
          },
          {
            "accuracy": 0.5691096901131333,
            "f1": 0.5636618561509215,
            "f1_weighted": 0.5703171072283864
          },
          {
            "accuracy": 0.6128873585833743,
            "f1": 0.5865514823644041,
            "f1_weighted": 0.607524579180776
          },
          {
            "accuracy": 0.5917363502213477,
            "f1": 0.5935623699347202,
            "f1_weighted": 0.589259469314696
          },
          {
            "accuracy": 0.5848499754058042,
            "f1": 0.5765184802825332,
            "f1_weighted": 0.5849857713967206
          }
        ]
      }
    ]
  },
  "task_name": "MassiveScenarioClassification"
}