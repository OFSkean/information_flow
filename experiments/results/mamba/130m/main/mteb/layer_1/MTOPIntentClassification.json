{
  "dataset_revision": "ae001d0e6b1228650b7bd1c2c65fb50ad11a8aba",
  "evaluation_time": 52.54566311836243,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.6472412220702235,
        "f1": 0.4610490366741288,
        "f1_weighted": 0.6902130035865525,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.6472412220702235,
        "scores_per_experiment": [
          {
            "accuracy": 0.6413588691290469,
            "f1": 0.4586898554384994,
            "f1_weighted": 0.6821044006563163
          },
          {
            "accuracy": 0.6554947560419516,
            "f1": 0.470340084833915,
            "f1_weighted": 0.6953547319122673
          },
          {
            "accuracy": 0.6310989512083903,
            "f1": 0.4439185815340423,
            "f1_weighted": 0.6767161339042097
          },
          {
            "accuracy": 0.6543547651618787,
            "f1": 0.4687103290129344,
            "f1_weighted": 0.6984007523518034
          },
          {
            "accuracy": 0.6440948472412221,
            "f1": 0.4636057036869777,
            "f1_weighted": 0.6873241842538595
          },
          {
            "accuracy": 0.6383948928408573,
            "f1": 0.44498522896885223,
            "f1_weighted": 0.6810115760848873
          },
          {
            "accuracy": 0.6386228910168719,
            "f1": 0.46552016744196295,
            "f1_weighted": 0.6805152451534654
          },
          {
            "accuracy": 0.6823985408116735,
            "f1": 0.4899189884430946,
            "f1_weighted": 0.7214216958708934
          },
          {
            "accuracy": 0.645234838121295,
            "f1": 0.4407426753809235,
            "f1_weighted": 0.6925995222487065
          },
          {
            "accuracy": 0.6413588691290469,
            "f1": 0.4640587520000853,
            "f1_weighted": 0.6866817934291155
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.6485458612975392,
        "f1": 0.44055773415207955,
        "f1_weighted": 0.6912382352448508,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.6485458612975392,
        "scores_per_experiment": [
          {
            "accuracy": 0.6340044742729306,
            "f1": 0.43148013492711557,
            "f1_weighted": 0.678229840801223
          },
          {
            "accuracy": 0.6434004474272931,
            "f1": 0.4162531981270299,
            "f1_weighted": 0.6838492605930375
          },
          {
            "accuracy": 0.6389261744966444,
            "f1": 0.42578508821394756,
            "f1_weighted": 0.6835964561759637
          },
          {
            "accuracy": 0.6487695749440716,
            "f1": 0.44324927241586576,
            "f1_weighted": 0.6927562399986127
          },
          {
            "accuracy": 0.6608501118568233,
            "f1": 0.4543442934333736,
            "f1_weighted": 0.7028470204310197
          },
          {
            "accuracy": 0.6442953020134228,
            "f1": 0.44094386004198455,
            "f1_weighted": 0.6861732631972975
          },
          {
            "accuracy": 0.639821029082774,
            "f1": 0.4565968150682063,
            "f1_weighted": 0.6788860669498664
          },
          {
            "accuracy": 0.680089485458613,
            "f1": 0.4673555251604902,
            "f1_weighted": 0.7209490861719388
          },
          {
            "accuracy": 0.6389261744966444,
            "f1": 0.4250164509651296,
            "f1_weighted": 0.6862980191352755
          },
          {
            "accuracy": 0.6563758389261745,
            "f1": 0.44455270316765283,
            "f1_weighted": 0.6987970989942741
          }
        ]
      }
    ]
  },
  "task_name": "MTOPIntentClassification"
}