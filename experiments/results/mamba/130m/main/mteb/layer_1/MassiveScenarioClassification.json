{
  "dataset_revision": "fad2c6e8459f9e1c45d9315f4953d921437d70f8",
  "evaluation_time": 23.7251455783844,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.5930060524546066,
        "f1": 0.5758745783253325,
        "f1_weighted": 0.5950505116559273,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5930060524546066,
        "scores_per_experiment": [
          {
            "accuracy": 0.6334902488231339,
            "f1": 0.6191046202347633,
            "f1_weighted": 0.6366325596832452
          },
          {
            "accuracy": 0.6062542030934768,
            "f1": 0.5866854895800429,
            "f1_weighted": 0.6105401448959112
          },
          {
            "accuracy": 0.5773369199731002,
            "f1": 0.5599959632788755,
            "f1_weighted": 0.5769242995418441
          },
          {
            "accuracy": 0.5817081371889711,
            "f1": 0.569233743946803,
            "f1_weighted": 0.5862354774871628
          },
          {
            "accuracy": 0.5827168796234028,
            "f1": 0.5583505508293743,
            "f1_weighted": 0.5820781628305988
          },
          {
            "accuracy": 0.5904505716207128,
            "f1": 0.5680929670317979,
            "f1_weighted": 0.5952226151790193
          },
          {
            "accuracy": 0.5783456624075319,
            "f1": 0.5660715288214975,
            "f1_weighted": 0.5832880165014129
          },
          {
            "accuracy": 0.6240753194351042,
            "f1": 0.5994628276260292,
            "f1_weighted": 0.6197983829645104
          },
          {
            "accuracy": 0.5790181573638198,
            "f1": 0.5662161210633203,
            "f1_weighted": 0.5783294134931217
          },
          {
            "accuracy": 0.5766644250168124,
            "f1": 0.5655319708408206,
            "f1_weighted": 0.5814560439824475
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.5833743236596163,
        "f1": 0.5734592515728988,
        "f1_weighted": 0.5838672947826358,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5833743236596163,
        "scores_per_experiment": [
          {
            "accuracy": 0.6232169208066897,
            "f1": 0.6121869416864139,
            "f1_weighted": 0.6280040018184643
          },
          {
            "accuracy": 0.5897688145597639,
            "f1": 0.5811741247337769,
            "f1_weighted": 0.5874972692881465
          },
          {
            "accuracy": 0.5696015740285293,
            "f1": 0.5619690194331856,
            "f1_weighted": 0.5707156313513783
          },
          {
            "accuracy": 0.5686178061977374,
            "f1": 0.5534364077066533,
            "f1_weighted": 0.5687198056684144
          },
          {
            "accuracy": 0.5892769306443679,
            "f1": 0.5777901724952287,
            "f1_weighted": 0.5858604805308999
          },
          {
            "accuracy": 0.5710772257747172,
            "f1": 0.56617585308663,
            "f1_weighted": 0.5765662238084327
          },
          {
            "accuracy": 0.558288243974422,
            "f1": 0.5537134250288408,
            "f1_weighted": 0.5621894809188882
          },
          {
            "accuracy": 0.6074766355140186,
            "f1": 0.5861720509631507,
            "f1_weighted": 0.6002989307460114
          },
          {
            "accuracy": 0.5863256271519921,
            "f1": 0.58237940307934,
            "f1_weighted": 0.5843050901412418
          },
          {
            "accuracy": 0.5700934579439252,
            "f1": 0.5595951175157678,
            "f1_weighted": 0.574516033554481
          }
        ]
      }
    ]
  },
  "task_name": "MassiveScenarioClassification"
}