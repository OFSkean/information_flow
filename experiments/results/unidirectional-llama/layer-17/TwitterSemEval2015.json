{"dataset_revision": "70970daeab8776df92f5ea462b6173c0b46fd2d1", "evaluation_time": 153.2616355419159, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"cosine": {"accuracy": 0.8123621624843536, "accuracy_threshold": 0.7806497812271118, "ap": 0.5734348184096337, "f1": 0.5605880445357259, "f1_threshold": 0.703229546546936, "precision": 0.4748214612708295, "recall": 0.6841688654353562}, "dot": {"accuracy": 0.7785658937831555, "accuracy_threshold": 29.253562927246094, "ap": 0.3992759649455119, "f1": 0.4498666403240146, "f1_threshold": 21.416229248046875, "precision": 0.359545239223117, "recall": 0.6007915567282321}, "euclidean": {"accuracy": 0.8103951838826966, "accuracy_threshold": 3.528409004211426, "ap": 0.5641230978861363, "f1": 0.5505579979709165, "f1_threshold": 4.208280086517334, "precision": 0.48061405235189925, "recall": 0.6443271767810026}, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.5734348184096337, "manhattan": {"accuracy": 0.8112892650652679, "accuracy_threshold": 171.337158203125, "ap": 0.5675484126022896, "f1": 0.5517089790086822, "f1_threshold": 207.49432373046875, "precision": 0.4727820681860991, "recall": 0.662269129287599}, "max": {"accuracy": 0.8123621624843536, "ap": 0.5734348184096337, "f1": 0.5605880445357259}, "similarity": {"accuracy": 0.8123621624843536, "accuracy_threshold": 0.7806497812271118, "ap": 0.5734348184096337, "f1": 0.5605880445357259, "f1_threshold": 0.703229546546936, "precision": 0.4748214612708295, "recall": 0.6841688654353562}}]}, "task_name": "TwitterSemEval2015", "avg_layerwise_entropy": 0.2633042362517115, "num_samples": 64}