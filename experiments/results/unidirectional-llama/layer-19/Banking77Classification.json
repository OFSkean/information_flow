{"dataset_revision": "0fd18e25b25c072e09e0d92ab615fda904d66300", "evaluation_time": 149.66753458976746, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"accuracy": 0.6463636363636364, "f1": 0.6450020651238453, "f1_weighted": 0.6450020651238453, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.6463636363636364, "scores_per_experiment": [{"accuracy": 0.637987012987013, "f1": 0.6359256939588027, "f1_weighted": 0.6359256939588025}, {"accuracy": 0.6454545454545455, "f1": 0.6437933709489135, "f1_weighted": 0.6437933709489135}, {"accuracy": 0.6396103896103896, "f1": 0.6388559111729712, "f1_weighted": 0.6388559111729712}, {"accuracy": 0.638961038961039, "f1": 0.6395228233544797, "f1_weighted": 0.6395228233544799}, {"accuracy": 0.6418831168831168, "f1": 0.6416706318754872, "f1_weighted": 0.6416706318754873}, {"accuracy": 0.6444805194805194, "f1": 0.6416395932076996, "f1_weighted": 0.6416395932076995}, {"accuracy": 0.6399350649350649, "f1": 0.6390751844312994, "f1_weighted": 0.6390751844312993}, {"accuracy": 0.6590909090909091, "f1": 0.6593139667841228, "f1_weighted": 0.6593139667841229}, {"accuracy": 0.6451298701298701, "f1": 0.6420837465514193, "f1_weighted": 0.6420837465514194}, {"accuracy": 0.6711038961038961, "f1": 0.6681397289532578, "f1_weighted": 0.668139728953258}]}]}, "task_name": "Banking77Classification", "avg_layerwise_entropy": 0.20611336378018158, "num_samples": 64}