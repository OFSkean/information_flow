{"dataset_revision": "70970daeab8776df92f5ea462b6173c0b46fd2d1", "evaluation_time": 149.72484922409058, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"cosine": {"accuracy": 0.812839005781725, "accuracy_threshold": 0.7720997929573059, "ap": 0.5707089039673336, "f1": 0.5579006141820211, "f1_threshold": 0.6956565380096436, "precision": 0.48363988383349465, "recall": 0.6591029023746702}, "dot": {"accuracy": 0.7770757584788699, "accuracy_threshold": 47.977054595947266, "ap": 0.3856843616641397, "f1": 0.4356343283582089, "f1_threshold": 33.4501838684082, "precision": 0.33694083694083693, "recall": 0.6160949868073878}, "euclidean": {"accuracy": 0.812839005781725, "accuracy_threshold": 4.704719543457031, "ap": 0.5703856982032638, "f1": 0.5533769063180828, "f1_threshold": 5.4303483963012695, "precision": 0.4893530723991077, "recall": 0.6366754617414248}, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.5746075751329087, "manhattan": {"accuracy": 0.8137330869642964, "accuracy_threshold": 229.04653930664062, "ap": 0.5746075751329087, "f1": 0.5523809523809524, "f1_threshold": 274.6675720214844, "precision": 0.4577623590633131, "recall": 0.6963060686015831}, "max": {"accuracy": 0.8137330869642964, "ap": 0.5746075751329087, "f1": 0.5579006141820211}, "similarity": {"accuracy": 0.812839005781725, "accuracy_threshold": 0.7720997929573059, "ap": 0.5707089039673336, "f1": 0.5579006141820211, "f1_threshold": 0.6956565380096436, "precision": 0.48363988383349465, "recall": 0.6591029023746702}}]}, "task_name": "TwitterSemEval2015", "avg_layerwise_entropy": 0.35795519905900586, "num_samples": 64}