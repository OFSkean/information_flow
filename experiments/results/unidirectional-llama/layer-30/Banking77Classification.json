{"dataset_revision": "0fd18e25b25c072e09e0d92ab615fda904d66300", "evaluation_time": 170.52813339233398, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"accuracy": 0.6387337662337662, "f1": 0.636643789991405, "f1_weighted": 0.636643789991405, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.6387337662337662, "scores_per_experiment": [{"accuracy": 0.6279220779220779, "f1": 0.6254070094902276, "f1_weighted": 0.6254070094902276}, {"accuracy": 0.6441558441558441, "f1": 0.6432240107116005, "f1_weighted": 0.6432240107116004}, {"accuracy": 0.6487012987012987, "f1": 0.6475878723430047, "f1_weighted": 0.6475878723430047}, {"accuracy": 0.6464285714285715, "f1": 0.645987151536968, "f1_weighted": 0.645987151536968}, {"accuracy": 0.6467532467532467, "f1": 0.6432114933710802, "f1_weighted": 0.6432114933710803}, {"accuracy": 0.6350649350649351, "f1": 0.6322555077897074, "f1_weighted": 0.6322555077897074}, {"accuracy": 0.6402597402597403, "f1": 0.6382113127105215, "f1_weighted": 0.6382113127105213}, {"accuracy": 0.615909090909091, "f1": 0.6145838743671821, "f1_weighted": 0.614583874367182}, {"accuracy": 0.624025974025974, "f1": 0.6209512450004644, "f1_weighted": 0.6209512450004643}, {"accuracy": 0.6581168831168831, "f1": 0.6550184225932937, "f1_weighted": 0.6550184225932938}]}]}, "task_name": "Banking77Classification", "avg_layerwise_entropy": 0.5888270799615504, "num_samples": 64}