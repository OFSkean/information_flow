{"dataset_revision": "70970daeab8776df92f5ea462b6173c0b46fd2d1", "evaluation_time": 153.7408163547516, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"cosine": {"accuracy": 0.79841449603624, "accuracy_threshold": 0.8807401657104492, "ap": 0.48628557908337317, "f1": 0.4817987152034261, "f1_threshold": 0.8293205499649048, "precision": 0.40540540540540543, "recall": 0.5936675461741425}, "dot": {"accuracy": 0.7740954878702986, "accuracy_threshold": 851.3917846679688, "ap": 0.22069929695138535, "f1": 0.36860532970239257, "f1_threshold": 215.9496612548828, "precision": 0.22594491474901635, "recall": 1.0}, "euclidean": {"accuracy": 0.7999046313405257, "accuracy_threshold": 12.073514938354492, "ap": 0.5116092580752376, "f1": 0.507453666398066, "f1_threshold": 14.806571960449219, "precision": 0.4103942652329749, "recall": 0.6646437994722955}, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.5165238101338697, "manhattan": {"accuracy": 0.8003814746378971, "accuracy_threshold": 593.0328979492188, "ap": 0.5165238101338697, "f1": 0.5111619296526189, "f1_threshold": 718.3757934570312, "precision": 0.43519376970146484, "recall": 0.6192612137203166}, "max": {"accuracy": 0.8003814746378971, "ap": 0.5165238101338697, "f1": 0.5111619296526189}, "similarity": {"accuracy": 0.79841449603624, "accuracy_threshold": 0.8807401657104492, "ap": 0.48628557908337317, "f1": 0.4817987152034261, "f1_threshold": 0.8293205499649048, "precision": 0.40540540540540543, "recall": 0.5936675461741425}}]}, "task_name": "TwitterSemEval2015", "avg_layerwise_entropy": 0.7770471681743147, "num_samples": 64}