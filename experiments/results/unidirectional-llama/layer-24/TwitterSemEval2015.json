{"dataset_revision": "70970daeab8776df92f5ea462b6173c0b46fd2d1", "evaluation_time": 148.2101583480835, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"cosine": {"accuracy": 0.8095607081122966, "accuracy_threshold": 0.817253053188324, "ap": 0.5480375944787781, "f1": 0.5285949635208285, "f1_threshold": 0.7655096054077148, "precision": 0.4770603228547154, "recall": 0.5926121372031662}, "dot": {"accuracy": 0.7745127257554986, "accuracy_threshold": 187.1405487060547, "ap": 0.2689800018922727, "f1": 0.3711381908277292, "f1_threshold": 77.14510345458984, "precision": 0.22956151873397135, "recall": 0.9683377308707124}, "euclidean": {"accuracy": 0.8097395243488109, "accuracy_threshold": 7.422961235046387, "ap": 0.5596884186455489, "f1": 0.542289142453829, "f1_threshold": 8.472000122070312, "precision": 0.489280407556782, "recall": 0.6081794195250659}, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.5646604191167904, "manhattan": {"accuracy": 0.8114680813017822, "accuracy_threshold": 354.32489013671875, "ap": 0.5646604191167904, "f1": 0.5465051672408046, "f1_threshold": 422.6328125, "precision": 0.47206757535035515, "recall": 0.6488126649076518}, "max": {"accuracy": 0.8114680813017822, "ap": 0.5646604191167904, "f1": 0.5465051672408046}, "similarity": {"accuracy": 0.8095607081122966, "accuracy_threshold": 0.817253053188324, "ap": 0.5480375944787781, "f1": 0.5285949635208285, "f1_threshold": 0.7655096054077148, "precision": 0.4770603228547154, "recall": 0.5926121372031662}}]}, "task_name": "TwitterSemEval2015", "avg_layerwise_entropy": 0.5824348493655525, "num_samples": 64}