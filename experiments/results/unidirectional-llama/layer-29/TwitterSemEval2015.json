{"dataset_revision": "70970daeab8776df92f5ea462b6173c0b46fd2d1", "evaluation_time": 143.91768288612366, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"cosine": {"accuracy": 0.8007987125230971, "accuracy_threshold": 0.8810548782348633, "ap": 0.4945101498064366, "f1": 0.485186476124085, "f1_threshold": 0.8311780691146851, "precision": 0.43346481212372845, "recall": 0.5509234828496042}, "dot": {"accuracy": 0.7740954878702986, "accuracy_threshold": 689.60302734375, "ap": 0.22087498972587893, "f1": 0.36891878716973775, "f1_threshold": 200.24832153320312, "precision": 0.22626165988997848, "recall": 0.9984168865435357}, "euclidean": {"accuracy": 0.8016927937056685, "accuracy_threshold": 10.83987808227539, "ap": 0.5225005697844505, "f1": 0.5144393779960248, "f1_threshold": 12.709301948547363, "precision": 0.4618937644341801, "recall": 0.5804749340369393}, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.5277282747177102, "manhattan": {"accuracy": 0.8031829290099541, "accuracy_threshold": 539.3076782226562, "ap": 0.5277282747177102, "f1": 0.5178109899988763, "f1_threshold": 635.2288818359375, "precision": 0.4509688784497945, "recall": 0.6079155672823219}, "max": {"accuracy": 0.8031829290099541, "ap": 0.5277282747177102, "f1": 0.5178109899988763}, "similarity": {"accuracy": 0.8007987125230971, "accuracy_threshold": 0.8810548782348633, "ap": 0.4945101498064366, "f1": 0.485186476124085, "f1_threshold": 0.8311780691146851, "precision": 0.43346481212372845, "recall": 0.5509234828496042}}]}, "task_name": "TwitterSemEval2015", "avg_layerwise_entropy": 0.7443063494922103, "num_samples": 64}