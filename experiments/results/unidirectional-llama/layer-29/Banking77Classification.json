{"dataset_revision": "0fd18e25b25c072e09e0d92ab615fda904d66300", "evaluation_time": 161.6881971359253, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"accuracy": 0.6474025974025974, "f1": 0.6453665153162307, "f1_weighted": 0.6453665153162307, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.6474025974025974, "scores_per_experiment": [{"accuracy": 0.6435064935064935, "f1": 0.641922355350112, "f1_weighted": 0.6419223553501118}, {"accuracy": 0.6506493506493507, "f1": 0.6492042013765706, "f1_weighted": 0.6492042013765705}, {"accuracy": 0.6538961038961039, "f1": 0.6515709611890655, "f1_weighted": 0.6515709611890655}, {"accuracy": 0.6428571428571429, "f1": 0.6398674995567113, "f1_weighted": 0.6398674995567112}, {"accuracy": 0.6584415584415585, "f1": 0.6560063501575216, "f1_weighted": 0.6560063501575214}, {"accuracy": 0.6425324675324675, "f1": 0.6415024941973411, "f1_weighted": 0.641502494197341}, {"accuracy": 0.6441558441558441, "f1": 0.6413942635219789, "f1_weighted": 0.6413942635219791}, {"accuracy": 0.6548701298701298, "f1": 0.6518035644666329, "f1_weighted": 0.651803564466633}, {"accuracy": 0.6314935064935064, "f1": 0.6297912203291482, "f1_weighted": 0.6297912203291482}, {"accuracy": 0.6516233766233767, "f1": 0.6506022430172247, "f1_weighted": 0.6506022430172248}]}]}, "task_name": "Banking77Classification", "avg_layerwise_entropy": 0.5433661044114165, "num_samples": 64}