{"dataset_revision": "70970daeab8776df92f5ea462b6173c0b46fd2d1", "evaluation_time": 140.55355596542358, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"cosine": {"accuracy": 0.795315014603326, "accuracy_threshold": 0.8678711652755737, "ap": 0.4712431895933652, "f1": 0.47309297100674014, "f1_threshold": 0.8122220039367676, "precision": 0.3978765520964549, "recall": 0.583377308707124}, "dot": {"accuracy": 0.7742146986946414, "accuracy_threshold": 1019.7703247070312, "ap": 0.2324846868824807, "f1": 0.36852599328891694, "f1_threshold": 283.5973205566406, "precision": 0.22589876587372562, "recall": 0.9997361477572559}, "euclidean": {"accuracy": 0.7968051499076116, "accuracy_threshold": 13.579837799072266, "ap": 0.4944598337352227, "f1": 0.4966165844706962, "f1_threshold": 16.530719757080078, "precision": 0.43923716778251165, "recall": 0.5712401055408971}, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.4982254169401345, "manhattan": {"accuracy": 0.797281993204983, "accuracy_threshold": 675.48583984375, "ap": 0.4982254169401345, "f1": 0.5015200990879405, "f1_threshold": 829.672607421875, "precision": 0.43743861716755056, "recall": 0.587598944591029}, "max": {"accuracy": 0.797281993204983, "ap": 0.4982254169401345, "f1": 0.5015200990879405}, "similarity": {"accuracy": 0.795315014603326, "accuracy_threshold": 0.8678711652755737, "ap": 0.4712431895933652, "f1": 0.47309297100674014, "f1_threshold": 0.8122220039367676, "precision": 0.3978765520964549, "recall": 0.583377308707124}}]}, "task_name": "TwitterSemEval2015", "avg_layerwise_entropy": 0.8099276026620653, "num_samples": 64}