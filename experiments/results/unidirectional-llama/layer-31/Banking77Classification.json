{"dataset_revision": "0fd18e25b25c072e09e0d92ab615fda904d66300", "evaluation_time": 154.44054126739502, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"accuracy": 0.6311688311688312, "f1": 0.6290528244444262, "f1_weighted": 0.6290528244444265, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.6311688311688312, "scores_per_experiment": [{"accuracy": 0.6269480519480519, "f1": 0.6236203866859108, "f1_weighted": 0.6236203866859107}, {"accuracy": 0.6220779220779221, "f1": 0.6209714242969471, "f1_weighted": 0.6209714242969471}, {"accuracy": 0.6568181818181819, "f1": 0.6554226610875481, "f1_weighted": 0.6554226610875481}, {"accuracy": 0.6444805194805194, "f1": 0.6432462160989153, "f1_weighted": 0.643246216098915}, {"accuracy": 0.6256493506493507, "f1": 0.6254112052433497, "f1_weighted": 0.6254112052433495}, {"accuracy": 0.6298701298701299, "f1": 0.6276277362292711, "f1_weighted": 0.6276277362292711}, {"accuracy": 0.6207792207792208, "f1": 0.6177174740635645, "f1_weighted": 0.6177174740635646}, {"accuracy": 0.6146103896103896, "f1": 0.6107563213906578, "f1_weighted": 0.6107563213906578}, {"accuracy": 0.6363636363636364, "f1": 0.6342132349902577, "f1_weighted": 0.634213234990258}, {"accuracy": 0.634090909090909, "f1": 0.6315415843578409, "f1_weighted": 0.6315415843578411}]}]}, "task_name": "Banking77Classification", "avg_layerwise_entropy": 0.6454227407328922, "num_samples": 64}