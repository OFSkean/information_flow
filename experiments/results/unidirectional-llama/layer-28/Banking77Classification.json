{"dataset_revision": "0fd18e25b25c072e09e0d92ab615fda904d66300", "evaluation_time": 166.30167818069458, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"accuracy": 0.6490909090909092, "f1": 0.6479592424337375, "f1_weighted": 0.6479592424337375, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.6490909090909092, "scores_per_experiment": [{"accuracy": 0.6275974025974026, "f1": 0.6270418437623018, "f1_weighted": 0.6270418437623019}, {"accuracy": 0.6386363636363637, "f1": 0.6372424970606949, "f1_weighted": 0.6372424970606947}, {"accuracy": 0.6548701298701298, "f1": 0.6549682110225662, "f1_weighted": 0.6549682110225663}, {"accuracy": 0.6610389610389611, "f1": 0.6617172361414491, "f1_weighted": 0.6617172361414491}, {"accuracy": 0.6353896103896104, "f1": 0.6325646760273694, "f1_weighted": 0.6325646760273695}, {"accuracy": 0.6441558441558441, "f1": 0.6445694085272724, "f1_weighted": 0.6445694085272724}, {"accuracy": 0.6506493506493507, "f1": 0.6490178998895526, "f1_weighted": 0.6490178998895524}, {"accuracy": 0.6525974025974026, "f1": 0.6500299697625729, "f1_weighted": 0.6500299697625728}, {"accuracy": 0.6574675324675324, "f1": 0.6568894457378339, "f1_weighted": 0.656889445737834}, {"accuracy": 0.6685064935064935, "f1": 0.6655512364057622, "f1_weighted": 0.6655512364057623}]}]}, "task_name": "Banking77Classification", "avg_layerwise_entropy": 0.5076077346601034, "num_samples": 64}