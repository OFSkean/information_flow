{"dataset_revision": "70970daeab8776df92f5ea462b6173c0b46fd2d1", "evaluation_time": 155.6734869480133, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"cosine": {"accuracy": 0.8021696370030399, "accuracy_threshold": 0.8700308799743652, "ap": 0.5013487399053789, "f1": 0.48898216159496327, "f1_threshold": 0.8230741024017334, "precision": 0.4380614163359098, "recall": 0.5532981530343007}, "dot": {"accuracy": 0.7740358824581272, "accuracy_threshold": 556.0594482421875, "ap": 0.22322496682029694, "f1": 0.3689083820662768, "f1_threshold": 150.15081787109375, "precision": 0.226240286909743, "recall": 0.9986807387862797}, "euclidean": {"accuracy": 0.8034213506586398, "accuracy_threshold": 9.856679916381836, "ap": 0.5310322837178925, "f1": 0.5198130008904719, "f1_threshold": 11.75486946105957, "precision": 0.4495571813631113, "recall": 0.6160949868073878}, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.5371592584496403, "manhattan": {"accuracy": 0.8052095130237825, "accuracy_threshold": 482.7830810546875, "ap": 0.5371592584496403, "f1": 0.525728317892622, "f1_threshold": 578.76220703125, "precision": 0.4556006964596634, "recall": 0.6213720316622692}, "max": {"accuracy": 0.8052095130237825, "ap": 0.5371592584496403, "f1": 0.525728317892622}, "similarity": {"accuracy": 0.8021696370030399, "accuracy_threshold": 0.8700308799743652, "ap": 0.5013487399053789, "f1": 0.48898216159496327, "f1_threshold": 0.8230741024017334, "precision": 0.4380614163359098, "recall": 0.5532981530343007}}]}, "task_name": "TwitterSemEval2015", "avg_layerwise_entropy": 0.7128699084207745, "num_samples": 64}