{"dataset_revision": "70970daeab8776df92f5ea462b6173c0b46fd2d1", "evaluation_time": 150.09050035476685, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"cosine": {"accuracy": 0.8115276867139536, "accuracy_threshold": 0.7984188795089722, "ap": 0.5619234742973331, "f1": 0.5427816699207534, "f1_threshold": 0.7283599972724915, "precision": 0.4805775879601383, "recall": 0.6234828496042216}, "dot": {"accuracy": 0.7758240448232699, "accuracy_threshold": 101.24311828613281, "ap": 0.32077422693423363, "f1": 0.38353888427375543, "f1_threshold": 58.50944519042969, "precision": 0.25305675014756723, "recall": 0.7918205804749341}, "euclidean": {"accuracy": 0.8110508434165822, "accuracy_threshold": 6.153911590576172, "ap": 0.5656375300771305, "f1": 0.5493369296471118, "f1_threshold": 7.17431640625, "precision": 0.47846515270164447, "recall": 0.6448548812664908}, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.5710376405515718, "manhattan": {"accuracy": 0.8129582166060678, "accuracy_threshold": 304.581787109375, "ap": 0.5710376405515718, "f1": 0.5540556683789273, "f1_threshold": 344.6556701660156, "precision": 0.5062213490504257, "recall": 0.6118733509234828}, "max": {"accuracy": 0.8129582166060678, "ap": 0.5710376405515718, "f1": 0.5540556683789273}, "similarity": {"accuracy": 0.8115276867139536, "accuracy_threshold": 0.7984188795089722, "ap": 0.5619234742973331, "f1": 0.5427816699207534, "f1_threshold": 0.7283599972724915, "precision": 0.4805775879601383, "recall": 0.6234828496042216}}]}, "task_name": "TwitterSemEval2015", "avg_layerwise_entropy": 0.5005034811973326, "num_samples": 64}