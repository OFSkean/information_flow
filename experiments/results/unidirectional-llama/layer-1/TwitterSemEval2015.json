{"dataset_revision": "70970daeab8776df92f5ea462b6173c0b46fd2d1", "evaluation_time": 143.20403456687927, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"cosine": {"accuracy": 0.8096203135244681, "accuracy_threshold": 0.8517519235610962, "ap": 0.5453008406376685, "f1": 0.5341495618773255, "f1_threshold": 0.8002838492393494, "precision": 0.4899801805769654, "recall": 0.5870712401055409}, "dot": {"accuracy": 0.7781486558979556, "accuracy_threshold": 0.23821388185024261, "ap": 0.36152976530547665, "f1": 0.41461282992483833, "f1_threshold": 0.16939660906791687, "precision": 0.30998431782540514, "recall": 0.6258575197889182}, "euclidean": {"accuracy": 0.7980568635632116, "accuracy_threshold": 0.2540284991264343, "ap": 0.4997034940316683, "f1": 0.5013216871623951, "f1_threshold": 0.30242496728897095, "precision": 0.44410507025045814, "recall": 0.5754617414248021}, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.5453008406376685, "manhattan": {"accuracy": 0.7979972581510402, "accuracy_threshold": 11.923873901367188, "ap": 0.4993699606861156, "f1": 0.5013656335352097, "f1_threshold": 14.521345138549805, "precision": 0.4558410710429713, "recall": 0.5569920844327176}, "max": {"accuracy": 0.8096203135244681, "ap": 0.5453008406376685, "f1": 0.5341495618773255}, "similarity": {"accuracy": 0.8096203135244681, "accuracy_threshold": 0.8517519235610962, "ap": 0.5453008406376685, "f1": 0.5341495618773255, "f1_threshold": 0.8002838492393494, "precision": 0.4899801805769654, "recall": 0.5870712401055409}}]}, "task_name": "TwitterSemEval2015", "avg_layerwise_entropy": 0.8054413461178636, "num_samples": 64}