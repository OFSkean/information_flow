{"dataset_revision": "70970daeab8776df92f5ea462b6173c0b46fd2d1", "evaluation_time": 154.0763454437256, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"cosine": {"accuracy": 0.8096203135244681, "accuracy_threshold": 0.8021298050880432, "ap": 0.5632656778713161, "f1": 0.5529487731381835, "f1_threshold": 0.7279231548309326, "precision": 0.4669211195928753, "recall": 0.6778364116094987}, "dot": {"accuracy": 0.7799368182630982, "accuracy_threshold": 18.225378036499023, "ap": 0.3959673231972524, "f1": 0.44253214638971317, "f1_threshold": 13.961755752563477, "precision": 0.35395569620253164, "recall": 0.5902374670184697}, "euclidean": {"accuracy": 0.8073553078619539, "accuracy_threshold": 2.8034191131591797, "ap": 0.5528778619157655, "f1": 0.5444174500897856, "f1_threshold": 3.2573087215423584, "precision": 0.45393693852386824, "recall": 0.6799472295514511}, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.5632656778713161, "manhattan": {"accuracy": 0.8086070215175538, "accuracy_threshold": 136.18740844726562, "ap": 0.5558861024870321, "f1": 0.5487683640782062, "f1_threshold": 156.58482360839844, "precision": 0.47197415922477676, "recall": 0.6554089709762533}, "max": {"accuracy": 0.8096203135244681, "ap": 0.5632656778713161, "f1": 0.5529487731381835}, "similarity": {"accuracy": 0.8096203135244681, "accuracy_threshold": 0.8021298050880432, "ap": 0.5632656778713161, "f1": 0.5529487731381835, "f1_threshold": 0.7279231548309326, "precision": 0.4669211195928753, "recall": 0.6778364116094987}}]}, "task_name": "TwitterSemEval2015", "avg_layerwise_entropy": 0.18961540288263648, "num_samples": 64}