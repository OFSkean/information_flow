{"dataset_revision": "0fd18e25b25c072e09e0d92ab615fda904d66300", "evaluation_time": 141.40848779678345, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"accuracy": 0.6202597402597403, "f1": 0.6186602041418845, "f1_weighted": 0.6186602041418844, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.6202597402597403, "scores_per_experiment": [{"accuracy": 0.6074675324675325, "f1": 0.6070197465523752, "f1_weighted": 0.6070197465523751}, {"accuracy": 0.6126623376623377, "f1": 0.6106109545455946, "f1_weighted": 0.6106109545455946}, {"accuracy": 0.6331168831168831, "f1": 0.6323022192047956, "f1_weighted": 0.6323022192047956}, {"accuracy": 0.625, "f1": 0.6202360980490983, "f1_weighted": 0.6202360980490981}, {"accuracy": 0.6074675324675325, "f1": 0.606123308309801, "f1_weighted": 0.606123308309801}, {"accuracy": 0.6178571428571429, "f1": 0.6155273612594583, "f1_weighted": 0.6155273612594583}, {"accuracy": 0.6282467532467533, "f1": 0.6287178457471039, "f1_weighted": 0.628717845747104}, {"accuracy": 0.6194805194805195, "f1": 0.6163056968044155, "f1_weighted": 0.6163056968044153}, {"accuracy": 0.6230519480519481, "f1": 0.6228873723081926, "f1_weighted": 0.6228873723081925}, {"accuracy": 0.6282467532467533, "f1": 0.6268714386380095, "f1_weighted": 0.6268714386380095}]}]}, "task_name": "Banking77Classification", "avg_layerwise_entropy": 0.0984505189456617, "num_samples": 64}