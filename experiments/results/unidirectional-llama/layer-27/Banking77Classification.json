{"dataset_revision": "0fd18e25b25c072e09e0d92ab615fda904d66300", "evaluation_time": 175.47089672088623, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"accuracy": 0.6545454545454545, "f1": 0.6531135976959106, "f1_weighted": 0.6531135976959106, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.6545454545454545, "scores_per_experiment": [{"accuracy": 0.6519480519480519, "f1": 0.6531870327576622, "f1_weighted": 0.6531870327576623}, {"accuracy": 0.6561688311688312, "f1": 0.6540055859736615, "f1_weighted": 0.6540055859736615}, {"accuracy": 0.6610389610389611, "f1": 0.6600386642132253, "f1_weighted": 0.6600386642132253}, {"accuracy": 0.6590909090909091, "f1": 0.6575049735385142, "f1_weighted": 0.6575049735385141}, {"accuracy": 0.650974025974026, "f1": 0.6495464709535305, "f1_weighted": 0.6495464709535306}, {"accuracy": 0.6490259740259741, "f1": 0.646876889461423, "f1_weighted": 0.6468768894614229}, {"accuracy": 0.6545454545454545, "f1": 0.6521980865310021, "f1_weighted": 0.6521980865310021}, {"accuracy": 0.6584415584415585, "f1": 0.6568529130697632, "f1_weighted": 0.6568529130697633}, {"accuracy": 0.6399350649350649, "f1": 0.6384073333565813, "f1_weighted": 0.6384073333565813}, {"accuracy": 0.6642857142857143, "f1": 0.662518027103742, "f1_weighted": 0.6625180271037417}]}]}, "task_name": "Banking77Classification", "avg_layerwise_entropy": 0.4769950647928666, "num_samples": 64}