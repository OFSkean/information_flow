{"dataset_revision": "70970daeab8776df92f5ea462b6173c0b46fd2d1", "evaluation_time": 157.54842138290405, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"cosine": {"accuracy": 0.8031233235977827, "accuracy_threshold": 0.8650916218757629, "ap": 0.5165377756778798, "f1": 0.5024407667579472, "f1_threshold": 0.8150683641433716, "precision": 0.4577999566066392, "recall": 0.5567282321899736}, "dot": {"accuracy": 0.77415509328247, "accuracy_threshold": 413.9705810546875, "ap": 0.2315857346823294, "f1": 0.36921573418030784, "f1_threshold": 126.25160217285156, "precision": 0.22656671664167916, "recall": 0.9968337730870712}, "euclidean": {"accuracy": 0.8051499076116111, "accuracy_threshold": 9.303953170776367, "ap": 0.5415074024492891, "f1": 0.5289459007250418, "f1_threshold": 10.849386215209961, "precision": 0.45816425120772947, "recall": 0.6255936675461742}, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.5469666906477574, "manhattan": {"accuracy": 0.8069976753889253, "accuracy_threshold": 464.0068359375, "ap": 0.5469666906477574, "f1": 0.5359477124183006, "f1_threshold": 532.287353515625, "precision": 0.46774193548387094, "recall": 0.6274406332453826}, "max": {"accuracy": 0.8069976753889253, "ap": 0.5469666906477574, "f1": 0.5359477124183006}, "similarity": {"accuracy": 0.8031233235977827, "accuracy_threshold": 0.8650916218757629, "ap": 0.5165377756778798, "f1": 0.5024407667579472, "f1_threshold": 0.8150683641433716, "precision": 0.4577999566066392, "recall": 0.5567282321899736}}]}, "task_name": "TwitterSemEval2015", "avg_layerwise_entropy": 0.6833349204534362, "num_samples": 64}