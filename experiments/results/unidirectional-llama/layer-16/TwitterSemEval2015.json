{"dataset_revision": "70970daeab8776df92f5ea462b6173c0b46fd2d1", "evaluation_time": 153.07201862335205, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"cosine": {"accuracy": 0.8167133575728677, "accuracy_threshold": 0.7704213261604309, "ap": 0.5884887040625535, "f1": 0.5685444480625202, "f1_threshold": 0.7059972286224365, "precision": 0.48294302046837545, "recall": 0.6910290237467018}, "dot": {"accuracy": 0.7830362996960124, "accuracy_threshold": 21.588109970092773, "ap": 0.4197498718795552, "f1": 0.4615534929164631, "f1_threshold": 16.8351993560791, "precision": 0.3664856477889837, "recall": 0.6232189973614776}, "euclidean": {"accuracy": 0.8124813733086964, "accuracy_threshold": 3.119948625564575, "ap": 0.571470611203474, "f1": 0.5570806829594911, "f1_threshold": 3.7384958267211914, "precision": 0.4826919357957842, "recall": 0.6585751978891821}, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.5884887040625535, "manhattan": {"accuracy": 0.8133754544912678, "accuracy_threshold": 154.43606567382812, "ap": 0.5728618987468856, "f1": 0.5566721196541248, "f1_threshold": 180.14849853515625, "precision": 0.49958053691275167, "recall": 0.6284960422163588}, "max": {"accuracy": 0.8167133575728677, "ap": 0.5884887040625535, "f1": 0.5685444480625202}, "similarity": {"accuracy": 0.8167133575728677, "accuracy_threshold": 0.7704213261604309, "ap": 0.5884887040625535, "f1": 0.5685444480625202, "f1_threshold": 0.7059972286224365, "precision": 0.48294302046837545, "recall": 0.6910290237467018}}]}, "task_name": "TwitterSemEval2015", "avg_layerwise_entropy": 0.2258653116236181, "num_samples": 64}