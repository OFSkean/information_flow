{"dataset_revision": "0fd18e25b25c072e09e0d92ab615fda904d66300", "evaluation_time": 140.33239889144897, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"accuracy": 0.6492207792207794, "f1": 0.6468022950050141, "f1_weighted": 0.6468022950050141, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.6492207792207794, "scores_per_experiment": [{"accuracy": 0.6551948051948052, "f1": 0.6547274831203733, "f1_weighted": 0.6547274831203735}, {"accuracy": 0.6568181818181819, "f1": 0.6539202742872008, "f1_weighted": 0.6539202742872007}, {"accuracy": 0.6519480519480519, "f1": 0.6485730466485768, "f1_weighted": 0.648573046648577}, {"accuracy": 0.650974025974026, "f1": 0.6483398056319364, "f1_weighted": 0.6483398056319364}, {"accuracy": 0.673051948051948, "f1": 0.6701446581816191, "f1_weighted": 0.670144658181619}, {"accuracy": 0.6396103896103896, "f1": 0.6387672115801708, "f1_weighted": 0.6387672115801707}, {"accuracy": 0.6438311688311689, "f1": 0.6425925034611387, "f1_weighted": 0.6425925034611387}, {"accuracy": 0.6331168831168831, "f1": 0.6299574351006341, "f1_weighted": 0.6299574351006341}, {"accuracy": 0.6525974025974026, "f1": 0.6492665026577128, "f1_weighted": 0.6492665026577129}, {"accuracy": 0.6350649350649351, "f1": 0.6317340293807786, "f1_weighted": 0.6317340293807785}]}]}, "task_name": "Banking77Classification", "avg_layerwise_entropy": 0.12415130485957462, "num_samples": 64}