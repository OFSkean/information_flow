{"dataset_revision": "70970daeab8776df92f5ea462b6173c0b46fd2d1", "evaluation_time": 142.8594982624054, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"cosine": {"accuracy": 0.8031829290099541, "accuracy_threshold": 0.7992671728134155, "ap": 0.5416399366259161, "f1": 0.5441985968699407, "f1_threshold": 0.7139163017272949, "precision": 0.4604566210045662, "recall": 0.6651715039577837}, "dot": {"accuracy": 0.7796983966144125, "accuracy_threshold": 11.590234756469727, "ap": 0.4111577502550989, "f1": 0.455290611028316, "f1_threshold": 8.762054443359375, "precision": 0.3518571839907861, "recall": 0.6448548812664908}, "euclidean": {"accuracy": 0.8014543720569828, "accuracy_threshold": 2.264381170272827, "ap": 0.5361776867375905, "f1": 0.5372647154825372, "f1_threshold": 2.677232027053833, "precision": 0.4571375671172005, "recall": 0.6514511873350923}, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.5416399366259161, "manhattan": {"accuracy": 0.8013351612326399, "accuracy_threshold": 114.67504119873047, "ap": 0.5351718097607238, "f1": 0.5369024014100022, "f1_threshold": 131.4634552001953, "precision": 0.4608547655068079, "recall": 0.6430079155672823}, "max": {"accuracy": 0.8031829290099541, "ap": 0.5416399366259161, "f1": 0.5441985968699407}, "similarity": {"accuracy": 0.8031829290099541, "accuracy_threshold": 0.7992671728134155, "ap": 0.5416399366259161, "f1": 0.5441985968699407, "f1_threshold": 0.7139163017272949, "precision": 0.4604566210045662, "recall": 0.6651715039577837}}]}, "task_name": "TwitterSemEval2015", "avg_layerwise_entropy": 0.13898709183386787, "num_samples": 64}