{"dataset_revision": "0fd18e25b25c072e09e0d92ab615fda904d66300", "evaluation_time": 122.89258766174316, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"accuracy": 0.6306168831168831, "f1": 0.6278167237853021, "f1_weighted": 0.6278167237853021, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.6306168831168831, "scores_per_experiment": [{"accuracy": 0.6311688311688312, "f1": 0.6279991316344675, "f1_weighted": 0.6279991316344675}, {"accuracy": 0.6275974025974026, "f1": 0.6238774274168121, "f1_weighted": 0.623877427416812}, {"accuracy": 0.6253246753246753, "f1": 0.621767669436456, "f1_weighted": 0.6217676694364559}, {"accuracy": 0.6347402597402597, "f1": 0.631030160309252, "f1_weighted": 0.6310301603092522}, {"accuracy": 0.6396103896103896, "f1": 0.6393882306381957, "f1_weighted": 0.6393882306381958}, {"accuracy": 0.6201298701298701, "f1": 0.6164771973633862, "f1_weighted": 0.6164771973633862}, {"accuracy": 0.6285714285714286, "f1": 0.6266415801802145, "f1_weighted": 0.6266415801802145}, {"accuracy": 0.6347402597402597, "f1": 0.6321321346463464, "f1_weighted": 0.6321321346463464}, {"accuracy": 0.6344155844155844, "f1": 0.631347084025986, "f1_weighted": 0.6313470840259862}, {"accuracy": 0.6298701298701299, "f1": 0.627506622201905, "f1_weighted": 0.627506622201905}]}]}, "task_name": "Banking77Classification", "avg_layerwise_entropy": 0.055046529246774285, "num_samples": 64}