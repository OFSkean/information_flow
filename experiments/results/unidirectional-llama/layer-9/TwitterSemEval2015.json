{"dataset_revision": "70970daeab8776df92f5ea462b6173c0b46fd2d1", "evaluation_time": 152.33435606956482, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"cosine": {"accuracy": 0.805269118435954, "accuracy_threshold": 0.8110229969024658, "ap": 0.5469715231060854, "f1": 0.5433133280489186, "f1_threshold": 0.7403491735458374, "precision": 0.47589763935727036, "recall": 0.632981530343008}, "dot": {"accuracy": 0.779042737080527, "accuracy_threshold": 9.204419136047363, "ap": 0.40915575758533207, "f1": 0.4504751305538909, "f1_threshold": 7.211728096008301, "precision": 0.3334178177670764, "recall": 0.6941952506596306}, "euclidean": {"accuracy": 0.8058651725576682, "accuracy_threshold": 2.0434255599975586, "ap": 0.5453042111812227, "f1": 0.5447027268663388, "f1_threshold": 2.316511631011963, "precision": 0.4724699495928655, "recall": 0.6430079155672823}, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.5469715231060854, "manhattan": {"accuracy": 0.8057459617333254, "accuracy_threshold": 99.46356201171875, "ap": 0.5451509588356416, "f1": 0.5441386116898783, "f1_threshold": 115.65901184082031, "precision": 0.4655657721899043, "recall": 0.6546174142480211}, "max": {"accuracy": 0.8058651725576682, "ap": 0.5469715231060854, "f1": 0.5447027268663388}, "similarity": {"accuracy": 0.805269118435954, "accuracy_threshold": 0.8110229969024658, "ap": 0.5469715231060854, "f1": 0.5433133280489186, "f1_threshold": 0.7403491735458374, "precision": 0.47589763935727036, "recall": 0.632981530343008}}]}, "task_name": "TwitterSemEval2015", "avg_layerwise_entropy": 0.11300131277137551, "num_samples": 64}