{"dataset_revision": "70970daeab8776df92f5ea462b6173c0b46fd2d1", "evaluation_time": 143.23509860038757, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"cosine": {"accuracy": 0.8105740001192108, "accuracy_threshold": 0.7898745536804199, "ap": 0.565829302318671, "f1": 0.5470704410796576, "f1_threshold": 0.7309298515319824, "precision": 0.46825694966190834, "recall": 0.6577836411609499}, "dot": {"accuracy": 0.7785062883709841, "accuracy_threshold": 4.350172519683838, "ap": 0.39059786328297375, "f1": 0.43882395909422933, "f1_threshold": 3.3406970500946045, "precision": 0.3355208042446244, "recall": 0.6340369393139842}, "euclidean": {"accuracy": 0.8081897836323538, "accuracy_threshold": 1.340058445930481, "ap": 0.5463424843255809, "f1": 0.5409906986805104, "f1_threshold": 1.5936362743377686, "precision": 0.45839442815249265, "recall": 0.6598944591029023}, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.565829302318671, "manhattan": {"accuracy": 0.8079513619836681, "accuracy_threshold": 68.10269927978516, "ap": 0.5451434516842956, "f1": 0.5393551188796006, "f1_threshold": 78.86006164550781, "precision": 0.4582180409518539, "recall": 0.6554089709762533}, "max": {"accuracy": 0.8105740001192108, "ap": 0.565829302318671, "f1": 0.5470704410796576}, "similarity": {"accuracy": 0.8105740001192108, "accuracy_threshold": 0.7898745536804199, "ap": 0.565829302318671, "f1": 0.5470704410796576, "f1_threshold": 0.7309298515319824, "precision": 0.46825694966190834, "recall": 0.6577836411609499}}]}, "task_name": "TwitterSemEval2015", "avg_layerwise_entropy": 0.07331723540342189, "num_samples": 64}