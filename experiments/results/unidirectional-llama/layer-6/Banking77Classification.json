{"dataset_revision": "0fd18e25b25c072e09e0d92ab615fda904d66300", "evaluation_time": 111.91812944412231, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"accuracy": 0.6254870129870129, "f1": 0.6207774216969374, "f1_weighted": 0.6207774216969374, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.6254870129870129, "scores_per_experiment": [{"accuracy": 0.6139610389610389, "f1": 0.6081909218976618, "f1_weighted": 0.6081909218976619}, {"accuracy": 0.6551948051948052, "f1": 0.6503829712867246, "f1_weighted": 0.6503829712867246}, {"accuracy": 0.6314935064935064, "f1": 0.6273710021708757, "f1_weighted": 0.6273710021708758}, {"accuracy": 0.6168831168831169, "f1": 0.6149493335464642, "f1_weighted": 0.6149493335464643}, {"accuracy": 0.6217532467532467, "f1": 0.6179830025203579, "f1_weighted": 0.617983002520358}, {"accuracy": 0.6224025974025974, "f1": 0.6177808882595484, "f1_weighted": 0.6177808882595485}, {"accuracy": 0.6172077922077922, "f1": 0.6116027967381652, "f1_weighted": 0.611602796738165}, {"accuracy": 0.625, "f1": 0.6197762966877726, "f1_weighted": 0.6197762966877726}, {"accuracy": 0.6237012987012988, "f1": 0.614982408794097, "f1_weighted": 0.614982408794097}, {"accuracy": 0.6272727272727273, "f1": 0.6247545950677065, "f1_weighted": 0.6247545950677064}]}]}, "task_name": "Banking77Classification", "avg_layerwise_entropy": 0.03739356317526361, "num_samples": 64}