{"dataset_revision": "70970daeab8776df92f5ea462b6173c0b46fd2d1", "evaluation_time": 142.35472750663757, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"cosine": {"accuracy": 0.8074745186862967, "accuracy_threshold": 0.8144547939300537, "ap": 0.5552545587179233, "f1": 0.5488587356684549, "f1_threshold": 0.7438952922821045, "precision": 0.45635822984082564, "recall": 0.6883905013192612}, "dot": {"accuracy": 0.781724980628241, "accuracy_threshold": 15.889524459838867, "ap": 0.4123155276833799, "f1": 0.44672376020713006, "f1_threshold": 13.424517631530762, "precision": 0.358765195137556, "recall": 0.591820580474934}, "euclidean": {"accuracy": 0.8072957024497824, "accuracy_threshold": 2.6179401874542236, "ap": 0.5491386751253418, "f1": 0.5423689943813783, "f1_threshold": 2.968345880508423, "precision": 0.4796187385925776, "recall": 0.6240105540897097}, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.5552545587179233, "manhattan": {"accuracy": 0.8074149132741253, "accuracy_threshold": 128.3242950439453, "ap": 0.5493657604502818, "f1": 0.5432549646199497, "f1_threshold": 145.40121459960938, "precision": 0.4786806114239743, "recall": 0.6279683377308707}, "max": {"accuracy": 0.8074745186862967, "ap": 0.5552545587179233, "f1": 0.5488587356684549}, "similarity": {"accuracy": 0.8074745186862967, "accuracy_threshold": 0.8144547939300537, "ap": 0.5552545587179233, "f1": 0.5488587356684549, "f1_threshold": 0.7438952922821045, "precision": 0.45635822984082564, "recall": 0.6883905013192612}}]}, "task_name": "TwitterSemEval2015", "avg_layerwise_entropy": 0.1649738605401335, "num_samples": 64}