{"dataset_revision": "70970daeab8776df92f5ea462b6173c0b46fd2d1", "evaluation_time": 142.761372089386, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"cosine": {"accuracy": 0.8050903021994397, "accuracy_threshold": 0.8515292406082153, "ap": 0.5262795815727529, "f1": 0.5111136441354155, "f1_threshold": 0.791811466217041, "precision": 0.44992976118803935, "recall": 0.59155672823219}, "dot": {"accuracy": 0.7740954878702986, "accuracy_threshold": 323.3165283203125, "ap": 0.24146674642301616, "f1": 0.3693315089913995, "f1_threshold": 96.55519104003906, "precision": 0.22664027827755787, "recall": 0.9970976253298153}, "euclidean": {"accuracy": 0.807653334922811, "accuracy_threshold": 8.74319076538086, "ap": 0.5484026893630984, "f1": 0.5335018963337547, "f1_threshold": 9.958874702453613, "precision": 0.47261250254530646, "recall": 0.6124010554089709}, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.5539168085272874, "manhattan": {"accuracy": 0.8086666269297252, "accuracy_threshold": 411.84173583984375, "ap": 0.5539168085272874, "f1": 0.5383462115096297, "f1_threshold": 488.2764892578125, "precision": 0.478180700676091, "recall": 0.6158311345646438}, "max": {"accuracy": 0.8086666269297252, "ap": 0.5539168085272874, "f1": 0.5383462115096297}, "similarity": {"accuracy": 0.8050903021994397, "accuracy_threshold": 0.8515292406082153, "ap": 0.5262795815727529, "f1": 0.5111136441354155, "f1_threshold": 0.791811466217041, "precision": 0.44992976118803935, "recall": 0.59155672823219}}]}, "task_name": "TwitterSemEval2015", "avg_layerwise_entropy": 0.6522633276907153, "num_samples": 64}