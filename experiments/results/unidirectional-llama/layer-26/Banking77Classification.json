{"dataset_revision": "0fd18e25b25c072e09e0d92ab615fda904d66300", "evaluation_time": 163.54073572158813, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"accuracy": 0.6487012987012987, "f1": 0.6470610973817899, "f1_weighted": 0.6470610973817899, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.6487012987012987, "scores_per_experiment": [{"accuracy": 0.6542207792207793, "f1": 0.6532411737747639, "f1_weighted": 0.653241173774764}, {"accuracy": 0.6431818181818182, "f1": 0.6433464536440122, "f1_weighted": 0.6433464536440121}, {"accuracy": 0.6366883116883116, "f1": 0.6342268376404715, "f1_weighted": 0.6342268376404713}, {"accuracy": 0.6659090909090909, "f1": 0.6640974448563338, "f1_weighted": 0.6640974448563339}, {"accuracy": 0.6503246753246753, "f1": 0.6496726209810895, "f1_weighted": 0.6496726209810895}, {"accuracy": 0.6558441558441559, "f1": 0.6571865221728165, "f1_weighted": 0.6571865221728165}, {"accuracy": 0.6425324675324675, "f1": 0.6407423474392784, "f1_weighted": 0.6407423474392786}, {"accuracy": 0.6311688311688312, "f1": 0.6269921267636509, "f1_weighted": 0.6269921267636509}, {"accuracy": 0.6506493506493507, "f1": 0.646565297134737, "f1_weighted": 0.646565297134737}, {"accuracy": 0.6564935064935065, "f1": 0.6545401494107437, "f1_weighted": 0.6545401494107437}]}]}, "task_name": "Banking77Classification", "avg_layerwise_entropy": 0.44740232916620903, "num_samples": 64}