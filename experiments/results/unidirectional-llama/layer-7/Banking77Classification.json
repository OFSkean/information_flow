{"dataset_revision": "0fd18e25b25c072e09e0d92ab615fda904d66300", "evaluation_time": 114.39670300483704, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"accuracy": 0.6242857142857143, "f1": 0.6205074147140432, "f1_weighted": 0.6205074147140432, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.6242857142857143, "scores_per_experiment": [{"accuracy": 0.6253246753246753, "f1": 0.6191176895507148, "f1_weighted": 0.6191176895507149}, {"accuracy": 0.6198051948051948, "f1": 0.6151486632546332, "f1_weighted": 0.6151486632546334}, {"accuracy": 0.6123376623376623, "f1": 0.6068310405535302, "f1_weighted": 0.6068310405535303}, {"accuracy": 0.6204545454545455, "f1": 0.616337967299473, "f1_weighted": 0.616337967299473}, {"accuracy": 0.6214285714285714, "f1": 0.6179111702909603, "f1_weighted": 0.6179111702909604}, {"accuracy": 0.6246753246753247, "f1": 0.61910634480401, "f1_weighted": 0.6191063448040098}, {"accuracy": 0.6363636363636364, "f1": 0.6361579959336272, "f1_weighted": 0.6361579959336271}, {"accuracy": 0.6396103896103896, "f1": 0.6370424556707591, "f1_weighted": 0.637042455670759}, {"accuracy": 0.615909090909091, "f1": 0.6135349722733563, "f1_weighted": 0.6135349722733563}, {"accuracy": 0.6269480519480519, "f1": 0.6238858475093688, "f1_weighted": 0.6238858475093687}]}]}, "task_name": "Banking77Classification", "avg_layerwise_entropy": 0.044103863121517574, "num_samples": 64}