{"dataset_revision": "70970daeab8776df92f5ea462b6173c0b46fd2d1", "evaluation_time": 148.72386264801025, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"cosine": {"accuracy": 0.8090242594027538, "accuracy_threshold": 0.7962736487388611, "ap": 0.5557618220235242, "f1": 0.5434021898634157, "f1_threshold": 0.7254195809364319, "precision": 0.47484710988360623, "recall": 0.6350923482849604}, "dot": {"accuracy": 0.7804136615604697, "accuracy_threshold": 5.917102813720703, "ap": 0.4145951606470301, "f1": 0.4584638054890403, "f1_threshold": 4.685259819030762, "precision": 0.3521505376344086, "recall": 0.6567282321899736}, "euclidean": {"accuracy": 0.8083089944566967, "accuracy_threshold": 1.6872096061706543, "ap": 0.5479946850288228, "f1": 0.5449160908193484, "f1_threshold": 1.932433843612671, "precision": 0.46630373568612726, "recall": 0.6554089709762533}, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.5557618220235242, "manhattan": {"accuracy": 0.8082493890445253, "accuracy_threshold": 84.12415313720703, "ap": 0.5473693205928729, "f1": 0.5435124803238138, "f1_threshold": 95.31782531738281, "precision": 0.47355015673981193, "recall": 0.6377308707124011}, "max": {"accuracy": 0.8090242594027538, "ap": 0.5557618220235242, "f1": 0.5449160908193484}, "similarity": {"accuracy": 0.8090242594027538, "accuracy_threshold": 0.7962736487388611, "ap": 0.5557618220235242, "f1": 0.5434021898634157, "f1_threshold": 0.7254195809364319, "precision": 0.47484710988360623, "recall": 0.6350923482849604}}]}, "task_name": "TwitterSemEval2015", "avg_layerwise_entropy": 0.08962905453860204, "num_samples": 64}