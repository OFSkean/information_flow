{"dataset_revision": "70970daeab8776df92f5ea462b6173c0b46fd2d1", "evaluation_time": 160.0415005683899, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"cosine": {"accuracy": 0.7888776300888121, "accuracy_threshold": 0.8841274380683899, "ap": 0.4368685189737961, "f1": 0.45157780195865066, "f1_threshold": 0.78900146484375, "precision": 0.38425925925925924, "recall": 0.5474934036939314}, "dot": {"accuracy": 0.7746915419920128, "accuracy_threshold": 11802.857421875, "ap": 0.3095521079831999, "f1": 0.38069778600315307, "f1_threshold": 7453.880859375, "precision": 0.2571534401333457, "recall": 0.7327176781002639}, "euclidean": {"accuracy": 0.7889372355009835, "accuracy_threshold": 49.831871032714844, "ap": 0.44237022779339524, "f1": 0.45745375408052236, "f1_threshold": 67.0690689086914, "precision": 0.38925925925925925, "recall": 0.5546174142480211}, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.4568545732358061, "manhattan": {"accuracy": 0.7898909220957263, "accuracy_threshold": 2447.867431640625, "ap": 0.4568545732358061, "f1": 0.46687697160883274, "f1_threshold": 3336.030029296875, "precision": 0.37999006128871954, "recall": 0.6052770448548813}, "max": {"accuracy": 0.7898909220957263, "ap": 0.4568545732358061, "f1": 0.46687697160883274}, "similarity": {"accuracy": 0.7888776300888121, "accuracy_threshold": 0.8841274380683899, "ap": 0.4368685189737961, "f1": 0.45157780195865066, "f1_threshold": 0.78900146484375, "precision": 0.38425925925925924, "recall": 0.5474934036939314}}]}, "task_name": "TwitterSemEval2015", "avg_layerwise_entropy": 0.8623326871017729, "num_samples": 64}