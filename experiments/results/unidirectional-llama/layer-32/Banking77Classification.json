{"dataset_revision": "0fd18e25b25c072e09e0d92ab615fda904d66300", "evaluation_time": 173.06939697265625, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"accuracy": 0.6072077922077923, "f1": 0.6043352181604463, "f1_weighted": 0.6043352181604463, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.6072077922077923, "scores_per_experiment": [{"accuracy": 0.6025974025974026, "f1": 0.6008746266008386, "f1_weighted": 0.6008746266008385}, {"accuracy": 0.6097402597402597, "f1": 0.6058004828649363, "f1_weighted": 0.6058004828649364}, {"accuracy": 0.6126623376623377, "f1": 0.6123502763289845, "f1_weighted": 0.6123502763289845}, {"accuracy": 0.6126623376623377, "f1": 0.6059784903990185, "f1_weighted": 0.6059784903990186}, {"accuracy": 0.6090909090909091, "f1": 0.6054308099083838, "f1_weighted": 0.6054308099083838}, {"accuracy": 0.6048701298701299, "f1": 0.6052778646118774, "f1_weighted": 0.6052778646118773}, {"accuracy": 0.6142857142857143, "f1": 0.6128848054253395, "f1_weighted": 0.6128848054253395}, {"accuracy": 0.5938311688311688, "f1": 0.5904050816809328, "f1_weighted": 0.5904050816809329}, {"accuracy": 0.6077922077922078, "f1": 0.6008120074918782, "f1_weighted": 0.6008120074918784}, {"accuracy": 0.6045454545454545, "f1": 0.6035377362922739, "f1_weighted": 0.6035377362922738}]}]}, "task_name": "Banking77Classification", "avg_layerwise_entropy": 0.847940778143286, "num_samples": 64}