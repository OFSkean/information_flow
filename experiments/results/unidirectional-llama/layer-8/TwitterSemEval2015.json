{"dataset_revision": "70970daeab8776df92f5ea462b6173c0b46fd2d1", "evaluation_time": 144.92726016044617, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"cosine": {"accuracy": 0.8043750372533826, "accuracy_threshold": 0.7996881008148193, "ap": 0.5489067045343307, "f1": 0.5430663721957457, "f1_threshold": 0.7312288880348206, "precision": 0.4853521712029919, "recall": 0.6163588390501319}, "dot": {"accuracy": 0.7800560290874411, "accuracy_threshold": 7.905850410461426, "ap": 0.41494885227529843, "f1": 0.45302990209050015, "f1_threshold": 5.969930648803711, "precision": 0.3402676560222605, "recall": 0.6775725593667546}, "euclidean": {"accuracy": 0.8070572808010967, "accuracy_threshold": 1.9207565784454346, "ap": 0.5472634664838726, "f1": 0.5478575996589213, "f1_threshold": 2.2029714584350586, "precision": 0.459585121602289, "recall": 0.6781002638522428}, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.5489067045343307, "manhattan": {"accuracy": 0.8069976753889253, "accuracy_threshold": 95.86373901367188, "ap": 0.5468797090751754, "f1": 0.5458901968955099, "f1_threshold": 109.93634033203125, "precision": 0.45102427268032363, "recall": 0.6912928759894459}, "max": {"accuracy": 0.8070572808010967, "ap": 0.5489067045343307, "f1": 0.5478575996589213}, "similarity": {"accuracy": 0.8043750372533826, "accuracy_threshold": 0.7996881008148193, "ap": 0.5489067045343307, "f1": 0.5430663721957457, "f1_threshold": 0.7312288880348206, "precision": 0.4853521712029919, "recall": 0.6163588390501319}}]}, "task_name": "TwitterSemEval2015", "avg_layerwise_entropy": 0.10512806031925694, "num_samples": 64}