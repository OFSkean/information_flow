{"dataset_revision": "0fd18e25b25c072e09e0d92ab615fda904d66300", "evaluation_time": 119.488285779953, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"accuracy": 0.6359415584415584, "f1": 0.631592052488043, "f1_weighted": 0.631592052488043, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.6359415584415584, "scores_per_experiment": [{"accuracy": 0.6373376623376623, "f1": 0.6318798471420625, "f1_weighted": 0.6318798471420626}, {"accuracy": 0.6392857142857142, "f1": 0.6358586129913696, "f1_weighted": 0.6358586129913697}, {"accuracy": 0.6298701298701299, "f1": 0.6256587098602294, "f1_weighted": 0.6256587098602293}, {"accuracy": 0.6285714285714286, "f1": 0.6232878089677301, "f1_weighted": 0.62328780896773}, {"accuracy": 0.6516233766233767, "f1": 0.6511948673764838, "f1_weighted": 0.6511948673764836}, {"accuracy": 0.6376623376623377, "f1": 0.6322926688072816, "f1_weighted": 0.6322926688072815}, {"accuracy": 0.6418831168831168, "f1": 0.6344669192328225, "f1_weighted": 0.6344669192328224}, {"accuracy": 0.6357142857142857, "f1": 0.6300065505746612, "f1_weighted": 0.6300065505746614}, {"accuracy": 0.6172077922077922, "f1": 0.6147432800592141, "f1_weighted": 0.6147432800592141}, {"accuracy": 0.6402597402597403, "f1": 0.6365312598685743, "f1_weighted": 0.6365312598685743}]}]}, "task_name": "Banking77Classification", "avg_layerwise_entropy": 0.051357357747797226, "num_samples": 64}