{"dataset_revision": "0fd18e25b25c072e09e0d92ab615fda904d66300", "evaluation_time": 125.86680817604065, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"accuracy": 0.6225, "f1": 0.619242315706731, "f1_weighted": 0.619242315706731, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.6225, "scores_per_experiment": [{"accuracy": 0.6149350649350649, "f1": 0.6135898284576691, "f1_weighted": 0.6135898284576692}, {"accuracy": 0.6275974025974026, "f1": 0.62490951872483, "f1_weighted": 0.6249095187248301}, {"accuracy": 0.6123376623376623, "f1": 0.6101404583985616, "f1_weighted": 0.6101404583985619}, {"accuracy": 0.6152597402597403, "f1": 0.6114068320916615, "f1_weighted": 0.6114068320916614}, {"accuracy": 0.6237012987012988, "f1": 0.6194972797032646, "f1_weighted": 0.6194972797032647}, {"accuracy": 0.6324675324675325, "f1": 0.62815250146011, "f1_weighted": 0.6281525014601101}, {"accuracy": 0.6256493506493507, "f1": 0.6228042047687246, "f1_weighted": 0.6228042047687246}, {"accuracy": 0.6256493506493507, "f1": 0.6202601802251622, "f1_weighted": 0.6202601802251622}, {"accuracy": 0.6314935064935064, "f1": 0.6282663647300418, "f1_weighted": 0.6282663647300419}, {"accuracy": 0.615909090909091, "f1": 0.6133959885072844, "f1_weighted": 0.6133959885072844}]}]}, "task_name": "Banking77Classification", "avg_layerwise_entropy": 0.06051620137029134, "num_samples": 64}