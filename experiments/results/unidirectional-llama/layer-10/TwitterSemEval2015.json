{"dataset_revision": "70970daeab8776df92f5ea462b6173c0b46fd2d1", "evaluation_time": 147.36492323875427, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"cosine": {"accuracy": 0.8030041127734399, "accuracy_threshold": 0.8183475732803345, "ap": 0.5435196820672403, "f1": 0.5455193724744475, "f1_threshold": 0.7585933208465576, "precision": 0.4963235294117647, "recall": 0.6055408970976254}, "dot": {"accuracy": 0.7799964236752697, "accuracy_threshold": 11.720407485961914, "ap": 0.4033674705191749, "f1": 0.4480223359702187, "f1_threshold": 8.91131591796875, "precision": 0.34608195542774983, "recall": 0.6350923482849604}, "euclidean": {"accuracy": 0.8035405614829827, "accuracy_threshold": 2.1228737831115723, "ap": 0.5414711165695434, "f1": 0.5403654869303725, "f1_threshold": 2.450193166732788, "precision": 0.48105436573311366, "recall": 0.6163588390501319}, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.5435196820672403, "manhattan": {"accuracy": 0.8038385885438398, "accuracy_threshold": 104.71849822998047, "ap": 0.5404221333427838, "f1": 0.5401049932853131, "f1_threshold": 119.44441986083984, "precision": 0.5026130424903431, "recall": 0.5836411609498681}, "max": {"accuracy": 0.8038385885438398, "ap": 0.5435196820672403, "f1": 0.5455193724744475}, "similarity": {"accuracy": 0.8030041127734399, "accuracy_threshold": 0.8183475732803345, "ap": 0.5435196820672403, "f1": 0.5455193724744475, "f1_threshold": 0.7585933208465576, "precision": 0.4963235294117647, "recall": 0.6055408970976254}}]}, "task_name": "TwitterSemEval2015", "avg_layerwise_entropy": 0.12338987461711627, "num_samples": 64}