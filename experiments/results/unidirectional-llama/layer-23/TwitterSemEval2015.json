{"dataset_revision": "70970daeab8776df92f5ea462b6173c0b46fd2d1", "evaluation_time": 146.42769479751587, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"cosine": {"accuracy": 0.8103951838826966, "accuracy_threshold": 0.8072757720947266, "ap": 0.5546452656646395, "f1": 0.5378266850068776, "f1_threshold": 0.7472680807113647, "precision": 0.47547628698824485, "recall": 0.6189973614775726}, "dot": {"accuracy": 0.7747511474041843, "accuracy_threshold": 147.54534912109375, "ap": 0.2864747189805209, "f1": 0.37560923924560286, "f1_threshold": 66.25880432128906, "precision": 0.23498607980909453, "recall": 0.9353562005277045}, "euclidean": {"accuracy": 0.8105740001192108, "accuracy_threshold": 6.7434868812561035, "ap": 0.5639493643896671, "f1": 0.547547211978992, "f1_threshold": 7.858461380004883, "precision": 0.47489823609226595, "recall": 0.6464379947229552}, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.5694530785100326, "manhattan": {"accuracy": 0.8126601895452107, "accuracy_threshold": 324.8576354980469, "ap": 0.5694530785100326, "f1": 0.5507584597432906, "f1_threshold": 379.6507568359375, "precision": 0.49372384937238495, "recall": 0.6226912928759895}, "max": {"accuracy": 0.8126601895452107, "ap": 0.5694530785100326, "f1": 0.5507584597432906}, "similarity": {"accuracy": 0.8103951838826966, "accuracy_threshold": 0.8072757720947266, "ap": 0.5546452656646395, "f1": 0.5378266850068776, "f1_threshold": 0.7472680807113647, "precision": 0.47547628698824485, "recall": 0.6189973614775726}}]}, "task_name": "TwitterSemEval2015", "avg_layerwise_entropy": 0.5414545517056878, "num_samples": 64}