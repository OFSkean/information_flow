{"dataset_revision": "70970daeab8776df92f5ea462b6173c0b46fd2d1", "evaluation_time": 141.87092685699463, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"cosine": {"accuracy": 0.8037789831316684, "accuracy_threshold": 0.8098108172416687, "ap": 0.5452867829126922, "f1": 0.5457922228670923, "f1_threshold": 0.7425128221511841, "precision": 0.4872538860103627, "recall": 0.6203166226912928}, "dot": {"accuracy": 0.7799368182630982, "accuracy_threshold": 11.667417526245117, "ap": 0.41400500599436885, "f1": 0.4537444933920705, "f1_threshold": 9.120346069335938, "precision": 0.36560180703452727, "recall": 0.5978891820580475}, "euclidean": {"accuracy": 0.8029445073612684, "accuracy_threshold": 2.253568649291992, "ap": 0.5422914316687877, "f1": 0.5412937577886032, "f1_threshold": 2.562668561935425, "precision": 0.4742902521342069, "recall": 0.6303430079155673}, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.5452867829126922, "manhattan": {"accuracy": 0.8030041127734399, "accuracy_threshold": 110.39805603027344, "ap": 0.5410260512168645, "f1": 0.5418653654208717, "f1_threshold": 127.30776977539062, "precision": 0.46738090682992156, "recall": 0.6445910290237467}, "max": {"accuracy": 0.8037789831316684, "ap": 0.5452867829126922, "f1": 0.5457922228670923}, "similarity": {"accuracy": 0.8037789831316684, "accuracy_threshold": 0.8098108172416687, "ap": 0.5452867829126922, "f1": 0.5457922228670923, "f1_threshold": 0.7425128221511841, "precision": 0.4872538860103627, "recall": 0.6203166226912928}}]}, "task_name": "TwitterSemEval2015", "avg_layerwise_entropy": 0.13004326318091927, "num_samples": 64}