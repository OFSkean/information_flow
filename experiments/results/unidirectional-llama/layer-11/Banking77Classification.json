{"dataset_revision": "0fd18e25b25c072e09e0d92ab615fda904d66300", "evaluation_time": 122.4498782157898, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"accuracy": 0.6185714285714285, "f1": 0.6154823668633304, "f1_weighted": 0.6154823668633302, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.6185714285714285, "scores_per_experiment": [{"accuracy": 0.6168831168831169, "f1": 0.61561967682542, "f1_weighted": 0.61561967682542}, {"accuracy": 0.6327922077922078, "f1": 0.6279843007435554, "f1_weighted": 0.6279843007435555}, {"accuracy": 0.6201298701298701, "f1": 0.6180362995661152, "f1_weighted": 0.6180362995661152}, {"accuracy": 0.6266233766233766, "f1": 0.6222372477316751, "f1_weighted": 0.6222372477316752}, {"accuracy": 0.6123376623376623, "f1": 0.6096225326754222, "f1_weighted": 0.6096225326754221}, {"accuracy": 0.6146103896103896, "f1": 0.6114715823555411, "f1_weighted": 0.611471582355541}, {"accuracy": 0.6064935064935065, "f1": 0.6048276689198435, "f1_weighted": 0.6048276689198435}, {"accuracy": 0.6090909090909091, "f1": 0.6037090032478081, "f1_weighted": 0.603709003247808}, {"accuracy": 0.6224025974025974, "f1": 0.618092924611257, "f1_weighted": 0.6180929246112569}, {"accuracy": 0.6243506493506493, "f1": 0.6232224319566659, "f1_weighted": 0.6232224319566659}]}]}, "task_name": "Banking77Classification", "avg_layerwise_entropy": 0.06409483612126618, "num_samples": 64}