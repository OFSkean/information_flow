{"dataset_revision": "70970daeab8776df92f5ea462b6173c0b46fd2d1", "evaluation_time": 186.07084608078003, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"cosine": {"accuracy": 0.8166537521606962, "accuracy_threshold": 0.768778383731842, "ap": 0.5829975337086547, "f1": 0.5633198187455283, "f1_threshold": 0.7185894846916199, "precision": 0.5139251523063534, "recall": 0.6232189973614776}, "dot": {"accuracy": 0.7792811587292127, "accuracy_threshold": 0.8113373517990112, "ap": 0.3837558689076941, "f1": 0.4253056884635832, "f1_threshold": 0.6213927268981934, "precision": 0.32017075773745995, "recall": 0.633245382585752}, "euclidean": {"accuracy": 0.8095607081122966, "accuracy_threshold": 0.6326621174812317, "ap": 0.5474411399417024, "f1": 0.5421294738023112, "f1_threshold": 0.7205790281295776, "precision": 0.4715986726527425, "recall": 0.637467018469657}, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.5829975337086547, "manhattan": {"accuracy": 0.8092030756392681, "accuracy_threshold": 31.198997497558594, "ap": 0.5462474412250118, "f1": 0.5410277420110031, "f1_threshold": 34.987892150878906, "precision": 0.48621922996002526, "recall": 0.6097625329815304}, "max": {"accuracy": 0.8166537521606962, "ap": 0.5829975337086547, "f1": 0.5633198187455283}, "similarity": {"accuracy": 0.8166537521606962, "accuracy_threshold": 0.768778383731842, "ap": 0.5829975337086547, "f1": 0.5633198187455283, "f1_threshold": 0.7185894846916199, "precision": 0.5139251523063534, "recall": 0.6232189973614776}}]}, "task_name": "TwitterSemEval2015", "avg_layerwise_entropy": 0.02402128910619714, "num_samples": 64}