{"dataset_revision": "70970daeab8776df92f5ea462b6173c0b46fd2d1", "evaluation_time": 158.65395760536194, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"cosine": {"accuracy": 0.8112892650652679, "accuracy_threshold": 0.7727139592170715, "ap": 0.5657307822687383, "f1": 0.5478856397434504, "f1_threshold": 0.7048554420471191, "precision": 0.46589018302828616, "recall": 0.6649076517150396}, "dot": {"accuracy": 0.7764797043571556, "accuracy_threshold": 77.84791564941406, "ap": 0.3443955621634846, "f1": 0.4033257747543462, "f1_threshold": 46.655662536621094, "precision": 0.2826271186440678, "recall": 0.7039577836411609}, "euclidean": {"accuracy": 0.8123025570721821, "accuracy_threshold": 5.596395492553711, "ap": 0.5679440226331852, "f1": 0.5528972783143108, "f1_threshold": 6.485732078552246, "precision": 0.4733183013904547, "recall": 0.6646437994722955}, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.5725373538859487, "manhattan": {"accuracy": 0.8133754544912678, "accuracy_threshold": 273.84271240234375, "ap": 0.5725373538859487, "f1": 0.5544997205142538, "f1_threshold": 317.50152587890625, "precision": 0.481086323957323, "recall": 0.6543535620052771}, "max": {"accuracy": 0.8133754544912678, "ap": 0.5725373538859487, "f1": 0.5544997205142538}, "similarity": {"accuracy": 0.8112892650652679, "accuracy_threshold": 0.7727139592170715, "ap": 0.5657307822687383, "f1": 0.5478856397434504, "f1_threshold": 0.7048554420471191, "precision": 0.46589018302828616, "recall": 0.6649076517150396}}]}, "task_name": "TwitterSemEval2015", "avg_layerwise_entropy": 0.4463240589824025, "num_samples": 64}