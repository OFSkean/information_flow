{"dataset_revision": "0fd18e25b25c072e09e0d92ab615fda904d66300", "evaluation_time": 167.93298411369324, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"accuracy": 0.6574675324675324, "f1": 0.6563135567651255, "f1_weighted": 0.6563135567651257, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.6574675324675324, "scores_per_experiment": [{"accuracy": 0.6584415584415585, "f1": 0.6566650317980061, "f1_weighted": 0.6566650317980061}, {"accuracy": 0.6538961038961039, "f1": 0.6532871538426648, "f1_weighted": 0.6532871538426647}, {"accuracy": 0.6623376623376623, "f1": 0.6613644816349562, "f1_weighted": 0.6613644816349563}, {"accuracy": 0.6633116883116883, "f1": 0.6613883593381586, "f1_weighted": 0.6613883593381588}, {"accuracy": 0.6636363636363637, "f1": 0.6648896917834761, "f1_weighted": 0.6648896917834761}, {"accuracy": 0.6496753246753246, "f1": 0.647080688181582, "f1_weighted": 0.6470806881815822}, {"accuracy": 0.6659090909090909, "f1": 0.664027382979022, "f1_weighted": 0.664027382979022}, {"accuracy": 0.6555194805194805, "f1": 0.6560238957510363, "f1_weighted": 0.6560238957510363}, {"accuracy": 0.6555194805194805, "f1": 0.656169093007111, "f1_weighted": 0.6561690930071111}, {"accuracy": 0.6464285714285715, "f1": 0.6422397893352423, "f1_weighted": 0.6422397893352422}]}]}, "task_name": "Banking77Classification", "avg_layerwise_entropy": 0.2726498012845856, "num_samples": 64}