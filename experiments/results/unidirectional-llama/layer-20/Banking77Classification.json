{"dataset_revision": "0fd18e25b25c072e09e0d92ab615fda904d66300", "evaluation_time": 166.52258729934692, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"accuracy": 0.6558116883116882, "f1": 0.6536181824739566, "f1_weighted": 0.6536181824739565, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.6558116883116882, "scores_per_experiment": [{"accuracy": 0.65, "f1": 0.6475786978889062, "f1_weighted": 0.6475786978889064}, {"accuracy": 0.6590909090909091, "f1": 0.6577347422296308, "f1_weighted": 0.6577347422296309}, {"accuracy": 0.6727272727272727, "f1": 0.6713620910406706, "f1_weighted": 0.6713620910406704}, {"accuracy": 0.6464285714285715, "f1": 0.6444882625163579, "f1_weighted": 0.644488262516358}, {"accuracy": 0.6555194805194805, "f1": 0.6538913179253676, "f1_weighted": 0.6538913179253675}, {"accuracy": 0.6626623376623376, "f1": 0.6593124194012722, "f1_weighted": 0.659312419401272}, {"accuracy": 0.6538961038961039, "f1": 0.6520375896037078, "f1_weighted": 0.6520375896037078}, {"accuracy": 0.663961038961039, "f1": 0.6624529902844799, "f1_weighted": 0.6624529902844799}, {"accuracy": 0.6405844155844156, "f1": 0.6391362906930872, "f1_weighted": 0.6391362906930871}, {"accuracy": 0.6532467532467533, "f1": 0.6481874231560861, "f1_weighted": 0.6481874231560861}]}]}, "task_name": "Banking77Classification", "avg_layerwise_entropy": 0.23838052020454178, "num_samples": 64}