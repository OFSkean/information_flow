{"dataset_revision": "70970daeab8776df92f5ea462b6173c0b46fd2d1", "evaluation_time": 158.64786529541016, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"cosine": {"accuracy": 0.8109316325922393, "accuracy_threshold": 0.7794573307037354, "ap": 0.5640538951294682, "f1": 0.5496345201603396, "f1_threshold": 0.7181206941604614, "precision": 0.4968030690537084, "recall": 0.6150395778364116}, "dot": {"accuracy": 0.7765989151814985, "accuracy_threshold": 67.19100189208984, "ap": 0.3480913541635662, "f1": 0.40698772426817753, "f1_threshold": 40.62547302246094, "precision": 0.2899753307916573, "recall": 0.6823218997361478}, "euclidean": {"accuracy": 0.8125409787208678, "accuracy_threshold": 5.160627365112305, "ap": 0.5688968682018882, "f1": 0.5528013582342954, "f1_threshold": 5.93105936050415, "precision": 0.4840436075322101, "recall": 0.6443271767810026}, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.5735980390189149, "manhattan": {"accuracy": 0.8133158490790964, "accuracy_threshold": 251.24542236328125, "ap": 0.5735980390189149, "f1": 0.5553240740740741, "f1_threshold": 289.8396911621094, "precision": 0.49463917525773193, "recall": 0.632981530343008}, "max": {"accuracy": 0.8133158490790964, "ap": 0.5735980390189149, "f1": 0.5553240740740741}, "similarity": {"accuracy": 0.8109316325922393, "accuracy_threshold": 0.7794573307037354, "ap": 0.5640538951294682, "f1": 0.5496345201603396, "f1_threshold": 0.7181206941604614, "precision": 0.4968030690537084, "recall": 0.6150395778364116}}]}, "task_name": "TwitterSemEval2015", "avg_layerwise_entropy": 0.40085761143601156, "num_samples": 64}