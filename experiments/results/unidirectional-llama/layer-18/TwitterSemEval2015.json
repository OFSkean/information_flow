{"dataset_revision": "70970daeab8776df92f5ea462b6173c0b46fd2d1", "evaluation_time": 157.69421887397766, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"cosine": {"accuracy": 0.8131370328425821, "accuracy_threshold": 0.7628494501113892, "ap": 0.5753069939643066, "f1": 0.5589440145653164, "f1_threshold": 0.6879538297653198, "precision": 0.4913965586234494, "recall": 0.6480211081794195}, "dot": {"accuracy": 0.7786851046074984, "accuracy_threshold": 36.11168670654297, "ap": 0.41682911047049465, "f1": 0.462213554363725, "f1_threshold": 25.832462310791016, "precision": 0.3665893271461717, "recall": 0.6253298153034301}, "euclidean": {"accuracy": 0.8125409787208678, "accuracy_threshold": 4.222681999206543, "ap": 0.5698211123982133, "f1": 0.5554691837867851, "f1_threshold": 4.934173107147217, "precision": 0.47957813998082455, "recall": 0.6598944591029023}, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.5753069939643066, "manhattan": {"accuracy": 0.8137926923764678, "accuracy_threshold": 204.28822326660156, "ap": 0.5734177980444634, "f1": 0.5550354099147762, "f1_threshold": 236.02484130859375, "precision": 0.5091389561770535, "recall": 0.6100263852242744}, "max": {"accuracy": 0.8137926923764678, "ap": 0.5753069939643066, "f1": 0.5589440145653164}, "similarity": {"accuracy": 0.8131370328425821, "accuracy_threshold": 0.7628494501113892, "ap": 0.5753069939643066, "f1": 0.5589440145653164, "f1_threshold": 0.6879538297653198, "precision": 0.4913965586234494, "recall": 0.6480211081794195}}]}, "task_name": "TwitterSemEval2015", "avg_layerwise_entropy": 0.3104329214250606, "num_samples": 64}