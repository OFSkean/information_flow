{"dataset_revision": "0fd18e25b25c072e09e0d92ab615fda904d66300", "evaluation_time": 161.9140613079071, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"accuracy": 0.6565584415584416, "f1": 0.6551892012085687, "f1_weighted": 0.6551892012085687, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.6565584415584416, "scores_per_experiment": [{"accuracy": 0.6545454545454545, "f1": 0.6535136326337873, "f1_weighted": 0.6535136326337871}, {"accuracy": 0.6610389610389611, "f1": 0.6610197237486801, "f1_weighted": 0.6610197237486801}, {"accuracy": 0.6519480519480519, "f1": 0.651422887029102, "f1_weighted": 0.651422887029102}, {"accuracy": 0.6516233766233767, "f1": 0.6510461443628992, "f1_weighted": 0.6510461443628994}, {"accuracy": 0.6538961038961039, "f1": 0.6531151178449884, "f1_weighted": 0.6531151178449883}, {"accuracy": 0.6665584415584416, "f1": 0.6661950047951544, "f1_weighted": 0.6661950047951545}, {"accuracy": 0.6415584415584416, "f1": 0.6370218245761512, "f1_weighted": 0.6370218245761511}, {"accuracy": 0.6490259740259741, "f1": 0.6495687526484427, "f1_weighted": 0.6495687526484428}, {"accuracy": 0.6724025974025974, "f1": 0.6701758710053265, "f1_weighted": 0.6701758710053266}, {"accuracy": 0.662987012987013, "f1": 0.6588130534411563, "f1_weighted": 0.6588130534411563}]}]}, "task_name": "Banking77Classification", "avg_layerwise_entropy": 0.41853726388941265, "num_samples": 64}