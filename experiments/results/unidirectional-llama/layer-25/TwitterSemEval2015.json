{"dataset_revision": "70970daeab8776df92f5ea462b6173c0b46fd2d1", "evaluation_time": 142.66527891159058, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"cosine": {"accuracy": 0.8074149132741253, "accuracy_threshold": 0.8397183418273926, "ap": 0.5384892109970855, "f1": 0.5219620958751394, "f1_threshold": 0.7793608903884888, "precision": 0.45193050193050194, "recall": 0.6176781002638523}, "dot": {"accuracy": 0.7742743041068129, "accuracy_threshold": 255.53988647460938, "ap": 0.2558343954140429, "f1": 0.3700296735905044, "f1_threshold": 90.90830993652344, "precision": 0.2276932440657334, "recall": 0.9870712401055409}, "euclidean": {"accuracy": 0.8095607081122966, "accuracy_threshold": 8.028383255004883, "ap": 0.5544838412832183, "f1": 0.5391441087273953, "f1_threshold": 9.269234657287598, "precision": 0.4693917465284569, "recall": 0.633245382585752}, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.5593158899019128, "manhattan": {"accuracy": 0.8110508434165822, "accuracy_threshold": 398.00421142578125, "ap": 0.5593158899019128, "f1": 0.5424626006904488, "f1_threshold": 452.2552490234375, "precision": 0.4810204081632653, "recall": 0.6218997361477573}, "max": {"accuracy": 0.8110508434165822, "ap": 0.5593158899019128, "f1": 0.5424626006904488}, "similarity": {"accuracy": 0.8074149132741253, "accuracy_threshold": 0.8397183418273926, "ap": 0.5384892109970855, "f1": 0.5219620958751394, "f1_threshold": 0.7793608903884888, "precision": 0.45193050193050194, "recall": 0.6176781002638523}}]}, "task_name": "TwitterSemEval2015", "avg_layerwise_entropy": 0.6186439636011458, "num_samples": 64}