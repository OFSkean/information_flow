{"dataset_revision": "70970daeab8776df92f5ea462b6173c0b46fd2d1", "evaluation_time": 147.00320887565613, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"cosine": {"accuracy": 0.8150444060320677, "accuracy_threshold": 0.7682980298995972, "ap": 0.5818712480425483, "f1": 0.5599459642012833, "f1_threshold": 0.7092875242233276, "precision": 0.4883172982525034, "recall": 0.6562005277044854}, "dot": {"accuracy": 0.7818441914525839, "accuracy_threshold": 2.82424259185791, "ap": 0.4075167421854644, "f1": 0.44080053817692566, "f1_threshold": 2.042327880859375, "precision": 0.32350037027894346, "recall": 0.6915567282321899}, "euclidean": {"accuracy": 0.810037551409668, "accuracy_threshold": 1.1506140232086182, "ap": 0.554898998573548, "f1": 0.5455103921367871, "f1_threshold": 1.3578786849975586, "precision": 0.44570854943951815, "recall": 0.7029023746701847}, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.5818712480425483, "manhattan": {"accuracy": 0.8094414972879538, "accuracy_threshold": 56.994163513183594, "ap": 0.5534399311170103, "f1": 0.5459156715629293, "f1_threshold": 66.43695831298828, "precision": 0.4553146483342147, "recall": 0.6815303430079156}, "max": {"accuracy": 0.8150444060320677, "ap": 0.5818712480425483, "f1": 0.5599459642012833}, "similarity": {"accuracy": 0.8150444060320677, "accuracy_threshold": 0.7682980298995972, "ap": 0.5818712480425483, "f1": 0.5599459642012833, "f1_threshold": 0.7092875242233276, "precision": 0.4883172982525034, "recall": 0.6562005277044854}}]}, "task_name": "TwitterSemEval2015", "avg_layerwise_entropy": 0.05619070009911233, "num_samples": 64}