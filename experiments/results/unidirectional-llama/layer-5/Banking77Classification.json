{"dataset_revision": "0fd18e25b25c072e09e0d92ab615fda904d66300", "evaluation_time": 111.18888759613037, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"accuracy": 0.6134090909090909, "f1": 0.6071326181353597, "f1_weighted": 0.6071326181353596, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.6134090909090909, "scores_per_experiment": [{"accuracy": 0.6126623376623377, "f1": 0.602218569599021, "f1_weighted": 0.6022185695990209}, {"accuracy": 0.6025974025974026, "f1": 0.6002332544403486, "f1_weighted": 0.6002332544403485}, {"accuracy": 0.6071428571428571, "f1": 0.6009632666461583, "f1_weighted": 0.6009632666461583}, {"accuracy": 0.6100649350649351, "f1": 0.604377247350803, "f1_weighted": 0.604377247350803}, {"accuracy": 0.612987012987013, "f1": 0.6109157626129541, "f1_weighted": 0.6109157626129542}, {"accuracy": 0.6211038961038962, "f1": 0.6150043845669997, "f1_weighted": 0.6150043845669996}, {"accuracy": 0.6295454545454545, "f1": 0.6202788193398543, "f1_weighted": 0.6202788193398544}, {"accuracy": 0.6136363636363636, "f1": 0.6039130398285156, "f1_weighted": 0.6039130398285155}, {"accuracy": 0.5987012987012987, "f1": 0.5940056076418132, "f1_weighted": 0.5940056076418132}, {"accuracy": 0.6256493506493507, "f1": 0.6194162293271278, "f1_weighted": 0.6194162293271278}]}]}, "task_name": "Banking77Classification", "avg_layerwise_entropy": 0.028673857739738275, "num_samples": 64}