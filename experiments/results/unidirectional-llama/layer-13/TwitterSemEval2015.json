{"dataset_revision": "70970daeab8776df92f5ea462b6173c0b46fd2d1", "evaluation_time": 151.4563591480255, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"cosine": {"accuracy": 0.8065208320915539, "accuracy_threshold": 0.8257589340209961, "ap": 0.5503387297143104, "f1": 0.5436144578313253, "f1_threshold": 0.7682170867919922, "precision": 0.5002217294900222, "recall": 0.5952506596306069}, "dot": {"accuracy": 0.7799368182630982, "accuracy_threshold": 15.561408996582031, "ap": 0.393667739490318, "f1": 0.42962822458270106, "f1_threshold": 12.123651504516602, "precision": 0.3353568255848386, "recall": 0.5976253298153035}, "euclidean": {"accuracy": 0.8038981939560113, "accuracy_threshold": 2.434234142303467, "ap": 0.5428931659975077, "f1": 0.5409995147986414, "f1_threshold": 2.7433395385742188, "precision": 0.5006735518634935, "recall": 0.5883905013192612}, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.5503387297143104, "manhattan": {"accuracy": 0.8043154318412112, "accuracy_threshold": 119.98231506347656, "ap": 0.542986991305815, "f1": 0.542555632025694, "f1_threshold": 136.27578735351562, "precision": 0.4799107142857143, "recall": 0.6240105540897097}, "max": {"accuracy": 0.8065208320915539, "ap": 0.5503387297143104, "f1": 0.5436144578313253}, "similarity": {"accuracy": 0.8065208320915539, "accuracy_threshold": 0.8257589340209961, "ap": 0.5503387297143104, "f1": 0.5436144578313253, "f1_threshold": 0.7682170867919922, "precision": 0.5002217294900222, "recall": 0.5952506596306069}}]}, "task_name": "TwitterSemEval2015", "avg_layerwise_entropy": 0.1502046418277658, "num_samples": 64}