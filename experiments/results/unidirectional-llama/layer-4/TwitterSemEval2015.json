{"dataset_revision": "70970daeab8776df92f5ea462b6173c0b46fd2d1", "evaluation_time": 181.39434337615967, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"cosine": {"accuracy": 0.816772962985039, "accuracy_threshold": 0.7696611881256104, "ap": 0.5807675029157163, "f1": 0.5605095541401274, "f1_threshold": 0.7005730867385864, "precision": 0.47432852183446006, "recall": 0.6849604221635884}, "dot": {"accuracy": 0.7808308994456696, "accuracy_threshold": 1.6334569454193115, "ap": 0.39265853495307956, "f1": 0.42783365044849136, "f1_threshold": 1.241195559501648, "precision": 0.3257899820615427, "recall": 0.6229551451187335}, "euclidean": {"accuracy": 0.8082493890445253, "accuracy_threshold": 0.8875775337219238, "ap": 0.545335172303116, "f1": 0.5402024337541226, "f1_threshold": 1.0200939178466797, "precision": 0.4747151708974615, "recall": 0.6266490765171504}, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.5807675029157163, "manhattan": {"accuracy": 0.8082493890445253, "accuracy_threshold": 43.79155731201172, "ap": 0.5439972729088106, "f1": 0.539733950260266, "f1_threshold": 50.27278518676758, "precision": 0.48053553038105046, "recall": 0.6155672823218997}, "max": {"accuracy": 0.816772962985039, "ap": 0.5807675029157163, "f1": 0.5605095541401274}, "similarity": {"accuracy": 0.816772962985039, "accuracy_threshold": 0.7696611881256104, "ap": 0.5807675029157163, "f1": 0.5605095541401274, "f1_threshold": 0.7005730867385864, "precision": 0.47432852183446006, "recall": 0.6849604221635884}}]}, "task_name": "TwitterSemEval2015", "avg_layerwise_entropy": 0.03868239630432544, "num_samples": 64}