{"dataset_revision": "70970daeab8776df92f5ea462b6173c0b46fd2d1", "evaluation_time": 154.06817722320557, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"cosine": {"accuracy": 0.8138522977886392, "accuracy_threshold": 0.7929747104644775, "ap": 0.5614063331552993, "f1": 0.5486151350098505, "f1_threshold": 0.7300524711608887, "precision": 0.48915065096094235, "recall": 0.6245382585751978}, "dot": {"accuracy": 0.7783274721344698, "accuracy_threshold": 0.38011524081230164, "ap": 0.3653654832615496, "f1": 0.4195712649320897, "f1_threshold": 0.2801237106323242, "precision": 0.3040796963946869, "recall": 0.6765171503957783}, "euclidean": {"accuracy": 0.8059247779698396, "accuracy_threshold": 0.4025430977344513, "ap": 0.53097704191634, "f1": 0.5299640663034659, "f1_threshold": 0.4672316312789917, "precision": 0.4726069878023568, "recall": 0.6031662269129288}, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.5614063331552993, "manhattan": {"accuracy": 0.8055671454968111, "accuracy_threshold": 19.27781105041504, "ap": 0.5301219258066143, "f1": 0.5314254265272427, "f1_threshold": 22.939664840698242, "precision": 0.4559017941454202, "recall": 0.6369393139841689}, "max": {"accuracy": 0.8138522977886392, "ap": 0.5614063331552993, "f1": 0.5486151350098505}, "similarity": {"accuracy": 0.8138522977886392, "accuracy_threshold": 0.7929747104644775, "ap": 0.5614063331552993, "f1": 0.5486151350098505, "f1_threshold": 0.7300524711608887, "precision": 0.48915065096094235, "recall": 0.6245382585751978}}]}, "task_name": "TwitterSemEval2015", "avg_layerwise_entropy": 0.012347024890493425, "num_samples": 64}