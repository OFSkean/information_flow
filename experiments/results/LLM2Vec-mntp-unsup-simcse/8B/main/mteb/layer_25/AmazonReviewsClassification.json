{
  "dataset_revision": "1399c76144fd37290681b995c656ef9b2e06e26d",
  "evaluation_time": 807.4400515556335,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.21",
  "scores": {
    "test": [
      {
        "accuracy": 0.38536,
        "f1": 0.38129269237549573,
        "f1_weighted": 0.38129269237549573,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.38536,
        "scores_per_experiment": [
          {
            "accuracy": 0.4058,
            "f1": 0.3964808979397101,
            "f1_weighted": 0.39648089793971014
          },
          {
            "accuracy": 0.4012,
            "f1": 0.4049763412470971,
            "f1_weighted": 0.4049763412470971
          },
          {
            "accuracy": 0.3716,
            "f1": 0.37034147815390134,
            "f1_weighted": 0.3703414781539014
          },
          {
            "accuracy": 0.3714,
            "f1": 0.37415687510560486,
            "f1_weighted": 0.37415687510560486
          },
          {
            "accuracy": 0.4056,
            "f1": 0.40022399475275544,
            "f1_weighted": 0.40022399475275555
          },
          {
            "accuracy": 0.3792,
            "f1": 0.36785662270034464,
            "f1_weighted": 0.3678566227003446
          },
          {
            "accuracy": 0.3752,
            "f1": 0.37495836215767575,
            "f1_weighted": 0.37495836215767575
          },
          {
            "accuracy": 0.3964,
            "f1": 0.3900731867528084,
            "f1_weighted": 0.3900731867528084
          },
          {
            "accuracy": 0.3726,
            "f1": 0.36546576904664757,
            "f1_weighted": 0.3654657690466476
          },
          {
            "accuracy": 0.3746,
            "f1": 0.368393395898412,
            "f1_weighted": 0.368393395898412
          }
        ]
      }
    ]
  },
  "task_name": "AmazonReviewsClassification"
}