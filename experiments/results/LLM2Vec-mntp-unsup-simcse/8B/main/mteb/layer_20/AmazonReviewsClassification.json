{
  "dataset_revision": "1399c76144fd37290681b995c656ef9b2e06e26d",
  "evaluation_time": 619.2119226455688,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.21",
  "scores": {
    "test": [
      {
        "accuracy": 0.40674,
        "f1": 0.40123442939262033,
        "f1_weighted": 0.40123442939262033,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.40674,
        "scores_per_experiment": [
          {
            "accuracy": 0.4196,
            "f1": 0.41198088576564923,
            "f1_weighted": 0.41198088576564923
          },
          {
            "accuracy": 0.4306,
            "f1": 0.43270007381973896,
            "f1_weighted": 0.432700073819739
          },
          {
            "accuracy": 0.3922,
            "f1": 0.3886168146584055,
            "f1_weighted": 0.3886168146584055
          },
          {
            "accuracy": 0.406,
            "f1": 0.4067853624588932,
            "f1_weighted": 0.40678536245889324
          },
          {
            "accuracy": 0.4276,
            "f1": 0.4149283707154107,
            "f1_weighted": 0.4149283707154107
          },
          {
            "accuracy": 0.3972,
            "f1": 0.37955835695249374,
            "f1_weighted": 0.3795583569524937
          },
          {
            "accuracy": 0.394,
            "f1": 0.3948806035328651,
            "f1_weighted": 0.394880603532865
          },
          {
            "accuracy": 0.4096,
            "f1": 0.405124257305898,
            "f1_weighted": 0.40512425730589796
          },
          {
            "accuracy": 0.3926,
            "f1": 0.38726082787060406,
            "f1_weighted": 0.38726082787060406
          },
          {
            "accuracy": 0.398,
            "f1": 0.3905087408462446,
            "f1_weighted": 0.39050874084624465
          }
        ]
      }
    ]
  },
  "task_name": "AmazonReviewsClassification"
}