{
  "dataset_revision": "1399c76144fd37290681b995c656ef9b2e06e26d",
  "evaluation_time": 653.6084742546082,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.21",
  "scores": {
    "test": [
      {
        "accuracy": 0.40902000000000005,
        "f1": 0.4033162567850416,
        "f1_weighted": 0.4033162567850416,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.40902000000000005,
        "scores_per_experiment": [
          {
            "accuracy": 0.42,
            "f1": 0.4117248400086647,
            "f1_weighted": 0.41172484000866477
          },
          {
            "accuracy": 0.4336,
            "f1": 0.43648934824438096,
            "f1_weighted": 0.4364893482443809
          },
          {
            "accuracy": 0.3974,
            "f1": 0.39265286000752087,
            "f1_weighted": 0.39265286000752087
          },
          {
            "accuracy": 0.4154,
            "f1": 0.41556928828801354,
            "f1_weighted": 0.41556928828801354
          },
          {
            "accuracy": 0.4344,
            "f1": 0.42181744103352853,
            "f1_weighted": 0.4218174410335286
          },
          {
            "accuracy": 0.3946,
            "f1": 0.3773737164479293,
            "f1_weighted": 0.37737371644792933
          },
          {
            "accuracy": 0.3946,
            "f1": 0.39585604027142673,
            "f1_weighted": 0.3958560402714268
          },
          {
            "accuracy": 0.4076,
            "f1": 0.4023274525357805,
            "f1_weighted": 0.40232745253578045
          },
          {
            "accuracy": 0.393,
            "f1": 0.3885176134203445,
            "f1_weighted": 0.3885176134203446
          },
          {
            "accuracy": 0.3996,
            "f1": 0.39083396759282707,
            "f1_weighted": 0.39083396759282707
          }
        ]
      }
    ]
  },
  "task_name": "AmazonReviewsClassification"
}