{
  "dataset_revision": "1399c76144fd37290681b995c656ef9b2e06e26d",
  "evaluation_time": 396.54946422576904,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.21",
  "scores": {
    "test": [
      {
        "accuracy": 0.43050000000000005,
        "f1": 0.42322118713105183,
        "f1_weighted": 0.42322118713105183,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.43050000000000005,
        "scores_per_experiment": [
          {
            "accuracy": 0.4388,
            "f1": 0.4386835515450702,
            "f1_weighted": 0.43868355154507005
          },
          {
            "accuracy": 0.4578,
            "f1": 0.4511754756898535,
            "f1_weighted": 0.45117547568985344
          },
          {
            "accuracy": 0.4002,
            "f1": 0.39512670550875406,
            "f1_weighted": 0.395126705508754
          },
          {
            "accuracy": 0.4312,
            "f1": 0.4275298330573872,
            "f1_weighted": 0.4275298330573872
          },
          {
            "accuracy": 0.4608,
            "f1": 0.43876310444631716,
            "f1_weighted": 0.43876310444631716
          },
          {
            "accuracy": 0.4212,
            "f1": 0.40036306333228244,
            "f1_weighted": 0.4003630633322825
          },
          {
            "accuracy": 0.4012,
            "f1": 0.4028923739317455,
            "f1_weighted": 0.40289237393174554
          },
          {
            "accuracy": 0.4224,
            "f1": 0.4223550669742756,
            "f1_weighted": 0.42235506697427555
          },
          {
            "accuracy": 0.4306,
            "f1": 0.4299554512448312,
            "f1_weighted": 0.42995545124483125
          },
          {
            "accuracy": 0.4408,
            "f1": 0.42536724558000116,
            "f1_weighted": 0.4253672455800012
          }
        ]
      }
    ]
  },
  "task_name": "AmazonReviewsClassification"
}