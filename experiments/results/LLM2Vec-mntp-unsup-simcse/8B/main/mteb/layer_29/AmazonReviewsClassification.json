{
  "dataset_revision": "1399c76144fd37290681b995c656ef9b2e06e26d",
  "evaluation_time": 904.9663460254669,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.21",
  "scores": {
    "test": [
      {
        "accuracy": 0.38927999999999996,
        "f1": 0.3861739679714925,
        "f1_weighted": 0.38617396797149256,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.38927999999999996,
        "scores_per_experiment": [
          {
            "accuracy": 0.409,
            "f1": 0.4018384194251329,
            "f1_weighted": 0.4018384194251328
          },
          {
            "accuracy": 0.405,
            "f1": 0.4079259260877399,
            "f1_weighted": 0.4079259260877399
          },
          {
            "accuracy": 0.372,
            "f1": 0.3716497960184846,
            "f1_weighted": 0.37164979601848463
          },
          {
            "accuracy": 0.3736,
            "f1": 0.37538671090317643,
            "f1_weighted": 0.3753867109031765
          },
          {
            "accuracy": 0.4176,
            "f1": 0.412497202446416,
            "f1_weighted": 0.41249720244641597
          },
          {
            "accuracy": 0.381,
            "f1": 0.37351873662314894,
            "f1_weighted": 0.373518736623149
          },
          {
            "accuracy": 0.3764,
            "f1": 0.37585644738295804,
            "f1_weighted": 0.375856447382958
          },
          {
            "accuracy": 0.4024,
            "f1": 0.39707054057743263,
            "f1_weighted": 0.39707054057743263
          },
          {
            "accuracy": 0.38,
            "f1": 0.37472276347942185,
            "f1_weighted": 0.3747227634794219
          },
          {
            "accuracy": 0.3758,
            "f1": 0.3712731367710139,
            "f1_weighted": 0.37127313677101387
          }
        ]
      }
    ]
  },
  "task_name": "AmazonReviewsClassification"
}