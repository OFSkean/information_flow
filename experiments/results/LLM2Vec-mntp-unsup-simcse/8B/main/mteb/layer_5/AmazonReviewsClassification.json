{
  "dataset_revision": "1399c76144fd37290681b995c656ef9b2e06e26d",
  "evaluation_time": 193.7332248687744,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.21",
  "scores": {
    "test": [
      {
        "accuracy": 0.37754000000000004,
        "f1": 0.37412807731047304,
        "f1_weighted": 0.37412807731047304,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.37754000000000004,
        "scores_per_experiment": [
          {
            "accuracy": 0.4028,
            "f1": 0.39719550321149033,
            "f1_weighted": 0.3971955032114904
          },
          {
            "accuracy": 0.389,
            "f1": 0.38777549031036623,
            "f1_weighted": 0.38777549031036623
          },
          {
            "accuracy": 0.3718,
            "f1": 0.37143636926949547,
            "f1_weighted": 0.37143636926949547
          },
          {
            "accuracy": 0.384,
            "f1": 0.3833823003088001,
            "f1_weighted": 0.3833823003088001
          },
          {
            "accuracy": 0.4098,
            "f1": 0.39874361068869546,
            "f1_weighted": 0.39874361068869546
          },
          {
            "accuracy": 0.345,
            "f1": 0.338576384797922,
            "f1_weighted": 0.338576384797922
          },
          {
            "accuracy": 0.3582,
            "f1": 0.35984718431268187,
            "f1_weighted": 0.35984718431268187
          },
          {
            "accuracy": 0.3838,
            "f1": 0.3816703866769224,
            "f1_weighted": 0.38167038667692244
          },
          {
            "accuracy": 0.3654,
            "f1": 0.36064367593562363,
            "f1_weighted": 0.36064367593562363
          },
          {
            "accuracy": 0.3656,
            "f1": 0.362009867592733,
            "f1_weighted": 0.362009867592733
          }
        ]
      }
    ]
  },
  "task_name": "AmazonReviewsClassification"
}