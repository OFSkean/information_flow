{
  "dataset_revision": "1399c76144fd37290681b995c656ef9b2e06e26d",
  "evaluation_time": 599.5196907520294,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.21",
  "scores": {
    "test": [
      {
        "accuracy": 0.41150000000000003,
        "f1": 0.40565986872820525,
        "f1_weighted": 0.40565986872820525,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.41150000000000003,
        "scores_per_experiment": [
          {
            "accuracy": 0.4234,
            "f1": 0.41577480090407476,
            "f1_weighted": 0.4157748009040747
          },
          {
            "accuracy": 0.4368,
            "f1": 0.43843383616780507,
            "f1_weighted": 0.438433836167805
          },
          {
            "accuracy": 0.3954,
            "f1": 0.3918482653167588,
            "f1_weighted": 0.3918482653167588
          },
          {
            "accuracy": 0.4152,
            "f1": 0.41556136867940563,
            "f1_weighted": 0.4155613686794057
          },
          {
            "accuracy": 0.43,
            "f1": 0.417564092581061,
            "f1_weighted": 0.417564092581061
          },
          {
            "accuracy": 0.4082,
            "f1": 0.38880857434324767,
            "f1_weighted": 0.38880857434324767
          },
          {
            "accuracy": 0.3946,
            "f1": 0.39566457157418544,
            "f1_weighted": 0.39566457157418544
          },
          {
            "accuracy": 0.4088,
            "f1": 0.40451343196061956,
            "f1_weighted": 0.40451343196061956
          },
          {
            "accuracy": 0.402,
            "f1": 0.396903503698384,
            "f1_weighted": 0.39690350369838406
          },
          {
            "accuracy": 0.4006,
            "f1": 0.3915262420565115,
            "f1_weighted": 0.39152624205651143
          }
        ]
      }
    ]
  },
  "task_name": "AmazonReviewsClassification"
}