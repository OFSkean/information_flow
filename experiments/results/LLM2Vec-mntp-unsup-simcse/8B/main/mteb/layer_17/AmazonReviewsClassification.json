{
  "dataset_revision": "1399c76144fd37290681b995c656ef9b2e06e26d",
  "evaluation_time": 543.1482081413269,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.21",
  "scores": {
    "test": [
      {
        "accuracy": 0.4120799999999999,
        "f1": 0.405209017380831,
        "f1_weighted": 0.405209017380831,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.4120799999999999,
        "scores_per_experiment": [
          {
            "accuracy": 0.425,
            "f1": 0.4162717763303876,
            "f1_weighted": 0.41627177633038753
          },
          {
            "accuracy": 0.4392,
            "f1": 0.4381704674752994,
            "f1_weighted": 0.4381704674752993
          },
          {
            "accuracy": 0.3928,
            "f1": 0.38417989558865073,
            "f1_weighted": 0.38417989558865073
          },
          {
            "accuracy": 0.4148,
            "f1": 0.414229304007088,
            "f1_weighted": 0.414229304007088
          },
          {
            "accuracy": 0.4342,
            "f1": 0.4171682660823997,
            "f1_weighted": 0.4171682660823997
          },
          {
            "accuracy": 0.411,
            "f1": 0.3892943563756911,
            "f1_weighted": 0.38929435637569104
          },
          {
            "accuracy": 0.3966,
            "f1": 0.39731945851326483,
            "f1_weighted": 0.3973194585132649
          },
          {
            "accuracy": 0.404,
            "f1": 0.4042952777054943,
            "f1_weighted": 0.4042952777054942
          },
          {
            "accuracy": 0.4008,
            "f1": 0.3971067300822147,
            "f1_weighted": 0.3971067300822147
          },
          {
            "accuracy": 0.4024,
            "f1": 0.39405464164781984,
            "f1_weighted": 0.39405464164781984
          }
        ]
      }
    ]
  },
  "task_name": "AmazonReviewsClassification"
}