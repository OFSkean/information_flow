{
  "dataset_revision": "1399c76144fd37290681b995c656ef9b2e06e26d",
  "evaluation_time": 839.9493095874786,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.21",
  "scores": {
    "test": [
      {
        "accuracy": 0.39268000000000003,
        "f1": 0.3887083866058104,
        "f1_weighted": 0.3887083866058104,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.39268000000000003,
        "scores_per_experiment": [
          {
            "accuracy": 0.4136,
            "f1": 0.40527450233866114,
            "f1_weighted": 0.4052745023386612
          },
          {
            "accuracy": 0.408,
            "f1": 0.41132820472626613,
            "f1_weighted": 0.4113282047262661
          },
          {
            "accuracy": 0.3768,
            "f1": 0.3759149644636679,
            "f1_weighted": 0.3759149644636679
          },
          {
            "accuracy": 0.3768,
            "f1": 0.37934421748665054,
            "f1_weighted": 0.37934421748665054
          },
          {
            "accuracy": 0.4112,
            "f1": 0.4053673457046362,
            "f1_weighted": 0.4053673457046362
          },
          {
            "accuracy": 0.3898,
            "f1": 0.37938322381143336,
            "f1_weighted": 0.3793832238114334
          },
          {
            "accuracy": 0.38,
            "f1": 0.38029664679719577,
            "f1_weighted": 0.38029664679719566
          },
          {
            "accuracy": 0.4048,
            "f1": 0.3980954424891693,
            "f1_weighted": 0.3980954424891692
          },
          {
            "accuracy": 0.3824,
            "f1": 0.37548391490720207,
            "f1_weighted": 0.375483914907202
          },
          {
            "accuracy": 0.3834,
            "f1": 0.3765954033332221,
            "f1_weighted": 0.3765954033332221
          }
        ]
      }
    ]
  },
  "task_name": "AmazonReviewsClassification"
}