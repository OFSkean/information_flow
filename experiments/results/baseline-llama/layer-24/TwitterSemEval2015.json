{"dataset_revision": "70970daeab8776df92f5ea462b6173c0b46fd2d1", "evaluation_time": 141.7989842891693, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"cosine": {"accuracy": 0.8230911366752101, "accuracy_threshold": 0.8481591939926147, "ap": 0.6041727626762055, "f1": 0.566042182160952, "f1_threshold": 0.8112427592277527, "precision": 0.5113902490951672, "recall": 0.6337730870712401}, "dot": {"accuracy": 0.7765393097693271, "accuracy_threshold": 213.77769470214844, "ap": 0.31596177530993236, "f1": 0.37443816987438056, "f1_threshold": 133.52700805664062, "precision": 0.23953111176644057, "recall": 0.8572559366754617}, "euclidean": {"accuracy": 0.8183227037014961, "accuracy_threshold": 7.602353572845459, "ap": 0.5818727672812851, "f1": 0.5623107971745711, "f1_threshold": 8.440336227416992, "precision": 0.5386660222329628, "recall": 0.5881266490765171}, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.6041727626762055, "manhattan": {"accuracy": 0.8173690171067532, "accuracy_threshold": 362.029296875, "ap": 0.5871091731987044, "f1": 0.5702914666978812, "f1_threshold": 417.53228759765625, "precision": 0.5125184094256259, "recall": 0.6427440633245383}, "max": {"accuracy": 0.8230911366752101, "ap": 0.6041727626762055, "f1": 0.5702914666978812}, "similarity": {"accuracy": 0.8230911366752101, "accuracy_threshold": 0.8481591939926147, "ap": 0.6041727626762055, "f1": 0.566042182160952, "f1_threshold": 0.8112427592277527, "precision": 0.5113902490951672, "recall": 0.6337730870712401}}]}, "task_name": "TwitterSemEval2015", "avg_layerwise_entropy": 0.8778328917645913, "num_samples": 64}