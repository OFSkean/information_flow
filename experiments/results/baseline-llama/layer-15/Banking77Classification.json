{"dataset_revision": "0fd18e25b25c072e09e0d92ab615fda904d66300", "evaluation_time": 133.6982798576355, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"accuracy": 0.6089610389610389, "f1": 0.606614429443136, "f1_weighted": 0.606614429443136, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.6089610389610389, "scores_per_experiment": [{"accuracy": 0.6081168831168832, "f1": 0.6052357162646945, "f1_weighted": 0.6052357162646944}, {"accuracy": 0.6009740259740259, "f1": 0.6021162716985352, "f1_weighted": 0.6021162716985353}, {"accuracy": 0.6269480519480519, "f1": 0.6245747896297719, "f1_weighted": 0.624574789629772}, {"accuracy": 0.6188311688311688, "f1": 0.6162427936423202, "f1_weighted": 0.6162427936423203}, {"accuracy": 0.6116883116883117, "f1": 0.6090933179089736, "f1_weighted": 0.6090933179089735}, {"accuracy": 0.6048701298701299, "f1": 0.6011944619673144, "f1_weighted": 0.6011944619673145}, {"accuracy": 0.6103896103896104, "f1": 0.6073535241910808, "f1_weighted": 0.6073535241910808}, {"accuracy": 0.5987012987012987, "f1": 0.5961899909430532, "f1_weighted": 0.5961899909430532}, {"accuracy": 0.6162337662337662, "f1": 0.6139688629123536, "f1_weighted": 0.6139688629123536}, {"accuracy": 0.5928571428571429, "f1": 0.5901745652732626, "f1_weighted": 0.5901745652732626}]}]}, "task_name": "Banking77Classification", "avg_layerwise_entropy": 0.30300196026437004, "num_samples": 64}