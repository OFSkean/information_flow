{"dataset_revision": "70970daeab8776df92f5ea462b6173c0b46fd2d1", "evaluation_time": 141.02268958091736, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"cosine": {"accuracy": 0.8080109673958396, "accuracy_threshold": 0.8424648642539978, "ap": 0.5399335319549288, "f1": 0.5277242317723098, "f1_threshold": 0.796907901763916, "precision": 0.4627162457745079, "recall": 0.6139841688654354}, "dot": {"accuracy": 0.7779102342492699, "accuracy_threshold": 20.290145874023438, "ap": 0.3176305492119738, "f1": 0.3771040017835247, "f1_threshold": 12.569109916687012, "precision": 0.23904748445449406, "recall": 0.8926121372031662}, "euclidean": {"accuracy": 0.8065208320915539, "accuracy_threshold": 2.3665008544921875, "ap": 0.5309203481003044, "f1": 0.5249094202898551, "f1_threshold": 2.7491769790649414, "precision": 0.45973819912733044, "recall": 0.6116094986807388}, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.5399335319549288, "manhattan": {"accuracy": 0.8061035942063539, "accuracy_threshold": 116.29246520996094, "ap": 0.5257361007521185, "f1": 0.5227828924367789, "f1_threshold": 132.61058044433594, "precision": 0.46816948445001044, "recall": 0.591820580474934}, "max": {"accuracy": 0.8080109673958396, "ap": 0.5399335319549288, "f1": 0.5277242317723098}, "similarity": {"accuracy": 0.8080109673958396, "accuracy_threshold": 0.8424648642539978, "ap": 0.5399335319549288, "f1": 0.5277242317723098, "f1_threshold": 0.796907901763916, "precision": 0.4627162457745079, "recall": 0.6139841688654354}}]}, "task_name": "TwitterSemEval2015", "avg_layerwise_entropy": 0.6228966414143469, "num_samples": 64}