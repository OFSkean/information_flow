{"dataset_revision": "0fd18e25b25c072e09e0d92ab615fda904d66300", "evaluation_time": 113.98733496665955, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"accuracy": 0.6038636363636364, "f1": 0.6009873334584863, "f1_weighted": 0.6009873334584863, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.6038636363636364, "scores_per_experiment": [{"accuracy": 0.5980519480519481, "f1": 0.5943680193806021, "f1_weighted": 0.5943680193806021}, {"accuracy": 0.5853896103896103, "f1": 0.5860039694398755, "f1_weighted": 0.5860039694398755}, {"accuracy": 0.6217532467532467, "f1": 0.6170331822968037, "f1_weighted": 0.6170331822968038}, {"accuracy": 0.6211038961038962, "f1": 0.6204877056151503, "f1_weighted": 0.6204877056151503}, {"accuracy": 0.6142857142857143, "f1": 0.612234068333383, "f1_weighted": 0.6122340683333829}, {"accuracy": 0.6, "f1": 0.5964162358957145, "f1_weighted": 0.5964162358957144}, {"accuracy": 0.5928571428571429, "f1": 0.5887182033393547, "f1_weighted": 0.5887182033393548}, {"accuracy": 0.6061688311688311, "f1": 0.6021536946232503, "f1_weighted": 0.6021536946232504}, {"accuracy": 0.6103896103896104, "f1": 0.608742390120763, "f1_weighted": 0.608742390120763}, {"accuracy": 0.5886363636363636, "f1": 0.5837158655399666, "f1_weighted": 0.5837158655399665}]}]}, "task_name": "Banking77Classification", "avg_layerwise_entropy": 0.1739261807023944, "num_samples": 64}