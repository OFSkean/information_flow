{"dataset_revision": "70970daeab8776df92f5ea462b6173c0b46fd2d1", "evaluation_time": 141.1555564403534, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"cosine": {"accuracy": 0.8083089944566967, "accuracy_threshold": 0.863297700881958, "ap": 0.5563447044795, "f1": 0.5399416909620991, "f1_threshold": 0.8033360242843628, "precision": 0.48380355276907, "recall": 0.6108179419525066}, "dot": {"accuracy": 0.7832151159325267, "accuracy_threshold": 3.1028285026550293, "ap": 0.42079544579483064, "f1": 0.4614160700079554, "f1_threshold": 2.6998136043548584, "precision": 0.37025215448451965, "recall": 0.6121372031662269}, "euclidean": {"accuracy": 0.8065208320915539, "accuracy_threshold": 0.9817949533462524, "ap": 0.53372308401248, "f1": 0.5223391019115485, "f1_threshold": 1.1658103466033936, "precision": 0.4571372005543457, "recall": 0.6092348284960423}, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.5563447044795, "manhattan": {"accuracy": 0.8053883292602968, "accuracy_threshold": 48.43885040283203, "ap": 0.531216646511404, "f1": 0.5216874628639334, "f1_threshold": 56.62205123901367, "precision": 0.4745945945945946, "recall": 0.579155672823219}, "max": {"accuracy": 0.8083089944566967, "ap": 0.5563447044795, "f1": 0.5399416909620991}, "similarity": {"accuracy": 0.8083089944566967, "accuracy_threshold": 0.863297700881958, "ap": 0.5563447044795, "f1": 0.5399416909620991, "f1_threshold": 0.8033360242843628, "precision": 0.48380355276907, "recall": 0.6108179419525066}}]}, "task_name": "TwitterSemEval2015", "avg_layerwise_entropy": 0.2957265109129203, "num_samples": 64}