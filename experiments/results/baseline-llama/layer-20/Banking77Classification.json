{"dataset_revision": "0fd18e25b25c072e09e0d92ab615fda904d66300", "evaluation_time": 161.6542203426361, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"accuracy": 0.6918181818181818, "f1": 0.690666611887332, "f1_weighted": 0.690666611887332, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.6918181818181818, "scores_per_experiment": [{"accuracy": 0.686038961038961, "f1": 0.6838508673840973, "f1_weighted": 0.6838508673840976}, {"accuracy": 0.6909090909090909, "f1": 0.691336088097347, "f1_weighted": 0.6913360880973469}, {"accuracy": 0.7016233766233766, "f1": 0.7019951502318329, "f1_weighted": 0.7019951502318329}, {"accuracy": 0.7016233766233766, "f1": 0.702171895069122, "f1_weighted": 0.7021718950691219}, {"accuracy": 0.6938311688311688, "f1": 0.6933056210946519, "f1_weighted": 0.6933056210946519}, {"accuracy": 0.6840909090909091, "f1": 0.6816013377823134, "f1_weighted": 0.6816013377823135}, {"accuracy": 0.6977272727272728, "f1": 0.6953489827149916, "f1_weighted": 0.6953489827149916}, {"accuracy": 0.6873376623376624, "f1": 0.6841569327918352, "f1_weighted": 0.6841569327918352}, {"accuracy": 0.6905844155844156, "f1": 0.6902466314646565, "f1_weighted": 0.6902466314646565}, {"accuracy": 0.6844155844155844, "f1": 0.6826526122424719, "f1_weighted": 0.6826526122424719}]}]}, "task_name": "Banking77Classification", "avg_layerwise_entropy": 0.5359006606710981, "num_samples": 64}