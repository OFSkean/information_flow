{
  "dataset_revision": "70970daeab8776df92f5ea462b6173c0b46fd2d1",
  "evaluation_time": 141.0655701160431,
  "kg_co2_emissions": null,
  "mteb_version": "1.12.49",
  "scores": {
    "test": [
      {
        "cosine": {
          "accuracy": 0.8196936281814389,
          "accuracy_threshold": 0.8232359886169434,
          "ap": 0.5878090753159535,
          "f1": 0.5564544495251494,
          "f1_threshold": 0.7720521688461304,
          "precision": 0.5007385524372231,
          "recall": 0.6261213720316623
        },
        "dot": {
          "accuracy": 0.7765989151814985,
          "accuracy_threshold": 66.84642791748047,
          "ap": 0.3339815511939114,
          "f1": 0.41302597877789976,
          "f1_threshold": 44.28453826904297,
          "precision": 0.2857721518987342,
          "recall": 0.7445910290237467
        },
        "euclidean": {
          "accuracy": 0.8137926923764678,
          "accuracy_threshold": 4.648977279663086,
          "ap": 0.5659664876154784,
          "f1": 0.5499001996007985,
          "f1_threshold": 5.2976861000061035,
          "precision": 0.5215333648840511,
          "recall": 0.5815303430079156
        },
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5878090753159535,
        "manhattan": {
          "accuracy": 0.8165345413363534,
          "accuracy_threshold": 226.0230712890625,
          "ap": 0.5759531721300938,
          "f1": 0.5633439004344253,
          "f1_threshold": 258.75946044921875,
          "precision": 0.5075100486566533,
          "recall": 0.632981530343008
        },
        "max": {
          "accuracy": 0.8196936281814389,
          "ap": 0.5878090753159535,
          "f1": 0.5633439004344253
        },
        "similarity": {
          "accuracy": 0.8196936281814389,
          "accuracy_threshold": 0.8232359886169434,
          "ap": 0.5878090753159535,
          "f1": 0.5564544495251494,
          "f1_threshold": 0.7720521688461304,
          "precision": 0.5007385524372231,
          "recall": 0.6261213720316623
        }
      }
    ]
  },
  "task_name": "TwitterSemEval2015"
}