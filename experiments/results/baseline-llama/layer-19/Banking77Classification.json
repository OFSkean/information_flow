{"dataset_revision": "0fd18e25b25c072e09e0d92ab615fda904d66300", "evaluation_time": 153.0148856639862, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"accuracy": 0.685357142857143, "f1": 0.6842024722426852, "f1_weighted": 0.6842024722426852, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.685357142857143, "scores_per_experiment": [{"accuracy": 0.6746753246753247, "f1": 0.6726924521212343, "f1_weighted": 0.6726924521212344}, {"accuracy": 0.6795454545454546, "f1": 0.6795187743407902, "f1_weighted": 0.6795187743407902}, {"accuracy": 0.6941558441558442, "f1": 0.6942199102109681, "f1_weighted": 0.6942199102109682}, {"accuracy": 0.6938311688311688, "f1": 0.6951326221166304, "f1_weighted": 0.6951326221166304}, {"accuracy": 0.6931818181818182, "f1": 0.6920819320053332, "f1_weighted": 0.6920819320053331}, {"accuracy": 0.6779220779220779, "f1": 0.675743502010452, "f1_weighted": 0.675743502010452}, {"accuracy": 0.6935064935064935, "f1": 0.6914537933164184, "f1_weighted": 0.6914537933164182}, {"accuracy": 0.6827922077922078, "f1": 0.6794594698015806, "f1_weighted": 0.6794594698015807}, {"accuracy": 0.685064935064935, "f1": 0.6843950415193142, "f1_weighted": 0.6843950415193142}, {"accuracy": 0.6788961038961039, "f1": 0.67732722498413, "f1_weighted": 0.6773272249841302}]}]}, "task_name": "Banking77Classification", "avg_layerwise_entropy": 0.4956766399019104, "num_samples": 64}