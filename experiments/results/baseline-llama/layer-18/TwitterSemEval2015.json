{"dataset_revision": "70970daeab8776df92f5ea462b6173c0b46fd2d1", "evaluation_time": 140.85642266273499, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"cosine": {"accuracy": 0.8214817905465817, "accuracy_threshold": 0.8093093633651733, "ap": 0.5912125942565207, "f1": 0.5566610673213592, "f1_threshold": 0.7727823257446289, "precision": 0.5454545454545454, "recall": 0.5683377308707124}, "dot": {"accuracy": 0.7810097156821839, "accuracy_threshold": 38.29948425292969, "ap": 0.3900866501374617, "f1": 0.43471918307804525, "f1_threshold": 30.67124366760254, "precision": 0.33212594037336307, "recall": 0.629023746701847}, "euclidean": {"accuracy": 0.8128986111938964, "accuracy_threshold": 3.899735450744629, "ap": 0.5618593528971352, "f1": 0.5424670774435182, "f1_threshold": 4.430270195007324, "precision": 0.5003342990862492, "recall": 0.5923482849604221}, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.5912125942565207, "manhattan": {"accuracy": 0.8142099302616678, "accuracy_threshold": 186.95510864257812, "ap": 0.565569921423384, "f1": 0.5511363636363636, "f1_threshold": 213.6396942138672, "precision": 0.49978531558608846, "recall": 0.6142480211081794}, "max": {"accuracy": 0.8214817905465817, "ap": 0.5912125942565207, "f1": 0.5566610673213592}, "similarity": {"accuracy": 0.8214817905465817, "accuracy_threshold": 0.8093093633651733, "ap": 0.5912125942565207, "f1": 0.5566610673213592, "f1_threshold": 0.7727823257446289, "precision": 0.5454545454545454, "recall": 0.5683377308707124}}]}, "task_name": "TwitterSemEval2015", "avg_layerwise_entropy": 0.7635199209556118, "num_samples": 64}