{"dataset_revision": "0fd18e25b25c072e09e0d92ab615fda904d66300", "evaluation_time": 148.63115310668945, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"accuracy": 0.6797402597402598, "f1": 0.678402671089694, "f1_weighted": 0.678402671089694, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.6797402597402598, "scores_per_experiment": [{"accuracy": 0.673051948051948, "f1": 0.6706355754998549, "f1_weighted": 0.670635575499855}, {"accuracy": 0.6762987012987013, "f1": 0.6766695367655129, "f1_weighted": 0.6766695367655131}, {"accuracy": 0.6902597402597402, "f1": 0.6899614200775339, "f1_weighted": 0.6899614200775341}, {"accuracy": 0.6824675324675324, "f1": 0.6835743967342922, "f1_weighted": 0.6835743967342923}, {"accuracy": 0.687012987012987, "f1": 0.6858336965240326, "f1_weighted": 0.6858336965240325}, {"accuracy": 0.6746753246753247, "f1": 0.6725666205226678, "f1_weighted": 0.6725666205226676}, {"accuracy": 0.6844155844155844, "f1": 0.681523820858304, "f1_weighted": 0.681523820858304}, {"accuracy": 0.676948051948052, "f1": 0.673888079893031, "f1_weighted": 0.6738880798930309}, {"accuracy": 0.6805194805194805, "f1": 0.6792817793128391, "f1_weighted": 0.679281779312839}, {"accuracy": 0.6717532467532468, "f1": 0.6700917847088719, "f1_weighted": 0.670091784708872}]}]}, "task_name": "Banking77Classification", "avg_layerwise_entropy": 0.45786013080117227, "num_samples": 64}