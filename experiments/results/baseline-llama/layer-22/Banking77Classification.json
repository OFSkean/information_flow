{"dataset_revision": "0fd18e25b25c072e09e0d92ab615fda904d66300", "evaluation_time": 160.08806705474854, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"accuracy": 0.7157142857142856, "f1": 0.7148763657394377, "f1_weighted": 0.7148763657394376, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.7157142857142856, "scores_per_experiment": [{"accuracy": 0.698051948051948, "f1": 0.6959817329958474, "f1_weighted": 0.6959817329958474}, {"accuracy": 0.714935064935065, "f1": 0.7158480978229762, "f1_weighted": 0.7158480978229763}, {"accuracy": 0.7227272727272728, "f1": 0.7229258121761062, "f1_weighted": 0.7229258121761063}, {"accuracy": 0.7253246753246754, "f1": 0.7263728456793586, "f1_weighted": 0.7263728456793586}, {"accuracy": 0.7185064935064935, "f1": 0.7180690975532975, "f1_weighted": 0.7180690975532974}, {"accuracy": 0.7126623376623377, "f1": 0.7108275717442816, "f1_weighted": 0.7108275717442816}, {"accuracy": 0.7224025974025974, "f1": 0.719621923203304, "f1_weighted": 0.719621923203304}, {"accuracy": 0.7136363636363636, "f1": 0.7114715592991065, "f1_weighted": 0.7114715592991064}, {"accuracy": 0.7175324675324676, "f1": 0.7173999123776975, "f1_weighted": 0.7173999123776974}, {"accuracy": 0.7113636363636363, "f1": 0.7102451045424009, "f1_weighted": 0.7102451045424011}]}]}, "task_name": "Banking77Classification", "avg_layerwise_entropy": 0.6329922659882871, "num_samples": 64}