{"dataset_revision": "70970daeab8776df92f5ea462b6173c0b46fd2d1", "evaluation_time": 141.68609380722046, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"cosine": {"accuracy": 0.8238660070334386, "accuracy_threshold": 0.8240261077880859, "ap": 0.6033587888857841, "f1": 0.5671751067724223, "f1_threshold": 0.780410885810852, "precision": 0.5275822928490352, "recall": 0.6131926121372032}, "dot": {"accuracy": 0.7783870775466413, "accuracy_threshold": 114.86325073242188, "ap": 0.34803486869001005, "f1": 0.4023781212841855, "f1_threshold": 79.9669418334961, "precision": 0.28759206798866854, "recall": 0.6696569920844327}, "euclidean": {"accuracy": 0.8155808547416106, "accuracy_threshold": 6.122369289398193, "ap": 0.5775731958244875, "f1": 0.5612383900928792, "f1_threshold": 6.930933952331543, "precision": 0.5288214702450409, "recall": 0.5978891820580475}, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.6033587888857841, "manhattan": {"accuracy": 0.8180246766406389, "accuracy_threshold": 296.518798828125, "ap": 0.5841018284701543, "f1": 0.5723120837297812, "f1_threshold": 334.66259765625, "precision": 0.5210047639670853, "recall": 0.6348284960422164}, "max": {"accuracy": 0.8238660070334386, "ap": 0.6033587888857841, "f1": 0.5723120837297812}, "similarity": {"accuracy": 0.8238660070334386, "accuracy_threshold": 0.8240261077880859, "ap": 0.6033587888857841, "f1": 0.5671751067724223, "f1_threshold": 0.780410885810852, "precision": 0.5275822928490352, "recall": 0.6131926121372032}}]}, "task_name": "TwitterSemEval2015", "avg_layerwise_entropy": 0.8601800562704754, "num_samples": 64}