{"dataset_revision": "0fd18e25b25c072e09e0d92ab615fda904d66300", "evaluation_time": 145.95849084854126, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"accuracy": 0.7423051948051949, "f1": 0.7413966893353401, "f1_weighted": 0.7413966893353401, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.7423051948051949, "scores_per_experiment": [{"accuracy": 0.7285714285714285, "f1": 0.7268393191566784, "f1_weighted": 0.7268393191566784}, {"accuracy": 0.7376623376623377, "f1": 0.7386342221286666, "f1_weighted": 0.7386342221286665}, {"accuracy": 0.7525974025974026, "f1": 0.7524005906706742, "f1_weighted": 0.7524005906706744}, {"accuracy": 0.7529220779220779, "f1": 0.7528515810989251, "f1_weighted": 0.7528515810989251}, {"accuracy": 0.7438311688311688, "f1": 0.7438732857541074, "f1_weighted": 0.7438732857541074}, {"accuracy": 0.7324675324675325, "f1": 0.7295777252755926, "f1_weighted": 0.7295777252755927}, {"accuracy": 0.7512987012987012, "f1": 0.7499491890701815, "f1_weighted": 0.7499491890701815}, {"accuracy": 0.7353896103896104, "f1": 0.7336168065591673, "f1_weighted": 0.7336168065591672}, {"accuracy": 0.7467532467532467, "f1": 0.7457557275719167, "f1_weighted": 0.7457557275719164}, {"accuracy": 0.7415584415584415, "f1": 0.7404684460674913, "f1_weighted": 0.7404684460674912}]}]}, "task_name": "Banking77Classification", "avg_layerwise_entropy": 0.8341042273065509, "num_samples": 64}