{"dataset_revision": "0fd18e25b25c072e09e0d92ab615fda904d66300", "evaluation_time": 120.36626863479614, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"accuracy": 0.5718506493506494, "f1": 0.5678523192987477, "f1_weighted": 0.5678523192987477, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.5718506493506494, "scores_per_experiment": [{"accuracy": 0.5626623376623376, "f1": 0.5576542563834893, "f1_weighted": 0.5576542563834894}, {"accuracy": 0.5707792207792208, "f1": 0.5724220878516089, "f1_weighted": 0.572422087851609}, {"accuracy": 0.5853896103896103, "f1": 0.5796629436386811, "f1_weighted": 0.5796629436386812}, {"accuracy": 0.5996753246753247, "f1": 0.5966233244436022, "f1_weighted": 0.5966233244436022}, {"accuracy": 0.5707792207792208, "f1": 0.5654658025814824, "f1_weighted": 0.5654658025814824}, {"accuracy": 0.5675324675324676, "f1": 0.563847987488428, "f1_weighted": 0.5638479874884281}, {"accuracy": 0.5668831168831169, "f1": 0.561790210284746, "f1_weighted": 0.5617902102847461}, {"accuracy": 0.5704545454545454, "f1": 0.5658419573159453, "f1_weighted": 0.5658419573159453}, {"accuracy": 0.5701298701298702, "f1": 0.5639544912883496, "f1_weighted": 0.5639544912883496}, {"accuracy": 0.5542207792207792, "f1": 0.5512601317111446, "f1_weighted": 0.5512601317111445}]}]}, "task_name": "Banking77Classification", "avg_layerwise_entropy": 0.2028911439988635, "num_samples": 64}