{
  "dataset_revision": "70970daeab8776df92f5ea462b6173c0b46fd2d1",
  "evaluation_time": 143.60616636276245,
  "kg_co2_emissions": null,
  "mteb_version": "1.12.49",
  "scores": {
    "test": [
      {
        "cosine": {
          "accuracy": 0.8078321511593253,
          "accuracy_threshold": 0.8551598787307739,
          "ap": 0.521959826649079,
          "f1": 0.5072480053938645,
          "f1_threshold": 0.8059272766113281,
          "precision": 0.44176942650225093,
          "recall": 0.5955145118733509
        },
        "dot": {
          "accuracy": 0.7762412827084699,
          "accuracy_threshold": 11.237188339233398,
          "ap": 0.3346550679099284,
          "f1": 0.39942506997503596,
          "f1_threshold": 8.354050636291504,
          "precision": 0.27998727330575884,
          "recall": 0.6965699208443272
        },
        "euclidean": {
          "accuracy": 0.806401621267211,
          "accuracy_threshold": 1.7863426208496094,
          "ap": 0.5184295339313157,
          "f1": 0.5099519560741249,
          "f1_threshold": 2.0637307167053223,
          "precision": 0.45012116316639744,
          "recall": 0.5881266490765171
        },
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.521959826649079,
        "manhattan": {
          "accuracy": 0.8039577993681827,
          "accuracy_threshold": 87.07379150390625,
          "ap": 0.5124232977746694,
          "f1": 0.5058119056005637,
          "f1_threshold": 101.6734619140625,
          "precision": 0.45568013539242647,
          "recall": 0.5683377308707124
        },
        "max": {
          "accuracy": 0.8078321511593253,
          "ap": 0.521959826649079,
          "f1": 0.5099519560741249
        },
        "similarity": {
          "accuracy": 0.8078321511593253,
          "accuracy_threshold": 0.8551598787307739,
          "ap": 0.521959826649079,
          "f1": 0.5072480053938645,
          "f1_threshold": 0.8059272766113281,
          "precision": 0.44176942650225093,
          "recall": 0.5955145118733509
        }
      }
    ]
  },
  "task_name": "TwitterSemEval2015"
}