{"dataset_revision": "0fd18e25b25c072e09e0d92ab615fda904d66300", "evaluation_time": 134.55031776428223, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"accuracy": 0.7295454545454546, "f1": 0.7282106153777874, "f1_weighted": 0.7282106153777874, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.7295454545454546, "scores_per_experiment": [{"accuracy": 0.714935064935065, "f1": 0.7129973536263531, "f1_weighted": 0.7129973536263531}, {"accuracy": 0.7227272727272728, "f1": 0.7231085453476402, "f1_weighted": 0.7231085453476402}, {"accuracy": 0.7444805194805195, "f1": 0.7437499463194661, "f1_weighted": 0.7437499463194662}, {"accuracy": 0.7399350649350649, "f1": 0.7401463830268299, "f1_weighted": 0.7401463830268301}, {"accuracy": 0.7275974025974026, "f1": 0.7269123236421952, "f1_weighted": 0.7269123236421954}, {"accuracy": 0.7191558441558441, "f1": 0.7164270621043516, "f1_weighted": 0.7164270621043515}, {"accuracy": 0.7376623376623377, "f1": 0.7356861643945122, "f1_weighted": 0.7356861643945121}, {"accuracy": 0.725, "f1": 0.7223310770762709, "f1_weighted": 0.7223310770762708}, {"accuracy": 0.7350649350649351, "f1": 0.7336961763838589, "f1_weighted": 0.7336961763838589}, {"accuracy": 0.7288961038961039, "f1": 0.7270511218563961, "f1_weighted": 0.7270511218563963}]}]}, "task_name": "Banking77Classification", "avg_layerwise_entropy": 0.8588742382856696, "num_samples": 64}