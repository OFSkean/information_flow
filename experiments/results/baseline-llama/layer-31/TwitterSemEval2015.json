{
  "dataset_revision": "70970daeab8776df92f5ea462b6173c0b46fd2d1",
  "evaluation_time": 142.37201952934265,
  "kg_co2_emissions": null,
  "mteb_version": "1.12.49",
  "scores": {
    "test": [
      {
        "cosine": {
          "accuracy": 0.8158192763902963,
          "accuracy_threshold": 0.8912100195884705,
          "ap": 0.5642741805604187,
          "f1": 0.5365730832179277,
          "f1_threshold": 0.859567403793335,
          "precision": 0.5131230435829521,
          "recall": 0.562269129287599
        },
        "dot": {
          "accuracy": 0.7792215533170412,
          "accuracy_threshold": 1176.4945068359375,
          "ap": 0.35041849374625494,
          "f1": 0.39539548268945673,
          "f1_threshold": 931.922607421875,
          "precision": 0.29529764230819333,
          "recall": 0.5981530343007916
        },
        "euclidean": {
          "accuracy": 0.8097991297609823,
          "accuracy_threshold": 15.39413833618164,
          "ap": 0.5397906344002716,
          "f1": 0.5231739868963843,
          "f1_threshold": 17.9610595703125,
          "precision": 0.48427672955974843,
          "recall": 0.5688654353562005
        },
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5642741805604187,
        "manhattan": {
          "accuracy": 0.8098587351731538,
          "accuracy_threshold": 786.642822265625,
          "ap": 0.5419074292106895,
          "f1": 0.5250531050855929,
          "f1_threshold": 892.357177734375,
          "precision": 0.49869451697127937,
          "recall": 0.5543535620052771
        },
        "max": {
          "accuracy": 0.8158192763902963,
          "ap": 0.5642741805604187,
          "f1": 0.5365730832179277
        },
        "similarity": {
          "accuracy": 0.8158192763902963,
          "accuracy_threshold": 0.8912100195884705,
          "ap": 0.5642741805604187,
          "f1": 0.5365730832179277,
          "f1_threshold": 0.859567403793335,
          "precision": 0.5131230435829521,
          "recall": 0.562269129287599
        }
      }
    ]
  },
  "task_name": "TwitterSemEval2015"
}