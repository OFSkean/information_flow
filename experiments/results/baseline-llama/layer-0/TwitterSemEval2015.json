{"dataset_revision": "70970daeab8776df92f5ea462b6173c0b46fd2d1", "evaluation_time": 140.86502051353455, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"cosine": {"accuracy": 0.803719377719497, "accuracy_threshold": 0.7342687845230103, "ap": 0.5071230018025468, "f1": 0.4919864011656144, "f1_threshold": 0.6517777442932129, "precision": 0.4556905083220873, "recall": 0.5345646437994723}, "dot": {"accuracy": 0.7814269535673839, "accuracy_threshold": 0.036527350544929504, "ap": 0.3765852204366263, "f1": 0.408834917282647, "f1_threshold": 0.024225369095802307, "precision": 0.306707156060206, "recall": 0.612928759894459}, "euclidean": {"accuracy": 0.7917386898730405, "accuracy_threshold": 0.1314677596092224, "ap": 0.4583711679176458, "f1": 0.4640346133044889, "f1_threshold": 0.17228156328201294, "precision": 0.39321723189734187, "recall": 0.5659630606860159}, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.5071230018025468, "manhattan": {"accuracy": 0.7906061870417834, "accuracy_threshold": 6.879528999328613, "ap": 0.45449048178929696, "f1": 0.4590780809031045, "f1_threshold": 8.773898124694824, "precision": 0.38012809416652243, "recall": 0.5794195250659631}, "max": {"accuracy": 0.803719377719497, "ap": 0.5071230018025468, "f1": 0.4919864011656144}, "similarity": {"accuracy": 0.803719377719497, "accuracy_threshold": 0.7342687845230103, "ap": 0.5071230018025468, "f1": 0.4919864011656144, "f1_threshold": 0.6517777442932129, "precision": 0.4556905083220873, "recall": 0.5345646437994723}}]}, "task_name": "TwitterSemEval2015", "avg_layerwise_entropy": 0.9069755018660108, "num_samples": 64}