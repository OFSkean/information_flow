{
  "dataset_revision": "70970daeab8776df92f5ea462b6173c0b46fd2d1",
  "evaluation_time": 140.71017718315125,
  "kg_co2_emissions": null,
  "mteb_version": "1.12.49",
  "scores": {
    "test": [
      {
        "cosine": {
          "accuracy": 0.8198128390057817,
          "accuracy_threshold": 0.8913893103599548,
          "ap": 0.5832813246278027,
          "f1": 0.548158985613265,
          "f1_threshold": 0.8620161414146423,
          "precision": 0.5095194922937444,
          "recall": 0.5931398416886543
        },
        "dot": {
          "accuracy": 0.7765393097693271,
          "accuracy_threshold": 504.5408935546875,
          "ap": 0.29095138584885644,
          "f1": 0.3686591119108993,
          "f1_threshold": 226.97267150878906,
          "precision": 0.22598533182278935,
          "recall": 1.0
        },
        "euclidean": {
          "accuracy": 0.8147463789712106,
          "accuracy_threshold": 9.95307731628418,
          "ap": 0.5685121502402247,
          "f1": 0.5467658843732112,
          "f1_threshold": 11.368159294128418,
          "precision": 0.48291203235591507,
          "recall": 0.6300791556728232
        },
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5832813246278027,
        "manhattan": {
          "accuracy": 0.8159384872146391,
          "accuracy_threshold": 487.7369384765625,
          "ap": 0.5727127229855803,
          "f1": 0.5551632833186231,
          "f1_threshold": 562.939208984375,
          "precision": 0.4770572620401972,
          "recall": 0.6638522427440633
        },
        "max": {
          "accuracy": 0.8198128390057817,
          "ap": 0.5832813246278027,
          "f1": 0.5551632833186231
        },
        "similarity": {
          "accuracy": 0.8198128390057817,
          "accuracy_threshold": 0.8913893103599548,
          "ap": 0.5832813246278027,
          "f1": 0.548158985613265,
          "f1_threshold": 0.8620161414146423,
          "precision": 0.5095194922937444,
          "recall": 0.5931398416886543
        }
      }
    ]
  },
  "task_name": "TwitterSemEval2015"
}