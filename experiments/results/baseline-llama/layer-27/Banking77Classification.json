{"dataset_revision": "0fd18e25b25c072e09e0d92ab615fda904d66300", "evaluation_time": 146.60531568527222, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"accuracy": 0.7382142857142858, "f1": 0.7373355133608324, "f1_weighted": 0.7373355133608324, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.7382142857142858, "scores_per_experiment": [{"accuracy": 0.7262987012987013, "f1": 0.7244149345090438, "f1_weighted": 0.7244149345090439}, {"accuracy": 0.7376623376623377, "f1": 0.7386545010624639, "f1_weighted": 0.7386545010624639}, {"accuracy": 0.750974025974026, "f1": 0.7505420816976843, "f1_weighted": 0.7505420816976841}, {"accuracy": 0.7470779220779221, "f1": 0.7477382671534124, "f1_weighted": 0.7477382671534126}, {"accuracy": 0.7418831168831169, "f1": 0.7417150768937838, "f1_weighted": 0.7417150768937838}, {"accuracy": 0.7308441558441559, "f1": 0.7277337013952128, "f1_weighted": 0.7277337013952127}, {"accuracy": 0.7454545454545455, "f1": 0.7439684312776695, "f1_weighted": 0.7439684312776694}, {"accuracy": 0.7314935064935065, "f1": 0.7297984070168511, "f1_weighted": 0.7297984070168511}, {"accuracy": 0.7350649350649351, "f1": 0.7343821109368983, "f1_weighted": 0.7343821109368984}, {"accuracy": 0.7353896103896104, "f1": 0.7344076216653039, "f1_weighted": 0.7344076216653042}]}]}, "task_name": "Banking77Classification", "avg_layerwise_entropy": 0.7643710295896899, "num_samples": 64}