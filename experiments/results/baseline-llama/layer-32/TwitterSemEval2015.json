{
  "dataset_revision": "70970daeab8776df92f5ea462b6173c0b46fd2d1",
  "evaluation_time": 141.38392543792725,
  "kg_co2_emissions": null,
  "mteb_version": "1.12.49",
  "scores": {
    "test": [
      {
        "cosine": {
          "accuracy": 0.812839005781725,
          "accuracy_threshold": 0.8849823474884033,
          "ap": 0.5497760125815141,
          "f1": 0.5231284628269554,
          "f1_threshold": 0.849528431892395,
          "precision": 0.5112062452782674,
          "recall": 0.5356200527704486
        },
        "dot": {
          "accuracy": 0.7779102342492699,
          "accuracy_threshold": 14245.279296875,
          "ap": 0.38452001485204734,
          "f1": 0.4272605423239509,
          "f1_threshold": 11825.6455078125,
          "precision": 0.3506515484853613,
          "recall": 0.5467018469656992
        },
        "euclidean": {
          "accuracy": 0.8096203135244681,
          "accuracy_threshold": 56.37617111206055,
          "ap": 0.5395626638240624,
          "f1": 0.51934611313654,
          "f1_threshold": 67.4321060180664,
          "precision": 0.4684914067472947,
          "recall": 0.5825857519788918
        },
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5544090980039961,
        "manhattan": {
          "accuracy": 0.812421767896525,
          "accuracy_threshold": 2840.380615234375,
          "ap": 0.5544090980039961,
          "f1": 0.5319014529374605,
          "f1_threshold": 3245.117431640625,
          "precision": 0.5103030303030303,
          "recall": 0.5554089709762533
        },
        "max": {
          "accuracy": 0.812839005781725,
          "ap": 0.5544090980039961,
          "f1": 0.5319014529374605
        },
        "similarity": {
          "accuracy": 0.812839005781725,
          "accuracy_threshold": 0.8849823474884033,
          "ap": 0.5497760125815141,
          "f1": 0.5231284628269554,
          "f1_threshold": 0.849528431892395,
          "precision": 0.5112062452782674,
          "recall": 0.5356200527704486
        }
      }
    ]
  },
  "task_name": "TwitterSemEval2015"
}