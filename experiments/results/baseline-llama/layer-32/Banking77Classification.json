{"dataset_revision": "0fd18e25b25c072e09e0d92ab615fda904d66300", "evaluation_time": 152.646244764328, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"accuracy": 0.7207142857142858, "f1": 0.7195082789020502, "f1_weighted": 0.71950827890205, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.7207142857142858, "scores_per_experiment": [{"accuracy": 0.7081168831168831, "f1": 0.7070878206235681, "f1_weighted": 0.707087820623568}, {"accuracy": 0.7146103896103896, "f1": 0.7151782116307236, "f1_weighted": 0.7151782116307233}, {"accuracy": 0.736038961038961, "f1": 0.7354899439090129, "f1_weighted": 0.7354899439090128}, {"accuracy": 0.7353896103896104, "f1": 0.7355549114521384, "f1_weighted": 0.7355549114521382}, {"accuracy": 0.7165584415584415, "f1": 0.7159052383666933, "f1_weighted": 0.7159052383666933}, {"accuracy": 0.7103896103896103, "f1": 0.7068450840131605, "f1_weighted": 0.7068450840131604}, {"accuracy": 0.725, "f1": 0.7221670303337913, "f1_weighted": 0.7221670303337914}, {"accuracy": 0.7181818181818181, "f1": 0.7160784490759008, "f1_weighted": 0.7160784490759008}, {"accuracy": 0.7227272727272728, "f1": 0.7221617015220536, "f1_weighted": 0.7221617015220536}, {"accuracy": 0.7201298701298702, "f1": 0.718614398093459, "f1_weighted": 0.7186143980934588}]}]}, "task_name": "Banking77Classification", "avg_layerwise_entropy": 0.8688325835387409, "num_samples": 64}