{"dataset_revision": "70970daeab8776df92f5ea462b6173c0b46fd2d1", "evaluation_time": 143.62436437606812, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"cosine": {"accuracy": 0.8012755558204685, "accuracy_threshold": 0.8671973943710327, "ap": 0.49320149930314755, "f1": 0.4808815426997245, "f1_threshold": 0.8090587258338928, "precision": 0.41286660359508043, "recall": 0.5757255936675462}, "dot": {"accuracy": 0.7772545747153842, "accuracy_threshold": 13.602058410644531, "ap": 0.33837783630063506, "f1": 0.39063631244469477, "f1_threshold": 9.616223335266113, "precision": 0.280985996991089, "recall": 0.6406332453825857}, "euclidean": {"accuracy": 0.8002622638135543, "accuracy_threshold": 1.878002643585205, "ap": 0.49551294695944137, "f1": 0.48608085209392404, "f1_threshold": 2.1489272117614746, "precision": 0.4490161001788909, "recall": 0.5298153034300792}, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.49551294695944137, "manhattan": {"accuracy": 0.7987721285092686, "accuracy_threshold": 91.03146362304688, "ap": 0.48840289046954316, "f1": 0.48341684562157006, "f1_threshold": 107.09239196777344, "precision": 0.4412020905923345, "recall": 0.5345646437994723}, "max": {"accuracy": 0.8012755558204685, "ap": 0.49551294695944137, "f1": 0.48608085209392404}, "similarity": {"accuracy": 0.8012755558204685, "accuracy_threshold": 0.8671973943710327, "ap": 0.49320149930314755, "f1": 0.4808815426997245, "f1_threshold": 0.8090587258338928, "precision": 0.41286660359508043, "recall": 0.5757255936675462}}]}, "task_name": "TwitterSemEval2015", "avg_layerwise_entropy": 0.4938105250066259, "num_samples": 64}