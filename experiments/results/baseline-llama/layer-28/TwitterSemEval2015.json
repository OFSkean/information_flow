{
  "dataset_revision": "70970daeab8776df92f5ea462b6173c0b46fd2d1",
  "evaluation_time": 151.74268007278442,
  "kg_co2_emissions": null,
  "mteb_version": "1.12.49",
  "scores": {
    "test": [
      {
        "cosine": {
          "accuracy": 0.8195148119449246,
          "accuracy_threshold": 0.8856682777404785,
          "ap": 0.5810180408220682,
          "f1": 0.5432772112876452,
          "f1_threshold": 0.846783459186554,
          "precision": 0.4933247200689061,
          "recall": 0.604485488126649
        },
        "dot": {
          "accuracy": 0.777791023424927,
          "accuracy_threshold": 502.92010498046875,
          "ap": 0.3156947080977482,
          "f1": 0.36988326451547127,
          "f1_threshold": 321.21917724609375,
          "precision": 0.22922853351867062,
          "recall": 0.9572559366754617
        },
        "euclidean": {
          "accuracy": 0.8145079573225249,
          "accuracy_threshold": 10.691160202026367,
          "ap": 0.5638216218236847,
          "f1": 0.5419174548581256,
          "f1_threshold": 12.489570617675781,
          "precision": 0.45719985491476245,
          "recall": 0.6651715039577837
        },
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5810180408220682,
        "manhattan": {
          "accuracy": 0.8157000655659534,
          "accuracy_threshold": 528.4899291992188,
          "ap": 0.5658855743412115,
          "f1": 0.5468732827783273,
          "f1_threshold": 611.07373046875,
          "precision": 0.4686381616123564,
          "recall": 0.6564643799472295
        },
        "max": {
          "accuracy": 0.8195148119449246,
          "ap": 0.5810180408220682,
          "f1": 0.5468732827783273
        },
        "similarity": {
          "accuracy": 0.8195148119449246,
          "accuracy_threshold": 0.8856682777404785,
          "ap": 0.5810180408220682,
          "f1": 0.5432772112876452,
          "f1_threshold": 0.846783459186554,
          "precision": 0.4933247200689061,
          "recall": 0.604485488126649
        }
      }
    ]
  },
  "task_name": "TwitterSemEval2015"
}