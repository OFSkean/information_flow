{
  "dataset_revision": "70970daeab8776df92f5ea462b6173c0b46fd2d1",
  "evaluation_time": 140.92021942138672,
  "kg_co2_emissions": null,
  "mteb_version": "1.12.49",
  "scores": {
    "test": [
      {
        "cosine": {
          "accuracy": 0.807653334922811,
          "accuracy_threshold": 0.8516169786453247,
          "ap": 0.5494733703954137,
          "f1": 0.5311516677155445,
          "f1_threshold": 0.8128514289855957,
          "precision": 0.5078219013237064,
          "recall": 0.5567282321899736
        },
        "dot": {
          "accuracy": 0.7815461643917267,
          "accuracy_threshold": 4.236311912536621,
          "ap": 0.4008871949036107,
          "f1": 0.43836813138813663,
          "f1_threshold": 3.579710006713867,
          "precision": 0.32767402376910015,
          "recall": 0.6620052770448549
        },
        "euclidean": {
          "accuracy": 0.8040174047803541,
          "accuracy_threshold": 1.1246365308761597,
          "ap": 0.5250880039610714,
          "f1": 0.5220272426064548,
          "f1_threshold": 1.3195642232894897,
          "precision": 0.4879559532002753,
          "recall": 0.5612137203166226
        },
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5494733703954137,
        "manhattan": {
          "accuracy": 0.8030637181856113,
          "accuracy_threshold": 56.78682327270508,
          "ap": 0.5242489928795397,
          "f1": 0.5186517932109873,
          "f1_threshold": 65.95419311523438,
          "precision": 0.4754783373652958,
          "recall": 0.5704485488126649
        },
        "max": {
          "accuracy": 0.807653334922811,
          "ap": 0.5494733703954137,
          "f1": 0.5311516677155445
        },
        "similarity": {
          "accuracy": 0.807653334922811,
          "accuracy_threshold": 0.8516169786453247,
          "ap": 0.5494733703954137,
          "f1": 0.5311516677155445,
          "f1_threshold": 0.8128514289855957,
          "precision": 0.5078219013237064,
          "recall": 0.5567282321899736
        }
      }
    ]
  },
  "task_name": "TwitterSemEval2015"
}