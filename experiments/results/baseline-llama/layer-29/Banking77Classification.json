{"dataset_revision": "0fd18e25b25c072e09e0d92ab615fda904d66300", "evaluation_time": 141.92186737060547, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"accuracy": 0.7386688311688312, "f1": 0.7377633137042288, "f1_weighted": 0.7377633137042288, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.7386688311688312, "scores_per_experiment": [{"accuracy": 0.7285714285714285, "f1": 0.7263236342787122, "f1_weighted": 0.7263236342787122}, {"accuracy": 0.7357142857142858, "f1": 0.7369616362581725, "f1_weighted": 0.7369616362581725}, {"accuracy": 0.750974025974026, "f1": 0.7505768963819565, "f1_weighted": 0.7505768963819563}, {"accuracy": 0.7519480519480519, "f1": 0.7517438636173916, "f1_weighted": 0.7517438636173918}, {"accuracy": 0.7366883116883117, "f1": 0.7366134287752257, "f1_weighted": 0.7366134287752256}, {"accuracy": 0.7327922077922078, "f1": 0.7299318949403272, "f1_weighted": 0.7299318949403273}, {"accuracy": 0.7467532467532467, "f1": 0.745294370320264, "f1_weighted": 0.7452943703202641}, {"accuracy": 0.7305194805194806, "f1": 0.7290230356135349, "f1_weighted": 0.7290230356135348}, {"accuracy": 0.736038961038961, "f1": 0.7350835135731807, "f1_weighted": 0.7350835135731806}, {"accuracy": 0.7366883116883117, "f1": 0.7360808632835224, "f1_weighted": 0.7360808632835226}]}]}, "task_name": "Banking77Classification", "avg_layerwise_entropy": 0.8096167305975412, "num_samples": 64}