{"dataset_revision": "70970daeab8776df92f5ea462b6173c0b46fd2d1", "evaluation_time": 190.116206407547, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"cosine": {"accuracy": 0.8211241580735531, "accuracy_threshold": 0.8846561312675476, "ap": 0.5837905206764153, "f1": 0.5473163841807911, "f1_threshold": 0.8441611528396606, "precision": 0.49405014874628134, "recall": 0.6134564643799473}, "dot": {"accuracy": 0.7785658937831555, "accuracy_threshold": 608.656494140625, "ap": 0.33471819302686096, "f1": 0.37667976018193094, "f1_threshold": 454.8988037109375, "precision": 0.2549202499766813, "recall": 0.7211081794195251}, "euclidean": {"accuracy": 0.8127794003695535, "accuracy_threshold": 11.583080291748047, "ap": 0.55732711626559, "f1": 0.535920832797841, "f1_threshold": 13.052637100219727, "precision": 0.5224254572788775, "recall": 0.5501319261213721}, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.5837905206764153, "manhattan": {"accuracy": 0.8131966382547535, "accuracy_threshold": 580.9635620117188, "ap": 0.5595911042072338, "f1": 0.5403770072143356, "f1_threshold": 667.7412719726562, "precision": 0.4833472106577852, "recall": 0.612664907651715}, "max": {"accuracy": 0.8211241580735531, "ap": 0.5837905206764153, "f1": 0.5473163841807911}, "similarity": {"accuracy": 0.8211241580735531, "accuracy_threshold": 0.8846561312675476, "ap": 0.5837905206764153, "f1": 0.5473163841807911, "f1_threshold": 0.8441611528396606, "precision": 0.49405014874628134, "recall": 0.6134564643799473}}]}, "task_name": "TwitterSemEval2015", "avg_layerwise_entropy": 0.9037510996473647, "num_samples": 64}