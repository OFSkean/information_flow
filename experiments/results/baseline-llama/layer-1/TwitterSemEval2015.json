{
  "dataset_revision": "70970daeab8776df92f5ea462b6173c0b46fd2d1",
  "evaluation_time": 141.01659750938416,
  "kg_co2_emissions": null,
  "mteb_version": "1.12.49",
  "scores": {
    "test": [
      {
        "cosine": {
          "accuracy": 0.8097395243488109,
          "accuracy_threshold": 0.8468890190124512,
          "ap": 0.5421795605014215,
          "f1": 0.5317289423685877,
          "f1_threshold": 0.8084383010864258,
          "precision": 0.5113276492082826,
          "recall": 0.553825857519789
        },
        "dot": {
          "accuracy": 0.7782678667222984,
          "accuracy_threshold": 0.24165569245815277,
          "ap": 0.3544112655815591,
          "f1": 0.40764222069910894,
          "f1_threshold": 0.1718863546848297,
          "precision": 0.30182694747526007,
          "recall": 0.6277044854881266
        },
        "euclidean": {
          "accuracy": 0.7973415986171545,
          "accuracy_threshold": 0.24675554037094116,
          "ap": 0.496197265102429,
          "f1": 0.4974446337308348,
          "f1_threshold": 0.29803267121315,
          "precision": 0.46160794941282746,
          "recall": 0.5393139841688654
        },
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5421795605014215,
        "manhattan": {
          "accuracy": 0.7968647553197831,
          "accuracy_threshold": 12.02579402923584,
          "ap": 0.4957909101330682,
          "f1": 0.4985905964595782,
          "f1_threshold": 14.758642196655273,
          "precision": 0.4353219137625517,
          "recall": 0.583377308707124
        },
        "max": {
          "accuracy": 0.8097395243488109,
          "ap": 0.5421795605014215,
          "f1": 0.5317289423685877
        },
        "similarity": {
          "accuracy": 0.8097395243488109,
          "accuracy_threshold": 0.8468890190124512,
          "ap": 0.5421795605014215,
          "f1": 0.5317289423685877,
          "f1_threshold": 0.8084383010864258,
          "precision": 0.5113276492082826,
          "recall": 0.553825857519789
        }
      }
    ]
  },
  "task_name": "TwitterSemEval2015"
}