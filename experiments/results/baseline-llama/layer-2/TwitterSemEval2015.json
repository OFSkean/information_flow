{
  "dataset_revision": "70970daeab8776df92f5ea462b6173c0b46fd2d1",
  "evaluation_time": 140.86730480194092,
  "kg_co2_emissions": null,
  "mteb_version": "1.12.49",
  "scores": {
    "test": [
      {
        "cosine": {
          "accuracy": 0.8131370328425821,
          "accuracy_threshold": 0.7878948450088501,
          "ap": 0.5624648146296151,
          "f1": 0.5475808300848429,
          "f1_threshold": 0.7216023206710815,
          "precision": 0.48418491484184917,
          "recall": 0.6300791556728232
        },
        "dot": {
          "accuracy": 0.7792215533170412,
          "accuracy_threshold": 0.39631351828575134,
          "ap": 0.38067003589253356,
          "f1": 0.42869445401308987,
          "f1_threshold": 0.28644195199012756,
          "precision": 0.3182050626438251,
          "recall": 0.6567282321899736
        },
        "euclidean": {
          "accuracy": 0.8049114859629254,
          "accuracy_threshold": 0.4086974561214447,
          "ap": 0.5290420810718685,
          "f1": 0.5280683991450107,
          "f1_threshold": 0.482774019241333,
          "precision": 0.46028633065306923,
          "recall": 0.6192612137203166
        },
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5624648146296151,
        "manhattan": {
          "accuracy": 0.8052095130237825,
          "accuracy_threshold": 19.469482421875,
          "ap": 0.5279749013917922,
          "f1": 0.5314852431592783,
          "f1_threshold": 22.945186614990234,
          "precision": 0.4856955667176239,
          "recall": 0.5868073878627968
        },
        "max": {
          "accuracy": 0.8131370328425821,
          "ap": 0.5624648146296151,
          "f1": 0.5475808300848429
        },
        "similarity": {
          "accuracy": 0.8131370328425821,
          "accuracy_threshold": 0.7878948450088501,
          "ap": 0.5624648146296151,
          "f1": 0.5475808300848429,
          "f1_threshold": 0.7216023206710815,
          "precision": 0.48418491484184917,
          "recall": 0.6300791556728232
        }
      }
    ]
  },
  "task_name": "TwitterSemEval2015"
}