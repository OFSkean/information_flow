{
  "dataset_revision": "70970daeab8776df92f5ea462b6173c0b46fd2d1",
  "evaluation_time": 140.98914861679077,
  "kg_co2_emissions": null,
  "mteb_version": "1.12.49",
  "scores": {
    "test": [
      {
        "cosine": {
          "accuracy": 0.8225546879656673,
          "accuracy_threshold": 0.8615083694458008,
          "ap": 0.6018865729868004,
          "f1": 0.5642770352369381,
          "f1_threshold": 0.8247941136360168,
          "precision": 0.522972972972973,
          "recall": 0.612664907651715
        },
        "dot": {
          "accuracy": 0.7764200989449842,
          "accuracy_threshold": 262.125732421875,
          "ap": 0.3142378296890109,
          "f1": 0.37328549105851033,
          "f1_threshold": 170.77931213378906,
          "precision": 0.23908369782785974,
          "recall": 0.8509234828496042
        },
        "euclidean": {
          "accuracy": 0.8174286225189247,
          "accuracy_threshold": 8.231201171875,
          "ap": 0.582032265043974,
          "f1": 0.5632436178814832,
          "f1_threshold": 9.409025192260742,
          "precision": 0.5009245942058763,
          "recall": 0.6432717678100264
        },
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.6018865729868004,
        "manhattan": {
          "accuracy": 0.8179054658162961,
          "accuracy_threshold": 405.5451965332031,
          "ap": 0.5861477891075056,
          "f1": 0.5697835099032703,
          "f1_threshold": 457.721923828125,
          "precision": 0.5055169595422967,
          "recall": 0.6527704485488126
        },
        "max": {
          "accuracy": 0.8225546879656673,
          "ap": 0.6018865729868004,
          "f1": 0.5697835099032703
        },
        "similarity": {
          "accuracy": 0.8225546879656673,
          "accuracy_threshold": 0.8615083694458008,
          "ap": 0.6018865729868004,
          "f1": 0.5642770352369381,
          "f1_threshold": 0.8247941136360168,
          "precision": 0.522972972972973,
          "recall": 0.612664907651715
        }
      }
    ]
  },
  "task_name": "TwitterSemEval2015"
}