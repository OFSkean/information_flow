{"dataset_revision": "0fd18e25b25c072e09e0d92ab615fda904d66300", "evaluation_time": 148.35034084320068, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"accuracy": 0.7408116883116883, "f1": 0.7400669897463177, "f1_weighted": 0.7400669897463177, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.7408116883116883, "scores_per_experiment": [{"accuracy": 0.7295454545454545, "f1": 0.7276285690629318, "f1_weighted": 0.7276285690629318}, {"accuracy": 0.7399350649350649, "f1": 0.7409565200863057, "f1_weighted": 0.7409565200863057}, {"accuracy": 0.7542207792207792, "f1": 0.7541612337973632, "f1_weighted": 0.7541612337973633}, {"accuracy": 0.7480519480519481, "f1": 0.7484524006381463, "f1_weighted": 0.7484524006381462}, {"accuracy": 0.7405844155844156, "f1": 0.7406510360206707, "f1_weighted": 0.7406510360206711}, {"accuracy": 0.7357142857142858, "f1": 0.7334126211684464, "f1_weighted": 0.7334126211684465}, {"accuracy": 0.7457792207792208, "f1": 0.743835424825843, "f1_weighted": 0.7438354248258429}, {"accuracy": 0.7357142857142858, "f1": 0.7342509406145538, "f1_weighted": 0.7342509406145541}, {"accuracy": 0.7405844155844156, "f1": 0.7399108952048642, "f1_weighted": 0.7399108952048643}, {"accuracy": 0.737987012987013, "f1": 0.737410256044052, "f1_weighted": 0.7374102560440521}]}]}, "task_name": "Banking77Classification", "avg_layerwise_entropy": 0.7198457488372886, "num_samples": 64}