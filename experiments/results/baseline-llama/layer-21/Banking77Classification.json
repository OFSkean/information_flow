{"dataset_revision": "0fd18e25b25c072e09e0d92ab615fda904d66300", "evaluation_time": 162.26183652877808, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"accuracy": 0.7086688311688312, "f1": 0.7076577430122405, "f1_weighted": 0.7076577430122406, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.7086688311688312, "scores_per_experiment": [{"accuracy": 0.6983766233766234, "f1": 0.6959645738139617, "f1_weighted": 0.6959645738139616}, {"accuracy": 0.7042207792207792, "f1": 0.7055858217162357, "f1_weighted": 0.7055858217162356}, {"accuracy": 0.7227272727272728, "f1": 0.7231710736773121, "f1_weighted": 0.7231710736773123}, {"accuracy": 0.7165584415584415, "f1": 0.717107889445592, "f1_weighted": 0.7171078894455922}, {"accuracy": 0.7136363636363636, "f1": 0.7131806466870385, "f1_weighted": 0.7131806466870385}, {"accuracy": 0.7035714285714286, "f1": 0.7011135085313407, "f1_weighted": 0.7011135085313408}, {"accuracy": 0.7146103896103896, "f1": 0.7123295157187651, "f1_weighted": 0.7123295157187652}, {"accuracy": 0.7064935064935065, "f1": 0.7035583784901924, "f1_weighted": 0.7035583784901926}, {"accuracy": 0.7071428571428572, "f1": 0.7067215324977021, "f1_weighted": 0.7067215324977023}, {"accuracy": 0.6993506493506494, "f1": 0.6978444895442645, "f1_weighted": 0.6978444895442644}]}]}, "task_name": "Banking77Classification", "avg_layerwise_entropy": 0.581468460232107, "num_samples": 64}