{"dataset_revision": "70970daeab8776df92f5ea462b6173c0b46fd2d1", "evaluation_time": 140.88328218460083, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"cosine": {"accuracy": 0.8179650712284675, "accuracy_threshold": 0.8103948831558228, "ap": 0.5791618459777907, "f1": 0.545407769488037, "f1_threshold": 0.7767165899276733, "precision": 0.5321285140562249, "recall": 0.5593667546174143}, "dot": {"accuracy": 0.7814865589795553, "accuracy_threshold": 31.457691192626953, "ap": 0.3787943278510664, "f1": 0.4097238992191902, "f1_threshold": 23.729793548583984, "precision": 0.29479902698945903, "recall": 0.6715039577836411}, "euclidean": {"accuracy": 0.8099779459974966, "accuracy_threshold": 3.3655943870544434, "ap": 0.5529045604191772, "f1": 0.5386344811264939, "f1_threshold": 3.8954086303710938, "precision": 0.48830723020810984, "recall": 0.6005277044854881}, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.5791618459777907, "manhattan": {"accuracy": 0.8103951838826966, "accuracy_threshold": 163.16778564453125, "ap": 0.5545894224269409, "f1": 0.5447284345047924, "f1_threshold": 190.22372436523438, "precision": 0.47989545637314035, "recall": 0.6298153034300792}, "max": {"accuracy": 0.8179650712284675, "ap": 0.5791618459777907, "f1": 0.545407769488037}, "similarity": {"accuracy": 0.8179650712284675, "accuracy_threshold": 0.8103948831558228, "ap": 0.5791618459777907, "f1": 0.545407769488037, "f1_threshold": 0.7767165899276733, "precision": 0.5321285140562249, "recall": 0.5593667546174143}}]}, "task_name": "TwitterSemEval2015", "avg_layerwise_entropy": 0.7400332932242226, "num_samples": 64}