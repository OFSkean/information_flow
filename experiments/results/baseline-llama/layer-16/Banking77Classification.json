{"dataset_revision": "0fd18e25b25c072e09e0d92ab615fda904d66300", "evaluation_time": 139.06238913536072, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"accuracy": 0.6614285714285714, "f1": 0.6597861565040043, "f1_weighted": 0.6597861565040042, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.6614285714285714, "scores_per_experiment": [{"accuracy": 0.6568181818181819, "f1": 0.6537058545447331, "f1_weighted": 0.6537058545447331}, {"accuracy": 0.6525974025974026, "f1": 0.6539769185244086, "f1_weighted": 0.6539769185244086}, {"accuracy": 0.6707792207792208, "f1": 0.6689165827511624, "f1_weighted": 0.6689165827511625}, {"accuracy": 0.6672077922077922, "f1": 0.6671090564198319, "f1_weighted": 0.6671090564198316}, {"accuracy": 0.6652597402597402, "f1": 0.6646477422663853, "f1_weighted": 0.6646477422663852}, {"accuracy": 0.6574675324675324, "f1": 0.6545229385564103, "f1_weighted": 0.6545229385564103}, {"accuracy": 0.6646103896103897, "f1": 0.6624956597278372, "f1_weighted": 0.6624956597278373}, {"accuracy": 0.6610389610389611, "f1": 0.6585371347191131, "f1_weighted": 0.6585371347191131}, {"accuracy": 0.6659090909090909, "f1": 0.664344975061401, "f1_weighted": 0.6643449750614012}, {"accuracy": 0.6525974025974026, "f1": 0.6496047024687592, "f1_weighted": 0.6496047024687591}]}]}, "task_name": "Banking77Classification", "avg_layerwise_entropy": 0.3823353127690412, "num_samples": 64}