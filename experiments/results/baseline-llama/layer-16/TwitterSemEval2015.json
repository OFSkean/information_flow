{
  "dataset_revision": "70970daeab8776df92f5ea462b6173c0b46fd2d1",
  "evaluation_time": 143.99398636817932,
  "kg_co2_emissions": null,
  "mteb_version": "1.12.49",
  "scores": {
    "test": [
      {
        "cosine": {
          "accuracy": 0.8176670441676104,
          "accuracy_threshold": 0.8280861377716064,
          "ap": 0.5763603315817747,
          "f1": 0.5509557874985341,
          "f1_threshold": 0.7827491164207458,
          "precision": 0.49588347055098164,
          "recall": 0.6197889182058047
        },
        "dot": {
          "accuracy": 0.7808905048578411,
          "accuracy_threshold": 27.982824325561523,
          "ap": 0.366293276612707,
          "f1": 0.39836132732486684,
          "f1_threshold": 21.73193359375,
          "precision": 0.28888888888888886,
          "recall": 0.641424802110818
        },
        "euclidean": {
          "accuracy": 0.8117065029504679,
          "accuracy_threshold": 3.1798768043518066,
          "ap": 0.5570208565911734,
          "f1": 0.5472258524414874,
          "f1_threshold": 3.5397207736968994,
          "precision": 0.4977307110438729,
          "recall": 0.6076517150395778
        },
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5763603315817747,
        "manhattan": {
          "accuracy": 0.8112892650652679,
          "accuracy_threshold": 151.3543701171875,
          "ap": 0.5544901828242415,
          "f1": 0.5473758365621697,
          "f1_threshold": 173.46910095214844,
          "precision": 0.4931246033425005,
          "recall": 0.6150395778364116
        },
        "max": {
          "accuracy": 0.8176670441676104,
          "ap": 0.5763603315817747,
          "f1": 0.5509557874985341
        },
        "similarity": {
          "accuracy": 0.8176670441676104,
          "accuracy_threshold": 0.8280861377716064,
          "ap": 0.5763603315817747,
          "f1": 0.5509557874985341,
          "f1_threshold": 0.7827491164207458,
          "precision": 0.49588347055098164,
          "recall": 0.6197889182058047
        }
      }
    ]
  },
  "task_name": "TwitterSemEval2015"
}