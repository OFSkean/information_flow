{"dataset_revision": "70970daeab8776df92f5ea462b6173c0b46fd2d1", "evaluation_time": 140.90226984024048, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"cosine": {"accuracy": 0.8111104488287536, "accuracy_threshold": 0.8591498732566833, "ap": 0.5566428172849959, "f1": 0.5404674046740467, "f1_threshold": 0.8140809535980225, "precision": 0.506221198156682, "recall": 0.5796833773087071}, "dot": {"accuracy": 0.7779698396614413, "accuracy_threshold": 2.6952462196350098, "ap": 0.34225012240369945, "f1": 0.40725552050473185, "f1_threshold": 1.9569129943847656, "precision": 0.2904386951631046, "recall": 0.6812664907651715}, "euclidean": {"accuracy": 0.8078321511593253, "accuracy_threshold": 0.8443644046783447, "ap": 0.5374204483996481, "f1": 0.5290773939658941, "f1_threshold": 1.0050525665283203, "precision": 0.45166106756252333, "recall": 0.6385224274406333}, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.5566428172849959, "manhattan": {"accuracy": 0.808070572808011, "accuracy_threshold": 41.86187744140625, "ap": 0.5383250935616857, "f1": 0.5305070851387751, "f1_threshold": 48.74642562866211, "precision": 0.47694251421351863, "recall": 0.5976253298153035}, "max": {"accuracy": 0.8111104488287536, "ap": 0.5566428172849959, "f1": 0.5404674046740467}, "similarity": {"accuracy": 0.8111104488287536, "accuracy_threshold": 0.8591498732566833, "ap": 0.5566428172849959, "f1": 0.5404674046740467, "f1_threshold": 0.8140809535980225, "precision": 0.506221198156682, "recall": 0.5796833773087071}}]}, "task_name": "TwitterSemEval2015", "avg_layerwise_entropy": 0.24879378441408542, "num_samples": 64}