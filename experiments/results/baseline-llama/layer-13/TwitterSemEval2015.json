{
  "dataset_revision": "70970daeab8776df92f5ea462b6173c0b46fd2d1",
  "evaluation_time": 140.99203157424927,
  "kg_co2_emissions": null,
  "mteb_version": "1.12.49",
  "scores": {
    "test": [
      {
        "cosine": {
          "accuracy": 0.8049114859629254,
          "accuracy_threshold": 0.8518013954162598,
          "ap": 0.5222246037729114,
          "f1": 0.5086783891094725,
          "f1_threshold": 0.7975075244903564,
          "precision": 0.4461691542288557,
          "recall": 0.59155672823219
        },
        "dot": {
          "accuracy": 0.7767777314180128,
          "accuracy_threshold": 16.336471557617188,
          "ap": 0.30679724080547166,
          "f1": 0.38347130629998594,
          "f1_threshold": 10.264629364013672,
          "precision": 0.26118119266055045,
          "recall": 0.7211081794195251
        },
        "euclidean": {
          "accuracy": 0.8016927937056685,
          "accuracy_threshold": 2.0242486000061035,
          "ap": 0.5099260416811692,
          "f1": 0.5029425079221367,
          "f1_threshold": 2.3864614963531494,
          "precision": 0.4403487911216805,
          "recall": 0.5862796833773087
        },
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5222246037729114,
        "manhattan": {
          "accuracy": 0.8000238421648685,
          "accuracy_threshold": 100.07191467285156,
          "ap": 0.5030349673444456,
          "f1": 0.49716336690980667,
          "f1_threshold": 116.52201843261719,
          "precision": 0.44295440478646586,
          "recall": 0.566490765171504
        },
        "max": {
          "accuracy": 0.8049114859629254,
          "ap": 0.5222246037729114,
          "f1": 0.5086783891094725
        },
        "similarity": {
          "accuracy": 0.8049114859629254,
          "accuracy_threshold": 0.8518013954162598,
          "ap": 0.5222246037729114,
          "f1": 0.5086783891094725,
          "f1_threshold": 0.7975075244903564,
          "precision": 0.4461691542288557,
          "recall": 0.59155672823219
        }
      }
    ]
  },
  "task_name": "TwitterSemEval2015"
}