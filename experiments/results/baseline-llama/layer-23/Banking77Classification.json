{"dataset_revision": "0fd18e25b25c072e09e0d92ab615fda904d66300", "evaluation_time": 153.92073726654053, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"accuracy": 0.7284415584415584, "f1": 0.7277832761614886, "f1_weighted": 0.7277832761614886, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.7284415584415584, "scores_per_experiment": [{"accuracy": 0.7152597402597403, "f1": 0.7136985950948777, "f1_weighted": 0.7136985950948779}, {"accuracy": 0.7305194805194806, "f1": 0.7317957612525624, "f1_weighted": 0.7317957612525624}, {"accuracy": 0.7392857142857143, "f1": 0.7396973305962605, "f1_weighted": 0.7396973305962604}, {"accuracy": 0.737012987012987, "f1": 0.7376833518881624, "f1_weighted": 0.7376833518881621}, {"accuracy": 0.7288961038961039, "f1": 0.7289599813338504, "f1_weighted": 0.7289599813338502}, {"accuracy": 0.7262987012987013, "f1": 0.7244712177298471, "f1_weighted": 0.7244712177298473}, {"accuracy": 0.7324675324675325, "f1": 0.7302134256421734, "f1_weighted": 0.7302134256421734}, {"accuracy": 0.7243506493506493, "f1": 0.7221798753319447, "f1_weighted": 0.7221798753319448}, {"accuracy": 0.727922077922078, "f1": 0.7277062315112321, "f1_weighted": 0.7277062315112321}, {"accuracy": 0.7224025974025974, "f1": 0.7214269912339757, "f1_weighted": 0.7214269912339759}]}]}, "task_name": "Banking77Classification", "avg_layerwise_entropy": 0.6671161706276105, "num_samples": 64}