{
  "dataset_revision": "70970daeab8776df92f5ea462b6173c0b46fd2d1",
  "evaluation_time": 141.76244401931763,
  "kg_co2_emissions": null,
  "mteb_version": "1.12.49",
  "scores": {
    "test": [
      {
        "cosine": {
          "accuracy": 0.82350837456041,
          "accuracy_threshold": 0.8265880346298218,
          "ap": 0.6032545825216765,
          "f1": 0.5652061187451386,
          "f1_threshold": 0.79585862159729,
          "precision": 0.5555555555555556,
          "recall": 0.575197889182058
        },
        "dot": {
          "accuracy": 0.7779102342492699,
          "accuracy_threshold": 148.5753173828125,
          "ap": 0.3319012626406036,
          "f1": 0.388832088084939,
          "f1_threshold": 104.36157989501953,
          "precision": 0.2769747899159664,
          "recall": 0.6522427440633245
        },
        "euclidean": {
          "accuracy": 0.8172498062824104,
          "accuracy_threshold": 7.005465507507324,
          "ap": 0.5806626663339101,
          "f1": 0.5623939907923431,
          "f1_threshold": 7.783769130706787,
          "precision": 0.5199372759856631,
          "recall": 0.6124010554089709
        },
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.6032545825216765,
        "manhattan": {
          "accuracy": 0.8183227037014961,
          "accuracy_threshold": 330.46038818359375,
          "ap": 0.5870146971017021,
          "f1": 0.5708154506437769,
          "f1_threshold": 375.5765380859375,
          "precision": 0.5206611570247934,
          "recall": 0.6316622691292876
        },
        "max": {
          "accuracy": 0.82350837456041,
          "ap": 0.6032545825216765,
          "f1": 0.5708154506437769
        },
        "similarity": {
          "accuracy": 0.82350837456041,
          "accuracy_threshold": 0.8265880346298218,
          "ap": 0.6032545825216765,
          "f1": 0.5652061187451386,
          "f1_threshold": 0.79585862159729,
          "precision": 0.5555555555555556,
          "recall": 0.575197889182058
        }
      }
    ]
  },
  "task_name": "TwitterSemEval2015"
}