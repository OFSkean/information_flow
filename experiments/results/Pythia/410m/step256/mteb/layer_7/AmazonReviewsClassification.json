{
  "dataset_revision": "1399c76144fd37290681b995c656ef9b2e06e26d",
  "evaluation_time": 31.25170922279358,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.20",
  "scores": {
    "test": [
      {
        "accuracy": 0.27081999999999995,
        "f1": 0.26893397697503507,
        "f1_weighted": 0.26893397697503507,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.27081999999999995,
        "scores_per_experiment": [
          {
            "accuracy": 0.2866,
            "f1": 0.28363542242907547,
            "f1_weighted": 0.2836354224290755
          },
          {
            "accuracy": 0.2774,
            "f1": 0.2756142411683505,
            "f1_weighted": 0.2756142411683505
          },
          {
            "accuracy": 0.2444,
            "f1": 0.24250759478919504,
            "f1_weighted": 0.24250759478919504
          },
          {
            "accuracy": 0.2622,
            "f1": 0.26099145938610585,
            "f1_weighted": 0.26099145938610585
          },
          {
            "accuracy": 0.3062,
            "f1": 0.30320385898333513,
            "f1_weighted": 0.30320385898333513
          },
          {
            "accuracy": 0.2622,
            "f1": 0.2624731681619408,
            "f1_weighted": 0.2624731681619408
          },
          {
            "accuracy": 0.2328,
            "f1": 0.23250217494880684,
            "f1_weighted": 0.2325021749488069
          },
          {
            "accuracy": 0.2944,
            "f1": 0.2912733765619257,
            "f1_weighted": 0.2912733765619257
          },
          {
            "accuracy": 0.2822,
            "f1": 0.28026240864118307,
            "f1_weighted": 0.2802624086411831
          },
          {
            "accuracy": 0.2598,
            "f1": 0.25687606468043167,
            "f1_weighted": 0.25687606468043167
          }
        ]
      }
    ]
  },
  "task_name": "AmazonReviewsClassification"
}