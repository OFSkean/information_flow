{
  "dataset_revision": "1399c76144fd37290681b995c656ef9b2e06e26d",
  "evaluation_time": 44.87277913093567,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.20",
  "scores": {
    "test": [
      {
        "accuracy": 0.26616,
        "f1": 0.2642330827743497,
        "f1_weighted": 0.26423308277434976,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.26616,
        "scores_per_experiment": [
          {
            "accuracy": 0.289,
            "f1": 0.28702362483073623,
            "f1_weighted": 0.28702362483073623
          },
          {
            "accuracy": 0.27,
            "f1": 0.2665967225728557,
            "f1_weighted": 0.2665967225728557
          },
          {
            "accuracy": 0.2444,
            "f1": 0.24176176335975233,
            "f1_weighted": 0.24176176335975233
          },
          {
            "accuracy": 0.2538,
            "f1": 0.25220723404558265,
            "f1_weighted": 0.25220723404558265
          },
          {
            "accuracy": 0.301,
            "f1": 0.2989721535735781,
            "f1_weighted": 0.2989721535735781
          },
          {
            "accuracy": 0.2566,
            "f1": 0.25703818414953294,
            "f1_weighted": 0.257038184149533
          },
          {
            "accuracy": 0.2348,
            "f1": 0.23252814135387476,
            "f1_weighted": 0.2325281413538748
          },
          {
            "accuracy": 0.2886,
            "f1": 0.2870577754177471,
            "f1_weighted": 0.2870577754177471
          },
          {
            "accuracy": 0.2738,
            "f1": 0.2718974377004419,
            "f1_weighted": 0.2718974377004419
          },
          {
            "accuracy": 0.2496,
            "f1": 0.24724779073939604,
            "f1_weighted": 0.24724779073939607
          }
        ]
      }
    ]
  },
  "task_name": "AmazonReviewsClassification"
}