{
  "dataset_revision": "1399c76144fd37290681b995c656ef9b2e06e26d",
  "evaluation_time": 31.567164659500122,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.20",
  "scores": {
    "test": [
      {
        "accuracy": 0.26925999999999994,
        "f1": 0.2671330938218862,
        "f1_weighted": 0.26713309382188627,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.26925999999999994,
        "scores_per_experiment": [
          {
            "accuracy": 0.2812,
            "f1": 0.27747623241631275,
            "f1_weighted": 0.2774762324163127
          },
          {
            "accuracy": 0.2726,
            "f1": 0.2700317537246423,
            "f1_weighted": 0.2700317537246423
          },
          {
            "accuracy": 0.2444,
            "f1": 0.24216496486532318,
            "f1_weighted": 0.24216496486532324
          },
          {
            "accuracy": 0.2644,
            "f1": 0.2626149812576143,
            "f1_weighted": 0.2626149812576143
          },
          {
            "accuracy": 0.3044,
            "f1": 0.301651701222936,
            "f1_weighted": 0.30165170122293605
          },
          {
            "accuracy": 0.2644,
            "f1": 0.26440844616862536,
            "f1_weighted": 0.2644084461686253
          },
          {
            "accuracy": 0.2352,
            "f1": 0.2345071136239909,
            "f1_weighted": 0.23450711362399088
          },
          {
            "accuracy": 0.2898,
            "f1": 0.2877416647518051,
            "f1_weighted": 0.28774166475180507
          },
          {
            "accuracy": 0.2804,
            "f1": 0.27776846584065995,
            "f1_weighted": 0.27776846584065995
          },
          {
            "accuracy": 0.2558,
            "f1": 0.2529656143469527,
            "f1_weighted": 0.25296561434695275
          }
        ]
      }
    ]
  },
  "task_name": "AmazonReviewsClassification"
}