{
  "dataset_revision": "1399c76144fd37290681b995c656ef9b2e06e26d",
  "evaluation_time": 58.96075797080994,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.20",
  "scores": {
    "test": [
      {
        "accuracy": 0.26392,
        "f1": 0.2619935454589321,
        "f1_weighted": 0.2619935454589321,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.26392,
        "scores_per_experiment": [
          {
            "accuracy": 0.284,
            "f1": 0.2826055513806179,
            "f1_weighted": 0.2826055513806179
          },
          {
            "accuracy": 0.257,
            "f1": 0.2533596179296499,
            "f1_weighted": 0.2533596179296499
          },
          {
            "accuracy": 0.24,
            "f1": 0.2359199091794042,
            "f1_weighted": 0.23591990917940425
          },
          {
            "accuracy": 0.2554,
            "f1": 0.25403717098933215,
            "f1_weighted": 0.2540371709893321
          },
          {
            "accuracy": 0.3006,
            "f1": 0.29802439259299957,
            "f1_weighted": 0.2980243925929996
          },
          {
            "accuracy": 0.2534,
            "f1": 0.25328953998457965,
            "f1_weighted": 0.2532895399845796
          },
          {
            "accuracy": 0.2322,
            "f1": 0.22933994374684397,
            "f1_weighted": 0.229339943746844
          },
          {
            "accuracy": 0.2838,
            "f1": 0.2820136815892773,
            "f1_weighted": 0.28201368158927737
          },
          {
            "accuracy": 0.2786,
            "f1": 0.2774488407918724,
            "f1_weighted": 0.27744884079187243
          },
          {
            "accuracy": 0.2542,
            "f1": 0.2538968064047446,
            "f1_weighted": 0.2538968064047446
          }
        ]
      }
    ]
  },
  "task_name": "AmazonReviewsClassification"
}