{
  "dataset_revision": "1399c76144fd37290681b995c656ef9b2e06e26d",
  "evaluation_time": 33.2017126083374,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.20",
  "scores": {
    "test": [
      {
        "accuracy": 0.26792,
        "f1": 0.265765679752956,
        "f1_weighted": 0.26576567975295606,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.26792,
        "scores_per_experiment": [
          {
            "accuracy": 0.2794,
            "f1": 0.27569283852846727,
            "f1_weighted": 0.27569283852846727
          },
          {
            "accuracy": 0.2734,
            "f1": 0.27000615605274664,
            "f1_weighted": 0.27000615605274664
          },
          {
            "accuracy": 0.242,
            "f1": 0.2392514834487533,
            "f1_weighted": 0.2392514834487533
          },
          {
            "accuracy": 0.263,
            "f1": 0.26124919961281756,
            "f1_weighted": 0.2612491996128175
          },
          {
            "accuracy": 0.3026,
            "f1": 0.29941906700192644,
            "f1_weighted": 0.2994190670019265
          },
          {
            "accuracy": 0.2622,
            "f1": 0.2628061741665252,
            "f1_weighted": 0.26280617416652513
          },
          {
            "accuracy": 0.2378,
            "f1": 0.23640200744756723,
            "f1_weighted": 0.2364020074475672
          },
          {
            "accuracy": 0.2882,
            "f1": 0.28679707384843656,
            "f1_weighted": 0.2867970738484365
          },
          {
            "accuracy": 0.2756,
            "f1": 0.2729554298333199,
            "f1_weighted": 0.27295542983331994
          },
          {
            "accuracy": 0.255,
            "f1": 0.25307736758900023,
            "f1_weighted": 0.2530773675890003
          }
        ]
      }
    ]
  },
  "task_name": "AmazonReviewsClassification"
}