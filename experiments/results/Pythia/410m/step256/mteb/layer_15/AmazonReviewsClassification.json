{
  "dataset_revision": "1399c76144fd37290681b995c656ef9b2e06e26d",
  "evaluation_time": 45.96697807312012,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.20",
  "scores": {
    "test": [
      {
        "accuracy": 0.26638,
        "f1": 0.2646904292931567,
        "f1_weighted": 0.2646904292931567,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.26638,
        "scores_per_experiment": [
          {
            "accuracy": 0.286,
            "f1": 0.28454506438611327,
            "f1_weighted": 0.28454506438611327
          },
          {
            "accuracy": 0.2712,
            "f1": 0.26795027100436386,
            "f1_weighted": 0.26795027100436386
          },
          {
            "accuracy": 0.246,
            "f1": 0.24377774471529837,
            "f1_weighted": 0.24377774471529834
          },
          {
            "accuracy": 0.256,
            "f1": 0.2549029960814504,
            "f1_weighted": 0.2549029960814504
          },
          {
            "accuracy": 0.3006,
            "f1": 0.29827379422360356,
            "f1_weighted": 0.2982737942236036
          },
          {
            "accuracy": 0.256,
            "f1": 0.2564022384422078,
            "f1_weighted": 0.2564022384422078
          },
          {
            "accuracy": 0.2348,
            "f1": 0.23243214783712976,
            "f1_weighted": 0.23243214783712973
          },
          {
            "accuracy": 0.2884,
            "f1": 0.2871057891973135,
            "f1_weighted": 0.28710578919731344
          },
          {
            "accuracy": 0.2738,
            "f1": 0.27200759068446867,
            "f1_weighted": 0.2720075906844686
          },
          {
            "accuracy": 0.251,
            "f1": 0.24950665635961816,
            "f1_weighted": 0.24950665635961813
          }
        ]
      }
    ]
  },
  "task_name": "AmazonReviewsClassification"
}