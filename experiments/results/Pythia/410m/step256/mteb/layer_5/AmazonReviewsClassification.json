{
  "dataset_revision": "1399c76144fd37290681b995c656ef9b2e06e26d",
  "evaluation_time": 25.831393241882324,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.20",
  "scores": {
    "test": [
      {
        "accuracy": 0.27224,
        "f1": 0.27061312834923557,
        "f1_weighted": 0.27061312834923557,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.27224,
        "scores_per_experiment": [
          {
            "accuracy": 0.2886,
            "f1": 0.2872773751351924,
            "f1_weighted": 0.2872773751351924
          },
          {
            "accuracy": 0.2808,
            "f1": 0.279922256108741,
            "f1_weighted": 0.279922256108741
          },
          {
            "accuracy": 0.2432,
            "f1": 0.24089355464148737,
            "f1_weighted": 0.2408935546414874
          },
          {
            "accuracy": 0.254,
            "f1": 0.2520760926304418,
            "f1_weighted": 0.2520760926304418
          },
          {
            "accuracy": 0.3064,
            "f1": 0.30283494984828413,
            "f1_weighted": 0.30283494984828413
          },
          {
            "accuracy": 0.2708,
            "f1": 0.2705905317417065,
            "f1_weighted": 0.2705905317417065
          },
          {
            "accuracy": 0.239,
            "f1": 0.23821666311152612,
            "f1_weighted": 0.23821666311152612
          },
          {
            "accuracy": 0.2842,
            "f1": 0.28167569311592583,
            "f1_weighted": 0.28167569311592583
          },
          {
            "accuracy": 0.2884,
            "f1": 0.287524416737911,
            "f1_weighted": 0.287524416737911
          },
          {
            "accuracy": 0.267,
            "f1": 0.2651197504211399,
            "f1_weighted": 0.2651197504211399
          }
        ]
      }
    ]
  },
  "task_name": "AmazonReviewsClassification"
}