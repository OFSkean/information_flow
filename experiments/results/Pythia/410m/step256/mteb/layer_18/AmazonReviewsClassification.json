{
  "dataset_revision": "1399c76144fd37290681b995c656ef9b2e06e26d",
  "evaluation_time": 53.95950770378113,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.20",
  "scores": {
    "test": [
      {
        "accuracy": 0.26546000000000003,
        "f1": 0.26344318059667154,
        "f1_weighted": 0.26344318059667154,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.26546000000000003,
        "scores_per_experiment": [
          {
            "accuracy": 0.2828,
            "f1": 0.2807306211285968,
            "f1_weighted": 0.2807306211285968
          },
          {
            "accuracy": 0.2614,
            "f1": 0.25782078279770004,
            "f1_weighted": 0.2578207827977001
          },
          {
            "accuracy": 0.2432,
            "f1": 0.23928281773750953,
            "f1_weighted": 0.2392828177375095
          },
          {
            "accuracy": 0.257,
            "f1": 0.25590818970115786,
            "f1_weighted": 0.25590818970115786
          },
          {
            "accuracy": 0.302,
            "f1": 0.29901025923675284,
            "f1_weighted": 0.2990102592367529
          },
          {
            "accuracy": 0.2546,
            "f1": 0.2546496952347399,
            "f1_weighted": 0.2546496952347399
          },
          {
            "accuracy": 0.2346,
            "f1": 0.2315552141535239,
            "f1_weighted": 0.23155521415352393
          },
          {
            "accuracy": 0.2874,
            "f1": 0.2856030436890699,
            "f1_weighted": 0.2856030436890699
          },
          {
            "accuracy": 0.2786,
            "f1": 0.27722763658375527,
            "f1_weighted": 0.27722763658375527
          },
          {
            "accuracy": 0.253,
            "f1": 0.2526435457039091,
            "f1_weighted": 0.2526435457039091
          }
        ]
      }
    ]
  },
  "task_name": "AmazonReviewsClassification"
}