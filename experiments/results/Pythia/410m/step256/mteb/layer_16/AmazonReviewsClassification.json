{
  "dataset_revision": "1399c76144fd37290681b995c656ef9b2e06e26d",
  "evaluation_time": 47.636242151260376,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.20",
  "scores": {
    "test": [
      {
        "accuracy": 0.26708,
        "f1": 0.26526260323023565,
        "f1_weighted": 0.2652626032302356,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.26708,
        "scores_per_experiment": [
          {
            "accuracy": 0.2882,
            "f1": 0.28672527059375963,
            "f1_weighted": 0.28672527059375963
          },
          {
            "accuracy": 0.269,
            "f1": 0.26604055513657066,
            "f1_weighted": 0.26604055513657066
          },
          {
            "accuracy": 0.2436,
            "f1": 0.2394360383281851,
            "f1_weighted": 0.23943603832818508
          },
          {
            "accuracy": 0.2546,
            "f1": 0.2535199285667134,
            "f1_weighted": 0.25351992856671335
          },
          {
            "accuracy": 0.3016,
            "f1": 0.2990859942130857,
            "f1_weighted": 0.2990859942130857
          },
          {
            "accuracy": 0.256,
            "f1": 0.2563704310200412,
            "f1_weighted": 0.2563704310200412
          },
          {
            "accuracy": 0.2366,
            "f1": 0.23440093930118433,
            "f1_weighted": 0.23440093930118433
          },
          {
            "accuracy": 0.2884,
            "f1": 0.286906798113103,
            "f1_weighted": 0.286906798113103
          },
          {
            "accuracy": 0.2784,
            "f1": 0.27693515338856367,
            "f1_weighted": 0.27693515338856367
          },
          {
            "accuracy": 0.2544,
            "f1": 0.25320492364114944,
            "f1_weighted": 0.25320492364114944
          }
        ]
      }
    ]
  },
  "task_name": "AmazonReviewsClassification"
}