{
  "dataset_revision": "1399c76144fd37290681b995c656ef9b2e06e26d",
  "evaluation_time": 60.553919076919556,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.20",
  "scores": {
    "test": [
      {
        "accuracy": 0.33938,
        "f1": 0.3388202542382074,
        "f1_weighted": 0.3388202542382074,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.33938,
        "scores_per_experiment": [
          {
            "accuracy": 0.3538,
            "f1": 0.35209272309277473,
            "f1_weighted": 0.3520927230927748
          },
          {
            "accuracy": 0.3412,
            "f1": 0.3421855956735348,
            "f1_weighted": 0.3421855956735348
          },
          {
            "accuracy": 0.3238,
            "f1": 0.3207573992933811,
            "f1_weighted": 0.32075739929338115
          },
          {
            "accuracy": 0.3444,
            "f1": 0.34951485236369184,
            "f1_weighted": 0.34951485236369184
          },
          {
            "accuracy": 0.3694,
            "f1": 0.3596959737166553,
            "f1_weighted": 0.3596959737166553
          },
          {
            "accuracy": 0.3254,
            "f1": 0.3244739111618086,
            "f1_weighted": 0.3244739111618087
          },
          {
            "accuracy": 0.3216,
            "f1": 0.3206642256549065,
            "f1_weighted": 0.3206642256549065
          },
          {
            "accuracy": 0.3472,
            "f1": 0.3466036286894905,
            "f1_weighted": 0.3466036286894906
          },
          {
            "accuracy": 0.325,
            "f1": 0.3255968086343214,
            "f1_weighted": 0.3255968086343214
          },
          {
            "accuracy": 0.342,
            "f1": 0.3466174241015093,
            "f1_weighted": 0.34661742410150925
          }
        ]
      }
    ]
  },
  "task_name": "AmazonReviewsClassification"
}