{
  "dataset_revision": "1399c76144fd37290681b995c656ef9b2e06e26d",
  "evaluation_time": 41.481903314590454,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.20",
  "scores": {
    "test": [
      {
        "accuracy": 0.35514,
        "f1": 0.3541017038098047,
        "f1_weighted": 0.3541017038098047,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.35514,
        "scores_per_experiment": [
          {
            "accuracy": 0.3662,
            "f1": 0.36848601425443517,
            "f1_weighted": 0.3684860142544352
          },
          {
            "accuracy": 0.3694,
            "f1": 0.37202320746679335,
            "f1_weighted": 0.3720232074667933
          },
          {
            "accuracy": 0.3302,
            "f1": 0.32647067203198443,
            "f1_weighted": 0.3264706720319844
          },
          {
            "accuracy": 0.37,
            "f1": 0.3733591138872995,
            "f1_weighted": 0.3733591138872995
          },
          {
            "accuracy": 0.4054,
            "f1": 0.3930451075847555,
            "f1_weighted": 0.39304510758475564
          },
          {
            "accuracy": 0.3222,
            "f1": 0.3224825224926047,
            "f1_weighted": 0.3224825224926046
          },
          {
            "accuracy": 0.321,
            "f1": 0.32211695082119485,
            "f1_weighted": 0.3221169508211948
          },
          {
            "accuracy": 0.3698,
            "f1": 0.3660746443711271,
            "f1_weighted": 0.36607464437112713
          },
          {
            "accuracy": 0.3348,
            "f1": 0.33491279787169403,
            "f1_weighted": 0.33491279787169403
          },
          {
            "accuracy": 0.3624,
            "f1": 0.3620460073161584,
            "f1_weighted": 0.3620460073161583
          }
        ]
      }
    ]
  },
  "task_name": "AmazonReviewsClassification"
}