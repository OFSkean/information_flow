{
  "dataset_revision": "1399c76144fd37290681b995c656ef9b2e06e26d",
  "evaluation_time": 41.848726987838745,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.20",
  "scores": {
    "test": [
      {
        "accuracy": 0.25962,
        "f1": 0.2580364988258231,
        "f1_weighted": 0.2580364988258231,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.25962,
        "scores_per_experiment": [
          {
            "accuracy": 0.278,
            "f1": 0.2770950616467659,
            "f1_weighted": 0.2770950616467659
          },
          {
            "accuracy": 0.2516,
            "f1": 0.2497728136692165,
            "f1_weighted": 0.24977281366921653
          },
          {
            "accuracy": 0.2516,
            "f1": 0.25231229175587516,
            "f1_weighted": 0.25231229175587516
          },
          {
            "accuracy": 0.2338,
            "f1": 0.23302477775859315,
            "f1_weighted": 0.2330247777585932
          },
          {
            "accuracy": 0.3046,
            "f1": 0.30267610117719956,
            "f1_weighted": 0.3026761011771996
          },
          {
            "accuracy": 0.2516,
            "f1": 0.25085975258559673,
            "f1_weighted": 0.2508597525855967
          },
          {
            "accuracy": 0.2186,
            "f1": 0.2167899384842884,
            "f1_weighted": 0.2167899384842884
          },
          {
            "accuracy": 0.2694,
            "f1": 0.26428533509691376,
            "f1_weighted": 0.26428533509691376
          },
          {
            "accuracy": 0.2702,
            "f1": 0.2684110042391637,
            "f1_weighted": 0.26841100423916364
          },
          {
            "accuracy": 0.2668,
            "f1": 0.2651379118446179,
            "f1_weighted": 0.2651379118446178
          }
        ]
      }
    ]
  },
  "task_name": "AmazonReviewsClassification"
}