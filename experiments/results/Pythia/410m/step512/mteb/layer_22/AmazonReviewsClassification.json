{
  "dataset_revision": "1399c76144fd37290681b995c656ef9b2e06e26d",
  "evaluation_time": 63.44373106956482,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.20",
  "scores": {
    "test": [
      {
        "accuracy": 0.25962,
        "f1": 0.25819155411624745,
        "f1_weighted": 0.25819155411624745,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.25962,
        "scores_per_experiment": [
          {
            "accuracy": 0.2746,
            "f1": 0.27420539767549995,
            "f1_weighted": 0.2742053976754999
          },
          {
            "accuracy": 0.2494,
            "f1": 0.24586348955471227,
            "f1_weighted": 0.24586348955471227
          },
          {
            "accuracy": 0.255,
            "f1": 0.255455109580503,
            "f1_weighted": 0.255455109580503
          },
          {
            "accuracy": 0.2438,
            "f1": 0.2427927128621798,
            "f1_weighted": 0.24279271286217982
          },
          {
            "accuracy": 0.302,
            "f1": 0.2996121639019254,
            "f1_weighted": 0.2996121639019254
          },
          {
            "accuracy": 0.2578,
            "f1": 0.25754951585898905,
            "f1_weighted": 0.2575495158589891
          },
          {
            "accuracy": 0.2186,
            "f1": 0.21835082863604086,
            "f1_weighted": 0.21835082863604088
          },
          {
            "accuracy": 0.2552,
            "f1": 0.2508219771407543,
            "f1_weighted": 0.2508219771407543
          },
          {
            "accuracy": 0.2708,
            "f1": 0.2694274736124763,
            "f1_weighted": 0.2694274736124763
          },
          {
            "accuracy": 0.269,
            "f1": 0.26783687233939335,
            "f1_weighted": 0.26783687233939335
          }
        ]
      }
    ]
  },
  "task_name": "AmazonReviewsClassification"
}