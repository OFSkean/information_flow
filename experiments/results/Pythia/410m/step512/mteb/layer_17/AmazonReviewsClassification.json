{
  "dataset_revision": "1399c76144fd37290681b995c656ef9b2e06e26d",
  "evaluation_time": 54.80446791648865,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.20",
  "scores": {
    "test": [
      {
        "accuracy": 0.259,
        "f1": 0.25725217565516767,
        "f1_weighted": 0.25725217565516767,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.259,
        "scores_per_experiment": [
          {
            "accuracy": 0.2772,
            "f1": 0.2752478426418753,
            "f1_weighted": 0.2752478426418753
          },
          {
            "accuracy": 0.2484,
            "f1": 0.24624794372638759,
            "f1_weighted": 0.2462479437263876
          },
          {
            "accuracy": 0.2576,
            "f1": 0.25828965723804076,
            "f1_weighted": 0.25828965723804076
          },
          {
            "accuracy": 0.2378,
            "f1": 0.23735279816256477,
            "f1_weighted": 0.23735279816256474
          },
          {
            "accuracy": 0.306,
            "f1": 0.30340973039461216,
            "f1_weighted": 0.30340973039461216
          },
          {
            "accuracy": 0.2512,
            "f1": 0.25069477787507854,
            "f1_weighted": 0.25069477787507854
          },
          {
            "accuracy": 0.2186,
            "f1": 0.21675501847984494,
            "f1_weighted": 0.21675501847984496
          },
          {
            "accuracy": 0.2604,
            "f1": 0.25480287103729526,
            "f1_weighted": 0.25480287103729526
          },
          {
            "accuracy": 0.2668,
            "f1": 0.2651252632149991,
            "f1_weighted": 0.265125263214999
          },
          {
            "accuracy": 0.266,
            "f1": 0.2645958537809783,
            "f1_weighted": 0.2645958537809783
          }
        ]
      }
    ]
  },
  "task_name": "AmazonReviewsClassification"
}