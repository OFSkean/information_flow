{
  "dataset_revision": "1399c76144fd37290681b995c656ef9b2e06e26d",
  "evaluation_time": 22.146108627319336,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.20",
  "scores": {
    "test": [
      {
        "accuracy": 0.29634,
        "f1": 0.29502928138749757,
        "f1_weighted": 0.29502928138749757,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.29634,
        "scores_per_experiment": [
          {
            "accuracy": 0.3276,
            "f1": 0.3276799348462495,
            "f1_weighted": 0.3276799348462495
          },
          {
            "accuracy": 0.2928,
            "f1": 0.295179057028132,
            "f1_weighted": 0.2951790570281321
          },
          {
            "accuracy": 0.2734,
            "f1": 0.2740382485496725,
            "f1_weighted": 0.2740382485496725
          },
          {
            "accuracy": 0.2742,
            "f1": 0.27271657556584905,
            "f1_weighted": 0.272716575565849
          },
          {
            "accuracy": 0.3508,
            "f1": 0.34622137704453804,
            "f1_weighted": 0.34622137704453804
          },
          {
            "accuracy": 0.2896,
            "f1": 0.29010105847875683,
            "f1_weighted": 0.29010105847875683
          },
          {
            "accuracy": 0.2514,
            "f1": 0.25523145817061327,
            "f1_weighted": 0.2552314581706132
          },
          {
            "accuracy": 0.31,
            "f1": 0.3026602647974419,
            "f1_weighted": 0.3026602647974419
          },
          {
            "accuracy": 0.29,
            "f1": 0.2880344345971433,
            "f1_weighted": 0.28803443459714334
          },
          {
            "accuracy": 0.3036,
            "f1": 0.2984304047965791,
            "f1_weighted": 0.2984304047965792
          }
        ]
      }
    ]
  },
  "task_name": "AmazonReviewsClassification"
}