{
  "dataset_revision": "1399c76144fd37290681b995c656ef9b2e06e26d",
  "evaluation_time": 60.4447078704834,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.20",
  "scores": {
    "test": [
      {
        "accuracy": 0.29606,
        "f1": 0.2946470483009177,
        "f1_weighted": 0.2946470483009177,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.29606,
        "scores_per_experiment": [
          {
            "accuracy": 0.326,
            "f1": 0.32601732077959933,
            "f1_weighted": 0.32601732077959933
          },
          {
            "accuracy": 0.2934,
            "f1": 0.29393312549524053,
            "f1_weighted": 0.2939331254952405
          },
          {
            "accuracy": 0.2758,
            "f1": 0.27779861592076055,
            "f1_weighted": 0.2777986159207606
          },
          {
            "accuracy": 0.2792,
            "f1": 0.28220704263543894,
            "f1_weighted": 0.28220704263543894
          },
          {
            "accuracy": 0.3504,
            "f1": 0.3430030281654189,
            "f1_weighted": 0.3430030281654189
          },
          {
            "accuracy": 0.2908,
            "f1": 0.28653046714888564,
            "f1_weighted": 0.2865304671488856
          },
          {
            "accuracy": 0.2638,
            "f1": 0.2645221164141799,
            "f1_weighted": 0.26452211641417994
          },
          {
            "accuracy": 0.3084,
            "f1": 0.2997584345526036,
            "f1_weighted": 0.29975843455260354
          },
          {
            "accuracy": 0.2858,
            "f1": 0.283428389435025,
            "f1_weighted": 0.2834283894350251
          },
          {
            "accuracy": 0.287,
            "f1": 0.28927194246202476,
            "f1_weighted": 0.2892719424620247
          }
        ]
      }
    ]
  },
  "task_name": "AmazonReviewsClassification"
}