{
  "dataset_revision": "1399c76144fd37290681b995c656ef9b2e06e26d",
  "evaluation_time": 68.9550154209137,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.20",
  "scores": {
    "test": [
      {
        "accuracy": 0.32996000000000003,
        "f1": 0.33117640449121055,
        "f1_weighted": 0.33117640449121055,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.32996000000000003,
        "scores_per_experiment": [
          {
            "accuracy": 0.3356,
            "f1": 0.3379850351122216,
            "f1_weighted": 0.33798503511222155
          },
          {
            "accuracy": 0.3354,
            "f1": 0.33428918691916115,
            "f1_weighted": 0.33428918691916115
          },
          {
            "accuracy": 0.3192,
            "f1": 0.3190545129802701,
            "f1_weighted": 0.31905451298027016
          },
          {
            "accuracy": 0.3308,
            "f1": 0.3368736215189737,
            "f1_weighted": 0.3368736215189737
          },
          {
            "accuracy": 0.3586,
            "f1": 0.35333841351493633,
            "f1_weighted": 0.3533384135149363
          },
          {
            "accuracy": 0.3362,
            "f1": 0.33643656276680173,
            "f1_weighted": 0.33643656276680173
          },
          {
            "accuracy": 0.3114,
            "f1": 0.3124758488018207,
            "f1_weighted": 0.31247584880182067
          },
          {
            "accuracy": 0.3366,
            "f1": 0.34019166097364656,
            "f1_weighted": 0.34019166097364656
          },
          {
            "accuracy": 0.3182,
            "f1": 0.3170111116628357,
            "f1_weighted": 0.31701111166283574
          },
          {
            "accuracy": 0.3176,
            "f1": 0.32410809066143786,
            "f1_weighted": 0.3241080906614378
          }
        ]
      }
    ]
  },
  "task_name": "AmazonReviewsClassification"
}