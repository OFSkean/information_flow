{
  "dataset_revision": "1399c76144fd37290681b995c656ef9b2e06e26d",
  "evaluation_time": 30.035194396972656,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.20",
  "scores": {
    "test": [
      {
        "accuracy": 0.3476,
        "f1": 0.3469642716115918,
        "f1_weighted": 0.3469642716115918,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.3476,
        "scores_per_experiment": [
          {
            "accuracy": 0.3564,
            "f1": 0.3576964766915349,
            "f1_weighted": 0.3576964766915349
          },
          {
            "accuracy": 0.3626,
            "f1": 0.36559884493378314,
            "f1_weighted": 0.3655988449337832
          },
          {
            "accuracy": 0.3246,
            "f1": 0.32700864876330166,
            "f1_weighted": 0.32700864876330166
          },
          {
            "accuracy": 0.3516,
            "f1": 0.35120088585238973,
            "f1_weighted": 0.35120088585238973
          },
          {
            "accuracy": 0.397,
            "f1": 0.39130005493214404,
            "f1_weighted": 0.39130005493214404
          },
          {
            "accuracy": 0.2968,
            "f1": 0.2943323482960345,
            "f1_weighted": 0.29433234829603455
          },
          {
            "accuracy": 0.3112,
            "f1": 0.30976576491690033,
            "f1_weighted": 0.3097657649169003
          },
          {
            "accuracy": 0.3726,
            "f1": 0.3650420832015334,
            "f1_weighted": 0.3650420832015334
          },
          {
            "accuracy": 0.3514,
            "f1": 0.3554626533275598,
            "f1_weighted": 0.3554626533275598
          },
          {
            "accuracy": 0.3518,
            "f1": 0.3522349552007368,
            "f1_weighted": 0.3522349552007368
          }
        ]
      }
    ]
  },
  "task_name": "AmazonReviewsClassification"
}