{
  "dataset_revision": "1399c76144fd37290681b995c656ef9b2e06e26d",
  "evaluation_time": 42.58127164840698,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.20",
  "scores": {
    "test": [
      {
        "accuracy": 0.36866,
        "f1": 0.3661568462450831,
        "f1_weighted": 0.366156846245083,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.36866,
        "scores_per_experiment": [
          {
            "accuracy": 0.3768,
            "f1": 0.37772087178978264,
            "f1_weighted": 0.37772087178978264
          },
          {
            "accuracy": 0.3788,
            "f1": 0.3769296841933688,
            "f1_weighted": 0.3769296841933688
          },
          {
            "accuracy": 0.3554,
            "f1": 0.34576020746072544,
            "f1_weighted": 0.3457602074607254
          },
          {
            "accuracy": 0.3794,
            "f1": 0.3834847678694931,
            "f1_weighted": 0.38348476786949304
          },
          {
            "accuracy": 0.4068,
            "f1": 0.3916135302657338,
            "f1_weighted": 0.3916135302657338
          },
          {
            "accuracy": 0.3502,
            "f1": 0.34893743711698355,
            "f1_weighted": 0.34893743711698355
          },
          {
            "accuracy": 0.3448,
            "f1": 0.34429137194138004,
            "f1_weighted": 0.34429137194138004
          },
          {
            "accuracy": 0.3742,
            "f1": 0.3718420292300422,
            "f1_weighted": 0.3718420292300422
          },
          {
            "accuracy": 0.3556,
            "f1": 0.35698380554035763,
            "f1_weighted": 0.35698380554035763
          },
          {
            "accuracy": 0.3646,
            "f1": 0.3640047570429636,
            "f1_weighted": 0.3640047570429636
          }
        ]
      }
    ]
  },
  "task_name": "AmazonReviewsClassification"
}