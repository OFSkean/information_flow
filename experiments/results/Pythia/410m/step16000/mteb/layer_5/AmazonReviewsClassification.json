{
  "dataset_revision": "1399c76144fd37290681b995c656ef9b2e06e26d",
  "evaluation_time": 26.773329973220825,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.20",
  "scores": {
    "test": [
      {
        "accuracy": 0.33366000000000007,
        "f1": 0.33257760263771197,
        "f1_weighted": 0.33257760263771197,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.33366000000000007,
        "scores_per_experiment": [
          {
            "accuracy": 0.3504,
            "f1": 0.35110217532834626,
            "f1_weighted": 0.35110217532834626
          },
          {
            "accuracy": 0.3326,
            "f1": 0.3365984093545176,
            "f1_weighted": 0.33659840935451757
          },
          {
            "accuracy": 0.3052,
            "f1": 0.30573919537239375,
            "f1_weighted": 0.30573919537239375
          },
          {
            "accuracy": 0.3356,
            "f1": 0.3336280400504494,
            "f1_weighted": 0.3336280400504494
          },
          {
            "accuracy": 0.3828,
            "f1": 0.37502883512203705,
            "f1_weighted": 0.37502883512203705
          },
          {
            "accuracy": 0.298,
            "f1": 0.29853254008186186,
            "f1_weighted": 0.29853254008186186
          },
          {
            "accuracy": 0.2988,
            "f1": 0.2968103629838859,
            "f1_weighted": 0.29681036298388586
          },
          {
            "accuracy": 0.3618,
            "f1": 0.354949373208777,
            "f1_weighted": 0.35494937320877695
          },
          {
            "accuracy": 0.3388,
            "f1": 0.34030521876833963,
            "f1_weighted": 0.34030521876833963
          },
          {
            "accuracy": 0.3326,
            "f1": 0.3330818761065114,
            "f1_weighted": 0.3330818761065113
          }
        ]
      }
    ]
  },
  "task_name": "AmazonReviewsClassification"
}