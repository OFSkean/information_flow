{
  "dataset_revision": "1399c76144fd37290681b995c656ef9b2e06e26d",
  "evaluation_time": 66.59434461593628,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.20",
  "scores": {
    "test": [
      {
        "accuracy": 0.33284,
        "f1": 0.33369602252060404,
        "f1_weighted": 0.3336960225206041,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.33284,
        "scores_per_experiment": [
          {
            "accuracy": 0.338,
            "f1": 0.33871914735514314,
            "f1_weighted": 0.3387191473551432
          },
          {
            "accuracy": 0.3406,
            "f1": 0.33774712168119203,
            "f1_weighted": 0.33774712168119203
          },
          {
            "accuracy": 0.3258,
            "f1": 0.32482689467662174,
            "f1_weighted": 0.32482689467662174
          },
          {
            "accuracy": 0.3364,
            "f1": 0.34282115422394827,
            "f1_weighted": 0.34282115422394827
          },
          {
            "accuracy": 0.359,
            "f1": 0.3537807600538797,
            "f1_weighted": 0.35378076005387976
          },
          {
            "accuracy": 0.3286,
            "f1": 0.3311641167549084,
            "f1_weighted": 0.3311641167549084
          },
          {
            "accuracy": 0.3152,
            "f1": 0.31503195750447727,
            "f1_weighted": 0.3150319575044773
          },
          {
            "accuracy": 0.3446,
            "f1": 0.34736483299860843,
            "f1_weighted": 0.34736483299860854
          },
          {
            "accuracy": 0.3214,
            "f1": 0.3209789582135922,
            "f1_weighted": 0.3209789582135922
          },
          {
            "accuracy": 0.3188,
            "f1": 0.3245252817436694,
            "f1_weighted": 0.32452528174366946
          }
        ]
      }
    ]
  },
  "task_name": "AmazonReviewsClassification"
}