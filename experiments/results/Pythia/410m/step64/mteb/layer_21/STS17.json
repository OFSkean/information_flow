{
  "dataset_revision": "faeb762787bd10488a50c8b5be4a3b82e411949c",
  "evaluation_time": 19.922069549560547,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.20",
  "scores": {
    "test": [
      {
        "cosine_pearson": 0.18247072107517234,
        "cosine_spearman": 0.24311198312324925,
        "euclidean_pearson": 0.23631273466313274,
        "euclidean_spearman": 0.2394559875259852,
        "hf_subset": "en-en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.24311198312324925,
        "manhattan_pearson": 0.23682257373504337,
        "manhattan_spearman": 0.2404189008864254,
        "pearson": 0.18247072107517234,
        "spearman": 0.24311198312324925
      },
      {
        "cosine_pearson": 0.013013742197106203,
        "cosine_spearman": 0.026848561506308734,
        "euclidean_pearson": 0.016239117263053413,
        "euclidean_spearman": 0.03538754649186471,
        "hf_subset": "en-de",
        "languages": [
          "eng-Latn",
          "deu-Latn"
        ],
        "main_score": 0.026848561506308734,
        "manhattan_pearson": 0.016767928286028282,
        "manhattan_spearman": 0.03738871493675551,
        "pearson": 0.013013742197106203,
        "spearman": 0.026848561506308734
      },
      {
        "cosine_pearson": 0.032226268085484774,
        "cosine_spearman": -0.005837446024608488,
        "euclidean_pearson": 0.028284563267444558,
        "euclidean_spearman": 5.650629300786564e-05,
        "hf_subset": "it-en",
        "languages": [
          "ita-Latn",
          "eng-Latn"
        ],
        "main_score": -0.005837446024608488,
        "manhattan_pearson": 0.03204015205382217,
        "manhattan_spearman": 0.006290265161773561,
        "pearson": 0.032226268085484774,
        "spearman": -0.005837446024608488
      },
      {
        "cosine_pearson": -0.011866578714286664,
        "cosine_spearman": -0.03469157138058484,
        "euclidean_pearson": -0.04073967354816624,
        "euclidean_spearman": -0.057126212895788835,
        "hf_subset": "en-tr",
        "languages": [
          "eng-Latn",
          "tur-Latn"
        ],
        "main_score": -0.03469157138058484,
        "manhattan_pearson": -0.03738871609910746,
        "manhattan_spearman": -0.05313194550647134,
        "pearson": -0.011866578714286664,
        "spearman": -0.03469157138058484
      },
      {
        "cosine_pearson": -0.12180377782056839,
        "cosine_spearman": -0.13611608485368318,
        "euclidean_pearson": -0.11409974869896539,
        "euclidean_spearman": -0.12439092072547915,
        "hf_subset": "es-en",
        "languages": [
          "spa-Latn",
          "eng-Latn"
        ],
        "main_score": -0.13611608485368318,
        "manhattan_pearson": -0.10379956391893902,
        "manhattan_spearman": -0.11306402774602325,
        "pearson": -0.12180377782056839,
        "spearman": -0.13611608485368318
      },
      {
        "cosine_pearson": 0.03432345042939322,
        "cosine_spearman": -0.016135814024429773,
        "euclidean_pearson": 0.022531706964411126,
        "euclidean_spearman": -0.014843860066337346,
        "hf_subset": "nl-en",
        "languages": [
          "nld-Latn",
          "eng-Latn"
        ],
        "main_score": -0.016135814024429773,
        "manhattan_pearson": 0.01946819367736245,
        "manhattan_spearman": -0.015100634112387715,
        "pearson": 0.03432345042939322,
        "spearman": -0.016135814024429773
      },
      {
        "cosine_pearson": 0.024277227623347074,
        "cosine_spearman": 0.06477121762751052,
        "euclidean_pearson": 0.05137520027015495,
        "euclidean_spearman": 0.0835398791816994,
        "hf_subset": "en-ar",
        "languages": [
          "eng-Latn",
          "ara-Arab"
        ],
        "main_score": 0.06477121762751052,
        "manhattan_pearson": 0.048738421623016226,
        "manhattan_spearman": 0.07542598682750658,
        "pearson": 0.024277227623347074,
        "spearman": 0.06477121762751052
      },
      {
        "cosine_pearson": 0.03990669949378524,
        "cosine_spearman": 0.03243461218651488,
        "euclidean_pearson": 0.04473471815804589,
        "euclidean_spearman": 0.04291595294464735,
        "hf_subset": "fr-en",
        "languages": [
          "fra-Latn",
          "eng-Latn"
        ],
        "main_score": 0.03243461218651488,
        "manhattan_pearson": 0.041245045270596616,
        "manhattan_spearman": 0.03980387846715292,
        "pearson": 0.03990669949378524,
        "spearman": 0.03243461218651488
      }
    ]
  },
  "task_name": "STS17"
}