{
  "dataset_revision": "d80d48c1eb48d3562165c59d59d0034df9fff0bf",
  "evaluation_time": 51.04131293296814,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.8176014591883265,
        "f1": 0.8129892233569451,
        "f1_weighted": 0.8180328373635865,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.8176014591883265,
        "scores_per_experiment": [
          {
            "accuracy": 0.781577747378021,
            "f1": 0.7774301913120922,
            "f1_weighted": 0.777804756373621
          },
          {
            "accuracy": 0.8267213862289101,
            "f1": 0.8226338203217002,
            "f1_weighted": 0.8292327388692551
          },
          {
            "accuracy": 0.8182854537163703,
            "f1": 0.8097868364430956,
            "f1_weighted": 0.8197534083976479
          },
          {
            "accuracy": 0.8267213862289101,
            "f1": 0.8206043633914116,
            "f1_weighted": 0.8274884523646198
          },
          {
            "accuracy": 0.8483812129502964,
            "f1": 0.8439667774154253,
            "f1_weighted": 0.8493271587898262
          },
          {
            "accuracy": 0.8331053351573188,
            "f1": 0.830124636881662,
            "f1_weighted": 0.833528258811013
          },
          {
            "accuracy": 0.7957136342909257,
            "f1": 0.79205539235024,
            "f1_weighted": 0.7930633613821987
          },
          {
            "accuracy": 0.8091655266757866,
            "f1": 0.8042804813855327,
            "f1_weighted": 0.8117320617276299
          },
          {
            "accuracy": 0.8189694482444141,
            "f1": 0.8181672297606639,
            "f1_weighted": 0.8198581378835181
          },
          {
            "accuracy": 0.8173734610123119,
            "f1": 0.8108425043076294,
            "f1_weighted": 0.818540039036535
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.8141834451901566,
        "f1": 0.8137843849488308,
        "f1_weighted": 0.8145568246792712,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.8141834451901566,
        "scores_per_experiment": [
          {
            "accuracy": 0.7736017897091723,
            "f1": 0.777663060727254,
            "f1_weighted": 0.7700640259618141
          },
          {
            "accuracy": 0.8228187919463087,
            "f1": 0.8240257692760352,
            "f1_weighted": 0.8235458912417765
          },
          {
            "accuracy": 0.8067114093959732,
            "f1": 0.7994920702316913,
            "f1_weighted": 0.8084429656177036
          },
          {
            "accuracy": 0.8375838926174497,
            "f1": 0.8393284908507368,
            "f1_weighted": 0.8394577305932122
          },
          {
            "accuracy": 0.8407158836689038,
            "f1": 0.8431901298917737,
            "f1_weighted": 0.8421726775497449
          },
          {
            "accuracy": 0.8277404921700223,
            "f1": 0.8273337084250197,
            "f1_weighted": 0.8279584139837852
          },
          {
            "accuracy": 0.789261744966443,
            "f1": 0.7863474419117289,
            "f1_weighted": 0.7878616635242252
          },
          {
            "accuracy": 0.8089485458612975,
            "f1": 0.8087596888026858,
            "f1_weighted": 0.810628312331193
          },
          {
            "accuracy": 0.821923937360179,
            "f1": 0.8211621217098503,
            "f1_weighted": 0.8223541769806798
          },
          {
            "accuracy": 0.8125279642058165,
            "f1": 0.8105413676615314,
            "f1_weighted": 0.8130823890085771
          }
        ]
      }
    ]
  },
  "task_name": "MTOPDomainClassification"
}