{
  "dataset_revision": "4672e20407010da34463acc759c162ca9734bca6",
  "evaluation_time": 87.4165906906128,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.5870208473436449,
        "f1": 0.5527657077691076,
        "f1_weighted": 0.5913075776265955,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5870208473436449,
        "scores_per_experiment": [
          {
            "accuracy": 0.5928043039677202,
            "f1": 0.5708967189347075,
            "f1_weighted": 0.5973931514881986
          },
          {
            "accuracy": 0.5921318090114324,
            "f1": 0.5588969661668713,
            "f1_weighted": 0.594489930507074
          },
          {
            "accuracy": 0.5864156018829859,
            "f1": 0.5539600171830136,
            "f1_weighted": 0.5875957285243472
          },
          {
            "accuracy": 0.6186953597848016,
            "f1": 0.5721732572375186,
            "f1_weighted": 0.6232098323059434
          },
          {
            "accuracy": 0.5944855413584398,
            "f1": 0.5502270329141172,
            "f1_weighted": 0.5961125535059989
          },
          {
            "accuracy": 0.5601882985877606,
            "f1": 0.5387033873472241,
            "f1_weighted": 0.5673452195486359
          },
          {
            "accuracy": 0.5749831876260928,
            "f1": 0.5453598055679684,
            "f1_weighted": 0.5765017808608593
          },
          {
            "accuracy": 0.5840618695359785,
            "f1": 0.5398145517806038,
            "f1_weighted": 0.5908553162433068
          },
          {
            "accuracy": 0.5696032279757902,
            "f1": 0.5394936503599975,
            "f1_weighted": 0.5783637526561277
          },
          {
            "accuracy": 0.5968392737054472,
            "f1": 0.5581316901990545,
            "f1_weighted": 0.6012085106254634
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.5970978848991638,
        "f1": 0.5629986222259917,
        "f1_weighted": 0.5996186329184787,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5970978848991638,
        "scores_per_experiment": [
          {
            "accuracy": 0.5922282341367437,
            "f1": 0.5672658482357976,
            "f1_weighted": 0.5959737091315065
          },
          {
            "accuracy": 0.5996064928676832,
            "f1": 0.5552419950841633,
            "f1_weighted": 0.6056812237410536
          },
          {
            "accuracy": 0.617314313821938,
            "f1": 0.578590182820799,
            "f1_weighted": 0.615034033382486
          },
          {
            "accuracy": 0.6187899655681259,
            "f1": 0.5700110436411947,
            "f1_weighted": 0.6198380332892871
          },
          {
            "accuracy": 0.6045253320216429,
            "f1": 0.5654156986425918,
            "f1_weighted": 0.6023720034083212
          },
          {
            "accuracy": 0.5838662075750123,
            "f1": 0.5670128043927479,
            "f1_weighted": 0.5869423952475288
          },
          {
            "accuracy": 0.5784554845056566,
            "f1": 0.5515133685767463,
            "f1_weighted": 0.5797073865952977
          },
          {
            "accuracy": 0.5794392523364486,
            "f1": 0.5432239996123671,
            "f1_weighted": 0.5844764573459333
          },
          {
            "accuracy": 0.5686178061977374,
            "f1": 0.5413790865098353,
            "f1_weighted": 0.5771627127721842
          },
          {
            "accuracy": 0.6281357599606493,
            "f1": 0.5903321947436745,
            "f1_weighted": 0.6289983742711885
          }
        ]
      }
    ]
  },
  "task_name": "MassiveIntentClassification"
}