{
  "dataset_revision": "ae001d0e6b1228650b7bd1c2c65fb50ad11a8aba",
  "evaluation_time": 109.46130609512329,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.7066575467396261,
        "f1": 0.48533099885846287,
        "f1_weighted": 0.7434034515669834,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.7066575467396261,
        "scores_per_experiment": [
          {
            "accuracy": 0.6979024167806658,
            "f1": 0.4707166877573486,
            "f1_weighted": 0.7353862859792581
          },
          {
            "accuracy": 0.6974464204286366,
            "f1": 0.47626264871022767,
            "f1_weighted": 0.7369383253841756
          },
          {
            "accuracy": 0.7033743730050159,
            "f1": 0.4864867446087065,
            "f1_weighted": 0.7410270382316946
          },
          {
            "accuracy": 0.6999544003647971,
            "f1": 0.49209103738570653,
            "f1_weighted": 0.7385569427972473
          },
          {
            "accuracy": 0.70953032375741,
            "f1": 0.4831126995658418,
            "f1_weighted": 0.7461397451996092
          },
          {
            "accuracy": 0.7250341997264022,
            "f1": 0.4991845505892276,
            "f1_weighted": 0.7613020686112344
          },
          {
            "accuracy": 0.6919744642042863,
            "f1": 0.48702439131267256,
            "f1_weighted": 0.7300277587319797
          },
          {
            "accuracy": 0.730734154126767,
            "f1": 0.5038713329283265,
            "f1_weighted": 0.764983291057974
          },
          {
            "accuracy": 0.7056543547651619,
            "f1": 0.48399926392574766,
            "f1_weighted": 0.7390222107821615
          },
          {
            "accuracy": 0.7049703602371181,
            "f1": 0.4705606318008232,
            "f1_weighted": 0.7406508488945004
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.7086800894854586,
        "f1": 0.47301037347532376,
        "f1_weighted": 0.7474922526026176,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.7086800894854586,
        "scores_per_experiment": [
          {
            "accuracy": 0.694407158836689,
            "f1": 0.46124019071313105,
            "f1_weighted": 0.7354180162857663
          },
          {
            "accuracy": 0.7087248322147651,
            "f1": 0.4574487416136166,
            "f1_weighted": 0.7484041888336537
          },
          {
            "accuracy": 0.7073825503355705,
            "f1": 0.4800217067081417,
            "f1_weighted": 0.7478767037807997
          },
          {
            "accuracy": 0.7015659955257271,
            "f1": 0.4719711850601231,
            "f1_weighted": 0.7404922847919475
          },
          {
            "accuracy": 0.7096196868008948,
            "f1": 0.476078563697504,
            "f1_weighted": 0.7463683272781934
          },
          {
            "accuracy": 0.7243847874720358,
            "f1": 0.4842799322744475,
            "f1_weighted": 0.7649314621980579
          },
          {
            "accuracy": 0.694407158836689,
            "f1": 0.4791495470495291,
            "f1_weighted": 0.7351381170166873
          },
          {
            "accuracy": 0.7239373601789709,
            "f1": 0.4812115019666479,
            "f1_weighted": 0.761514504181726
          },
          {
            "accuracy": 0.7131991051454138,
            "f1": 0.4685398937742084,
            "f1_weighted": 0.7467142162459142
          },
          {
            "accuracy": 0.70917225950783,
            "f1": 0.4701624718958883,
            "f1_weighted": 0.7480647054134301
          }
        ]
      }
    ]
  },
  "task_name": "MTOPIntentClassification"
}