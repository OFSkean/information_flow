{
  "dataset_revision": "fad2c6e8459f9e1c45d9315f4953d921437d70f8",
  "evaluation_time": 65.99579119682312,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.6179892400806993,
        "f1": 0.6030143486480137,
        "f1_weighted": 0.6194129619301304,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.6179892400806993,
        "scores_per_experiment": [
          {
            "accuracy": 0.6371889710827169,
            "f1": 0.6284069182494636,
            "f1_weighted": 0.6397397523655555
          },
          {
            "accuracy": 0.6344989912575656,
            "f1": 0.6196752962263389,
            "f1_weighted": 0.6362853173956741
          },
          {
            "accuracy": 0.6139878950907868,
            "f1": 0.6104676862684999,
            "f1_weighted": 0.6176611034715621
          },
          {
            "accuracy": 0.6123066577000672,
            "f1": 0.5904053135435752,
            "f1_weighted": 0.6133195597453444
          },
          {
            "accuracy": 0.6429051782111634,
            "f1": 0.6141785642551453,
            "f1_weighted": 0.6415274245277978
          },
          {
            "accuracy": 0.6049092131809012,
            "f1": 0.5873039546216634,
            "f1_weighted": 0.6039139061056602
          },
          {
            "accuracy": 0.6089441829186281,
            "f1": 0.594566833333673,
            "f1_weighted": 0.6107397410806249
          },
          {
            "accuracy": 0.6277740416946873,
            "f1": 0.6088803024312304,
            "f1_weighted": 0.6298449031125202
          },
          {
            "accuracy": 0.6156691324815063,
            "f1": 0.5994125343474127,
            "f1_weighted": 0.6173280171629685
          },
          {
            "accuracy": 0.5817081371889711,
            "f1": 0.5768460832031338,
            "f1_weighted": 0.583769894333596
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.6048204623708805,
        "f1": 0.5958593927647561,
        "f1_weighted": 0.6057449975686684,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.6048204623708805,
        "scores_per_experiment": [
          {
            "accuracy": 0.6163305459911461,
            "f1": 0.6104136833111395,
            "f1_weighted": 0.61934015995335
          },
          {
            "accuracy": 0.6414166256763404,
            "f1": 0.6377521070163199,
            "f1_weighted": 0.6427781264613313
          },
          {
            "accuracy": 0.602557796360059,
            "f1": 0.6023556673001005,
            "f1_weighted": 0.6031034704970241
          },
          {
            "accuracy": 0.5794392523364486,
            "f1": 0.5617441918818287,
            "f1_weighted": 0.5808460800410817
          },
          {
            "accuracy": 0.6305951795376291,
            "f1": 0.6090341500698844,
            "f1_weighted": 0.6262673737665896
          },
          {
            "accuracy": 0.5922282341367437,
            "f1": 0.5908161376791764,
            "f1_weighted": 0.5871557608041246
          },
          {
            "accuracy": 0.5956714215445155,
            "f1": 0.5904665915350055,
            "f1_weighted": 0.6006783969176077
          },
          {
            "accuracy": 0.6158386620757501,
            "f1": 0.6019842064508559,
            "f1_weighted": 0.6194535508083509
          },
          {
            "accuracy": 0.5927201180521396,
            "f1": 0.5760489571907509,
            "f1_weighted": 0.5965254495964514
          },
          {
            "accuracy": 0.5814067879980325,
            "f1": 0.577978235212499,
            "f1_weighted": 0.5813016068407734
          }
        ]
      }
    ]
  },
  "task_name": "MassiveScenarioClassification"
}