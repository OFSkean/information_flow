{
  "dataset_revision": "ae001d0e6b1228650b7bd1c2c65fb50ad11a8aba",
  "evaluation_time": 73.02566576004028,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.6509119927040583,
        "f1": 0.46310303203369535,
        "f1_weighted": 0.6940830065388323,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.6509119927040583,
        "scores_per_experiment": [
          {
            "accuracy": 0.6395348837209303,
            "f1": 0.4442737115229027,
            "f1_weighted": 0.6793971317354656
          },
          {
            "accuracy": 0.6481988144094847,
            "f1": 0.4647946388338074,
            "f1_weighted": 0.6888362448920639
          },
          {
            "accuracy": 0.6488828089375285,
            "f1": 0.4641691627063916,
            "f1_weighted": 0.6943985112220101
          },
          {
            "accuracy": 0.6488828089375285,
            "f1": 0.45907909249891304,
            "f1_weighted": 0.691998113284975
          },
          {
            "accuracy": 0.6568627450980392,
            "f1": 0.47042182218106593,
            "f1_weighted": 0.7029108094082982
          },
          {
            "accuracy": 0.640218878248974,
            "f1": 0.46062615426138304,
            "f1_weighted": 0.6887607640319746
          },
          {
            "accuracy": 0.6513907888736891,
            "f1": 0.461627583200943,
            "f1_weighted": 0.6945950684364032
          },
          {
            "accuracy": 0.6764705882352942,
            "f1": 0.48442035043111914,
            "f1_weighted": 0.7140650640511749
          },
          {
            "accuracy": 0.6383948928408573,
            "f1": 0.4555003832003196,
            "f1_weighted": 0.6829125810746736
          },
          {
            "accuracy": 0.6602827177382581,
            "f1": 0.46611742150010865,
            "f1_weighted": 0.702955777251284
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.6528859060402684,
        "f1": 0.4514408538867845,
        "f1_weighted": 0.6961651668316888,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.6528859060402684,
        "scores_per_experiment": [
          {
            "accuracy": 0.6366890380313199,
            "f1": 0.4423225375431921,
            "f1_weighted": 0.6755401885347794
          },
          {
            "accuracy": 0.6586129753914989,
            "f1": 0.45670808543517305,
            "f1_weighted": 0.6972713384773278
          },
          {
            "accuracy": 0.6429530201342282,
            "f1": 0.45484273642710643,
            "f1_weighted": 0.6864993694738751
          },
          {
            "accuracy": 0.6411633109619687,
            "f1": 0.4438635049001527,
            "f1_weighted": 0.6827731075448537
          },
          {
            "accuracy": 0.6711409395973155,
            "f1": 0.45885835689464294,
            "f1_weighted": 0.7173173390618801
          },
          {
            "accuracy": 0.6465324384787472,
            "f1": 0.44791653548613375,
            "f1_weighted": 0.6928567857471128
          },
          {
            "accuracy": 0.6563758389261745,
            "f1": 0.4526978725549293,
            "f1_weighted": 0.7017481264210645
          },
          {
            "accuracy": 0.6711409395973155,
            "f1": 0.46212615022680975,
            "f1_weighted": 0.7120325641678057
          },
          {
            "accuracy": 0.6375838926174496,
            "f1": 0.44296246654272725,
            "f1_weighted": 0.6853122189004215
          },
          {
            "accuracy": 0.6666666666666666,
            "f1": 0.452110292856977,
            "f1_weighted": 0.710300629987768
          }
        ]
      }
    ]
  },
  "task_name": "MTOPIntentClassification"
}