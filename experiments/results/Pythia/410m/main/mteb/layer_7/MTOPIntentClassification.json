{
  "dataset_revision": "ae001d0e6b1228650b7bd1c2c65fb50ad11a8aba",
  "evaluation_time": 105.18672347068787,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.7096443228454172,
        "f1": 0.4911309878631334,
        "f1_weighted": 0.7460768353825774,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.7096443228454172,
        "scores_per_experiment": [
          {
            "accuracy": 0.7049703602371181,
            "f1": 0.4929762726610278,
            "f1_weighted": 0.7401356744465388
          },
          {
            "accuracy": 0.7077063383492932,
            "f1": 0.47590005929532964,
            "f1_weighted": 0.7449355877731118
          },
          {
            "accuracy": 0.6988144094847242,
            "f1": 0.4868433867588353,
            "f1_weighted": 0.7375455444025206
          },
          {
            "accuracy": 0.7111263109895121,
            "f1": 0.49682584514069006,
            "f1_weighted": 0.7487074853330966
          },
          {
            "accuracy": 0.7104423164614683,
            "f1": 0.47856702120275574,
            "f1_weighted": 0.7475893347232128
          },
          {
            "accuracy": 0.7161422708618331,
            "f1": 0.49356412014400924,
            "f1_weighted": 0.7531358026181229
          },
          {
            "accuracy": 0.6965344277245782,
            "f1": 0.49227866413628424,
            "f1_weighted": 0.7339214031842956
          },
          {
            "accuracy": 0.7327861377108983,
            "f1": 0.5121016822918883,
            "f1_weighted": 0.7671517210512772
          },
          {
            "accuracy": 0.7031463748290013,
            "f1": 0.4934661925849394,
            "f1_weighted": 0.7387835757405028
          },
          {
            "accuracy": 0.7147742818057455,
            "f1": 0.48878663441557413,
            "f1_weighted": 0.7488622245530941
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.7108724832214766,
        "f1": 0.48077682589100446,
        "f1_weighted": 0.749628486169087,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.7108724832214766,
        "scores_per_experiment": [
          {
            "accuracy": 0.6957494407158836,
            "f1": 0.46723738922919894,
            "f1_weighted": 0.73476374479829
          },
          {
            "accuracy": 0.70917225950783,
            "f1": 0.4762569903312819,
            "f1_weighted": 0.747871348010986
          },
          {
            "accuracy": 0.7136465324384788,
            "f1": 0.47357508623278594,
            "f1_weighted": 0.7537793870133399
          },
          {
            "accuracy": 0.7033557046979866,
            "f1": 0.4819079326303533,
            "f1_weighted": 0.7433660665521692
          },
          {
            "accuracy": 0.7149888143176734,
            "f1": 0.49034284360992875,
            "f1_weighted": 0.7535120905680147
          },
          {
            "accuracy": 0.7172259507829978,
            "f1": 0.48752162404357485,
            "f1_weighted": 0.7583697249946332
          },
          {
            "accuracy": 0.6966442953020134,
            "f1": 0.48037453638661193,
            "f1_weighted": 0.7358337928106284
          },
          {
            "accuracy": 0.7360178970917226,
            "f1": 0.4985954993011047,
            "f1_weighted": 0.7720324573181564
          },
          {
            "accuracy": 0.7038031319910515,
            "f1": 0.4652466613303164,
            "f1_weighted": 0.7404191754661448
          },
          {
            "accuracy": 0.7181208053691275,
            "f1": 0.48670969581488793,
            "f1_weighted": 0.7563370741585068
          }
        ]
      }
    ]
  },
  "task_name": "MTOPIntentClassification"
}