{
  "dataset_revision": "4672e20407010da34463acc759c162ca9734bca6",
  "evaluation_time": 85.43392157554626,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.5903833221250842,
        "f1": 0.5585524808430373,
        "f1_weighted": 0.5941537175798947,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5903833221250842,
        "scores_per_experiment": [
          {
            "accuracy": 0.5917955615332885,
            "f1": 0.5661041642371037,
            "f1_weighted": 0.5953671028650249
          },
          {
            "accuracy": 0.5914593140551446,
            "f1": 0.5629585922354664,
            "f1_weighted": 0.5959896301080922
          },
          {
            "accuracy": 0.5948217888365838,
            "f1": 0.5623495601329984,
            "f1_weighted": 0.5954068527935156
          },
          {
            "accuracy": 0.601546738399462,
            "f1": 0.5543297476861808,
            "f1_weighted": 0.6057894190953355
          },
          {
            "accuracy": 0.6028917283120376,
            "f1": 0.5618045262746584,
            "f1_weighted": 0.6029641571897583
          },
          {
            "accuracy": 0.570275722932078,
            "f1": 0.5516421955057532,
            "f1_weighted": 0.5732936096827245
          },
          {
            "accuracy": 0.5830531271015468,
            "f1": 0.5574905941479658,
            "f1_weighted": 0.5849878320540605
          },
          {
            "accuracy": 0.5917955615332885,
            "f1": 0.5509755984572512,
            "f1_weighted": 0.5987359550971771
          },
          {
            "accuracy": 0.5766644250168124,
            "f1": 0.5533948485665141,
            "f1_weighted": 0.5850158797786865
          },
          {
            "accuracy": 0.5995292535305985,
            "f1": 0.5644749811864799,
            "f1_weighted": 0.6039867371345713
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.6013772749631088,
        "f1": 0.5679415106112657,
        "f1_weighted": 0.6042837312319079,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.6013772749631088,
        "scores_per_experiment": [
          {
            "accuracy": 0.6010821446138711,
            "f1": 0.5758780848027635,
            "f1_weighted": 0.604723587747251
          },
          {
            "accuracy": 0.6060009837678307,
            "f1": 0.5645893189912489,
            "f1_weighted": 0.6107260195268533
          },
          {
            "accuracy": 0.6197737333989178,
            "f1": 0.5824477800841329,
            "f1_weighted": 0.6189537416278303
          },
          {
            "accuracy": 0.6123954746679784,
            "f1": 0.5674685460653935,
            "f1_weighted": 0.6128080255848829
          },
          {
            "accuracy": 0.6187899655681259,
            "f1": 0.5787600948558631,
            "f1_weighted": 0.6171508316307722
          },
          {
            "accuracy": 0.5818986719134285,
            "f1": 0.5679410238509982,
            "f1_weighted": 0.5842160833103853
          },
          {
            "accuracy": 0.5774717166748647,
            "f1": 0.549737900688271,
            "f1_weighted": 0.5807258066778118
          },
          {
            "accuracy": 0.5863256271519921,
            "f1": 0.5505404535778022,
            "f1_weighted": 0.5937410660769941
          },
          {
            "accuracy": 0.573536645351697,
            "f1": 0.5489625189042285,
            "f1_weighted": 0.5804758871646216
          },
          {
            "accuracy": 0.6364977865223808,
            "f1": 0.5930893842919548,
            "f1_weighted": 0.6393162629716765
          }
        ]
      }
    ]
  },
  "task_name": "MassiveIntentClassification"
}