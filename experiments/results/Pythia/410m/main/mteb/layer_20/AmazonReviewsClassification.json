{
  "dataset_revision": "1399c76144fd37290681b995c656ef9b2e06e26d",
  "evaluation_time": 269.2716906070709,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.33788,
        "f1": 0.337370304889742,
        "f1_weighted": 0.33737030488974196,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.33788,
        "scores_per_experiment": [
          {
            "accuracy": 0.348,
            "f1": 0.3453293739866944,
            "f1_weighted": 0.34532937398669433
          },
          {
            "accuracy": 0.3608,
            "f1": 0.361377596991575,
            "f1_weighted": 0.361377596991575
          },
          {
            "accuracy": 0.3326,
            "f1": 0.32777275089972513,
            "f1_weighted": 0.3277727508997251
          },
          {
            "accuracy": 0.3412,
            "f1": 0.3434141059640264,
            "f1_weighted": 0.34341410596402633
          },
          {
            "accuracy": 0.3656,
            "f1": 0.3598284235409522,
            "f1_weighted": 0.3598284235409521
          },
          {
            "accuracy": 0.3198,
            "f1": 0.32022826820609346,
            "f1_weighted": 0.32022826820609346
          },
          {
            "accuracy": 0.3066,
            "f1": 0.3060845743750623,
            "f1_weighted": 0.3060845743750622
          },
          {
            "accuracy": 0.3398,
            "f1": 0.3392083356802118,
            "f1_weighted": 0.3392083356802118
          },
          {
            "accuracy": 0.3358,
            "f1": 0.33528215699116865,
            "f1_weighted": 0.3352821569911687
          },
          {
            "accuracy": 0.3286,
            "f1": 0.3351774622619108,
            "f1_weighted": 0.3351774622619108
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.34219999999999995,
        "f1": 0.34142642097786746,
        "f1_weighted": 0.34142642097786746,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.34219999999999995,
        "scores_per_experiment": [
          {
            "accuracy": 0.3378,
            "f1": 0.3379331804781097,
            "f1_weighted": 0.3379331804781097
          },
          {
            "accuracy": 0.3584,
            "f1": 0.3577329233606055,
            "f1_weighted": 0.3577329233606056
          },
          {
            "accuracy": 0.3468,
            "f1": 0.3418558238477117,
            "f1_weighted": 0.3418558238477117
          },
          {
            "accuracy": 0.327,
            "f1": 0.3285741756394785,
            "f1_weighted": 0.3285741756394785
          },
          {
            "accuracy": 0.374,
            "f1": 0.3650317626269469,
            "f1_weighted": 0.3650317626269469
          },
          {
            "accuracy": 0.3202,
            "f1": 0.3200437221497424,
            "f1_weighted": 0.32004372214974236
          },
          {
            "accuracy": 0.333,
            "f1": 0.33132091353591486,
            "f1_weighted": 0.33132091353591486
          },
          {
            "accuracy": 0.3306,
            "f1": 0.33139348676781555,
            "f1_weighted": 0.33139348676781555
          },
          {
            "accuracy": 0.3546,
            "f1": 0.3546542800673902,
            "f1_weighted": 0.35465428006739014
          },
          {
            "accuracy": 0.3396,
            "f1": 0.3457239413049595,
            "f1_weighted": 0.3457239413049595
          }
        ]
      }
    ]
  },
  "task_name": "AmazonReviewsClassification"
}