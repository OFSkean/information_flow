{
  "dataset_revision": "ae001d0e6b1228650b7bd1c2c65fb50ad11a8aba",
  "evaluation_time": 133.01294207572937,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.6795485636114911,
        "f1": 0.45355392837347336,
        "f1_weighted": 0.7191520597928863,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.6795485636114911,
        "scores_per_experiment": [
          {
            "accuracy": 0.6684906520747834,
            "f1": 0.44682820145491325,
            "f1_weighted": 0.7083754066896498
          },
          {
            "accuracy": 0.686046511627907,
            "f1": 0.4432000646410292,
            "f1_weighted": 0.7246534585816155
          },
          {
            "accuracy": 0.6963064295485636,
            "f1": 0.4651099416370542,
            "f1_weighted": 0.7371330015176757
          },
          {
            "accuracy": 0.6835385316917465,
            "f1": 0.4681026138764057,
            "f1_weighted": 0.7251017342917747
          },
          {
            "accuracy": 0.6933424532603739,
            "f1": 0.4666111017697723,
            "f1_weighted": 0.7258803698845057
          },
          {
            "accuracy": 0.6698586411308709,
            "f1": 0.4495009454011952,
            "f1_weighted": 0.7158733773845259
          },
          {
            "accuracy": 0.6666666666666666,
            "f1": 0.4561899464382679,
            "f1_weighted": 0.7118598409672828
          },
          {
            "accuracy": 0.6937984496124031,
            "f1": 0.46569444165018276,
            "f1_weighted": 0.7311561092251934
          },
          {
            "accuracy": 0.6694026447788418,
            "f1": 0.443759531282246,
            "f1_weighted": 0.70513936788076
          },
          {
            "accuracy": 0.6680346557227542,
            "f1": 0.4305424955836674,
            "f1_weighted": 0.7063479315058808
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.6789709172259508,
        "f1": 0.4353351875176784,
        "f1_weighted": 0.7211790597115655,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.6789709172259508,
        "scores_per_experiment": [
          {
            "accuracy": 0.6532438478747203,
            "f1": 0.40758291629170834,
            "f1_weighted": 0.6986325197540327
          },
          {
            "accuracy": 0.69082774049217,
            "f1": 0.4518806805093555,
            "f1_weighted": 0.7295150839770236
          },
          {
            "accuracy": 0.6948545861297539,
            "f1": 0.4315624160190551,
            "f1_weighted": 0.7376077244157124
          },
          {
            "accuracy": 0.6782997762863535,
            "f1": 0.45248835911779284,
            "f1_weighted": 0.7247069529538884
          },
          {
            "accuracy": 0.6814317673378076,
            "f1": 0.4280030501183592,
            "f1_weighted": 0.7170262538040291
          },
          {
            "accuracy": 0.6702460850111857,
            "f1": 0.431625684741209,
            "f1_weighted": 0.7198029009603311
          },
          {
            "accuracy": 0.6675615212527964,
            "f1": 0.4163363557732156,
            "f1_weighted": 0.7147322153750436
          },
          {
            "accuracy": 0.7015659955257271,
            "f1": 0.45373849380802367,
            "f1_weighted": 0.7419628980288004
          },
          {
            "accuracy": 0.6720357941834452,
            "f1": 0.4303298090334591,
            "f1_weighted": 0.708272508187683
          },
          {
            "accuracy": 0.6796420581655481,
            "f1": 0.44980410976460544,
            "f1_weighted": 0.7195315396591107
          }
        ]
      }
    ]
  },
  "task_name": "MTOPIntentClassification"
}