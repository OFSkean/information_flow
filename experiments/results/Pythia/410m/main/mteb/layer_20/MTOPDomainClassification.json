{
  "dataset_revision": "d80d48c1eb48d3562165c59d59d0034df9fff0bf",
  "evaluation_time": 55.80534029006958,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.8282717738258094,
        "f1": 0.8256165348813006,
        "f1_weighted": 0.8291003302306761,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.8282717738258094,
        "scores_per_experiment": [
          {
            "accuracy": 0.8023255813953488,
            "f1": 0.7999520851821831,
            "f1_weighted": 0.8006201557430392
          },
          {
            "accuracy": 0.8522571819425444,
            "f1": 0.8496606940752044,
            "f1_weighted": 0.8527528144644719
          },
          {
            "accuracy": 0.8267213862289101,
            "f1": 0.8215630833392793,
            "f1_weighted": 0.8286899605374682
          },
          {
            "accuracy": 0.8431372549019608,
            "f1": 0.8387950651604843,
            "f1_weighted": 0.8442761248628263
          },
          {
            "accuracy": 0.852485180118559,
            "f1": 0.8497219401213012,
            "f1_weighted": 0.854032502173459
          },
          {
            "accuracy": 0.8221614227086184,
            "f1": 0.8203088311521658,
            "f1_weighted": 0.8226184094578212
          },
          {
            "accuracy": 0.8146374829001368,
            "f1": 0.8131558964774762,
            "f1_weighted": 0.8120598166050838
          },
          {
            "accuracy": 0.8153214774281806,
            "f1": 0.8116421116616498,
            "f1_weighted": 0.8182861747022399
          },
          {
            "accuracy": 0.8349293205654355,
            "f1": 0.8330945870029304,
            "f1_weighted": 0.8369157277900906
          },
          {
            "accuracy": 0.8187414500683995,
            "f1": 0.818271054640332,
            "f1_weighted": 0.8207516159702609
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.8254138702460849,
        "f1": 0.826264088566564,
        "f1_weighted": 0.8260899823569829,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.8254138702460849,
        "scores_per_experiment": [
          {
            "accuracy": 0.785234899328859,
            "f1": 0.7890950414798396,
            "f1_weighted": 0.7835883127303068
          },
          {
            "accuracy": 0.8626398210290828,
            "f1": 0.8638256098331812,
            "f1_weighted": 0.8629929468798184
          },
          {
            "accuracy": 0.8259507829977628,
            "f1": 0.823594255628099,
            "f1_weighted": 0.8271668775191702
          },
          {
            "accuracy": 0.8541387024608501,
            "f1": 0.8536781694027472,
            "f1_weighted": 0.855107315355421
          },
          {
            "accuracy": 0.8416107382550335,
            "f1": 0.8464086853146949,
            "f1_weighted": 0.8435205045375591
          },
          {
            "accuracy": 0.8170022371364654,
            "f1": 0.8170313732594224,
            "f1_weighted": 0.8169802726256786
          },
          {
            "accuracy": 0.8080536912751678,
            "f1": 0.80801843649068,
            "f1_weighted": 0.8074367271601607
          },
          {
            "accuracy": 0.8116331096196868,
            "f1": 0.8127601005053624,
            "f1_weighted": 0.8139222649935118
          },
          {
            "accuracy": 0.8326621923937361,
            "f1": 0.831053857112185,
            "f1_weighted": 0.8336190534163685
          },
          {
            "accuracy": 0.8152125279642058,
            "f1": 0.8171753566394283,
            "f1_weighted": 0.8165655483518336
          }
        ]
      }
    ]
  },
  "task_name": "MTOPDomainClassification"
}