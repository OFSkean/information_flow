{
  "dataset_revision": "ae001d0e6b1228650b7bd1c2c65fb50ad11a8aba",
  "evaluation_time": 112.7515811920166,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.7039671682626539,
        "f1": 0.48632105104194984,
        "f1_weighted": 0.7403082883526672,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.7039671682626539,
        "scores_per_experiment": [
          {
            "accuracy": 0.6942544459644323,
            "f1": 0.4711235913314582,
            "f1_weighted": 0.7300444528747901
          },
          {
            "accuracy": 0.702234382124943,
            "f1": 0.48717408459672773,
            "f1_weighted": 0.7406651512558119
          },
          {
            "accuracy": 0.6963064295485636,
            "f1": 0.48401251317196653,
            "f1_weighted": 0.7347560693385636
          },
          {
            "accuracy": 0.7013223894208847,
            "f1": 0.5018188156023923,
            "f1_weighted": 0.7396121585604338
          },
          {
            "accuracy": 0.7063383492932056,
            "f1": 0.47400006401475064,
            "f1_weighted": 0.7424382368501429
          },
          {
            "accuracy": 0.7202462380300958,
            "f1": 0.4976955845949532,
            "f1_weighted": 0.7569062532319902
          },
          {
            "accuracy": 0.6917464660282717,
            "f1": 0.48518789806420776,
            "f1_weighted": 0.73037689886436
          },
          {
            "accuracy": 0.718422252621979,
            "f1": 0.49713811276584163,
            "f1_weighted": 0.7525061734190871
          },
          {
            "accuracy": 0.7065663474692202,
            "f1": 0.4814474835818185,
            "f1_weighted": 0.7389754276339353
          },
          {
            "accuracy": 0.702234382124943,
            "f1": 0.4836123626953817,
            "f1_weighted": 0.7368020614975572
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.7062192393736018,
        "f1": 0.4733499319529641,
        "f1_weighted": 0.744375623402103,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.7062192393736018,
        "scores_per_experiment": [
          {
            "accuracy": 0.6854586129753915,
            "f1": 0.4578301783006392,
            "f1_weighted": 0.7248711249833714
          },
          {
            "accuracy": 0.7127516778523489,
            "f1": 0.4675517060733948,
            "f1_weighted": 0.751876563034529
          },
          {
            "accuracy": 0.7109619686800895,
            "f1": 0.492196793505054,
            "f1_weighted": 0.7495321179347267
          },
          {
            "accuracy": 0.705592841163311,
            "f1": 0.48454162862772754,
            "f1_weighted": 0.74571577122026
          },
          {
            "accuracy": 0.7069351230425056,
            "f1": 0.47985233142881734,
            "f1_weighted": 0.7428178734611943
          },
          {
            "accuracy": 0.7145413870246085,
            "f1": 0.475105427255534,
            "f1_weighted": 0.7545639671105546
          },
          {
            "accuracy": 0.6876957494407159,
            "f1": 0.47129079468883894,
            "f1_weighted": 0.7275393930291868
          },
          {
            "accuracy": 0.7221476510067114,
            "f1": 0.4755101316039617,
            "f1_weighted": 0.7596488072580827
          },
          {
            "accuracy": 0.70917225950783,
            "f1": 0.4622179106061425,
            "f1_weighted": 0.7411852848388985
          },
          {
            "accuracy": 0.7069351230425056,
            "f1": 0.4674024174395307,
            "f1_weighted": 0.7460053311502247
          }
        ]
      }
    ]
  },
  "task_name": "MTOPIntentClassification"
}