{
  "dataset_revision": "1399c76144fd37290681b995c656ef9b2e06e26d",
  "evaluation_time": 165.36993169784546,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.36157999999999996,
        "f1": 0.359717138344899,
        "f1_weighted": 0.35971713834489905,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.36157999999999996,
        "scores_per_experiment": [
          {
            "accuracy": 0.3678,
            "f1": 0.37438124327824546,
            "f1_weighted": 0.37438124327824546
          },
          {
            "accuracy": 0.3832,
            "f1": 0.3834820766935139,
            "f1_weighted": 0.3834820766935139
          },
          {
            "accuracy": 0.351,
            "f1": 0.3457143038547619,
            "f1_weighted": 0.3457143038547619
          },
          {
            "accuracy": 0.3766,
            "f1": 0.3758410228496804,
            "f1_weighted": 0.3758410228496804
          },
          {
            "accuracy": 0.4154,
            "f1": 0.39345517536185526,
            "f1_weighted": 0.39345517536185526
          },
          {
            "accuracy": 0.3212,
            "f1": 0.3207764518299702,
            "f1_weighted": 0.3207764518299703
          },
          {
            "accuracy": 0.326,
            "f1": 0.32769644724867764,
            "f1_weighted": 0.32769644724867764
          },
          {
            "accuracy": 0.3706,
            "f1": 0.3694625360830403,
            "f1_weighted": 0.3694625360830403
          },
          {
            "accuracy": 0.348,
            "f1": 0.34982585333521066,
            "f1_weighted": 0.34982585333521077
          },
          {
            "accuracy": 0.356,
            "f1": 0.3565362729140342,
            "f1_weighted": 0.3565362729140342
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.36316000000000004,
        "f1": 0.3609840814971176,
        "f1_weighted": 0.3609840814971175,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.36316000000000004,
        "scores_per_experiment": [
          {
            "accuracy": 0.3616,
            "f1": 0.36829760476012774,
            "f1_weighted": 0.3682976047601278
          },
          {
            "accuracy": 0.3894,
            "f1": 0.39070616350724147,
            "f1_weighted": 0.39070616350724147
          },
          {
            "accuracy": 0.3524,
            "f1": 0.3470138363261891,
            "f1_weighted": 0.3470138363261891
          },
          {
            "accuracy": 0.3644,
            "f1": 0.36330500232457447,
            "f1_weighted": 0.36330500232457447
          },
          {
            "accuracy": 0.4142,
            "f1": 0.38967067879613426,
            "f1_weighted": 0.3896706787961342
          },
          {
            "accuracy": 0.3262,
            "f1": 0.3249283889041453,
            "f1_weighted": 0.32492838890414527
          },
          {
            "accuracy": 0.3334,
            "f1": 0.3343911569799536,
            "f1_weighted": 0.3343911569799536
          },
          {
            "accuracy": 0.3576,
            "f1": 0.3546680343050932,
            "f1_weighted": 0.35466803430509325
          },
          {
            "accuracy": 0.366,
            "f1": 0.37017723295258603,
            "f1_weighted": 0.3701772329525861
          },
          {
            "accuracy": 0.3664,
            "f1": 0.36668271611513015,
            "f1_weighted": 0.36668271611513015
          }
        ]
      }
    ]
  },
  "task_name": "AmazonReviewsClassification"
}