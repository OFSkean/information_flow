{
  "dataset_revision": "4672e20407010da34463acc759c162ca9734bca6",
  "evaluation_time": 88.0849175453186,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.5836247478143913,
        "f1": 0.5503522415215657,
        "f1_weighted": 0.5877293871272183,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5836247478143913,
        "scores_per_experiment": [
          {
            "accuracy": 0.5884330867518494,
            "f1": 0.56370703108574,
            "f1_weighted": 0.593788507981895
          },
          {
            "accuracy": 0.5870880968392737,
            "f1": 0.5583546928108145,
            "f1_weighted": 0.5906775060064506
          },
          {
            "accuracy": 0.5786819098856758,
            "f1": 0.5476919380599251,
            "f1_weighted": 0.5801430020926449
          },
          {
            "accuracy": 0.609280430396772,
            "f1": 0.5636883790843155,
            "f1_weighted": 0.6112118018213626
          },
          {
            "accuracy": 0.5854068594485541,
            "f1": 0.5405799587223682,
            "f1_weighted": 0.5881859300603492
          },
          {
            "accuracy": 0.558843308675185,
            "f1": 0.5397355070746084,
            "f1_weighted": 0.5656279271685716
          },
          {
            "accuracy": 0.5817081371889711,
            "f1": 0.5491371705223528,
            "f1_weighted": 0.5830163942033824
          },
          {
            "accuracy": 0.5840618695359785,
            "f1": 0.5398553972247471,
            "f1_weighted": 0.591062674154704
          },
          {
            "accuracy": 0.5682582380632145,
            "f1": 0.5416507818745875,
            "f1_weighted": 0.5757202873798913
          },
          {
            "accuracy": 0.5944855413584398,
            "f1": 0.5591215587561972,
            "f1_weighted": 0.5978598404029315
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.5919331037875061,
        "f1": 0.558192676719982,
        "f1_weighted": 0.5946251623436954,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5919331037875061,
        "scores_per_experiment": [
          {
            "accuracy": 0.5937038858829317,
            "f1": 0.5697756538424773,
            "f1_weighted": 0.59538297206015
          },
          {
            "accuracy": 0.5941957697983276,
            "f1": 0.5525044424376696,
            "f1_weighted": 0.5991198367059577
          },
          {
            "accuracy": 0.6084604033448107,
            "f1": 0.5722829562901618,
            "f1_weighted": 0.607103587921155
          },
          {
            "accuracy": 0.6010821446138711,
            "f1": 0.5588813611547399,
            "f1_weighted": 0.5999868074069763
          },
          {
            "accuracy": 0.5956714215445155,
            "f1": 0.5517191170298201,
            "f1_weighted": 0.5968361057350812
          },
          {
            "accuracy": 0.5823905558288244,
            "f1": 0.5689238423604912,
            "f1_weighted": 0.5865901669314377
          },
          {
            "accuracy": 0.5794392523364486,
            "f1": 0.5527072754993301,
            "f1_weighted": 0.5807692624932889
          },
          {
            "accuracy": 0.5809149040826365,
            "f1": 0.539859302816002,
            "f1_weighted": 0.5859475504393752
          },
          {
            "accuracy": 0.5661583866207575,
            "f1": 0.5388165474634733,
            "f1_weighted": 0.5744789032936123
          },
          {
            "accuracy": 0.617314313821938,
            "f1": 0.5764562683056546,
            "f1_weighted": 0.6200364304499203
          }
        ]
      }
    ]
  },
  "task_name": "MassiveIntentClassification"
}