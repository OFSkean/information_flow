{
  "dataset_revision": "ae001d0e6b1228650b7bd1c2c65fb50ad11a8aba",
  "evaluation_time": 139.98525881767273,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.6899452804377566,
        "f1": 0.46968282627411123,
        "f1_weighted": 0.7283001808184589,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.6899452804377566,
        "scores_per_experiment": [
          {
            "accuracy": 0.6746466028271774,
            "f1": 0.45575065860656033,
            "f1_weighted": 0.7152898728390729
          },
          {
            "accuracy": 0.6990424076607387,
            "f1": 0.47335940528247467,
            "f1_weighted": 0.7357344444786779
          },
          {
            "accuracy": 0.7017783857729138,
            "f1": 0.47196527533751537,
            "f1_weighted": 0.7406454520216809
          },
          {
            "accuracy": 0.6890104879160966,
            "f1": 0.4726005147286934,
            "f1_weighted": 0.729690951444178
          },
          {
            "accuracy": 0.6942544459644323,
            "f1": 0.4785115121873018,
            "f1_weighted": 0.7282020313757006
          },
          {
            "accuracy": 0.6880984952120383,
            "f1": 0.4706071235914493,
            "f1_weighted": 0.7312444410734462
          },
          {
            "accuracy": 0.6835385316917465,
            "f1": 0.48577606644522275,
            "f1_weighted": 0.7242738483383028
          },
          {
            "accuracy": 0.7127222982216143,
            "f1": 0.490537088131894,
            "f1_weighted": 0.7489629074223036
          },
          {
            "accuracy": 0.6782945736434108,
            "f1": 0.4578502923707615,
            "f1_weighted": 0.7128936255195368
          },
          {
            "accuracy": 0.6780665754673962,
            "f1": 0.4398703260592394,
            "f1_weighted": 0.7160642336716897
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.6885458612975393,
        "f1": 0.44838967427155724,
        "f1_weighted": 0.7289137454757806,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.6885458612975393,
        "scores_per_experiment": [
          {
            "accuracy": 0.6572706935123043,
            "f1": 0.4175372772916062,
            "f1_weighted": 0.6990763773786601
          },
          {
            "accuracy": 0.7033557046979866,
            "f1": 0.4587726057601852,
            "f1_weighted": 0.7412372496991446
          },
          {
            "accuracy": 0.7002237136465325,
            "f1": 0.4447625644299133,
            "f1_weighted": 0.7414163867169047
          },
          {
            "accuracy": 0.687248322147651,
            "f1": 0.4547336062311762,
            "f1_weighted": 0.7311422105324697
          },
          {
            "accuracy": 0.6885906040268457,
            "f1": 0.4432325922378353,
            "f1_weighted": 0.7221950804458945
          },
          {
            "accuracy": 0.6832214765100671,
            "f1": 0.44811612321539196,
            "f1_weighted": 0.7311102807634852
          },
          {
            "accuracy": 0.6787472035794183,
            "f1": 0.42914367669369163,
            "f1_weighted": 0.7208338104141893
          },
          {
            "accuracy": 0.7172259507829978,
            "f1": 0.48067583382497164,
            "f1_weighted": 0.7556779741604847
          },
          {
            "accuracy": 0.6760626398210291,
            "f1": 0.449080718243574,
            "f1_weighted": 0.7134076820044976
          },
          {
            "accuracy": 0.6935123042505593,
            "f1": 0.4578417447872267,
            "f1_weighted": 0.7330404026420757
          }
        ]
      }
    ]
  },
  "task_name": "MTOPIntentClassification"
}