{
  "dataset_revision": "d80d48c1eb48d3562165c59d59d0034df9fff0bf",
  "evaluation_time": 54.89601469039917,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.8333105335157318,
        "f1": 0.8300297362680483,
        "f1_weighted": 0.8339034347945654,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.8333105335157318,
        "scores_per_experiment": [
          {
            "accuracy": 0.8050615595075239,
            "f1": 0.8028152385695904,
            "f1_weighted": 0.8043970601707677
          },
          {
            "accuracy": 0.8588691290469677,
            "f1": 0.8551718989247467,
            "f1_weighted": 0.8591149003269338
          },
          {
            "accuracy": 0.8290013679890561,
            "f1": 0.8223422001050914,
            "f1_weighted": 0.8303162680233874
          },
          {
            "accuracy": 0.8497492020063839,
            "f1": 0.8451746322082054,
            "f1_weighted": 0.850975371555062
          },
          {
            "accuracy": 0.8581851345189239,
            "f1": 0.8543196636420956,
            "f1_weighted": 0.8594159960058699
          },
          {
            "accuracy": 0.8255813953488372,
            "f1": 0.8229569856214254,
            "f1_weighted": 0.8260980556291284
          },
          {
            "accuracy": 0.8157774737802097,
            "f1": 0.8146872116780505,
            "f1_weighted": 0.8128432878635139
          },
          {
            "accuracy": 0.8255813953488372,
            "f1": 0.8205168540155531,
            "f1_weighted": 0.8282355098264136
          },
          {
            "accuracy": 0.8404012767897857,
            "f1": 0.8395377981363374,
            "f1_weighted": 0.841565769491703
          },
          {
            "accuracy": 0.8248974008207934,
            "f1": 0.8227748797793875,
            "f1_weighted": 0.8260721290528734
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.8319463087248321,
        "f1": 0.8319279897748821,
        "f1_weighted": 0.8325049488948764,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.8319463087248321,
        "scores_per_experiment": [
          {
            "accuracy": 0.8022371364653244,
            "f1": 0.8046284826678988,
            "f1_weighted": 0.8011576856065563
          },
          {
            "accuracy": 0.868903803131991,
            "f1": 0.8684207507958035,
            "f1_weighted": 0.8690320598817242
          },
          {
            "accuracy": 0.8259507829977628,
            "f1": 0.820815591429491,
            "f1_weighted": 0.8269337362074592
          },
          {
            "accuracy": 0.854586129753915,
            "f1": 0.8534571513001247,
            "f1_weighted": 0.8557540166155669
          },
          {
            "accuracy": 0.8465324384787472,
            "f1": 0.8488062458949318,
            "f1_weighted": 0.848255301780678
          },
          {
            "accuracy": 0.8196868008948546,
            "f1": 0.8198693070568331,
            "f1_weighted": 0.8194551400025819
          },
          {
            "accuracy": 0.8093959731543624,
            "f1": 0.8090857934749074,
            "f1_weighted": 0.8083616627615563
          },
          {
            "accuracy": 0.8322147651006712,
            "f1": 0.8343037588351684,
            "f1_weighted": 0.8343832638280375
          },
          {
            "accuracy": 0.8375838926174497,
            "f1": 0.836180042432651,
            "f1_weighted": 0.8382052006859927
          },
          {
            "accuracy": 0.8223713646532439,
            "f1": 0.8237127738610106,
            "f1_weighted": 0.8235114215786107
          }
        ]
      }
    ]
  },
  "task_name": "MTOPDomainClassification"
}