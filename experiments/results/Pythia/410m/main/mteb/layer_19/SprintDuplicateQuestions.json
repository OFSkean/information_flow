{
  "dataset_revision": "d66bd1f72af766a5cc4b0ca5e00c162f89e8cc46",
  "evaluation_time": 32.77627515792847,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "cosine_accuracy": 0.991960396039604,
        "cosine_accuracy_threshold": 0.930939793586731,
        "cosine_ap": 0.44575998213865703,
        "cosine_f1": 0.4748201438848921,
        "cosine_f1_threshold": 0.918001115322113,
        "cosine_precision": 0.4883720930232558,
        "cosine_recall": 0.462,
        "dot_accuracy": 0.9901386138613861,
        "dot_accuracy_threshold": 3777.15869140625,
        "dot_ap": 0.05533434544627766,
        "dot_f1": 0.11017325633051976,
        "dot_f1_threshold": 3304.30859375,
        "dot_precision": 0.0991207034372502,
        "dot_recall": 0.124,
        "euclidean_accuracy": 0.992029702970297,
        "euclidean_accuracy_threshold": 21.42510986328125,
        "euclidean_ap": 0.4522429012514302,
        "euclidean_f1": 0.4894859813084112,
        "euclidean_f1_threshold": 22.83014488220215,
        "euclidean_precision": 0.5884831460674157,
        "euclidean_recall": 0.419,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.45713692283654134,
        "manhattan_accuracy": 0.9920792079207921,
        "manhattan_accuracy_threshold": 535.8681640625,
        "manhattan_ap": 0.45713692283654134,
        "manhattan_f1": 0.4930555555555555,
        "manhattan_f1_threshold": 582.5029296875,
        "manhattan_precision": 0.5851648351648352,
        "manhattan_recall": 0.426,
        "max_accuracy": 0.9920792079207921,
        "max_ap": 0.45713692283654134,
        "max_f1": 0.4930555555555555,
        "max_precision": 0.5884831460674157,
        "max_recall": 0.462,
        "similarity_accuracy": 0.991960396039604,
        "similarity_accuracy_threshold": 0.930939793586731,
        "similarity_ap": 0.44575998213865703,
        "similarity_f1": 0.4748201438848921,
        "similarity_f1_threshold": 0.918001115322113,
        "similarity_precision": 0.4883720930232558,
        "similarity_recall": 0.462
      }
    ],
    "validation": [
      {
        "cosine_accuracy": 0.9911386138613861,
        "cosine_accuracy_threshold": 0.9311792850494385,
        "cosine_ap": 0.36352640876851217,
        "cosine_f1": 0.4173998044965787,
        "cosine_f1_threshold": 0.9119006395339966,
        "cosine_precision": 0.40822179732313574,
        "cosine_recall": 0.427,
        "dot_accuracy": 0.9901287128712871,
        "dot_accuracy_threshold": 3820.4013671875,
        "dot_ap": 0.06091097560672179,
        "dot_f1": 0.11940298507462686,
        "dot_f1_threshold": 3289.59912109375,
        "dot_precision": 0.10900082576383155,
        "dot_recall": 0.132,
        "euclidean_accuracy": 0.9913168316831683,
        "euclidean_accuracy_threshold": 22.38764190673828,
        "euclidean_ap": 0.3856389255259023,
        "euclidean_f1": 0.43814432989690716,
        "euclidean_f1_threshold": 24.276575088500977,
        "euclidean_precision": 0.4521276595744681,
        "euclidean_recall": 0.425,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.3905228000316939,
        "manhattan_accuracy": 0.9913069306930693,
        "manhattan_accuracy_threshold": 569.3076782226562,
        "manhattan_ap": 0.3905228000316939,
        "manhattan_f1": 0.4405888538380652,
        "manhattan_f1_threshold": 615.4384155273438,
        "manhattan_precision": 0.46452328159645234,
        "manhattan_recall": 0.419,
        "max_accuracy": 0.9913168316831683,
        "max_ap": 0.3905228000316939,
        "max_f1": 0.4405888538380652,
        "max_precision": 0.46452328159645234,
        "max_recall": 0.427,
        "similarity_accuracy": 0.9911386138613861,
        "similarity_accuracy_threshold": 0.9311792850494385,
        "similarity_ap": 0.36352640876851217,
        "similarity_f1": 0.4173998044965787,
        "similarity_f1_threshold": 0.9119006395339966,
        "similarity_precision": 0.40822179732313574,
        "similarity_recall": 0.427
      }
    ]
  },
  "task_name": "SprintDuplicateQuestions"
}