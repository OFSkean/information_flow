{
  "dataset_revision": "4672e20407010da34463acc759c162ca9734bca6",
  "evaluation_time": 93.67183136940002,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.5853732347007397,
        "f1": 0.5542699213294566,
        "f1_weighted": 0.5886310734960712,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5853732347007397,
        "scores_per_experiment": [
          {
            "accuracy": 0.5894418291862811,
            "f1": 0.5668160353402549,
            "f1_weighted": 0.591775984490643
          },
          {
            "accuracy": 0.5954942837928715,
            "f1": 0.5608665953914672,
            "f1_weighted": 0.6008333081924201
          },
          {
            "accuracy": 0.5847343644922663,
            "f1": 0.5522047383522578,
            "f1_weighted": 0.5875502642556036
          },
          {
            "accuracy": 0.5985205110961668,
            "f1": 0.5610250016929416,
            "f1_weighted": 0.5986247659840979
          },
          {
            "accuracy": 0.5924680564895763,
            "f1": 0.5501676815562492,
            "f1_weighted": 0.59498966601681
          },
          {
            "accuracy": 0.5524546065904505,
            "f1": 0.538759476268405,
            "f1_weighted": 0.5534996589480082
          },
          {
            "accuracy": 0.5722932078009415,
            "f1": 0.5513553687645695,
            "f1_weighted": 0.575598753830038
          },
          {
            "accuracy": 0.5830531271015468,
            "f1": 0.5478990214330977,
            "f1_weighted": 0.5904322355128966
          },
          {
            "accuracy": 0.5800268997982515,
            "f1": 0.543184606332663,
            "f1_weighted": 0.585879963775448
          },
          {
            "accuracy": 0.605245460659045,
            "f1": 0.5704206881626604,
            "f1_weighted": 0.6071261339547473
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.5960649286768323,
        "f1": 0.5601032409931136,
        "f1_weighted": 0.5988621093783558,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5960649286768323,
        "scores_per_experiment": [
          {
            "accuracy": 0.5976389572060994,
            "f1": 0.5718130699139162,
            "f1_weighted": 0.5974591058028778
          },
          {
            "accuracy": 0.6133792424987703,
            "f1": 0.567740104838977,
            "f1_weighted": 0.6199644119385158
          },
          {
            "accuracy": 0.6153467781603541,
            "f1": 0.5787819042905888,
            "f1_weighted": 0.616041194360039
          },
          {
            "accuracy": 0.5912444663059518,
            "f1": 0.5474347348894824,
            "f1_weighted": 0.5917543658007104
          },
          {
            "accuracy": 0.5941957697983276,
            "f1": 0.5551273681717813,
            "f1_weighted": 0.5973201221098418
          },
          {
            "accuracy": 0.5686178061977374,
            "f1": 0.5478443727354868,
            "f1_weighted": 0.570962346173211
          },
          {
            "accuracy": 0.5681259222823414,
            "f1": 0.5480029313260907,
            "f1_weighted": 0.5715417921128402
          },
          {
            "accuracy": 0.5897688145597639,
            "f1": 0.5483370588196974,
            "f1_weighted": 0.5930473577497795
          },
          {
            "accuracy": 0.5912444663059518,
            "f1": 0.5514545936254521,
            "f1_weighted": 0.5970256572590052
          },
          {
            "accuracy": 0.6310870634530251,
            "f1": 0.5844962713196635,
            "f1_weighted": 0.6335047404767377
          }
        ]
      }
    ]
  },
  "task_name": "MassiveIntentClassification"
}