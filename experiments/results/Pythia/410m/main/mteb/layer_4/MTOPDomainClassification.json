{
  "dataset_revision": "d80d48c1eb48d3562165c59d59d0034df9fff0bf",
  "evaluation_time": 47.89439582824707,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.8323301413588691,
        "f1": 0.8255312631231495,
        "f1_weighted": 0.8339442816959922,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.8323301413588691,
        "scores_per_experiment": [
          {
            "accuracy": 0.7979936160510716,
            "f1": 0.7909850019009336,
            "f1_weighted": 0.7990656919919469
          },
          {
            "accuracy": 0.8372093023255814,
            "f1": 0.8330452674967567,
            "f1_weighted": 0.8400692698931163
          },
          {
            "accuracy": 0.8392612859097127,
            "f1": 0.827035987154711,
            "f1_weighted": 0.8419957242330816
          },
          {
            "accuracy": 0.8294573643410853,
            "f1": 0.8230761996249913,
            "f1_weighted": 0.8319905494131296
          },
          {
            "accuracy": 0.8508891928864569,
            "f1": 0.8425642365510645,
            "f1_weighted": 0.8532854307768731
          },
          {
            "accuracy": 0.8472412220702235,
            "f1": 0.8416854779743109,
            "f1_weighted": 0.8485278835064185
          },
          {
            "accuracy": 0.8242134062927496,
            "f1": 0.8179239160253928,
            "f1_weighted": 0.8225612596622497
          },
          {
            "accuracy": 0.8214774281805746,
            "f1": 0.8166437669688373,
            "f1_weighted": 0.8247567025504398
          },
          {
            "accuracy": 0.832421340629275,
            "f1": 0.8285720747509462,
            "f1_weighted": 0.8336334949963236
          },
          {
            "accuracy": 0.8431372549019608,
            "f1": 0.833780702783552,
            "f1_weighted": 0.8435568099363436
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.8289038031319912,
        "f1": 0.8257277246996676,
        "f1_weighted": 0.8301774919685577,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.8289038031319912,
        "scores_per_experiment": [
          {
            "accuracy": 0.7919463087248322,
            "f1": 0.7896061140791443,
            "f1_weighted": 0.7923442967858286
          },
          {
            "accuracy": 0.8353467561521253,
            "f1": 0.8349044929324756,
            "f1_weighted": 0.8370529252239907
          },
          {
            "accuracy": 0.836241610738255,
            "f1": 0.8278996415358054,
            "f1_weighted": 0.8374303313434238
          },
          {
            "accuracy": 0.8263982102908277,
            "f1": 0.8251737276413766,
            "f1_weighted": 0.8302693652175842
          },
          {
            "accuracy": 0.8550335570469799,
            "f1": 0.85212251458339,
            "f1_weighted": 0.8568302638304005
          },
          {
            "accuracy": 0.8442953020134228,
            "f1": 0.8427777357791458,
            "f1_weighted": 0.8447109721325193
          },
          {
            "accuracy": 0.814317673378076,
            "f1": 0.8074237347668111,
            "f1_weighted": 0.8131198114921292
          },
          {
            "accuracy": 0.81834451901566,
            "f1": 0.8168851758390889,
            "f1_weighted": 0.821861020349663
          },
          {
            "accuracy": 0.8308724832214766,
            "f1": 0.8300358835795127,
            "f1_weighted": 0.8317672470132469
          },
          {
            "accuracy": 0.836241610738255,
            "f1": 0.8304482262599244,
            "f1_weighted": 0.8363886862967904
          }
        ]
      }
    ]
  },
  "task_name": "MTOPDomainClassification"
}