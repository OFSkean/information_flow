{
  "dataset_revision": "4672e20407010da34463acc759c162ca9734bca6",
  "evaluation_time": 83.36550283432007,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.594250168123739,
        "f1": 0.5638699974009875,
        "f1_weighted": 0.5963056430404933,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.594250168123739,
        "scores_per_experiment": [
          {
            "accuracy": 0.5981842636180229,
            "f1": 0.5740164866266095,
            "f1_weighted": 0.5991921086459311
          },
          {
            "accuracy": 0.6106254203093476,
            "f1": 0.5785000117266308,
            "f1_weighted": 0.6108850799118
          },
          {
            "accuracy": 0.5931405514458642,
            "f1": 0.5554695322749005,
            "f1_weighted": 0.59147834813348
          },
          {
            "accuracy": 0.6049092131809012,
            "f1": 0.5601940910360473,
            "f1_weighted": 0.6069083985256836
          },
          {
            "accuracy": 0.6022192333557498,
            "f1": 0.5621955568011893,
            "f1_weighted": 0.6021715469784473
          },
          {
            "accuracy": 0.5706119704102219,
            "f1": 0.5545571874199331,
            "f1_weighted": 0.5726471891905779
          },
          {
            "accuracy": 0.5800268997982515,
            "f1": 0.5565461534933057,
            "f1_weighted": 0.5832952701609011
          },
          {
            "accuracy": 0.5917955615332885,
            "f1": 0.5586085036144242,
            "f1_weighted": 0.5982282486892064
          },
          {
            "accuracy": 0.5817081371889711,
            "f1": 0.5589815926380581,
            "f1_weighted": 0.5865960824013206
          },
          {
            "accuracy": 0.609280430396772,
            "f1": 0.5796308583787769,
            "f1_weighted": 0.6116541577675854
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.6008853910477128,
        "f1": 0.5703942443188789,
        "f1_weighted": 0.6029802603185208,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.6008853910477128,
        "scores_per_experiment": [
          {
            "accuracy": 0.6035415641908509,
            "f1": 0.5813483657900715,
            "f1_weighted": 0.604752803197559
          },
          {
            "accuracy": 0.6207575012297097,
            "f1": 0.5799690946874628,
            "f1_weighted": 0.6203308465732369
          },
          {
            "accuracy": 0.6202656173143138,
            "f1": 0.5836214712783412,
            "f1_weighted": 0.6206725682593739
          },
          {
            "accuracy": 0.6015740285292671,
            "f1": 0.5565920188814524,
            "f1_weighted": 0.6056052394298268
          },
          {
            "accuracy": 0.6217412690605018,
            "f1": 0.5849116200458292,
            "f1_weighted": 0.6206662160475962
          },
          {
            "accuracy": 0.5784554845056566,
            "f1": 0.5656500347256196,
            "f1_weighted": 0.5812644981537128
          },
          {
            "accuracy": 0.5705853418593212,
            "f1": 0.555775622402751,
            "f1_weighted": 0.5739025037838015
          },
          {
            "accuracy": 0.5818986719134285,
            "f1": 0.5495776583165064,
            "f1_weighted": 0.5837586124655195
          },
          {
            "accuracy": 0.5823905558288244,
            "f1": 0.5551159450730253,
            "f1_weighted": 0.5898122045929202
          },
          {
            "accuracy": 0.6276438760452533,
            "f1": 0.5913806119877301,
            "f1_weighted": 0.6290371106816617
          }
        ]
      }
    ]
  },
  "task_name": "MassiveIntentClassification"
}