{
  "dataset_revision": "ae001d0e6b1228650b7bd1c2c65fb50ad11a8aba",
  "evaluation_time": 124.47540235519409,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.6935248518011855,
        "f1": 0.47487782457303185,
        "f1_weighted": 0.7315349972356977,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.6935248518011855,
        "scores_per_experiment": [
          {
            "accuracy": 0.6901504787961696,
            "f1": 0.469324508411473,
            "f1_weighted": 0.7247885632203592
          },
          {
            "accuracy": 0.6892384860921112,
            "f1": 0.4666708990985418,
            "f1_weighted": 0.7282961635936076
          },
          {
            "accuracy": 0.6876424988600092,
            "f1": 0.47884708068374454,
            "f1_weighted": 0.726870622414447
          },
          {
            "accuracy": 0.6994984040127679,
            "f1": 0.4787291217323209,
            "f1_weighted": 0.7367059827065148
          },
          {
            "accuracy": 0.6915184678522572,
            "f1": 0.47589575878502155,
            "f1_weighted": 0.730452703236822
          },
          {
            "accuracy": 0.6919744642042863,
            "f1": 0.47161096413881404,
            "f1_weighted": 0.7327910313054943
          },
          {
            "accuracy": 0.6928864569083447,
            "f1": 0.47282397832954925,
            "f1_weighted": 0.7317589671000662
          },
          {
            "accuracy": 0.7216142270861833,
            "f1": 0.5045667355821452,
            "f1_weighted": 0.7562598879592278
          },
          {
            "accuracy": 0.6780665754673962,
            "f1": 0.45742318564836665,
            "f1_weighted": 0.7166424777080657
          },
          {
            "accuracy": 0.6926584587323301,
            "f1": 0.4728860133203422,
            "f1_weighted": 0.730783573112373
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.6998657718120807,
        "f1": 0.46951392860900737,
        "f1_weighted": 0.7391875336817575,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.6998657718120807,
        "scores_per_experiment": [
          {
            "accuracy": 0.6863534675615213,
            "f1": 0.45822813213028085,
            "f1_weighted": 0.7266976360782583
          },
          {
            "accuracy": 0.6917225950782998,
            "f1": 0.47020026407401866,
            "f1_weighted": 0.7318957639104594
          },
          {
            "accuracy": 0.6970917225950783,
            "f1": 0.4545053662879913,
            "f1_weighted": 0.739795643573345
          },
          {
            "accuracy": 0.7015659955257271,
            "f1": 0.4723017929489081,
            "f1_weighted": 0.7394493307287711
          },
          {
            "accuracy": 0.705592841163311,
            "f1": 0.4775874527862571,
            "f1_weighted": 0.7443740324114081
          },
          {
            "accuracy": 0.6953020134228188,
            "f1": 0.46426605904565577,
            "f1_weighted": 0.7391139696333054
          },
          {
            "accuracy": 0.6894854586129754,
            "f1": 0.47148827317789305,
            "f1_weighted": 0.7284647777725255
          },
          {
            "accuracy": 0.7217002237136465,
            "f1": 0.4805101747848519,
            "f1_weighted": 0.7575684683602517
          },
          {
            "accuracy": 0.6953020134228188,
            "f1": 0.4669762550644169,
            "f1_weighted": 0.7343348741642153
          },
          {
            "accuracy": 0.7145413870246085,
            "f1": 0.47907551578979973,
            "f1_weighted": 0.7501808401850341
          }
        ]
      }
    ]
  },
  "task_name": "MTOPIntentClassification"
}