{
  "dataset_revision": "ae001d0e6b1228650b7bd1c2c65fb50ad11a8aba",
  "evaluation_time": 149.53068661689758,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.6699042407660739,
        "f1": 0.43583757501714826,
        "f1_weighted": 0.7115530081513806,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.6699042407660739,
        "scores_per_experiment": [
          {
            "accuracy": 0.6486548107615139,
            "f1": 0.419726739267417,
            "f1_weighted": 0.6913854246024934
          },
          {
            "accuracy": 0.6808025535795713,
            "f1": 0.4314853983049009,
            "f1_weighted": 0.7226479001300506
          },
          {
            "accuracy": 0.6833105335157319,
            "f1": 0.43757519092868646,
            "f1_weighted": 0.7257973991504097
          },
          {
            "accuracy": 0.677610579115367,
            "f1": 0.4446222625962266,
            "f1_weighted": 0.7191587612273987
          },
          {
            "accuracy": 0.67875056999544,
            "f1": 0.44517992925632655,
            "f1_weighted": 0.7192461839305209
          },
          {
            "accuracy": 0.6652986776105791,
            "f1": 0.43445948584132743,
            "f1_weighted": 0.7105971073115714
          },
          {
            "accuracy": 0.6497948016415869,
            "f1": 0.43438858420738546,
            "f1_weighted": 0.696063989957563
          },
          {
            "accuracy": 0.6855905152758778,
            "f1": 0.46189111821350787,
            "f1_weighted": 0.7241666970605207
          },
          {
            "accuracy": 0.6646146830825354,
            "f1": 0.42880358267587415,
            "f1_weighted": 0.7026124001615385
          },
          {
            "accuracy": 0.6646146830825354,
            "f1": 0.4202434588798302,
            "f1_weighted": 0.7038542179817385
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.6749888143176734,
        "f1": 0.4338920750556838,
        "f1_weighted": 0.7158545794090467,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.6749888143176734,
        "scores_per_experiment": [
          {
            "accuracy": 0.6527964205816554,
            "f1": 0.4115601391306947,
            "f1_weighted": 0.6985911673513182
          },
          {
            "accuracy": 0.6953020134228188,
            "f1": 0.4513718839044958,
            "f1_weighted": 0.7318742936990584
          },
          {
            "accuracy": 0.6885906040268457,
            "f1": 0.42886862229047923,
            "f1_weighted": 0.7306350459084071
          },
          {
            "accuracy": 0.6863534675615213,
            "f1": 0.4592968417372969,
            "f1_weighted": 0.7265771752607989
          },
          {
            "accuracy": 0.6818791946308724,
            "f1": 0.4262424733263863,
            "f1_weighted": 0.7216125115596698
          },
          {
            "accuracy": 0.6787472035794183,
            "f1": 0.43983973222411293,
            "f1_weighted": 0.7214157960147307
          },
          {
            "accuracy": 0.6577181208053692,
            "f1": 0.4308475635025627,
            "f1_weighted": 0.7027296679855997
          },
          {
            "accuracy": 0.6760626398210291,
            "f1": 0.43804401433489465,
            "f1_weighted": 0.7173653665000405
          },
          {
            "accuracy": 0.6577181208053692,
            "f1": 0.4097123226064596,
            "f1_weighted": 0.6948482789854409
          },
          {
            "accuracy": 0.6747203579418345,
            "f1": 0.443137157499455,
            "f1_weighted": 0.7128964908254032
          }
        ]
      }
    ]
  },
  "task_name": "MTOPIntentClassification"
}