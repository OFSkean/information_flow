{
  "dataset_revision": "d66bd1f72af766a5cc4b0ca5e00c162f89e8cc46",
  "evaluation_time": 34.2968168258667,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "cosine_accuracy": 0.9910990099009901,
        "cosine_accuracy_threshold": 0.9162168502807617,
        "cosine_ap": 0.3241992645293968,
        "cosine_f1": 0.38244514106583066,
        "cosine_f1_threshold": 0.8966507911682129,
        "cosine_precision": 0.40043763676148797,
        "cosine_recall": 0.366,
        "dot_accuracy": 0.9901584158415841,
        "dot_accuracy_threshold": 5764.7744140625,
        "dot_ap": 0.060059722338716606,
        "dot_f1": 0.12334801762114539,
        "dot_f1_threshold": 5077.34130859375,
        "dot_precision": 0.12080536912751678,
        "dot_recall": 0.126,
        "euclidean_accuracy": 0.990960396039604,
        "euclidean_accuracy_threshold": 27.388992309570312,
        "euclidean_ap": 0.3172597424407516,
        "euclidean_f1": 0.3835005574136009,
        "euclidean_f1_threshold": 31.78441619873047,
        "euclidean_precision": 0.4332493702770781,
        "euclidean_recall": 0.344,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.3925070518482718,
        "manhattan_accuracy": 0.9913465346534653,
        "manhattan_accuracy_threshold": 739.0579833984375,
        "manhattan_ap": 0.3925070518482718,
        "manhattan_f1": 0.4491440080563947,
        "manhattan_f1_threshold": 799.8646240234375,
        "manhattan_precision": 0.45233265720081134,
        "manhattan_recall": 0.446,
        "max_accuracy": 0.9913465346534653,
        "max_ap": 0.3925070518482718,
        "max_f1": 0.4491440080563947,
        "max_precision": 0.45233265720081134,
        "max_recall": 0.446,
        "similarity_accuracy": 0.9910990099009901,
        "similarity_accuracy_threshold": 0.9162168502807617,
        "similarity_ap": 0.3241992645293968,
        "similarity_f1": 0.38244514106583066,
        "similarity_f1_threshold": 0.8966507911682129,
        "similarity_precision": 0.40043763676148797,
        "similarity_recall": 0.366
      }
    ],
    "validation": [
      {
        "cosine_accuracy": 0.9907524752475247,
        "cosine_accuracy_threshold": 0.9194662570953369,
        "cosine_ap": 0.24509991098088693,
        "cosine_f1": 0.3032329988851728,
        "cosine_f1_threshold": 0.8954194188117981,
        "cosine_precision": 0.3425692695214106,
        "cosine_recall": 0.272,
        "dot_accuracy": 0.9901485148514851,
        "dot_accuracy_threshold": 5926.42724609375,
        "dot_ap": 0.055201806894792635,
        "dot_f1": 0.11578947368421053,
        "dot_f1_threshold": 5026.38916015625,
        "dot_precision": 0.103125,
        "dot_recall": 0.132,
        "euclidean_accuracy": 0.9907821782178218,
        "euclidean_accuracy_threshold": 28.732383728027344,
        "euclidean_ap": 0.25111216485384,
        "euclidean_f1": 0.31300813008130085,
        "euclidean_f1_threshold": 33.34770202636719,
        "euclidean_precision": 0.3181818181818182,
        "euclidean_recall": 0.308,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.31807374792732274,
        "manhattan_accuracy": 0.991009900990099,
        "manhattan_accuracy_threshold": 728.380859375,
        "manhattan_ap": 0.31807374792732274,
        "manhattan_f1": 0.3663845223700121,
        "manhattan_f1_threshold": 802.678466796875,
        "manhattan_precision": 0.463302752293578,
        "manhattan_recall": 0.303,
        "max_accuracy": 0.991009900990099,
        "max_ap": 0.31807374792732274,
        "max_f1": 0.3663845223700121,
        "max_precision": 0.463302752293578,
        "max_recall": 0.308,
        "similarity_accuracy": 0.9907524752475247,
        "similarity_accuracy_threshold": 0.9194662570953369,
        "similarity_ap": 0.24509991098088693,
        "similarity_f1": 0.3032329988851728,
        "similarity_f1_threshold": 0.8954194188117981,
        "similarity_precision": 0.3425692695214106,
        "similarity_recall": 0.272
      }
    ]
  },
  "task_name": "SprintDuplicateQuestions"
}