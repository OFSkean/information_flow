{
  "dataset_revision": "d80d48c1eb48d3562165c59d59d0034df9fff0bf",
  "evaluation_time": 56.357632637023926,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.8203830369357046,
        "f1": 0.8176215461044153,
        "f1_weighted": 0.8212337438340003,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.8203830369357046,
        "scores_per_experiment": [
          {
            "accuracy": 0.7877336981304149,
            "f1": 0.7839499263023849,
            "f1_weighted": 0.7850738813309762
          },
          {
            "accuracy": 0.8433652530779754,
            "f1": 0.8413131212427374,
            "f1_weighted": 0.8440213555557669
          },
          {
            "accuracy": 0.8125854993160054,
            "f1": 0.8077452800640015,
            "f1_weighted": 0.8143183355809666
          },
          {
            "accuracy": 0.8333333333333334,
            "f1": 0.8270984253443163,
            "f1_weighted": 0.8354600161986724
          },
          {
            "accuracy": 0.8417692658458732,
            "f1": 0.8383975231809544,
            "f1_weighted": 0.8432024521637783
          },
          {
            "accuracy": 0.8219334245326038,
            "f1": 0.8213248031672366,
            "f1_weighted": 0.8219885528652191
          },
          {
            "accuracy": 0.8007295941632467,
            "f1": 0.8015173422196846,
            "f1_weighted": 0.7985021708452549
          },
          {
            "accuracy": 0.808937528499772,
            "f1": 0.804701239226899,
            "f1_weighted": 0.8119426662303877
          },
          {
            "accuracy": 0.8344733242134063,
            "f1": 0.8322969190743694,
            "f1_weighted": 0.8363541524920258
          },
          {
            "accuracy": 0.8189694482444141,
            "f1": 0.8178708812215674,
            "f1_weighted": 0.8214738550769535
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.8191946308724832,
        "f1": 0.8201275999084755,
        "f1_weighted": 0.8199183895638015,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.8191946308724832,
        "scores_per_experiment": [
          {
            "accuracy": 0.7758389261744967,
            "f1": 0.778562207982786,
            "f1_weighted": 0.772884231801851
          },
          {
            "accuracy": 0.8554809843400447,
            "f1": 0.857753173121782,
            "f1_weighted": 0.8555517661717464
          },
          {
            "accuracy": 0.8138702460850112,
            "f1": 0.8117179530775918,
            "f1_weighted": 0.8157261061353568
          },
          {
            "accuracy": 0.8492170022371365,
            "f1": 0.8475028508545381,
            "f1_weighted": 0.8512653715721344
          },
          {
            "accuracy": 0.8335570469798658,
            "f1": 0.837931190978534,
            "f1_weighted": 0.8352398245010162
          },
          {
            "accuracy": 0.8152125279642058,
            "f1": 0.8154068656146507,
            "f1_weighted": 0.8156726303322234
          },
          {
            "accuracy": 0.8017897091722596,
            "f1": 0.8025338000033666,
            "f1_weighted": 0.8007525491901409
          },
          {
            "accuracy": 0.8062639821029083,
            "f1": 0.8080187653890246,
            "f1_weighted": 0.808375272774492
          },
          {
            "accuracy": 0.8295302013422818,
            "f1": 0.828036745196368,
            "f1_weighted": 0.8304928156242048
          },
          {
            "accuracy": 0.8111856823266219,
            "f1": 0.8138124468661136,
            "f1_weighted": 0.813223327534848
          }
        ]
      }
    ]
  },
  "task_name": "MTOPDomainClassification"
}