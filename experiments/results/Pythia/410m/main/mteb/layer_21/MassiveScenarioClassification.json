{
  "dataset_revision": "fad2c6e8459f9e1c45d9315f4953d921437d70f8",
  "evaluation_time": 66.02795600891113,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.6304640215198386,
        "f1": 0.6219323961544456,
        "f1_weighted": 0.6334221899330034,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.6304640215198386,
        "scores_per_experiment": [
          {
            "accuracy": 0.636516476126429,
            "f1": 0.638830394364755,
            "f1_weighted": 0.6404804332728433
          },
          {
            "accuracy": 0.632481506388702,
            "f1": 0.6201802597819026,
            "f1_weighted": 0.6269500835561393
          },
          {
            "accuracy": 0.6099529253530599,
            "f1": 0.6079150307667559,
            "f1_weighted": 0.615526771642044
          },
          {
            "accuracy": 0.64862138533961,
            "f1": 0.6332932951969377,
            "f1_weighted": 0.6494594956404383
          },
          {
            "accuracy": 0.6607262945527909,
            "f1": 0.641365113081311,
            "f1_weighted": 0.6631029208232662
          },
          {
            "accuracy": 0.6173503698722259,
            "f1": 0.6104502818654394,
            "f1_weighted": 0.6171544413673046
          },
          {
            "accuracy": 0.613315400134499,
            "f1": 0.6031741652008101,
            "f1_weighted": 0.6180001956503534
          },
          {
            "accuracy": 0.6435776731674513,
            "f1": 0.631078263238859,
            "f1_weighted": 0.6484474638094716
          },
          {
            "accuracy": 0.6378614660390047,
            "f1": 0.6347657835372784,
            "f1_weighted": 0.6427271272586322
          },
          {
            "accuracy": 0.6042367182246133,
            "f1": 0.598271374510406,
            "f1_weighted": 0.6123729663095414
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.6246925725528776,
        "f1": 0.6193533340859136,
        "f1_weighted": 0.6276537054437304,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.6246925725528776,
        "scores_per_experiment": [
          {
            "accuracy": 0.6374815543531727,
            "f1": 0.6353392945103226,
            "f1_weighted": 0.6413253457922046
          },
          {
            "accuracy": 0.6266601082144614,
            "f1": 0.624537572222223,
            "f1_weighted": 0.6244141374233724
          },
          {
            "accuracy": 0.6079685194294147,
            "f1": 0.6088199954154373,
            "f1_weighted": 0.6137192822941943
          },
          {
            "accuracy": 0.6138711264141663,
            "f1": 0.6033537707148624,
            "f1_weighted": 0.6146820941276067
          },
          {
            "accuracy": 0.6556812592228234,
            "f1": 0.643078963115493,
            "f1_weighted": 0.6588572321103143
          },
          {
            "accuracy": 0.6084604033448107,
            "f1": 0.6092516675089436,
            "f1_weighted": 0.6102818850729872
          },
          {
            "accuracy": 0.6074766355140186,
            "f1": 0.600017562425922,
            "f1_weighted": 0.6100116869483596
          },
          {
            "accuracy": 0.6369896704377767,
            "f1": 0.6282561500229202,
            "f1_weighted": 0.6413404749718218
          },
          {
            "accuracy": 0.6350221347761928,
            "f1": 0.6300499955429769,
            "f1_weighted": 0.6387973395221835
          },
          {
            "accuracy": 0.617314313821938,
            "f1": 0.6108283693800365,
            "f1_weighted": 0.6231075761742606
          }
        ]
      }
    ]
  },
  "task_name": "MassiveScenarioClassification"
}