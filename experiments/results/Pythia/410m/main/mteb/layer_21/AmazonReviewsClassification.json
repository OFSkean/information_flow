{
  "dataset_revision": "1399c76144fd37290681b995c656ef9b2e06e26d",
  "evaluation_time": 280.14167165756226,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.33848,
        "f1": 0.3385866361074444,
        "f1_weighted": 0.33858663610744444,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.33848,
        "scores_per_experiment": [
          {
            "accuracy": 0.3586,
            "f1": 0.35662705095905983,
            "f1_weighted": 0.35662705095905983
          },
          {
            "accuracy": 0.3578,
            "f1": 0.3574262434243472,
            "f1_weighted": 0.3574262434243472
          },
          {
            "accuracy": 0.3324,
            "f1": 0.32760020617906804,
            "f1_weighted": 0.32760020617906804
          },
          {
            "accuracy": 0.3454,
            "f1": 0.34799622828167043,
            "f1_weighted": 0.3479962282816705
          },
          {
            "accuracy": 0.3568,
            "f1": 0.35229492612028246,
            "f1_weighted": 0.35229492612028246
          },
          {
            "accuracy": 0.3252,
            "f1": 0.32661130965444163,
            "f1_weighted": 0.32661130965444163
          },
          {
            "accuracy": 0.3094,
            "f1": 0.3113883786970587,
            "f1_weighted": 0.31138837869705865
          },
          {
            "accuracy": 0.3334,
            "f1": 0.3351954322449079,
            "f1_weighted": 0.3351954322449079
          },
          {
            "accuracy": 0.333,
            "f1": 0.33189776342058164,
            "f1_weighted": 0.3318977634205816
          },
          {
            "accuracy": 0.3328,
            "f1": 0.33882882209302584,
            "f1_weighted": 0.33882882209302584
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.3433399999999999,
        "f1": 0.34297956314054673,
        "f1_weighted": 0.3429795631405467,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.3433399999999999,
        "scores_per_experiment": [
          {
            "accuracy": 0.3632,
            "f1": 0.36221132717384075,
            "f1_weighted": 0.3622113271738407
          },
          {
            "accuracy": 0.3584,
            "f1": 0.35740356532902895,
            "f1_weighted": 0.35740356532902895
          },
          {
            "accuracy": 0.3502,
            "f1": 0.3453681346786763,
            "f1_weighted": 0.34536813467867633
          },
          {
            "accuracy": 0.3334,
            "f1": 0.3350887535045966,
            "f1_weighted": 0.3350887535045966
          },
          {
            "accuracy": 0.3598,
            "f1": 0.35273709481508453,
            "f1_weighted": 0.3527370948150845
          },
          {
            "accuracy": 0.3264,
            "f1": 0.32744526054061085,
            "f1_weighted": 0.3274452605406109
          },
          {
            "accuracy": 0.3272,
            "f1": 0.3279315100449408,
            "f1_weighted": 0.32793151004494075
          },
          {
            "accuracy": 0.3224,
            "f1": 0.3243805625388364,
            "f1_weighted": 0.32438056253883646
          },
          {
            "accuracy": 0.3564,
            "f1": 0.3557561268426971,
            "f1_weighted": 0.355756126842697
          },
          {
            "accuracy": 0.336,
            "f1": 0.34147329593715525,
            "f1_weighted": 0.3414732959371553
          }
        ]
      }
    ]
  },
  "task_name": "AmazonReviewsClassification"
}