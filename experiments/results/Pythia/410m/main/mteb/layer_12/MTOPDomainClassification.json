{
  "dataset_revision": "d80d48c1eb48d3562165c59d59d0034df9fff0bf",
  "evaluation_time": 52.38688111305237,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.8308937528499772,
        "f1": 0.8261439108330402,
        "f1_weighted": 0.8311769723419811,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.8308937528499772,
        "scores_per_experiment": [
          {
            "accuracy": 0.7991336069311445,
            "f1": 0.7940588866019898,
            "f1_weighted": 0.7960688942059599
          },
          {
            "accuracy": 0.844733242134063,
            "f1": 0.839047723028314,
            "f1_weighted": 0.8469484465536761
          },
          {
            "accuracy": 0.8305973552211582,
            "f1": 0.820409679452192,
            "f1_weighted": 0.8316054994002756
          },
          {
            "accuracy": 0.836297309621523,
            "f1": 0.8301210892052023,
            "f1_weighted": 0.8362078891933717
          },
          {
            "accuracy": 0.8632010943912448,
            "f1": 0.8582822933138249,
            "f1_weighted": 0.8640302721065503
          },
          {
            "accuracy": 0.8406292749658003,
            "f1": 0.8383290789609145,
            "f1_weighted": 0.8410943120080387
          },
          {
            "accuracy": 0.8105335157318742,
            "f1": 0.80714080803262,
            "f1_weighted": 0.8074063953385856
          },
          {
            "accuracy": 0.8274053807569539,
            "f1": 0.821093614415637,
            "f1_weighted": 0.8301740647035474
          },
          {
            "accuracy": 0.8308253533971728,
            "f1": 0.830667295980355,
            "f1_weighted": 0.8316032574944735
          },
          {
            "accuracy": 0.8255813953488372,
            "f1": 0.822288639339352,
            "f1_weighted": 0.826630692415332
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.8247427293064877,
        "f1": 0.8246405979536904,
        "f1_weighted": 0.8248605198103898,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.8247427293064877,
        "scores_per_experiment": [
          {
            "accuracy": 0.7870246085011185,
            "f1": 0.7887613581810818,
            "f1_weighted": 0.784251137347877
          },
          {
            "accuracy": 0.8420581655480984,
            "f1": 0.8425649637349409,
            "f1_weighted": 0.8428049934045054
          },
          {
            "accuracy": 0.8308724832214766,
            "f1": 0.8215233249473066,
            "f1_weighted": 0.8313364990806434
          },
          {
            "accuracy": 0.8380313199105145,
            "f1": 0.8405152120611916,
            "f1_weighted": 0.8384028418392917
          },
          {
            "accuracy": 0.8447427293064877,
            "f1": 0.8483576589581149,
            "f1_weighted": 0.8464409002181912
          },
          {
            "accuracy": 0.8326621923937361,
            "f1": 0.8327834791403821,
            "f1_weighted": 0.8322983320229566
          },
          {
            "accuracy": 0.8,
            "f1": 0.7978900340200649,
            "f1_weighted": 0.7984648765780794
          },
          {
            "accuracy": 0.8237136465324385,
            "f1": 0.8237813603470241,
            "f1_weighted": 0.8257596646425417
          },
          {
            "accuracy": 0.8313199105145413,
            "f1": 0.8320950734366858,
            "f1_weighted": 0.8316149945338687
          },
          {
            "accuracy": 0.8170022371364654,
            "f1": 0.8181335147101115,
            "f1_weighted": 0.8172309584359422
          }
        ]
      }
    ]
  },
  "task_name": "MTOPDomainClassification"
}