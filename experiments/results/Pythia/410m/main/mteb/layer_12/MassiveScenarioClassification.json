{
  "dataset_revision": "fad2c6e8459f9e1c45d9315f4953d921437d70f8",
  "evaluation_time": 67.33745813369751,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.6296570275722932,
        "f1": 0.6146914071824653,
        "f1_weighted": 0.6311884430907639,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.6296570275722932,
        "scores_per_experiment": [
          {
            "accuracy": 0.6472763954270343,
            "f1": 0.6412059478213847,
            "f1_weighted": 0.6485509476972781
          },
          {
            "accuracy": 0.644586415601883,
            "f1": 0.6347850720649666,
            "f1_weighted": 0.6450708408674848
          },
          {
            "accuracy": 0.6213853396099529,
            "f1": 0.6155484973619189,
            "f1_weighted": 0.6259595644662932
          },
          {
            "accuracy": 0.6217215870880969,
            "f1": 0.6005127132562004,
            "f1_weighted": 0.6244923555665839
          },
          {
            "accuracy": 0.656018829858776,
            "f1": 0.6271499769144548,
            "f1_weighted": 0.6541647506653364
          },
          {
            "accuracy": 0.6082716879623403,
            "f1": 0.5939238515659169,
            "f1_weighted": 0.6068400307631758
          },
          {
            "accuracy": 0.6264290517821116,
            "f1": 0.608650055664119,
            "f1_weighted": 0.6305973926484646
          },
          {
            "accuracy": 0.640551445864156,
            "f1": 0.6190380065557616,
            "f1_weighted": 0.6429325157538852
          },
          {
            "accuracy": 0.6271015467383995,
            "f1": 0.6132616476463166,
            "f1_weighted": 0.6281262934076796
          },
          {
            "accuracy": 0.6032279757901816,
            "f1": 0.592838302973613,
            "f1_weighted": 0.6051497390714585
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.6180521396950319,
        "f1": 0.6077512632390015,
        "f1_weighted": 0.6192976447197265,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.6180521396950319,
        "scores_per_experiment": [
          {
            "accuracy": 0.6330545991146089,
            "f1": 0.6246276710199314,
            "f1_weighted": 0.634681663381144
          },
          {
            "accuracy": 0.6340383669454008,
            "f1": 0.6326819463683415,
            "f1_weighted": 0.6349235139399613
          },
          {
            "accuracy": 0.6138711264141663,
            "f1": 0.612503335969893,
            "f1_weighted": 0.6160315408200118
          },
          {
            "accuracy": 0.5996064928676832,
            "f1": 0.5818948457782556,
            "f1_weighted": 0.6028084015641196
          },
          {
            "accuracy": 0.6419085095917364,
            "f1": 0.618744379379288,
            "f1_weighted": 0.638938145350841
          },
          {
            "accuracy": 0.5951795376291196,
            "f1": 0.5947314551268993,
            "f1_weighted": 0.591818120403166
          },
          {
            "accuracy": 0.6123954746679784,
            "f1": 0.6007811662831082,
            "f1_weighted": 0.6174345441151229
          },
          {
            "accuracy": 0.6197737333989178,
            "f1": 0.6049902438818444,
            "f1_weighted": 0.6223565603148143
          },
          {
            "accuracy": 0.617806197737334,
            "f1": 0.6058671955513106,
            "f1_weighted": 0.6209179450416723
          },
          {
            "accuracy": 0.6128873585833743,
            "f1": 0.6006903930311426,
            "f1_weighted": 0.6130660122664123
          }
        ]
      }
    ]
  },
  "task_name": "MassiveScenarioClassification"
}