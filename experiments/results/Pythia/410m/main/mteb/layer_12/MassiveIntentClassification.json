{
  "dataset_revision": "4672e20407010da34463acc759c162ca9734bca6",
  "evaluation_time": 90.09273266792297,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.5950571620712843,
        "f1": 0.5615609947489478,
        "f1_weighted": 0.5989545026601942,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5950571620712843,
        "scores_per_experiment": [
          {
            "accuracy": 0.5941492938802959,
            "f1": 0.5761128510720963,
            "f1_weighted": 0.5988812618809055
          },
          {
            "accuracy": 0.6069266980497646,
            "f1": 0.5747154870002186,
            "f1_weighted": 0.609219963181677
          },
          {
            "accuracy": 0.5958305312710155,
            "f1": 0.5618381113256005,
            "f1_weighted": 0.597706427060236
          },
          {
            "accuracy": 0.6146603900470746,
            "f1": 0.5711373999829779,
            "f1_weighted": 0.615504423731038
          },
          {
            "accuracy": 0.597511768661735,
            "f1": 0.5558120242402198,
            "f1_weighted": 0.5995623688483713
          },
          {
            "accuracy": 0.5679219905850706,
            "f1": 0.5450119013949679,
            "f1_weighted": 0.5749570466168163
          },
          {
            "accuracy": 0.5891055817081372,
            "f1": 0.5583167864053477,
            "f1_weighted": 0.5914476796281568
          },
          {
            "accuracy": 0.5954942837928715,
            "f1": 0.5524765073761897,
            "f1_weighted": 0.6037472697382448
          },
          {
            "accuracy": 0.5843981170141224,
            "f1": 0.5514329938728549,
            "f1_weighted": 0.5912997688816222
          },
          {
            "accuracy": 0.6045729657027572,
            "f1": 0.5687558848190045,
            "f1_weighted": 0.6072188170348742
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.6058042302016724,
        "f1": 0.5700591371080842,
        "f1_weighted": 0.6077320434461249,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.6058042302016724,
        "scores_per_experiment": [
          {
            "accuracy": 0.6010821446138711,
            "f1": 0.5735928352191281,
            "f1_weighted": 0.6040848579772188
          },
          {
            "accuracy": 0.6060009837678307,
            "f1": 0.559306910866137,
            "f1_weighted": 0.6084169996166
          },
          {
            "accuracy": 0.6227250368912937,
            "f1": 0.5861072684217593,
            "f1_weighted": 0.6204475581811866
          },
          {
            "accuracy": 0.6138711264141663,
            "f1": 0.5669452520706112,
            "f1_weighted": 0.613259285871209
          },
          {
            "accuracy": 0.6099360550909986,
            "f1": 0.5676465800481516,
            "f1_weighted": 0.6092753031801706
          },
          {
            "accuracy": 0.5927201180521396,
            "f1": 0.576242639714737,
            "f1_weighted": 0.5949450557790179
          },
          {
            "accuracy": 0.5961633054599115,
            "f1": 0.5657643927628502,
            "f1_weighted": 0.5998243036666997
          },
          {
            "accuracy": 0.5937038858829317,
            "f1": 0.553805075520338,
            "f1_weighted": 0.5961937281240323
          },
          {
            "accuracy": 0.5789473684210527,
            "f1": 0.5513553946024675,
            "f1_weighted": 0.5857844786276921
          },
          {
            "accuracy": 0.6428922774225283,
            "f1": 0.5998250218546627,
            "f1_weighted": 0.6450888634374218
          }
        ]
      }
    ]
  },
  "task_name": "MassiveIntentClassification"
}