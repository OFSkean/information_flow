{
  "dataset_revision": "ae001d0e6b1228650b7bd1c2c65fb50ad11a8aba",
  "evaluation_time": 121.1010844707489,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.70843593251254,
        "f1": 0.4822348592096035,
        "f1_weighted": 0.7446024808505041,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.70843593251254,
        "scores_per_experiment": [
          {
            "accuracy": 0.6949384404924761,
            "f1": 0.46690460733586464,
            "f1_weighted": 0.7314067856353687
          },
          {
            "accuracy": 0.70953032375741,
            "f1": 0.4815968651787057,
            "f1_weighted": 0.7481202417436322
          },
          {
            "accuracy": 0.7045143638850889,
            "f1": 0.48118920636484963,
            "f1_weighted": 0.7429260720044883
          },
          {
            "accuracy": 0.7086183310533516,
            "f1": 0.49673988299098976,
            "f1_weighted": 0.7456623346242649
          },
          {
            "accuracy": 0.7102143182854537,
            "f1": 0.4769035266829256,
            "f1_weighted": 0.7449250432442275
          },
          {
            "accuracy": 0.7209302325581395,
            "f1": 0.4930893444528525,
            "f1_weighted": 0.7582840095268282
          },
          {
            "accuracy": 0.6969904240766074,
            "f1": 0.48841798976670137,
            "f1_weighted": 0.735183570554358
          },
          {
            "accuracy": 0.728454172366621,
            "f1": 0.48551282870880574,
            "f1_weighted": 0.7617371671783711
          },
          {
            "accuracy": 0.7061103511171911,
            "f1": 0.47003424547916656,
            "f1_weighted": 0.737635982867051
          },
          {
            "accuracy": 0.7040583675330597,
            "f1": 0.4819600951351736,
            "f1_weighted": 0.7401436011264516
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.7094407158836689,
        "f1": 0.47610875946404774,
        "f1_weighted": 0.7477906060929576,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.7094407158836689,
        "scores_per_experiment": [
          {
            "accuracy": 0.6890380313199105,
            "f1": 0.44854836975097245,
            "f1_weighted": 0.7276919725662638
          },
          {
            "accuracy": 0.7145413870246085,
            "f1": 0.47546089893387067,
            "f1_weighted": 0.7559378311750489
          },
          {
            "accuracy": 0.7127516778523489,
            "f1": 0.4862788918074823,
            "f1_weighted": 0.7512702602898444
          },
          {
            "accuracy": 0.7127516778523489,
            "f1": 0.49806241187303274,
            "f1_weighted": 0.7520753953897589
          },
          {
            "accuracy": 0.7033557046979866,
            "f1": 0.4718416496996414,
            "f1_weighted": 0.7388539206180736
          },
          {
            "accuracy": 0.7167785234899329,
            "f1": 0.4680031518213014,
            "f1_weighted": 0.7570211738385559
          },
          {
            "accuracy": 0.6921700223713646,
            "f1": 0.4696442120000725,
            "f1_weighted": 0.7331696958505138
          },
          {
            "accuracy": 0.723489932885906,
            "f1": 0.48627275047399054,
            "f1_weighted": 0.762812335908564
          },
          {
            "accuracy": 0.7154362416107383,
            "f1": 0.473152107945461,
            "f1_weighted": 0.7472454931600624
          },
          {
            "accuracy": 0.7140939597315437,
            "f1": 0.4838231503346514,
            "f1_weighted": 0.7518279821328894
          }
        ]
      }
    ]
  },
  "task_name": "MTOPIntentClassification"
}