{
  "dataset_revision": "fad2c6e8459f9e1c45d9315f4953d921437d70f8",
  "evaluation_time": 67.7372534275055,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.627404169468729,
        "f1": 0.6134571873593065,
        "f1_weighted": 0.6287474226128804,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.627404169468729,
        "scores_per_experiment": [
          {
            "accuracy": 0.6378614660390047,
            "f1": 0.6339878902910449,
            "f1_weighted": 0.6396218219632722
          },
          {
            "accuracy": 0.6449226630800269,
            "f1": 0.6309523837112561,
            "f1_weighted": 0.6418802442084676
          },
          {
            "accuracy": 0.6096166778749159,
            "f1": 0.6062792443265795,
            "f1_weighted": 0.6122505685652172
          },
          {
            "accuracy": 0.6358439811701412,
            "f1": 0.6148561835864964,
            "f1_weighted": 0.6378805795568225
          },
          {
            "accuracy": 0.65635507733692,
            "f1": 0.6293357133876595,
            "f1_weighted": 0.6571591220474393
          },
          {
            "accuracy": 0.6099529253530599,
            "f1": 0.6003705859288944,
            "f1_weighted": 0.6085498806399201
          },
          {
            "accuracy": 0.6271015467383995,
            "f1": 0.6073762369876495,
            "f1_weighted": 0.6292439638487973
          },
          {
            "accuracy": 0.6361802286482852,
            "f1": 0.6155328216787367,
            "f1_weighted": 0.6388177612813487
          },
          {
            "accuracy": 0.6291190316072629,
            "f1": 0.6153877062599314,
            "f1_weighted": 0.6327202968917536
          },
          {
            "accuracy": 0.5870880968392737,
            "f1": 0.5804931074348172,
            "f1_weighted": 0.5893499871257655
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.6215445154943433,
        "f1": 0.6111213601574944,
        "f1_weighted": 0.6229766458336864,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.6215445154943433,
        "scores_per_experiment": [
          {
            "accuracy": 0.6266601082144614,
            "f1": 0.6176414224899507,
            "f1_weighted": 0.6286939561807375
          },
          {
            "accuracy": 0.6355140186915887,
            "f1": 0.6313780979063915,
            "f1_weighted": 0.6345402380091226
          },
          {
            "accuracy": 0.617314313821938,
            "f1": 0.6181504204896977,
            "f1_weighted": 0.6220417542852725
          },
          {
            "accuracy": 0.6069847515986228,
            "f1": 0.5896075890433246,
            "f1_weighted": 0.6088882725492217
          },
          {
            "accuracy": 0.6522380718150517,
            "f1": 0.6309391682419495,
            "f1_weighted": 0.6523654521961522
          },
          {
            "accuracy": 0.5971470732907034,
            "f1": 0.597553103676578,
            "f1_weighted": 0.5928233865378155
          },
          {
            "accuracy": 0.6207575012297097,
            "f1": 0.6048247995920512,
            "f1_weighted": 0.6227662570661346
          },
          {
            "accuracy": 0.631578947368421,
            "f1": 0.6089496677814221,
            "f1_weighted": 0.6341877819162626
          },
          {
            "accuracy": 0.617314313821938,
            "f1": 0.6108207954130555,
            "f1_weighted": 0.6217762714676973
          },
          {
            "accuracy": 0.6099360550909986,
            "f1": 0.6013485369405231,
            "f1_weighted": 0.6116830881284466
          }
        ]
      }
    ]
  },
  "task_name": "MassiveScenarioClassification"
}