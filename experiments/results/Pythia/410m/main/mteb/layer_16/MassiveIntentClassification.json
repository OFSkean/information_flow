{
  "dataset_revision": "4672e20407010da34463acc759c162ca9734bca6",
  "evaluation_time": 91.25930833816528,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.584969737726967,
        "f1": 0.5525015590302,
        "f1_weighted": 0.5887605869288001,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.584969737726967,
        "scores_per_experiment": [
          {
            "accuracy": 0.5911230665770006,
            "f1": 0.5667382722388593,
            "f1_weighted": 0.5952986088671504
          },
          {
            "accuracy": 0.5880968392737055,
            "f1": 0.5568536211363094,
            "f1_weighted": 0.5925127097801721
          },
          {
            "accuracy": 0.5843981170141224,
            "f1": 0.5481755263270585,
            "f1_weighted": 0.5866613712246941
          },
          {
            "accuracy": 0.6035642232683255,
            "f1": 0.5603586189336264,
            "f1_weighted": 0.6046490172067115
          },
          {
            "accuracy": 0.5877605917955615,
            "f1": 0.5460520535404962,
            "f1_weighted": 0.5904532636294044
          },
          {
            "accuracy": 0.5504371217215871,
            "f1": 0.5367967805688387,
            "f1_weighted": 0.5539281437871644
          },
          {
            "accuracy": 0.5766644250168124,
            "f1": 0.5513184647594005,
            "f1_weighted": 0.5787777555726544
          },
          {
            "accuracy": 0.589778076664425,
            "f1": 0.5498412286013535,
            "f1_weighted": 0.5987315128095461
          },
          {
            "accuracy": 0.5719569603227975,
            "f1": 0.544805434202684,
            "f1_weighted": 0.5779526909507829
          },
          {
            "accuracy": 0.6059179556153329,
            "f1": 0.5640755899933737,
            "f1_weighted": 0.608640795459721
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.5986227250368913,
        "f1": 0.5638747704662685,
        "f1_weighted": 0.601272141120645,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5986227250368913,
        "scores_per_experiment": [
          {
            "accuracy": 0.6060009837678307,
            "f1": 0.5782609946570124,
            "f1_weighted": 0.6070040362335343
          },
          {
            "accuracy": 0.6045253320216429,
            "f1": 0.5581525860028751,
            "f1_weighted": 0.6095289535539788
          },
          {
            "accuracy": 0.6182980816527299,
            "f1": 0.5840870354719431,
            "f1_weighted": 0.6173264345709705
          },
          {
            "accuracy": 0.602557796360059,
            "f1": 0.561438585543263,
            "f1_weighted": 0.6016977020643454
          },
          {
            "accuracy": 0.5922282341367437,
            "f1": 0.5545112338781688,
            "f1_weighted": 0.5953171468951227
          },
          {
            "accuracy": 0.5750122970978849,
            "f1": 0.5557385184191703,
            "f1_weighted": 0.5775410645518436
          },
          {
            "accuracy": 0.5789473684210527,
            "f1": 0.5548120076079932,
            "f1_weighted": 0.5803405926627649
          },
          {
            "accuracy": 0.5966551893753075,
            "f1": 0.552595921429588,
            "f1_weighted": 0.6020458453146978
          },
          {
            "accuracy": 0.5779636005902608,
            "f1": 0.5487581945248614,
            "f1_weighted": 0.5878073489240759
          },
          {
            "accuracy": 0.6340383669454008,
            "f1": 0.5903926271278098,
            "f1_weighted": 0.6341122864351154
          }
        ]
      }
    ]
  },
  "task_name": "MassiveIntentClassification"
}