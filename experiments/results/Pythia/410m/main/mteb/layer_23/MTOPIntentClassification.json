{
  "dataset_revision": "ae001d0e6b1228650b7bd1c2c65fb50ad11a8aba",
  "evaluation_time": 134.95495867729187,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.670018239854081,
        "f1": 0.43924693913007024,
        "f1_weighted": 0.7103340670047769,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.670018239854081,
        "scores_per_experiment": [
          {
            "accuracy": 0.6593707250341997,
            "f1": 0.4256355796549708,
            "f1_weighted": 0.6995111871476126
          },
          {
            "accuracy": 0.6880984952120383,
            "f1": 0.43926613150159605,
            "f1_weighted": 0.7264243753738777
          },
          {
            "accuracy": 0.6885544915640675,
            "f1": 0.4509488060872739,
            "f1_weighted": 0.7286796694417113
          },
          {
            "accuracy": 0.6782945736434108,
            "f1": 0.461534874699534,
            "f1_weighted": 0.718967949284142
          },
          {
            "accuracy": 0.677610579115367,
            "f1": 0.44838026230309896,
            "f1_weighted": 0.7142879624390588
          },
          {
            "accuracy": 0.6643866849065208,
            "f1": 0.43442861546336303,
            "f1_weighted": 0.7080888617545809
          },
          {
            "accuracy": 0.6561787505699954,
            "f1": 0.43495802816812373,
            "f1_weighted": 0.7022106041126879
          },
          {
            "accuracy": 0.677610579115367,
            "f1": 0.45538049140816456,
            "f1_weighted": 0.7170730050402835
          },
          {
            "accuracy": 0.6621067031463749,
            "f1": 0.4314563596198859,
            "f1_weighted": 0.6996862197861466
          },
          {
            "accuracy": 0.6479708162334701,
            "f1": 0.4104802423946919,
            "f1_weighted": 0.6884108356676677
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.6714541387024608,
        "f1": 0.4271273586676106,
        "f1_weighted": 0.7124769078545189,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.6714541387024608,
        "scores_per_experiment": [
          {
            "accuracy": 0.6514541387024608,
            "f1": 0.41118130811159004,
            "f1_weighted": 0.698983823320413
          },
          {
            "accuracy": 0.687248322147651,
            "f1": 0.43384165689775095,
            "f1_weighted": 0.7230400712700495
          },
          {
            "accuracy": 0.6841163310961969,
            "f1": 0.4198407092442915,
            "f1_weighted": 0.724015499409476
          },
          {
            "accuracy": 0.676510067114094,
            "f1": 0.44577310790839186,
            "f1_weighted": 0.7199935361554616
          },
          {
            "accuracy": 0.6697986577181209,
            "f1": 0.41953126506043004,
            "f1_weighted": 0.7054465867122128
          },
          {
            "accuracy": 0.6702460850111857,
            "f1": 0.42651787836597904,
            "f1_weighted": 0.7160989516045986
          },
          {
            "accuracy": 0.6612975391498881,
            "f1": 0.417646901665591,
            "f1_weighted": 0.7092688895331404
          },
          {
            "accuracy": 0.687248322147651,
            "f1": 0.4362666362019666,
            "f1_weighted": 0.7298215741982048
          },
          {
            "accuracy": 0.6662192393736018,
            "f1": 0.4270831128635733,
            "f1_weighted": 0.7008165742995416
          },
          {
            "accuracy": 0.6604026845637584,
            "f1": 0.4335910103565412,
            "f1_weighted": 0.6972835720420897
          }
        ]
      }
    ]
  },
  "task_name": "MTOPIntentClassification"
}