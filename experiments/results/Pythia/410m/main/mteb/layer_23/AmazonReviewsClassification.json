{
  "dataset_revision": "1399c76144fd37290681b995c656ef9b2e06e26d",
  "evaluation_time": 298.892906665802,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.33464,
        "f1": 0.33528308423862413,
        "f1_weighted": 0.3352830842386242,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.33464,
        "scores_per_experiment": [
          {
            "accuracy": 0.3534,
            "f1": 0.3543835048880343,
            "f1_weighted": 0.35438350488803433
          },
          {
            "accuracy": 0.3494,
            "f1": 0.35081773581004944,
            "f1_weighted": 0.3508177358100495
          },
          {
            "accuracy": 0.3236,
            "f1": 0.3191192525135255,
            "f1_weighted": 0.3191192525135255
          },
          {
            "accuracy": 0.3338,
            "f1": 0.33534006780952685,
            "f1_weighted": 0.33534006780952685
          },
          {
            "accuracy": 0.3524,
            "f1": 0.34869754965213035,
            "f1_weighted": 0.34869754965213035
          },
          {
            "accuracy": 0.3314,
            "f1": 0.3330365075760699,
            "f1_weighted": 0.33303650757606995
          },
          {
            "accuracy": 0.3102,
            "f1": 0.30919819176371677,
            "f1_weighted": 0.30919819176371677
          },
          {
            "accuracy": 0.3508,
            "f1": 0.35169195702731254,
            "f1_weighted": 0.3516919570273125
          },
          {
            "accuracy": 0.3262,
            "f1": 0.3266629592032101,
            "f1_weighted": 0.3266629592032101
          },
          {
            "accuracy": 0.3152,
            "f1": 0.3238831161426655,
            "f1_weighted": 0.3238831161426656
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.34006000000000003,
        "f1": 0.3405086016283613,
        "f1_weighted": 0.3405086016283613,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.34006000000000003,
        "scores_per_experiment": [
          {
            "accuracy": 0.3546,
            "f1": 0.3564273965470726,
            "f1_weighted": 0.35642739654707256
          },
          {
            "accuracy": 0.3536,
            "f1": 0.35469704965753274,
            "f1_weighted": 0.35469704965753274
          },
          {
            "accuracy": 0.3376,
            "f1": 0.334159760041109,
            "f1_weighted": 0.33415976004110903
          },
          {
            "accuracy": 0.3266,
            "f1": 0.3274635007292258,
            "f1_weighted": 0.32746350072922575
          },
          {
            "accuracy": 0.3552,
            "f1": 0.35012577722515353,
            "f1_weighted": 0.3501257772251536
          },
          {
            "accuracy": 0.3338,
            "f1": 0.3350651760080202,
            "f1_weighted": 0.3350651760080202
          },
          {
            "accuracy": 0.3284,
            "f1": 0.326690212352911,
            "f1_weighted": 0.326690212352911
          },
          {
            "accuracy": 0.3444,
            "f1": 0.34582704415670357,
            "f1_weighted": 0.34582704415670357
          },
          {
            "accuracy": 0.3436,
            "f1": 0.34419074450382076,
            "f1_weighted": 0.34419074450382076
          },
          {
            "accuracy": 0.3228,
            "f1": 0.33043935506206323,
            "f1_weighted": 0.33043935506206323
          }
        ]
      }
    ]
  },
  "task_name": "AmazonReviewsClassification"
}