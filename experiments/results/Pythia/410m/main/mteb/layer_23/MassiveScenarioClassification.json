{
  "dataset_revision": "fad2c6e8459f9e1c45d9315f4953d921437d70f8",
  "evaluation_time": 69.85008335113525,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.6191661062542031,
        "f1": 0.6108089592259882,
        "f1_weighted": 0.6224020795239381,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.6191661062542031,
        "scores_per_experiment": [
          {
            "accuracy": 0.6193678547410895,
            "f1": 0.6170177502424536,
            "f1_weighted": 0.6234038187165356
          },
          {
            "accuracy": 0.6217215870880969,
            "f1": 0.6125481206040768,
            "f1_weighted": 0.6160610894495397
          },
          {
            "accuracy": 0.6075991930060525,
            "f1": 0.607096886716648,
            "f1_weighted": 0.6107360670175116
          },
          {
            "accuracy": 0.6271015467383995,
            "f1": 0.6143517892470388,
            "f1_weighted": 0.629352044141744
          },
          {
            "accuracy": 0.6499663752521856,
            "f1": 0.628690206134338,
            "f1_weighted": 0.6537831707988746
          },
          {
            "accuracy": 0.6055817081371889,
            "f1": 0.5980007560910215,
            "f1_weighted": 0.6046494018161946
          },
          {
            "accuracy": 0.6086079354404842,
            "f1": 0.6000106439645962,
            "f1_weighted": 0.6144069165637973
          },
          {
            "accuracy": 0.636852723604573,
            "f1": 0.6238685279566532,
            "f1_weighted": 0.6439236391809126
          },
          {
            "accuracy": 0.6213853396099529,
            "f1": 0.6178210740846989,
            "f1_weighted": 0.6265998240716171
          },
          {
            "accuracy": 0.5934767989240081,
            "f1": 0.5886838372183576,
            "f1_weighted": 0.6011048234826548
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.6091982292179046,
        "f1": 0.6027620439707915,
        "f1_weighted": 0.6123822766688186,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.6091982292179046,
        "scores_per_experiment": [
          {
            "accuracy": 0.616822429906542,
            "f1": 0.6114179374234388,
            "f1_weighted": 0.6206758116162839
          },
          {
            "accuracy": 0.6045253320216429,
            "f1": 0.6027336144403017,
            "f1_weighted": 0.5998884310531754
          },
          {
            "accuracy": 0.6000983767830792,
            "f1": 0.5985296522619873,
            "f1_weighted": 0.6044219346212867
          },
          {
            "accuracy": 0.5981308411214953,
            "f1": 0.5894664074906113,
            "f1_weighted": 0.5988401422991879
          },
          {
            "accuracy": 0.6379734382685687,
            "f1": 0.6212233239222651,
            "f1_weighted": 0.6415702304743649
          },
          {
            "accuracy": 0.588293162813576,
            "f1": 0.5872973966534726,
            "f1_weighted": 0.5883594425657869
          },
          {
            "accuracy": 0.5941957697983276,
            "f1": 0.5873725513704281,
            "f1_weighted": 0.5998741599322817
          },
          {
            "accuracy": 0.631578947368421,
            "f1": 0.6183470677949539,
            "f1_weighted": 0.6377479707467891
          },
          {
            "accuracy": 0.6202656173143138,
            "f1": 0.6162501712959366,
            "f1_weighted": 0.6253527467975826
          },
          {
            "accuracy": 0.6000983767830792,
            "f1": 0.5949823170545187,
            "f1_weighted": 0.6070918965814477
          }
        ]
      }
    ]
  },
  "task_name": "MassiveScenarioClassification"
}