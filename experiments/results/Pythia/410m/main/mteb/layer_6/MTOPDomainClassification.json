{
  "dataset_revision": "d80d48c1eb48d3562165c59d59d0034df9fff0bf",
  "evaluation_time": 50.26783514022827,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.8312129502963976,
        "f1": 0.8249554736918773,
        "f1_weighted": 0.8317585392207313,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.8312129502963976,
        "scores_per_experiment": [
          {
            "accuracy": 0.8011855905152758,
            "f1": 0.7963721405631141,
            "f1_weighted": 0.8001777174692292
          },
          {
            "accuracy": 0.8331053351573188,
            "f1": 0.8280403375266316,
            "f1_weighted": 0.835382183683807
          },
          {
            "accuracy": 0.8328773369813042,
            "f1": 0.8226727038003674,
            "f1_weighted": 0.8339219651408157
          },
          {
            "accuracy": 0.837437300501596,
            "f1": 0.8286800830960803,
            "f1_weighted": 0.8380591589028661
          },
          {
            "accuracy": 0.8579571363429093,
            "f1": 0.8503893445209937,
            "f1_weighted": 0.8589187176831036
          },
          {
            "accuracy": 0.8415412676698586,
            "f1": 0.8383237645468843,
            "f1_weighted": 0.8413874351288909
          },
          {
            "accuracy": 0.8137254901960784,
            "f1": 0.8092570795299171,
            "f1_weighted": 0.8111957518147507
          },
          {
            "accuracy": 0.8305973552211582,
            "f1": 0.8244869503740415,
            "f1_weighted": 0.832471868386958
          },
          {
            "accuracy": 0.8301413588691291,
            "f1": 0.8260527335670621,
            "f1_weighted": 0.8316734748479488
          },
          {
            "accuracy": 0.833561331509348,
            "f1": 0.8252795993936798,
            "f1_weighted": 0.8343971191489431
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.8266219239373601,
        "f1": 0.8238716805844184,
        "f1_weighted": 0.8269525898263703,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.8266219239373601,
        "scores_per_experiment": [
          {
            "accuracy": 0.7897091722595079,
            "f1": 0.7907464081156674,
            "f1_weighted": 0.7868237066638504
          },
          {
            "accuracy": 0.8353467561521253,
            "f1": 0.8342266048256944,
            "f1_weighted": 0.8372105895755039
          },
          {
            "accuracy": 0.8313199105145413,
            "f1": 0.8235286639901257,
            "f1_weighted": 0.8316407322621789
          },
          {
            "accuracy": 0.8353467561521253,
            "f1": 0.8339150176151281,
            "f1_weighted": 0.8373321529997658
          },
          {
            "accuracy": 0.8523489932885906,
            "f1": 0.8496114655562174,
            "f1_weighted": 0.8534217454913077
          },
          {
            "accuracy": 0.8313199105145413,
            "f1": 0.8303563980954334,
            "f1_weighted": 0.8308493429460966
          },
          {
            "accuracy": 0.8013422818791947,
            "f1": 0.7945706521228814,
            "f1_weighted": 0.7996362045970236
          },
          {
            "accuracy": 0.8313199105145413,
            "f1": 0.8302457557314789,
            "f1_weighted": 0.8327845468861412
          },
          {
            "accuracy": 0.8263982102908277,
            "f1": 0.8249486961978036,
            "f1_weighted": 0.8275424336616618
          },
          {
            "accuracy": 0.8317673378076063,
            "f1": 0.8265671435937538,
            "f1_weighted": 0.8322844431801742
          }
        ]
      }
    ]
  },
  "task_name": "MTOPDomainClassification"
}