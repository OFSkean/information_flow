{
  "dataset_revision": "4672e20407010da34463acc759c162ca9734bca6",
  "evaluation_time": 84.23174500465393,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.5870544720914592,
        "f1": 0.5543608068268352,
        "f1_weighted": 0.5901160009851267,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5870544720914592,
        "scores_per_experiment": [
          {
            "accuracy": 0.5911230665770006,
            "f1": 0.5676910379909673,
            "f1_weighted": 0.5941904908377907
          },
          {
            "accuracy": 0.5978480161398789,
            "f1": 0.5605648800002353,
            "f1_weighted": 0.6021120594905145
          },
          {
            "accuracy": 0.5877605917955615,
            "f1": 0.5514099157592096,
            "f1_weighted": 0.5893237109796857
          },
          {
            "accuracy": 0.5951580363147276,
            "f1": 0.550150968695092,
            "f1_weighted": 0.598871675764325
          },
          {
            "accuracy": 0.601546738399462,
            "f1": 0.5620029343897013,
            "f1_weighted": 0.6016264983473626
          },
          {
            "accuracy": 0.558843308675185,
            "f1": 0.5412025679059644,
            "f1_weighted": 0.561074958902072
          },
          {
            "accuracy": 0.5776731674512441,
            "f1": 0.5468871664367129,
            "f1_weighted": 0.5798968517324874
          },
          {
            "accuracy": 0.5810356422326832,
            "f1": 0.5431475179721134,
            "f1_weighted": 0.5883317249191325
          },
          {
            "accuracy": 0.5753194351042367,
            "f1": 0.5518638373275913,
            "f1_weighted": 0.5794532621246582
          },
          {
            "accuracy": 0.6042367182246133,
            "f1": 0.5686872417907638,
            "f1_weighted": 0.6062787767532378
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.596212493851451,
        "f1": 0.5625661344288939,
        "f1_weighted": 0.59893316906638,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.596212493851451,
        "scores_per_experiment": [
          {
            "accuracy": 0.5932120019675357,
            "f1": 0.5650124327111496,
            "f1_weighted": 0.5971344348370221
          },
          {
            "accuracy": 0.6069847515986228,
            "f1": 0.5594643265042785,
            "f1_weighted": 0.6110819971068413
          },
          {
            "accuracy": 0.6192818494835219,
            "f1": 0.5802157501925177,
            "f1_weighted": 0.6208913722435396
          },
          {
            "accuracy": 0.6055090998524348,
            "f1": 0.5618017536736669,
            "f1_weighted": 0.6079677822368047
          },
          {
            "accuracy": 0.6114117068371864,
            "f1": 0.5785243938524672,
            "f1_weighted": 0.6095428045111966
          },
          {
            "accuracy": 0.5700934579439252,
            "f1": 0.5547599564477543,
            "f1_weighted": 0.5733623184005389
          },
          {
            "accuracy": 0.5779636005902608,
            "f1": 0.5526308377394675,
            "f1_weighted": 0.5814101133918799
          },
          {
            "accuracy": 0.5745204131824889,
            "f1": 0.5405464729541305,
            "f1_weighted": 0.5795558391638912
          },
          {
            "accuracy": 0.5764879488440728,
            "f1": 0.5460422278383672,
            "f1_weighted": 0.5824698256406377
          },
          {
            "accuracy": 0.6266601082144614,
            "f1": 0.5866631923751411,
            "f1_weighted": 0.6259152031314472
          }
        ]
      }
    ]
  },
  "task_name": "MassiveIntentClassification"
}