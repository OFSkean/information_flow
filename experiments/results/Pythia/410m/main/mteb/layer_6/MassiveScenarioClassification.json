{
  "dataset_revision": "fad2c6e8459f9e1c45d9315f4953d921437d70f8",
  "evaluation_time": 65.00068402290344,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.6173839946200402,
        "f1": 0.599603649084718,
        "f1_weighted": 0.6194307736199671,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.6173839946200402,
        "scores_per_experiment": [
          {
            "accuracy": 0.6318090114324143,
            "f1": 0.6167873482539351,
            "f1_weighted": 0.6353749844303885
          },
          {
            "accuracy": 0.6462676529926026,
            "f1": 0.6255463216073518,
            "f1_weighted": 0.6481693323403872
          },
          {
            "accuracy": 0.6062542030934768,
            "f1": 0.5983726781541671,
            "f1_weighted": 0.6093699121146723
          },
          {
            "accuracy": 0.6028917283120376,
            "f1": 0.5844078272140627,
            "f1_weighted": 0.605522377821316
          },
          {
            "accuracy": 0.6422326832548756,
            "f1": 0.6090798551398505,
            "f1_weighted": 0.6404007317504727
          },
          {
            "accuracy": 0.601546738399462,
            "f1": 0.5821406707521579,
            "f1_weighted": 0.6017587600516273
          },
          {
            "accuracy": 0.6180228648285138,
            "f1": 0.5975370133167588,
            "f1_weighted": 0.6230598956186333
          },
          {
            "accuracy": 0.6334902488231339,
            "f1": 0.6177163693720706,
            "f1_weighted": 0.6360394679640113
          },
          {
            "accuracy": 0.6059179556153329,
            "f1": 0.5886529582757265,
            "f1_weighted": 0.6056915616585618
          },
          {
            "accuracy": 0.5854068594485541,
            "f1": 0.5757954487610992,
            "f1_weighted": 0.5889207124496003
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.6083128381701919,
        "f1": 0.5983781484077881,
        "f1_weighted": 0.6098641181673442,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.6083128381701919,
        "scores_per_experiment": [
          {
            "accuracy": 0.6330545991146089,
            "f1": 0.6285782561443755,
            "f1_weighted": 0.6367902169115811
          },
          {
            "accuracy": 0.6276438760452533,
            "f1": 0.6267010254666093,
            "f1_weighted": 0.629629967799826
          },
          {
            "accuracy": 0.5937038858829317,
            "f1": 0.5907733145002666,
            "f1_weighted": 0.5956639062270295
          },
          {
            "accuracy": 0.5823905558288244,
            "f1": 0.5701534559730044,
            "f1_weighted": 0.5848590155542984
          },
          {
            "accuracy": 0.6330545991146089,
            "f1": 0.6123694518529147,
            "f1_weighted": 0.6296820591454815
          },
          {
            "accuracy": 0.5897688145597639,
            "f1": 0.5814467884385117,
            "f1_weighted": 0.5886287086315071
          },
          {
            "accuracy": 0.6005902606984752,
            "f1": 0.5902899211207884,
            "f1_weighted": 0.6070699667606542
          },
          {
            "accuracy": 0.6148548942449582,
            "f1": 0.5984554446021414,
            "f1_weighted": 0.6191606655552249
          },
          {
            "accuracy": 0.6040334481062469,
            "f1": 0.5909118281201438,
            "f1_weighted": 0.6024115612429385
          },
          {
            "accuracy": 0.6040334481062469,
            "f1": 0.5941019978591263,
            "f1_weighted": 0.6047451138449016
          }
        ]
      }
    ]
  },
  "task_name": "MassiveScenarioClassification"
}