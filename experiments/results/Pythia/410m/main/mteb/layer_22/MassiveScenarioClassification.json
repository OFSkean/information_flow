{
  "dataset_revision": "fad2c6e8459f9e1c45d9315f4953d921437d70f8",
  "evaluation_time": 67.44825911521912,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.6306657700067249,
        "f1": 0.6232609946289434,
        "f1_weighted": 0.633985128330595,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.6306657700067249,
        "scores_per_experiment": [
          {
            "accuracy": 0.6361802286482852,
            "f1": 0.6357905876969626,
            "f1_weighted": 0.6399757119249884
          },
          {
            "accuracy": 0.6331540013449899,
            "f1": 0.6217402199073547,
            "f1_weighted": 0.6287701368516596
          },
          {
            "accuracy": 0.6166778749159382,
            "f1": 0.6198080996808607,
            "f1_weighted": 0.6220046477891369
          },
          {
            "accuracy": 0.6422326832548756,
            "f1": 0.630523272502103,
            "f1_weighted": 0.6448837804439539
          },
          {
            "accuracy": 0.6650975117686617,
            "f1": 0.6441524969860549,
            "f1_weighted": 0.6681097537997993
          },
          {
            "accuracy": 0.6163416274377942,
            "f1": 0.6079933599974185,
            "f1_weighted": 0.6155590404753372
          },
          {
            "accuracy": 0.6126429051782112,
            "f1": 0.6028084358574552,
            "f1_weighted": 0.6169574145489797
          },
          {
            "accuracy": 0.6472763954270343,
            "f1": 0.6369094135144885,
            "f1_weighted": 0.6531420899076179
          },
          {
            "accuracy": 0.6334902488231339,
            "f1": 0.6323518929925156,
            "f1_weighted": 0.6394019411326739
          },
          {
            "accuracy": 0.6035642232683255,
            "f1": 0.6005321671542193,
            "f1_weighted": 0.6110467664318046
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.6232661091982291,
        "f1": 0.6179414602269322,
        "f1_weighted": 0.6262735426777414,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.6232661091982291,
        "scores_per_experiment": [
          {
            "accuracy": 0.6350221347761928,
            "f1": 0.6313244001404424,
            "f1_weighted": 0.6387052574161884
          },
          {
            "accuracy": 0.6232169208066897,
            "f1": 0.621197466551299,
            "f1_weighted": 0.6202061103913501
          },
          {
            "accuracy": 0.6050172159370388,
            "f1": 0.6056533750652877,
            "f1_weighted": 0.6097872825567021
          },
          {
            "accuracy": 0.6133792424987703,
            "f1": 0.6056282119708722,
            "f1_weighted": 0.6145413024435407
          },
          {
            "accuracy": 0.6517461878996557,
            "f1": 0.6366030924179511,
            "f1_weighted": 0.6537924208742141
          },
          {
            "accuracy": 0.6069847515986228,
            "f1": 0.6063166783140931,
            "f1_weighted": 0.6079428796220836
          },
          {
            "accuracy": 0.6094441711756026,
            "f1": 0.6005954676113139,
            "f1_weighted": 0.6137299514429205
          },
          {
            "accuracy": 0.6409247417609444,
            "f1": 0.6329099769278007,
            "f1_weighted": 0.6456984728690892
          },
          {
            "accuracy": 0.6310870634530251,
            "f1": 0.6262851879134349,
            "f1_weighted": 0.6360075940525878
          },
          {
            "accuracy": 0.6158386620757501,
            "f1": 0.6129007453568263,
            "f1_weighted": 0.6223241551087373
          }
        ]
      }
    ]
  },
  "task_name": "MassiveScenarioClassification"
}