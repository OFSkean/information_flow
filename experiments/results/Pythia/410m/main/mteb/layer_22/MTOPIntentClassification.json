{
  "dataset_revision": "ae001d0e6b1228650b7bd1c2c65fb50ad11a8aba",
  "evaluation_time": 136.4537513256073,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.6746922024623803,
        "f1": 0.44383159692278545,
        "f1_weighted": 0.7141228505881719,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.6746922024623803,
        "scores_per_experiment": [
          {
            "accuracy": 0.6639306885544916,
            "f1": 0.4324608484253951,
            "f1_weighted": 0.7038104182334878
          },
          {
            "accuracy": 0.68718650250798,
            "f1": 0.4427447429949548,
            "f1_weighted": 0.7252016747645034
          },
          {
            "accuracy": 0.6937984496124031,
            "f1": 0.45208811994523257,
            "f1_weighted": 0.7327919168850904
          },
          {
            "accuracy": 0.6778385772913816,
            "f1": 0.46600011311705786,
            "f1_weighted": 0.7179389984607961
          },
          {
            "accuracy": 0.6846785225718194,
            "f1": 0.44824928376781026,
            "f1_weighted": 0.7191418638296491
          },
          {
            "accuracy": 0.6714546283629731,
            "f1": 0.43855032382496045,
            "f1_weighted": 0.7152431058476889
          },
          {
            "accuracy": 0.65640674874601,
            "f1": 0.4410307411342953,
            "f1_weighted": 0.7026672962669487
          },
          {
            "accuracy": 0.6880984952120383,
            "f1": 0.46160478218662576,
            "f1_weighted": 0.7279460737439097
          },
          {
            "accuracy": 0.6668946648426812,
            "f1": 0.4403923913013263,
            "f1_weighted": 0.7020338732666878
          },
          {
            "accuracy": 0.6566347469220246,
            "f1": 0.41519462253019596,
            "f1_weighted": 0.6944532845829574
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.6756599552572707,
        "f1": 0.42773694840302473,
        "f1_weighted": 0.7173625098101897,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.6756599552572707,
        "scores_per_experiment": [
          {
            "accuracy": 0.6563758389261745,
            "f1": 0.4041166845949024,
            "f1_weighted": 0.7022887705023274
          },
          {
            "accuracy": 0.6935123042505593,
            "f1": 0.4411352653796912,
            "f1_weighted": 0.7315063088529926
          },
          {
            "accuracy": 0.6814317673378076,
            "f1": 0.4202398692018586,
            "f1_weighted": 0.72464200229457
          },
          {
            "accuracy": 0.672930648769575,
            "f1": 0.44760488959285033,
            "f1_weighted": 0.7166036347410519
          },
          {
            "accuracy": 0.6778523489932886,
            "f1": 0.4124917026847801,
            "f1_weighted": 0.713378902081714
          },
          {
            "accuracy": 0.6697986577181209,
            "f1": 0.43056469619492116,
            "f1_weighted": 0.7178654500458631
          },
          {
            "accuracy": 0.6630872483221476,
            "f1": 0.41219804754074557,
            "f1_weighted": 0.7097034701085211
          },
          {
            "accuracy": 0.698434004474273,
            "f1": 0.4321023757150208,
            "f1_weighted": 0.7399015593012607
          },
          {
            "accuracy": 0.6706935123042506,
            "f1": 0.43598437281290714,
            "f1_weighted": 0.7085619676731083
          },
          {
            "accuracy": 0.6724832214765101,
            "f1": 0.4409315803125698,
            "f1_weighted": 0.7091730325004878
          }
        ]
      }
    ]
  },
  "task_name": "MTOPIntentClassification"
}