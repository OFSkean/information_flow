{
  "dataset_revision": "d80d48c1eb48d3562165c59d59d0034df9fff0bf",
  "evaluation_time": 55.662301778793335,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.8378020975832193,
        "f1": 0.8336936335688263,
        "f1_weighted": 0.8381511368603576,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.8378020975832193,
        "scores_per_experiment": [
          {
            "accuracy": 0.808937528499772,
            "f1": 0.8035990375669044,
            "f1_weighted": 0.8080609001059497
          },
          {
            "accuracy": 0.863657090743274,
            "f1": 0.859699690455884,
            "f1_weighted": 0.863649867849259
          },
          {
            "accuracy": 0.8315093479252166,
            "f1": 0.8236934704152982,
            "f1_weighted": 0.8331835999255908
          },
          {
            "accuracy": 0.8495212038303693,
            "f1": 0.8436763137380404,
            "f1_weighted": 0.8501503683804625
          },
          {
            "accuracy": 0.8638850889192886,
            "f1": 0.8615293051464942,
            "f1_weighted": 0.8645804364555237
          },
          {
            "accuracy": 0.8319653442772458,
            "f1": 0.830131154095922,
            "f1_weighted": 0.8321478403218512
          },
          {
            "accuracy": 0.8205654354765162,
            "f1": 0.8166057904135521,
            "f1_weighted": 0.8180307532554886
          },
          {
            "accuracy": 0.8390332877336981,
            "f1": 0.8336322392342919,
            "f1_weighted": 0.8416592076155544
          },
          {
            "accuracy": 0.841313269493844,
            "f1": 0.8402997378238836,
            "f1_weighted": 0.8418721221740819
          },
          {
            "accuracy": 0.8276333789329685,
            "f1": 0.8240695967979913,
            "f1_weighted": 0.828176272519814
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.8342729306487696,
        "f1": 0.8340433041643408,
        "f1_weighted": 0.8343898328194204,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.8342729306487696,
        "scores_per_experiment": [
          {
            "accuracy": 0.7982102908277405,
            "f1": 0.8008546104244277,
            "f1_weighted": 0.7966654942574796
          },
          {
            "accuracy": 0.8662192393736018,
            "f1": 0.8663073312172618,
            "f1_weighted": 0.8663043684424621
          },
          {
            "accuracy": 0.8295302013422818,
            "f1": 0.8234316349271134,
            "f1_weighted": 0.8311427969069789
          },
          {
            "accuracy": 0.8523489932885906,
            "f1": 0.8524109641679428,
            "f1_weighted": 0.8525935419512752
          },
          {
            "accuracy": 0.8527964205816555,
            "f1": 0.8567012511480989,
            "f1_weighted": 0.854041817978938
          },
          {
            "accuracy": 0.8210290827740492,
            "f1": 0.821247719104829,
            "f1_weighted": 0.8204688094676631
          },
          {
            "accuracy": 0.8134228187919463,
            "f1": 0.8117640825034438,
            "f1_weighted": 0.8113166105331606
          },
          {
            "accuracy": 0.8407158836689038,
            "f1": 0.8400737934405118,
            "f1_weighted": 0.8425942433731608
          },
          {
            "accuracy": 0.843400447427293,
            "f1": 0.8420870150773664,
            "f1_weighted": 0.8440694529891891
          },
          {
            "accuracy": 0.8250559284116331,
            "f1": 0.8255546396324132,
            "f1_weighted": 0.8247011922938969
          }
        ]
      }
    ]
  },
  "task_name": "MTOPDomainClassification"
}