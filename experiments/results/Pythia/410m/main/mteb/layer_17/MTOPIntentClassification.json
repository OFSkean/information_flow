{
  "dataset_revision": "ae001d0e6b1228650b7bd1c2c65fb50ad11a8aba",
  "evaluation_time": 130.04754328727722,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.6978796169630643,
        "f1": 0.47973336655455584,
        "f1_weighted": 0.7353367151834493,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.6978796169630643,
        "scores_per_experiment": [
          {
            "accuracy": 0.6803465572275422,
            "f1": 0.4580266023114212,
            "f1_weighted": 0.7201691748332725
          },
          {
            "accuracy": 0.7033743730050159,
            "f1": 0.4811181115408507,
            "f1_weighted": 0.7417457652177721
          },
          {
            "accuracy": 0.7070223438212494,
            "f1": 0.4821001457018814,
            "f1_weighted": 0.7454096477309328
          },
          {
            "accuracy": 0.6988144094847242,
            "f1": 0.4804561524861449,
            "f1_weighted": 0.7380927742284861
          },
          {
            "accuracy": 0.6985864113087096,
            "f1": 0.4886399039054553,
            "f1_weighted": 0.7320170911489666
          },
          {
            "accuracy": 0.6953944368445052,
            "f1": 0.4794625418776037,
            "f1_weighted": 0.737160387615388
          },
          {
            "accuracy": 0.6926584587323301,
            "f1": 0.4965324898401358,
            "f1_weighted": 0.7333894913946162
          },
          {
            "accuracy": 0.7163702690378477,
            "f1": 0.4972579880749872,
            "f1_weighted": 0.7501022300571103
          },
          {
            "accuracy": 0.691062471500228,
            "f1": 0.4715779756375087,
            "f1_weighted": 0.7244257015072133
          },
          {
            "accuracy": 0.6951664386684907,
            "f1": 0.4621617541695694,
            "f1_weighted": 0.7308548881007361
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.698076062639821,
        "f1": 0.4632754138561658,
        "f1_weighted": 0.7380180807012713,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.698076062639821,
        "scores_per_experiment": [
          {
            "accuracy": 0.6733780760626398,
            "f1": 0.44224129521639055,
            "f1_weighted": 0.7141784980774306
          },
          {
            "accuracy": 0.7176733780760627,
            "f1": 0.4784950455612822,
            "f1_weighted": 0.7575097753181542
          },
          {
            "accuracy": 0.7006711409395974,
            "f1": 0.4527111955481343,
            "f1_weighted": 0.7421266446303859
          },
          {
            "accuracy": 0.6953020134228188,
            "f1": 0.45974755328515865,
            "f1_weighted": 0.737877268461304
          },
          {
            "accuracy": 0.6948545861297539,
            "f1": 0.4660606879134673,
            "f1_weighted": 0.7304415652069187
          },
          {
            "accuracy": 0.6997762863534676,
            "f1": 0.468621897822045,
            "f1_weighted": 0.7460078213371094
          },
          {
            "accuracy": 0.6836689038031319,
            "f1": 0.45357739628527577,
            "f1_weighted": 0.7253460335248727
          },
          {
            "accuracy": 0.723489932885906,
            "f1": 0.4838125219347119,
            "f1_weighted": 0.7612636270457243
          },
          {
            "accuracy": 0.69082774049217,
            "f1": 0.45771621559032016,
            "f1_weighted": 0.7272627682364587
          },
          {
            "accuracy": 0.7011185682326622,
            "f1": 0.46977032940487207,
            "f1_weighted": 0.7381668051743547
          }
        ]
      }
    ]
  },
  "task_name": "MTOPIntentClassification"
}