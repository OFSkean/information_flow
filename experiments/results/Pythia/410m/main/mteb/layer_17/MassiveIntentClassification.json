{
  "dataset_revision": "4672e20407010da34463acc759c162ca9734bca6",
  "evaluation_time": 92.62302303314209,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.5946872898453262,
        "f1": 0.5627077583892824,
        "f1_weighted": 0.5985901559418417,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5946872898453262,
        "scores_per_experiment": [
          {
            "accuracy": 0.5968392737054472,
            "f1": 0.5742317870885475,
            "f1_weighted": 0.5995327724543564
          },
          {
            "accuracy": 0.5971755211835911,
            "f1": 0.5665692174512443,
            "f1_weighted": 0.6026449398505072
          },
          {
            "accuracy": 0.597511768661735,
            "f1": 0.5612338142169558,
            "f1_weighted": 0.6003871353048107
          },
          {
            "accuracy": 0.613315400134499,
            "f1": 0.5751766784128161,
            "f1_weighted": 0.6141783236508968
          },
          {
            "accuracy": 0.5995292535305985,
            "f1": 0.5569857661000385,
            "f1_weighted": 0.6011670047533019
          },
          {
            "accuracy": 0.5558170813718897,
            "f1": 0.5418922686553584,
            "f1_weighted": 0.5580971087233353
          },
          {
            "accuracy": 0.5806993947545394,
            "f1": 0.5570995697869514,
            "f1_weighted": 0.5834223314410273
          },
          {
            "accuracy": 0.597511768661735,
            "f1": 0.5580428512700255,
            "f1_weighted": 0.6070877445744252
          },
          {
            "accuracy": 0.5894418291862811,
            "f1": 0.5582436755518418,
            "f1_weighted": 0.596410232612938
          },
          {
            "accuracy": 0.6190316072629455,
            "f1": 0.5776019553590451,
            "f1_weighted": 0.622973966052818
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.6045745204131825,
        "f1": 0.5673410050860517,
        "f1_weighted": 0.6068917356033572,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.6045745204131825,
        "scores_per_experiment": [
          {
            "accuracy": 0.6123954746679784,
            "f1": 0.5769627147020048,
            "f1_weighted": 0.6119259191696995
          },
          {
            "accuracy": 0.6123954746679784,
            "f1": 0.5693890907668588,
            "f1_weighted": 0.6174410093763274
          },
          {
            "accuracy": 0.6296114117068372,
            "f1": 0.594691533210828,
            "f1_weighted": 0.6303873100239231
          },
          {
            "accuracy": 0.6000983767830792,
            "f1": 0.5553226585877283,
            "f1_weighted": 0.5983178467613093
          },
          {
            "accuracy": 0.602557796360059,
            "f1": 0.5640017560819273,
            "f1_weighted": 0.6049162653724183
          },
          {
            "accuracy": 0.5804230201672406,
            "f1": 0.5619774022010626,
            "f1_weighted": 0.581081429399107
          },
          {
            "accuracy": 0.5814067879980325,
            "f1": 0.5548844853933862,
            "f1_weighted": 0.5839624858424551
          },
          {
            "accuracy": 0.6045253320216429,
            "f1": 0.5552796114574416,
            "f1_weighted": 0.6092571434333098
          },
          {
            "accuracy": 0.5848499754058042,
            "f1": 0.5511349538327123,
            "f1_weighted": 0.5917699214109647
          },
          {
            "accuracy": 0.6374815543531727,
            "f1": 0.5897658446265667,
            "f1_weighted": 0.639858025244058
          }
        ]
      }
    ]
  },
  "task_name": "MassiveIntentClassification"
}