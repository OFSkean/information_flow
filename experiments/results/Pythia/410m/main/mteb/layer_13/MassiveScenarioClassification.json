{
  "dataset_revision": "fad2c6e8459f9e1c45d9315f4953d921437d70f8",
  "evaluation_time": 69.1234061717987,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.636247478143914,
        "f1": 0.6203518625495535,
        "f1_weighted": 0.638080036188176,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.636247478143914,
        "scores_per_experiment": [
          {
            "accuracy": 0.6536650975117687,
            "f1": 0.6469805930744308,
            "f1_weighted": 0.6557741473903514
          },
          {
            "accuracy": 0.6526563550773369,
            "f1": 0.6429326547295264,
            "f1_weighted": 0.6530939301501432
          },
          {
            "accuracy": 0.6217215870880969,
            "f1": 0.6152683078437485,
            "f1_weighted": 0.6245225435383062
          },
          {
            "accuracy": 0.6314727639542703,
            "f1": 0.6080841578955085,
            "f1_weighted": 0.6341622142137259
          },
          {
            "accuracy": 0.660390047074647,
            "f1": 0.6292934558834502,
            "f1_weighted": 0.6594967902354691
          },
          {
            "accuracy": 0.6163416274377942,
            "f1": 0.6017041405743558,
            "f1_weighted": 0.6152537543812678
          },
          {
            "accuracy": 0.6338264963012777,
            "f1": 0.6137986106911683,
            "f1_weighted": 0.6384455261566631
          },
          {
            "accuracy": 0.6479488903833222,
            "f1": 0.6276884566648587,
            "f1_weighted": 0.6504348201916776
          },
          {
            "accuracy": 0.6355077336919973,
            "f1": 0.6186951538659009,
            "f1_weighted": 0.6380135247850838
          },
          {
            "accuracy": 0.6089441829186281,
            "f1": 0.5990730942725871,
            "f1_weighted": 0.6116031108390722
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.6291195277914412,
        "f1": 0.6175088896133215,
        "f1_weighted": 0.6306293521936392,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.6291195277914412,
        "scores_per_experiment": [
          {
            "accuracy": 0.647319232661092,
            "f1": 0.6379704155023919,
            "f1_weighted": 0.6502728623280972
          },
          {
            "accuracy": 0.6350221347761928,
            "f1": 0.6315227704879486,
            "f1_weighted": 0.6365927656367669
          },
          {
            "accuracy": 0.6217412690605018,
            "f1": 0.6208691881462838,
            "f1_weighted": 0.6239257336327206
          },
          {
            "accuracy": 0.6079685194294147,
            "f1": 0.5874341012281235,
            "f1_weighted": 0.6087836076212285
          },
          {
            "accuracy": 0.6660108214461387,
            "f1": 0.6390123026792873,
            "f1_weighted": 0.664816343293469
          },
          {
            "accuracy": 0.6069847515986228,
            "f1": 0.6073902252683905,
            "f1_weighted": 0.6030408142371507
          },
          {
            "accuracy": 0.6291195277914412,
            "f1": 0.6117376496661175,
            "f1_weighted": 0.6341336446984174
          },
          {
            "accuracy": 0.6296114117068372,
            "f1": 0.6145528646250966,
            "f1_weighted": 0.6320471978298289
          },
          {
            "accuracy": 0.6197737333989178,
            "f1": 0.6089984153602441,
            "f1_weighted": 0.6237312059603243
          },
          {
            "accuracy": 0.6276438760452533,
            "f1": 0.6156009631693325,
            "f1_weighted": 0.6289493466983886
          }
        ]
      }
    ]
  },
  "task_name": "MassiveScenarioClassification"
}