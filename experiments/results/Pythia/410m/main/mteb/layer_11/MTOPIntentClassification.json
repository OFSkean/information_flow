{
  "dataset_revision": "ae001d0e6b1228650b7bd1c2c65fb50ad11a8aba",
  "evaluation_time": 117.05845046043396,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.7105335157318742,
        "f1": 0.4886551357910368,
        "f1_weighted": 0.746250020161208,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.7105335157318742,
        "scores_per_experiment": [
          {
            "accuracy": 0.7033743730050159,
            "f1": 0.47034500318048655,
            "f1_weighted": 0.7388574163443096
          },
          {
            "accuracy": 0.7120383036935705,
            "f1": 0.48374276717856807,
            "f1_weighted": 0.7495676249589474
          },
          {
            "accuracy": 0.7024623803009576,
            "f1": 0.4860955116527333,
            "f1_weighted": 0.7396616936250384
          },
          {
            "accuracy": 0.7097583219334246,
            "f1": 0.5026697430284198,
            "f1_weighted": 0.7471999017481248
          },
          {
            "accuracy": 0.70953032375741,
            "f1": 0.4832548962093645,
            "f1_weighted": 0.7437993018428976
          },
          {
            "accuracy": 0.7200182398540812,
            "f1": 0.48528910169766243,
            "f1_weighted": 0.7572426739973237
          },
          {
            "accuracy": 0.70109439124487,
            "f1": 0.5035527477924665,
            "f1_weighted": 0.7380905068535959
          },
          {
            "accuracy": 0.7282261741906064,
            "f1": 0.49709943633284304,
            "f1_weighted": 0.7614348032197921
          },
          {
            "accuracy": 0.7099863201094391,
            "f1": 0.4860071438041463,
            "f1_weighted": 0.7428907391074385
          },
          {
            "accuracy": 0.7088463292293662,
            "f1": 0.48849500703367743,
            "f1_weighted": 0.7437555399146119
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.7146308724832215,
        "f1": 0.4782433662114366,
        "f1_weighted": 0.7518877374425424,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.7146308724832215,
        "scores_per_experiment": [
          {
            "accuracy": 0.697986577181208,
            "f1": 0.46055353382336023,
            "f1_weighted": 0.7357407764520941
          },
          {
            "accuracy": 0.7176733780760627,
            "f1": 0.4700059866340916,
            "f1_weighted": 0.7568343058754773
          },
          {
            "accuracy": 0.7158836689038032,
            "f1": 0.48729574295915595,
            "f1_weighted": 0.7529817604093382
          },
          {
            "accuracy": 0.716331096196868,
            "f1": 0.4880235231897496,
            "f1_weighted": 0.7559246345095391
          },
          {
            "accuracy": 0.7109619686800895,
            "f1": 0.4731634109329167,
            "f1_weighted": 0.7453856343509945
          },
          {
            "accuracy": 0.7194630872483222,
            "f1": 0.4759894285169172,
            "f1_weighted": 0.759321906027739
          },
          {
            "accuracy": 0.7002237136465325,
            "f1": 0.4830420633440381,
            "f1_weighted": 0.7400224438012839
          },
          {
            "accuracy": 0.7315436241610739,
            "f1": 0.49038635202115777,
            "f1_weighted": 0.7671739605942607
          },
          {
            "accuracy": 0.7194630872483222,
            "f1": 0.4729290756421131,
            "f1_weighted": 0.751787644237589
          },
          {
            "accuracy": 0.7167785234899329,
            "f1": 0.4810445450508663,
            "f1_weighted": 0.7537043081671089
          }
        ]
      }
    ]
  },
  "task_name": "MTOPIntentClassification"
}