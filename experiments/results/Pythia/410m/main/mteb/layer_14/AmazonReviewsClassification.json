{
  "dataset_revision": "1399c76144fd37290681b995c656ef9b2e06e26d",
  "evaluation_time": 206.85855531692505,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.38428000000000007,
        "f1": 0.38117392733868977,
        "f1_weighted": 0.38117392733868977,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.38428000000000007,
        "scores_per_experiment": [
          {
            "accuracy": 0.3948,
            "f1": 0.3956515667725056,
            "f1_weighted": 0.3956515667725056
          },
          {
            "accuracy": 0.408,
            "f1": 0.40829618586439287,
            "f1_weighted": 0.4082961858643929
          },
          {
            "accuracy": 0.3734,
            "f1": 0.36402637665402454,
            "f1_weighted": 0.36402637665402454
          },
          {
            "accuracy": 0.3988,
            "f1": 0.3975726532140125,
            "f1_weighted": 0.3975726532140125
          },
          {
            "accuracy": 0.4174,
            "f1": 0.4012637499985089,
            "f1_weighted": 0.4012637499985089
          },
          {
            "accuracy": 0.3522,
            "f1": 0.35487279379867503,
            "f1_weighted": 0.35487279379867503
          },
          {
            "accuracy": 0.3624,
            "f1": 0.3621146761840583,
            "f1_weighted": 0.3621146761840583
          },
          {
            "accuracy": 0.379,
            "f1": 0.37036045115225813,
            "f1_weighted": 0.37036045115225813
          },
          {
            "accuracy": 0.3832,
            "f1": 0.3842250634203707,
            "f1_weighted": 0.3842250634203707
          },
          {
            "accuracy": 0.3736,
            "f1": 0.3733557563280912,
            "f1_weighted": 0.3733557563280912
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.38871999999999995,
        "f1": 0.38489015569436585,
        "f1_weighted": 0.3848901556943658,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.38871999999999995,
        "scores_per_experiment": [
          {
            "accuracy": 0.3946,
            "f1": 0.3958337582599022,
            "f1_weighted": 0.39583375825990214
          },
          {
            "accuracy": 0.4082,
            "f1": 0.4082085459838665,
            "f1_weighted": 0.4082085459838665
          },
          {
            "accuracy": 0.3842,
            "f1": 0.3757698402228636,
            "f1_weighted": 0.3757698402228636
          },
          {
            "accuracy": 0.3976,
            "f1": 0.3974472363415708,
            "f1_weighted": 0.3974472363415708
          },
          {
            "accuracy": 0.4194,
            "f1": 0.3967471059686088,
            "f1_weighted": 0.3967471059686088
          },
          {
            "accuracy": 0.3468,
            "f1": 0.3489409163119759,
            "f1_weighted": 0.3489409163119759
          },
          {
            "accuracy": 0.3746,
            "f1": 0.3728187515852064,
            "f1_weighted": 0.37281875158520644
          },
          {
            "accuracy": 0.3738,
            "f1": 0.36502351262248595,
            "f1_weighted": 0.36502351262248595
          },
          {
            "accuracy": 0.3992,
            "f1": 0.4007821233775289,
            "f1_weighted": 0.4007821233775288
          },
          {
            "accuracy": 0.3888,
            "f1": 0.38732976626964916,
            "f1_weighted": 0.3873297662696491
          }
        ]
      }
    ]
  },
  "task_name": "AmazonReviewsClassification"
}