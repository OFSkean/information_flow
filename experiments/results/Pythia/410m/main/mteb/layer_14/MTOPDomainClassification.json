{
  "dataset_revision": "d80d48c1eb48d3562165c59d59d0034df9fff0bf",
  "evaluation_time": 52.80021929740906,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.8369585043319654,
        "f1": 0.8323270631175834,
        "f1_weighted": 0.8371724292172544,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.8369585043319654,
        "scores_per_experiment": [
          {
            "accuracy": 0.811217510259918,
            "f1": 0.8067564235924425,
            "f1_weighted": 0.8078287465332185
          },
          {
            "accuracy": 0.8581851345189239,
            "f1": 0.8521765362446735,
            "f1_weighted": 0.859781228644277
          },
          {
            "accuracy": 0.8353853169174646,
            "f1": 0.8271592309237102,
            "f1_weighted": 0.836821731972779
          },
          {
            "accuracy": 0.8410852713178295,
            "f1": 0.8341135989771165,
            "f1_weighted": 0.8410387471681472
          },
          {
            "accuracy": 0.8616051071591427,
            "f1": 0.8580550400451311,
            "f1_weighted": 0.8623395917634202
          },
          {
            "accuracy": 0.837437300501596,
            "f1": 0.8362314904622479,
            "f1_weighted": 0.8377976475687066
          },
          {
            "accuracy": 0.8178294573643411,
            "f1": 0.8128704749875406,
            "f1_weighted": 0.8150961632897629
          },
          {
            "accuracy": 0.8344733242134063,
            "f1": 0.8283201837443898,
            "f1_weighted": 0.836899084467724
          },
          {
            "accuracy": 0.8419972640218878,
            "f1": 0.8416547048217183,
            "f1_weighted": 0.8426093349729713
          },
          {
            "accuracy": 0.8303693570451436,
            "f1": 0.8259329473768644,
            "f1_weighted": 0.8315120157915372
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.8310514541387025,
        "f1": 0.8302402990933363,
        "f1_weighted": 0.8310494248512391,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.8310514541387025,
        "scores_per_experiment": [
          {
            "accuracy": 0.8013422818791947,
            "f1": 0.8033499189824148,
            "f1_weighted": 0.7978446716674659
          },
          {
            "accuracy": 0.8541387024608501,
            "f1": 0.8531006309133794,
            "f1_weighted": 0.8550460773151334
          },
          {
            "accuracy": 0.8304250559284116,
            "f1": 0.8235741495078738,
            "f1_weighted": 0.8309565004360461
          },
          {
            "accuracy": 0.8416107382550335,
            "f1": 0.8425051652148638,
            "f1_weighted": 0.841787587921269
          },
          {
            "accuracy": 0.8483221476510067,
            "f1": 0.851372487554002,
            "f1_weighted": 0.8494048326333109
          },
          {
            "accuracy": 0.839821029082774,
            "f1": 0.8401037006370249,
            "f1_weighted": 0.8400141162948821
          },
          {
            "accuracy": 0.8080536912751678,
            "f1": 0.8040180939030488,
            "f1_weighted": 0.8058712071537626
          },
          {
            "accuracy": 0.8281879194630872,
            "f1": 0.8250135345911755,
            "f1_weighted": 0.830369467704772
          },
          {
            "accuracy": 0.8366890380313199,
            "f1": 0.836708687832396,
            "f1_weighted": 0.8369607262951608
          },
          {
            "accuracy": 0.821923937360179,
            "f1": 0.8226566217971847,
            "f1_weighted": 0.8222390610905878
          }
        ]
      }
    ]
  },
  "task_name": "MTOPDomainClassification"
}