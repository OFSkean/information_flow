{
  "dataset_revision": "ae001d0e6b1228650b7bd1c2c65fb50ad11a8aba",
  "evaluation_time": 126.0224289894104,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.7066575467396261,
        "f1": 0.48074480079956033,
        "f1_weighted": 0.7429218062221652,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.7066575467396261,
        "scores_per_experiment": [
          {
            "accuracy": 0.6942544459644323,
            "f1": 0.4642457448211835,
            "f1_weighted": 0.7311603333991943
          },
          {
            "accuracy": 0.7102143182854537,
            "f1": 0.4835681239207136,
            "f1_weighted": 0.7485460356930131
          },
          {
            "accuracy": 0.7063383492932056,
            "f1": 0.4720776921492996,
            "f1_weighted": 0.7445568033705193
          },
          {
            "accuracy": 0.70953032375741,
            "f1": 0.4947462398582956,
            "f1_weighted": 0.7475626127816075
          },
          {
            "accuracy": 0.707250341997264,
            "f1": 0.48211601061752085,
            "f1_weighted": 0.7377136365465652
          },
          {
            "accuracy": 0.7150022799817601,
            "f1": 0.48580365422472843,
            "f1_weighted": 0.7538984655914881
          },
          {
            "accuracy": 0.6935704514363885,
            "f1": 0.5000279423386883,
            "f1_weighted": 0.7326098387945068
          },
          {
            "accuracy": 0.7218422252621979,
            "f1": 0.4854308250249253,
            "f1_weighted": 0.7558552306433345
          },
          {
            "accuracy": 0.7036023711810305,
            "f1": 0.4687500371536178,
            "f1_weighted": 0.7349849600579444
          },
          {
            "accuracy": 0.7049703602371181,
            "f1": 0.4706817378866303,
            "f1_weighted": 0.7423301453434782
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.7061297539149888,
        "f1": 0.47343884525921187,
        "f1_weighted": 0.7448147336538762,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.7061297539149888,
        "scores_per_experiment": [
          {
            "accuracy": 0.6894854586129754,
            "f1": 0.45335589495722955,
            "f1_weighted": 0.7286236016106491
          },
          {
            "accuracy": 0.7105145413870246,
            "f1": 0.4818130292991003,
            "f1_weighted": 0.7526824993791973
          },
          {
            "accuracy": 0.7109619686800895,
            "f1": 0.4776693547962287,
            "f1_weighted": 0.7514783257459344
          },
          {
            "accuracy": 0.7073825503355705,
            "f1": 0.4792645917898732,
            "f1_weighted": 0.7461955904674261
          },
          {
            "accuracy": 0.7046979865771812,
            "f1": 0.46956613898406024,
            "f1_weighted": 0.7373869465166518
          },
          {
            "accuracy": 0.7064876957494407,
            "f1": 0.4652613617585928,
            "f1_weighted": 0.7502852643429972
          },
          {
            "accuracy": 0.6859060402684564,
            "f1": 0.46306418345649775,
            "f1_weighted": 0.7272463732492317
          },
          {
            "accuracy": 0.7302013422818792,
            "f1": 0.4955983212202802,
            "f1_weighted": 0.7646367867097108
          },
          {
            "accuracy": 0.7033557046979866,
            "f1": 0.4674152497135685,
            "f1_weighted": 0.7384329374296693
          },
          {
            "accuracy": 0.7123042505592841,
            "f1": 0.4813803266166875,
            "f1_weighted": 0.7511790110872937
          }
        ]
      }
    ]
  },
  "task_name": "MTOPIntentClassification"
}