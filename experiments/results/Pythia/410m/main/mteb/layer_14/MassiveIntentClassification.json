{
  "dataset_revision": "4672e20407010da34463acc759c162ca9734bca6",
  "evaluation_time": 89.99857997894287,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.5960659045057162,
        "f1": 0.5624313313032198,
        "f1_weighted": 0.6003175380278976,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5960659045057162,
        "scores_per_experiment": [
          {
            "accuracy": 0.5995292535305985,
            "f1": 0.5802666063487948,
            "f1_weighted": 0.604170213136148
          },
          {
            "accuracy": 0.6042367182246133,
            "f1": 0.5712099474905101,
            "f1_weighted": 0.6082515642506575
          },
          {
            "accuracy": 0.5998655010087425,
            "f1": 0.5621177251521582,
            "f1_weighted": 0.6028935144658986
          },
          {
            "accuracy": 0.6160053799596503,
            "f1": 0.5701028233916943,
            "f1_weighted": 0.6164305934824996
          },
          {
            "accuracy": 0.5978480161398789,
            "f1": 0.5566603317014327,
            "f1_weighted": 0.6009527787297159
          },
          {
            "accuracy": 0.5682582380632145,
            "f1": 0.5490846005628889,
            "f1_weighted": 0.5742670381817574
          },
          {
            "accuracy": 0.5874243443174176,
            "f1": 0.5622586488327516,
            "f1_weighted": 0.5896841260219459
          },
          {
            "accuracy": 0.6005379959650302,
            "f1": 0.5547085146269876,
            "f1_weighted": 0.610444351051861
          },
          {
            "accuracy": 0.5766644250168124,
            "f1": 0.5452384102612258,
            "f1_weighted": 0.5828289435571634
          },
          {
            "accuracy": 0.6102891728312038,
            "f1": 0.5726657046637544,
            "f1_weighted": 0.6132522574013286
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.6060993605509101,
        "f1": 0.5718946579972904,
        "f1_weighted": 0.609355744629456,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.6060993605509101,
        "scores_per_experiment": [
          {
            "accuracy": 0.6069847515986228,
            "f1": 0.5800954670200767,
            "f1_weighted": 0.6079280982009729
          },
          {
            "accuracy": 0.6074766355140186,
            "f1": 0.5609989778950444,
            "f1_weighted": 0.6143185541877092
          },
          {
            "accuracy": 0.6291195277914412,
            "f1": 0.5936183020020297,
            "f1_weighted": 0.6297876720132504
          },
          {
            "accuracy": 0.6074766355140186,
            "f1": 0.5618854577142511,
            "f1_weighted": 0.6084889935317969
          },
          {
            "accuracy": 0.6099360550909986,
            "f1": 0.5727215802396342,
            "f1_weighted": 0.6124427134385281
          },
          {
            "accuracy": 0.5932120019675357,
            "f1": 0.5794943212729105,
            "f1_weighted": 0.5961455907875686
          },
          {
            "accuracy": 0.5828824397442204,
            "f1": 0.5555320402330881,
            "f1_weighted": 0.5835762685354933
          },
          {
            "accuracy": 0.6015740285292671,
            "f1": 0.5597463945191201,
            "f1_weighted": 0.6081839696467808
          },
          {
            "accuracy": 0.5833743236596163,
            "f1": 0.5569670951568161,
            "f1_weighted": 0.5908922961433811
          },
          {
            "accuracy": 0.6389572060993606,
            "f1": 0.5978869439199328,
            "f1_weighted": 0.641793289809079
          }
        ]
      }
    ]
  },
  "task_name": "MassiveIntentClassification"
}