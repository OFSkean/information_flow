{
  "dataset_revision": "4672e20407010da34463acc759c162ca9734bca6",
  "evaluation_time": 87.08040833473206,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.5887693342299932,
        "f1": 0.555721015473108,
        "f1_weighted": 0.5929491109998016,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5887693342299932,
        "scores_per_experiment": [
          {
            "accuracy": 0.5904505716207128,
            "f1": 0.5690877362584702,
            "f1_weighted": 0.5943662572649426
          },
          {
            "accuracy": 0.5965030262273033,
            "f1": 0.5618578222299836,
            "f1_weighted": 0.6015134263131079
          },
          {
            "accuracy": 0.5944855413584398,
            "f1": 0.5614727255093399,
            "f1_weighted": 0.5956058467904676
          },
          {
            "accuracy": 0.6059179556153329,
            "f1": 0.5618803863509075,
            "f1_weighted": 0.610146501215596
          },
          {
            "accuracy": 0.5978480161398789,
            "f1": 0.551889301635903,
            "f1_weighted": 0.5987906667617725
          },
          {
            "accuracy": 0.5672494956287828,
            "f1": 0.5461916253195824,
            "f1_weighted": 0.5728250164064622
          },
          {
            "accuracy": 0.5833893745796906,
            "f1": 0.5539977547313194,
            "f1_weighted": 0.5861354095770496
          },
          {
            "accuracy": 0.5776731674512441,
            "f1": 0.5343764813224466,
            "f1_weighted": 0.5844089313273986
          },
          {
            "accuracy": 0.5739744451916611,
            "f1": 0.5524048397637464,
            "f1_weighted": 0.5822880039365038
          },
          {
            "accuracy": 0.6002017484868863,
            "f1": 0.5640514816093817,
            "f1_weighted": 0.6034110504047154
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.5976881455976389,
        "f1": 0.5642058142466316,
        "f1_weighted": 0.6011239091254184,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5976881455976389,
        "scores_per_experiment": [
          {
            "accuracy": 0.5941957697983276,
            "f1": 0.5692239483695706,
            "f1_weighted": 0.5979608568713795
          },
          {
            "accuracy": 0.6015740285292671,
            "f1": 0.5570912552498617,
            "f1_weighted": 0.6074629724789875
          },
          {
            "accuracy": 0.616822429906542,
            "f1": 0.5792894588908182,
            "f1_weighted": 0.6172580969032098
          },
          {
            "accuracy": 0.6099360550909986,
            "f1": 0.5654523196256082,
            "f1_weighted": 0.6108721237042779
          },
          {
            "accuracy": 0.6094441711756026,
            "f1": 0.5727962638800128,
            "f1_weighted": 0.6089406755361273
          },
          {
            "accuracy": 0.5794392523364486,
            "f1": 0.563825207735004,
            "f1_weighted": 0.5822184706820862
          },
          {
            "accuracy": 0.5740285292670929,
            "f1": 0.5448454432380319,
            "f1_weighted": 0.5777067962064556
          },
          {
            "accuracy": 0.5858337432365962,
            "f1": 0.5522267671820242,
            "f1_weighted": 0.5905135315688543
          },
          {
            "accuracy": 0.5705853418593212,
            "f1": 0.5428693342006086,
            "f1_weighted": 0.5815027990950974
          },
          {
            "accuracy": 0.6350221347761928,
            "f1": 0.5944381440947771,
            "f1_weighted": 0.6368027682077083
          }
        ]
      }
    ]
  },
  "task_name": "MassiveIntentClassification"
}