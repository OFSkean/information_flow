{
  "dataset_revision": "d80d48c1eb48d3562165c59d59d0034df9fff0bf",
  "evaluation_time": 50.67855191230774,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.8252165982672139,
        "f1": 0.8207324478808571,
        "f1_weighted": 0.8256520544555543,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.8252165982672139,
        "scores_per_experiment": [
          {
            "accuracy": 0.7886456908344733,
            "f1": 0.7844249488022562,
            "f1_weighted": 0.7859549794682968
          },
          {
            "accuracy": 0.8317373461012312,
            "f1": 0.827806109038228,
            "f1_weighted": 0.8344168084582296
          },
          {
            "accuracy": 0.8317373461012312,
            "f1": 0.8227752712309312,
            "f1_weighted": 0.8328124901859015
          },
          {
            "accuracy": 0.8331053351573188,
            "f1": 0.8272872488474318,
            "f1_weighted": 0.8333545949907729
          },
          {
            "accuracy": 0.8497492020063839,
            "f1": 0.8449625040534504,
            "f1_weighted": 0.8507395245298868
          },
          {
            "accuracy": 0.8369813041495668,
            "f1": 0.8349492962276236,
            "f1_weighted": 0.837258588838836
          },
          {
            "accuracy": 0.803921568627451,
            "f1": 0.8017896925834903,
            "f1_weighted": 0.8010132301413189
          },
          {
            "accuracy": 0.823985408116735,
            "f1": 0.8186887432912289,
            "f1_weighted": 0.8264571006525954
          },
          {
            "accuracy": 0.8278613771089831,
            "f1": 0.825931958620681,
            "f1_weighted": 0.8287165248713103
          },
          {
            "accuracy": 0.8244414044687642,
            "f1": 0.8187087061132502,
            "f1_weighted": 0.8257967024183965
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.8222818791946308,
        "f1": 0.8216870641530096,
        "f1_weighted": 0.8227835966432628,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.8222818791946308,
        "scores_per_experiment": [
          {
            "accuracy": 0.7856823266219239,
            "f1": 0.7886210567128834,
            "f1_weighted": 0.7830677822633303
          },
          {
            "accuracy": 0.8304250559284116,
            "f1": 0.83167808381754,
            "f1_weighted": 0.8316059823443367
          },
          {
            "accuracy": 0.8281879194630872,
            "f1": 0.8218262076621431,
            "f1_weighted": 0.8291410652196014
          },
          {
            "accuracy": 0.8442953020134228,
            "f1": 0.8454165930941815,
            "f1_weighted": 0.8464515210648679
          },
          {
            "accuracy": 0.8402684563758389,
            "f1": 0.8418592790170135,
            "f1_weighted": 0.8421654228600165
          },
          {
            "accuracy": 0.8340044742729307,
            "f1": 0.832885547829992,
            "f1_weighted": 0.8344214958978722
          },
          {
            "accuracy": 0.7906040268456376,
            "f1": 0.7883720057101953,
            "f1_weighted": 0.788471995751078
          },
          {
            "accuracy": 0.8170022371364654,
            "f1": 0.8165293813636094,
            "f1_weighted": 0.8189589666663317
          },
          {
            "accuracy": 0.8304250559284116,
            "f1": 0.8303892982869047,
            "f1_weighted": 0.8307203172869211
          },
          {
            "accuracy": 0.821923937360179,
            "f1": 0.819293188035634,
            "f1_weighted": 0.8228314170782732
          }
        ]
      }
    ]
  },
  "task_name": "MTOPDomainClassification"
}