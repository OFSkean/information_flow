{
  "dataset_revision": "4672e20407010da34463acc759c162ca9734bca6",
  "evaluation_time": 75.42788815498352,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.5686617350369871,
        "f1": 0.5419113532388858,
        "f1_weighted": 0.5708512868371576,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5686617350369871,
        "scores_per_experiment": [
          {
            "accuracy": 0.5833893745796906,
            "f1": 0.5599542607233524,
            "f1_weighted": 0.5851660201900364
          },
          {
            "accuracy": 0.5884330867518494,
            "f1": 0.5518011322673074,
            "f1_weighted": 0.5934976839890772
          },
          {
            "accuracy": 0.5564895763281775,
            "f1": 0.5249104951115544,
            "f1_weighted": 0.5545639687620254
          },
          {
            "accuracy": 0.5827168796234028,
            "f1": 0.5474745901768295,
            "f1_weighted": 0.5864906469300849
          },
          {
            "accuracy": 0.5759919300605245,
            "f1": 0.5386130856085286,
            "f1_weighted": 0.5741270496631915
          },
          {
            "accuracy": 0.5494283792871554,
            "f1": 0.5345009588115599,
            "f1_weighted": 0.5532373173820859
          },
          {
            "accuracy": 0.5504371217215871,
            "f1": 0.5389874456376885,
            "f1_weighted": 0.5478325004378998
          },
          {
            "accuracy": 0.5601882985877606,
            "f1": 0.5355799724196554,
            "f1_weighted": 0.5652305255137416
          },
          {
            "accuracy": 0.5608607935440484,
            "f1": 0.5357112019068677,
            "f1_weighted": 0.5632283772959936
          },
          {
            "accuracy": 0.5786819098856758,
            "f1": 0.5515803897255146,
            "f1_weighted": 0.5851387782074404
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.5782095425479588,
        "f1": 0.5529189289310237,
        "f1_weighted": 0.5797173905903691,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5782095425479588,
        "scores_per_experiment": [
          {
            "accuracy": 0.5627151992129857,
            "f1": 0.5464861299621969,
            "f1_weighted": 0.5631769765618748
          },
          {
            "accuracy": 0.5981308411214953,
            "f1": 0.5517361107732411,
            "f1_weighted": 0.5995133550178485
          },
          {
            "accuracy": 0.5848499754058042,
            "f1": 0.5604981548829873,
            "f1_weighted": 0.5833410678413594
          },
          {
            "accuracy": 0.5838662075750123,
            "f1": 0.5425195003178611,
            "f1_weighted": 0.588221666397138
          },
          {
            "accuracy": 0.5986227250368913,
            "f1": 0.5607382356551107,
            "f1_weighted": 0.5951085255554692
          },
          {
            "accuracy": 0.5661583866207575,
            "f1": 0.5608087833057064,
            "f1_weighted": 0.5679773756822925
          },
          {
            "accuracy": 0.5489424495818986,
            "f1": 0.545663913021842,
            "f1_weighted": 0.5488200540256265
          },
          {
            "accuracy": 0.5656665027053616,
            "f1": 0.5324021082739163,
            "f1_weighted": 0.569660263325038
          },
          {
            "accuracy": 0.5681259222823414,
            "f1": 0.5518705912016152,
            "f1_weighted": 0.5742315188304753
          },
          {
            "accuracy": 0.6050172159370388,
            "f1": 0.5764657619157602,
            "f1_weighted": 0.607123102666568
          }
        ]
      }
    ]
  },
  "task_name": "MassiveIntentClassification"
}