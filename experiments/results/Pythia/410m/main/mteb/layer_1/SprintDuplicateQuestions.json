{
  "dataset_revision": "d66bd1f72af766a5cc4b0ca5e00c162f89e8cc46",
  "evaluation_time": 21.113576889038086,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "cosine_accuracy": 0.9917722772277228,
        "cosine_accuracy_threshold": 0.9315342903137207,
        "cosine_ap": 0.4365030563081158,
        "cosine_f1": 0.4648705758055996,
        "cosine_f1_threshold": 0.9133244752883911,
        "cosine_precision": 0.49272116461366183,
        "cosine_recall": 0.44,
        "dot_accuracy": 0.9902574257425742,
        "dot_accuracy_threshold": 4691.23193359375,
        "dot_ap": 0.10651513766183417,
        "dot_f1": 0.16898008449004226,
        "dot_f1_threshold": 4335.84033203125,
        "dot_precision": 0.213089802130898,
        "dot_recall": 0.14,
        "euclidean_accuracy": 0.9916633663366337,
        "euclidean_accuracy_threshold": 24.558313369750977,
        "euclidean_ap": 0.4125668117344105,
        "euclidean_f1": 0.44930875576036866,
        "euclidean_f1_threshold": 26.596805572509766,
        "euclidean_precision": 0.529891304347826,
        "euclidean_recall": 0.39,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.4365030563081158,
        "manhattan_accuracy": 0.9916732673267327,
        "manhattan_accuracy_threshold": 579.7745971679688,
        "manhattan_ap": 0.4159086303236124,
        "manhattan_f1": 0.45042979942693406,
        "manhattan_f1_threshold": 669.5658569335938,
        "manhattan_precision": 0.5275167785234899,
        "manhattan_recall": 0.393,
        "max_accuracy": 0.9917722772277228,
        "max_ap": 0.4365030563081158,
        "max_f1": 0.4648705758055996,
        "max_precision": 0.529891304347826,
        "max_recall": 0.44,
        "similarity_accuracy": 0.9917722772277228,
        "similarity_accuracy_threshold": 0.9315342903137207,
        "similarity_ap": 0.4365030563081158,
        "similarity_f1": 0.4648705758055996,
        "similarity_f1_threshold": 0.9133244752883911,
        "similarity_precision": 0.49272116461366183,
        "similarity_recall": 0.44
      }
    ],
    "validation": [
      {
        "cosine_accuracy": 0.9912772277227723,
        "cosine_accuracy_threshold": 0.9305696487426758,
        "cosine_ap": 0.357606725269385,
        "cosine_f1": 0.4035874439461883,
        "cosine_f1_threshold": 0.9124590158462524,
        "cosine_precision": 0.45918367346938777,
        "cosine_recall": 0.36,
        "dot_accuracy": 0.9901881188118812,
        "dot_accuracy_threshold": 4932.0869140625,
        "dot_ap": 0.10029825332072088,
        "dot_f1": 0.1697530864197531,
        "dot_f1_threshold": 4300.22900390625,
        "dot_precision": 0.17478813559322035,
        "dot_recall": 0.165,
        "euclidean_accuracy": 0.9911386138613861,
        "euclidean_accuracy_threshold": 24.315174102783203,
        "euclidean_ap": 0.3281008597186973,
        "euclidean_f1": 0.38003220611916255,
        "euclidean_f1_threshold": 27.668317794799805,
        "euclidean_precision": 0.4101969872537659,
        "euclidean_recall": 0.354,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.357606725269385,
        "manhattan_accuracy": 0.9911188118811881,
        "manhattan_accuracy_threshold": 608.926025390625,
        "manhattan_ap": 0.3286511313342551,
        "manhattan_f1": 0.37946210268948655,
        "manhattan_f1_threshold": 708.279296875,
        "manhattan_precision": 0.3712918660287081,
        "manhattan_recall": 0.388,
        "max_accuracy": 0.9912772277227723,
        "max_ap": 0.357606725269385,
        "max_f1": 0.4035874439461883,
        "max_precision": 0.45918367346938777,
        "max_recall": 0.388,
        "similarity_accuracy": 0.9912772277227723,
        "similarity_accuracy_threshold": 0.9305696487426758,
        "similarity_ap": 0.357606725269385,
        "similarity_f1": 0.4035874439461883,
        "similarity_f1_threshold": 0.9124590158462524,
        "similarity_precision": 0.45918367346938777,
        "similarity_recall": 0.36
      }
    ]
  },
  "task_name": "SprintDuplicateQuestions"
}