{
  "dataset_revision": "ae001d0e6b1228650b7bd1c2c65fb50ad11a8aba",
  "evaluation_time": 107.58235383033752,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.6681942544459644,
        "f1": 0.4604667704392614,
        "f1_weighted": 0.7096208478123477,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.6681942544459644,
        "scores_per_experiment": [
          {
            "accuracy": 0.6577747378020976,
            "f1": 0.45173208411971394,
            "f1_weighted": 0.6946880190021768
          },
          {
            "accuracy": 0.668718650250798,
            "f1": 0.4449967505405277,
            "f1_weighted": 0.7094072854860438
          },
          {
            "accuracy": 0.655266757865937,
            "f1": 0.4451918171424376,
            "f1_weighted": 0.701498281499097
          },
          {
            "accuracy": 0.6739626082991336,
            "f1": 0.4633117702364089,
            "f1_weighted": 0.7152903243993342
          },
          {
            "accuracy": 0.6673506611947104,
            "f1": 0.4670717540418182,
            "f1_weighted": 0.7120121717474595
          },
          {
            "accuracy": 0.6529867761057911,
            "f1": 0.4525979755450184,
            "f1_weighted": 0.6989107584295313
          },
          {
            "accuracy": 0.6646146830825354,
            "f1": 0.4818047495767029,
            "f1_weighted": 0.7047885183208743
          },
          {
            "accuracy": 0.6963064295485636,
            "f1": 0.4840873880198649,
            "f1_weighted": 0.7323996459055193
          },
          {
            "accuracy": 0.6691746466028272,
            "f1": 0.45622037178986796,
            "f1_weighted": 0.7120919797463293
          },
          {
            "accuracy": 0.6757865937072504,
            "f1": 0.4576530433802531,
            "f1_weighted": 0.7151214935871117
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.6664876957494407,
        "f1": 0.4574848906445005,
        "f1_weighted": 0.7100753451787243,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.6664876957494407,
        "scores_per_experiment": [
          {
            "accuracy": 0.6451901565995526,
            "f1": 0.4506574807237381,
            "f1_weighted": 0.6854965451657957
          },
          {
            "accuracy": 0.6608501118568233,
            "f1": 0.4476097670901105,
            "f1_weighted": 0.7023800478966526
          },
          {
            "accuracy": 0.6742729306487696,
            "f1": 0.4504040394862098,
            "f1_weighted": 0.7199007898389994
          },
          {
            "accuracy": 0.6733780760626398,
            "f1": 0.4644196204065123,
            "f1_weighted": 0.7150726908728107
          },
          {
            "accuracy": 0.6859060402684564,
            "f1": 0.47789069293021413,
            "f1_weighted": 0.7319685639110516
          },
          {
            "accuracy": 0.6514541387024608,
            "f1": 0.4489843822587532,
            "f1_weighted": 0.6963211256684221
          },
          {
            "accuracy": 0.6577181208053692,
            "f1": 0.44544248708078044,
            "f1_weighted": 0.7015999791059981
          },
          {
            "accuracy": 0.6881431767337808,
            "f1": 0.4797714345618443,
            "f1_weighted": 0.7302969733208025
          },
          {
            "accuracy": 0.643847874720358,
            "f1": 0.433332702399588,
            "f1_weighted": 0.6941048794313587
          },
          {
            "accuracy": 0.6841163310961969,
            "f1": 0.47633629950725376,
            "f1_weighted": 0.7236118565753514
          }
        ]
      }
    ]
  },
  "task_name": "MTOPIntentClassification"
}