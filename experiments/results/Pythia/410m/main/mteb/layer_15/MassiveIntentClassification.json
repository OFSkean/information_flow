{
  "dataset_revision": "4672e20407010da34463acc759c162ca9734bca6",
  "evaluation_time": 91.14213991165161,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.5910221923335576,
        "f1": 0.5587019867378304,
        "f1_weighted": 0.5948076322749011,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5910221923335576,
        "scores_per_experiment": [
          {
            "accuracy": 0.5948217888365838,
            "f1": 0.5728235178271209,
            "f1_weighted": 0.5990609893575866
          },
          {
            "accuracy": 0.6012104909213181,
            "f1": 0.5725016887303543,
            "f1_weighted": 0.6034515691886355
          },
          {
            "accuracy": 0.5921318090114324,
            "f1": 0.5560942995626996,
            "f1_weighted": 0.5936117253976142
          },
          {
            "accuracy": 0.6149966375252186,
            "f1": 0.5709376268149817,
            "f1_weighted": 0.6151421614809307
          },
          {
            "accuracy": 0.5917955615332885,
            "f1": 0.5486442141674346,
            "f1_weighted": 0.5944323747111775
          },
          {
            "accuracy": 0.5571620712844654,
            "f1": 0.5390695308925417,
            "f1_weighted": 0.5619060581340316
          },
          {
            "accuracy": 0.5823806321452589,
            "f1": 0.5590031631503123,
            "f1_weighted": 0.5841759197128277
          },
          {
            "accuracy": 0.5951580363147276,
            "f1": 0.5518616910016495,
            "f1_weighted": 0.6058442680889037
          },
          {
            "accuracy": 0.5739744451916611,
            "f1": 0.544071298006904,
            "f1_weighted": 0.5796417829579543
          },
          {
            "accuracy": 0.6065904505716208,
            "f1": 0.5720128372243061,
            "f1_weighted": 0.6108094737193499
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.6013280865715692,
        "f1": 0.5673590749383717,
        "f1_weighted": 0.6035799994032034,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.6013280865715692,
        "scores_per_experiment": [
          {
            "accuracy": 0.6069847515986228,
            "f1": 0.5834192663847312,
            "f1_weighted": 0.6062302424521083
          },
          {
            "accuracy": 0.6010821446138711,
            "f1": 0.5559400078625172,
            "f1_weighted": 0.6040010309186828
          },
          {
            "accuracy": 0.617806197737334,
            "f1": 0.5831948283441398,
            "f1_weighted": 0.6167320238823694
          },
          {
            "accuracy": 0.6040334481062469,
            "f1": 0.5629253351507398,
            "f1_weighted": 0.6033306317767777
          },
          {
            "accuracy": 0.6005902606984752,
            "f1": 0.560940233958013,
            "f1_weighted": 0.6039006653877347
          },
          {
            "accuracy": 0.5853418593212002,
            "f1": 0.5644580645724472,
            "f1_weighted": 0.5886992209502142
          },
          {
            "accuracy": 0.5774717166748647,
            "f1": 0.5564842741159483,
            "f1_weighted": 0.5775937028853548
          },
          {
            "accuracy": 0.6010821446138711,
            "f1": 0.5540003311290259,
            "f1_weighted": 0.6067983519773977
          },
          {
            "accuracy": 0.5887850467289719,
            "f1": 0.5624983177832277,
            "f1_weighted": 0.5962507396292792
          },
          {
            "accuracy": 0.6301032956222331,
            "f1": 0.5897300900829261,
            "f1_weighted": 0.6322633841721151
          }
        ]
      }
    ]
  },
  "task_name": "MassiveIntentClassification"
}