{
  "dataset_revision": "ae001d0e6b1228650b7bd1c2c65fb50ad11a8aba",
  "evaluation_time": 126.36848664283752,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.6993844049247606,
        "f1": 0.47627092670864846,
        "f1_weighted": 0.7366805421521123,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.6993844049247606,
        "scores_per_experiment": [
          {
            "accuracy": 0.6865025079799362,
            "f1": 0.4622429347403914,
            "f1_weighted": 0.7261425207291924
          },
          {
            "accuracy": 0.6967624259005928,
            "f1": 0.4777287919471182,
            "f1_weighted": 0.7371832737928509
          },
          {
            "accuracy": 0.6988144094847242,
            "f1": 0.4768261317570836,
            "f1_weighted": 0.7380050295464061
          },
          {
            "accuracy": 0.7026903784769721,
            "f1": 0.49743012932970276,
            "f1_weighted": 0.7410805737663395
          },
          {
            "accuracy": 0.7015503875968992,
            "f1": 0.48423547560341046,
            "f1_weighted": 0.7351736448853232
          },
          {
            "accuracy": 0.7049703602371181,
            "f1": 0.47667478796314905,
            "f1_weighted": 0.7444248847284028
          },
          {
            "accuracy": 0.6901504787961696,
            "f1": 0.48373789854550503,
            "f1_weighted": 0.7301255865952927
          },
          {
            "accuracy": 0.7197902416780666,
            "f1": 0.4848787969624836,
            "f1_weighted": 0.7530669770653625
          },
          {
            "accuracy": 0.6908344733242134,
            "f1": 0.4587656737651499,
            "f1_weighted": 0.7241315960051148
          },
          {
            "accuracy": 0.7017783857729138,
            "f1": 0.46018864647249025,
            "f1_weighted": 0.7374713344068385
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.7024161073825503,
        "f1": 0.4692277722195336,
        "f1_weighted": 0.7416053538548952,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.7024161073825503,
        "scores_per_experiment": [
          {
            "accuracy": 0.6863534675615213,
            "f1": 0.455806469265899,
            "f1_weighted": 0.7268467946194479
          },
          {
            "accuracy": 0.7051454138702461,
            "f1": 0.4668010144001924,
            "f1_weighted": 0.7465677735450563
          },
          {
            "accuracy": 0.705592841163311,
            "f1": 0.4668966344362293,
            "f1_weighted": 0.7477412077636475
          },
          {
            "accuracy": 0.7073825503355705,
            "f1": 0.47113990393622207,
            "f1_weighted": 0.746925899863904
          },
          {
            "accuracy": 0.6961968680089485,
            "f1": 0.4703513547526325,
            "f1_weighted": 0.7314672077331481
          },
          {
            "accuracy": 0.7042505592841163,
            "f1": 0.46960305607104574,
            "f1_weighted": 0.7462032278633778
          },
          {
            "accuracy": 0.6868008948545862,
            "f1": 0.4658215303795553,
            "f1_weighted": 0.7277776929306078
          },
          {
            "accuracy": 0.7248322147651006,
            "f1": 0.48256296802806187,
            "f1_weighted": 0.7617632591150935
          },
          {
            "accuracy": 0.697986577181208,
            "f1": 0.46696060666713474,
            "f1_weighted": 0.732890573403064
          },
          {
            "accuracy": 0.7096196868008948,
            "f1": 0.4763341842583635,
            "f1_weighted": 0.7478699017116046
          }
        ]
      }
    ]
  },
  "task_name": "MTOPIntentClassification"
}