{
  "dataset_revision": "fad2c6e8459f9e1c45d9315f4953d921437d70f8",
  "evaluation_time": 68.05382490158081,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.6366173503698722,
        "f1": 0.623322435251447,
        "f1_weighted": 0.6384980178258264,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.6366173503698722,
        "scores_per_experiment": [
          {
            "accuracy": 0.656018829858776,
            "f1": 0.6515842079520948,
            "f1_weighted": 0.6581604701143667
          },
          {
            "accuracy": 0.644250168123739,
            "f1": 0.6360539229712443,
            "f1_weighted": 0.6434640224401706
          },
          {
            "accuracy": 0.6254203093476799,
            "f1": 0.6216043904081917,
            "f1_weighted": 0.6305098700131728
          },
          {
            "accuracy": 0.6429051782111634,
            "f1": 0.6232624134943271,
            "f1_weighted": 0.6446799564417935
          },
          {
            "accuracy": 0.6671149966375253,
            "f1": 0.6399295668550424,
            "f1_weighted": 0.6683082130654742
          },
          {
            "accuracy": 0.6200403496973773,
            "f1": 0.6079610847629354,
            "f1_weighted": 0.6176585888468236
          },
          {
            "accuracy": 0.6254203093476799,
            "f1": 0.6059343808367765,
            "f1_weighted": 0.630208285779524
          },
          {
            "accuracy": 0.6469401479488904,
            "f1": 0.6285017513668947,
            "f1_weighted": 0.6489178318970732
          },
          {
            "accuracy": 0.6334902488231339,
            "f1": 0.6191482649741691,
            "f1_weighted": 0.6357502536337694
          },
          {
            "accuracy": 0.6045729657027572,
            "f1": 0.5992443688927954,
            "f1_weighted": 0.607322686026096
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.6309886866699459,
        "f1": 0.6207425913839302,
        "f1_weighted": 0.6325099686502269,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.6309886866699459,
        "scores_per_experiment": [
          {
            "accuracy": 0.6399409739301525,
            "f1": 0.6314870733386594,
            "f1_weighted": 0.6425937657913757
          },
          {
            "accuracy": 0.6374815543531727,
            "f1": 0.6349764869597277,
            "f1_weighted": 0.6379753906055682
          },
          {
            "accuracy": 0.6261682242990654,
            "f1": 0.6285961224415354,
            "f1_weighted": 0.6305003500252426
          },
          {
            "accuracy": 0.6212493851451057,
            "f1": 0.60348552930563,
            "f1_weighted": 0.6225474697549495
          },
          {
            "accuracy": 0.6591244466305952,
            "f1": 0.6352910263450232,
            "f1_weighted": 0.6591404239429542
          },
          {
            "accuracy": 0.5991146089522873,
            "f1": 0.6010119600148417,
            "f1_weighted": 0.594034230202004
          },
          {
            "accuracy": 0.6291195277914412,
            "f1": 0.6131911157299125,
            "f1_weighted": 0.6321519916590157
          },
          {
            "accuracy": 0.6428922774225283,
            "f1": 0.6235804128694457,
            "f1_weighted": 0.6449011419658441
          },
          {
            "accuracy": 0.6310870634530251,
            "f1": 0.6215573005250439,
            "f1_weighted": 0.6347611896255159
          },
          {
            "accuracy": 0.6237088047220856,
            "f1": 0.6142488863094818,
            "f1_weighted": 0.6264937329297993
          }
        ]
      }
    ]
  },
  "task_name": "MassiveScenarioClassification"
}