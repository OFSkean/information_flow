{
  "dataset_revision": "fad2c6e8459f9e1c45d9315f4953d921437d70f8",
  "evaluation_time": 62.44416570663452,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.5936785474108944,
        "f1": 0.5740962868599842,
        "f1_weighted": 0.5957757445765264,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5936785474108944,
        "scores_per_experiment": [
          {
            "accuracy": 0.620712844653665,
            "f1": 0.6047930647693538,
            "f1_weighted": 0.6225422640179687
          },
          {
            "accuracy": 0.6116341627437795,
            "f1": 0.5869583729649289,
            "f1_weighted": 0.6170217671867047
          },
          {
            "accuracy": 0.5770006724949562,
            "f1": 0.5649490684562974,
            "f1_weighted": 0.5773669039775765
          },
          {
            "accuracy": 0.5874243443174176,
            "f1": 0.5658660337658001,
            "f1_weighted": 0.5910105891816918
          },
          {
            "accuracy": 0.589778076664425,
            "f1": 0.5676209058496288,
            "f1_weighted": 0.5862687234639432
          },
          {
            "accuracy": 0.5847343644922663,
            "f1": 0.5612828257324887,
            "f1_weighted": 0.5913652669421625
          },
          {
            "accuracy": 0.6039004707464694,
            "f1": 0.5844178434537174,
            "f1_weighted": 0.610910668462149
          },
          {
            "accuracy": 0.6153328850033625,
            "f1": 0.5943691762859489,
            "f1_weighted": 0.6160342879139296
          },
          {
            "accuracy": 0.5722932078009415,
            "f1": 0.553097143520101,
            "f1_weighted": 0.5702256277544469
          },
          {
            "accuracy": 0.5739744451916611,
            "f1": 0.5576084338015771,
            "f1_weighted": 0.5750113468646917
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.5848007870142646,
        "f1": 0.5711481745411193,
        "f1_weighted": 0.5846125899112653,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5848007870142646,
        "scores_per_experiment": [
          {
            "accuracy": 0.6148548942449582,
            "f1": 0.5978135128166049,
            "f1_weighted": 0.6167070762497602
          },
          {
            "accuracy": 0.6084604033448107,
            "f1": 0.6060736008949485,
            "f1_weighted": 0.6102696654234043
          },
          {
            "accuracy": 0.5602557796360059,
            "f1": 0.551588744371855,
            "f1_weighted": 0.5624871879915335
          },
          {
            "accuracy": 0.5745204131824889,
            "f1": 0.5570829263825301,
            "f1_weighted": 0.5765437278522566
          },
          {
            "accuracy": 0.5863256271519921,
            "f1": 0.5674925853216253,
            "f1_weighted": 0.5790328020103014
          },
          {
            "accuracy": 0.5681259222823414,
            "f1": 0.5554608367937554,
            "f1_weighted": 0.570322919400176
          },
          {
            "accuracy": 0.5848499754058042,
            "f1": 0.5717389630324412,
            "f1_weighted": 0.592888529096477
          },
          {
            "accuracy": 0.5922282341367437,
            "f1": 0.5731465440440342,
            "f1_weighted": 0.589672533434994
          },
          {
            "accuracy": 0.5769798327594687,
            "f1": 0.5662096323240537,
            "f1_weighted": 0.5701834646139521
          },
          {
            "accuracy": 0.5814067879980325,
            "f1": 0.5648743994293445,
            "f1_weighted": 0.5780179930397986
          }
        ]
      }
    ]
  },
  "task_name": "MassiveScenarioClassification"
}