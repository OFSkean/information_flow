{
  "dataset_revision": "ae001d0e6b1228650b7bd1c2c65fb50ad11a8aba",
  "evaluation_time": 118.57685279846191,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.6654354765161878,
        "f1": 0.4595655302528258,
        "f1_weighted": 0.7061089735987002,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.6654354765161878,
        "scores_per_experiment": [
          {
            "accuracy": 0.6584587323301414,
            "f1": 0.45361190322116574,
            "f1_weighted": 0.6960172803360045
          },
          {
            "accuracy": 0.6650706794345645,
            "f1": 0.44996983707412797,
            "f1_weighted": 0.7031524823322006
          },
          {
            "accuracy": 0.6568627450980392,
            "f1": 0.452965019135029,
            "f1_weighted": 0.6995756741950169
          },
          {
            "accuracy": 0.667578659370725,
            "f1": 0.4570139579284286,
            "f1_weighted": 0.7076654569909047
          },
          {
            "accuracy": 0.6689466484268126,
            "f1": 0.4693981978849541,
            "f1_weighted": 0.7124405614868753
          },
          {
            "accuracy": 0.6543547651618787,
            "f1": 0.4540618093493394,
            "f1_weighted": 0.6986647100011169
          },
          {
            "accuracy": 0.6605107159142727,
            "f1": 0.4657357703266519,
            "f1_weighted": 0.7003241671766398
          },
          {
            "accuracy": 0.6949384404924761,
            "f1": 0.4840521596650679,
            "f1_weighted": 0.7306291325074633
          },
          {
            "accuracy": 0.6657546739626083,
            "f1": 0.46035113900865515,
            "f1_weighted": 0.7079749740967068
          },
          {
            "accuracy": 0.6618787049703603,
            "f1": 0.44849550893483847,
            "f1_weighted": 0.704645296864073
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.6671140939597315,
        "f1": 0.4533828564553629,
        "f1_weighted": 0.7094649124705844,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.6671140939597315,
        "scores_per_experiment": [
          {
            "accuracy": 0.6465324384787472,
            "f1": 0.4394619845882298,
            "f1_weighted": 0.6877030340632148
          },
          {
            "accuracy": 0.6662192393736018,
            "f1": 0.439928488641226,
            "f1_weighted": 0.7043645495051636
          },
          {
            "accuracy": 0.6742729306487696,
            "f1": 0.4391048780844171,
            "f1_weighted": 0.7180296974132739
          },
          {
            "accuracy": 0.6671140939597315,
            "f1": 0.4484537509966951,
            "f1_weighted": 0.7081310144202692
          },
          {
            "accuracy": 0.6930648769574944,
            "f1": 0.47361289613865065,
            "f1_weighted": 0.7361850826854297
          },
          {
            "accuracy": 0.6612975391498881,
            "f1": 0.4531513677880039,
            "f1_weighted": 0.7056703684646733
          },
          {
            "accuracy": 0.6527964205816554,
            "f1": 0.4535025531705821,
            "f1_weighted": 0.6973919346264785
          },
          {
            "accuracy": 0.6885906040268457,
            "f1": 0.46960182857918276,
            "f1_weighted": 0.7298669616711847
          },
          {
            "accuracy": 0.6478747203579418,
            "f1": 0.4323962766381409,
            "f1_weighted": 0.6935063104937608
          },
          {
            "accuracy": 0.6733780760626398,
            "f1": 0.48461453992850145,
            "f1_weighted": 0.7138001713623962
          }
        ]
      }
    ]
  },
  "task_name": "MTOPIntentClassification"
}