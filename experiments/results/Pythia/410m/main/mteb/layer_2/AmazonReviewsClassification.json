{
  "dataset_revision": "1399c76144fd37290681b995c656ef9b2e06e26d",
  "evaluation_time": 75.13818955421448,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.31886000000000003,
        "f1": 0.316874248211868,
        "f1_weighted": 0.316874248211868,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.31886000000000003,
        "scores_per_experiment": [
          {
            "accuracy": 0.3374,
            "f1": 0.33939646650587707,
            "f1_weighted": 0.33939646650587707
          },
          {
            "accuracy": 0.3216,
            "f1": 0.3220417157367427,
            "f1_weighted": 0.3220417157367427
          },
          {
            "accuracy": 0.3128,
            "f1": 0.3123638107447077,
            "f1_weighted": 0.31236381074470776
          },
          {
            "accuracy": 0.3242,
            "f1": 0.31677643892565305,
            "f1_weighted": 0.316776438925653
          },
          {
            "accuracy": 0.368,
            "f1": 0.36213099450450476,
            "f1_weighted": 0.3621309945045048
          },
          {
            "accuracy": 0.2776,
            "f1": 0.2784573073504565,
            "f1_weighted": 0.2784573073504565
          },
          {
            "accuracy": 0.2808,
            "f1": 0.2806982270768642,
            "f1_weighted": 0.28069822707686415
          },
          {
            "accuracy": 0.327,
            "f1": 0.32219056713852395,
            "f1_weighted": 0.32219056713852395
          },
          {
            "accuracy": 0.3086,
            "f1": 0.30706774141667104,
            "f1_weighted": 0.30706774141667104
          },
          {
            "accuracy": 0.3306,
            "f1": 0.32761921271867916,
            "f1_weighted": 0.3276192127186791
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.31526,
        "f1": 0.3133781402858335,
        "f1_weighted": 0.31337814028583344,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.31526,
        "scores_per_experiment": [
          {
            "accuracy": 0.321,
            "f1": 0.3234434659763859,
            "f1_weighted": 0.32344346597638585
          },
          {
            "accuracy": 0.3222,
            "f1": 0.3248174015202984,
            "f1_weighted": 0.3248174015202984
          },
          {
            "accuracy": 0.3086,
            "f1": 0.3093671203658975,
            "f1_weighted": 0.3093671203658974
          },
          {
            "accuracy": 0.3088,
            "f1": 0.3002608420868103,
            "f1_weighted": 0.3002608420868103
          },
          {
            "accuracy": 0.3614,
            "f1": 0.3526121963707363,
            "f1_weighted": 0.35261219637073626
          },
          {
            "accuracy": 0.2782,
            "f1": 0.2766631691211287,
            "f1_weighted": 0.2766631691211287
          },
          {
            "accuracy": 0.2794,
            "f1": 0.27913215786233725,
            "f1_weighted": 0.27913215786233725
          },
          {
            "accuracy": 0.3196,
            "f1": 0.31634071564188415,
            "f1_weighted": 0.3163407156418841
          },
          {
            "accuracy": 0.314,
            "f1": 0.3136206377419029,
            "f1_weighted": 0.31362063774190285
          },
          {
            "accuracy": 0.3394,
            "f1": 0.3375236961709533,
            "f1_weighted": 0.3375236961709533
          }
        ]
      }
    ]
  },
  "task_name": "AmazonReviewsClassification"
}