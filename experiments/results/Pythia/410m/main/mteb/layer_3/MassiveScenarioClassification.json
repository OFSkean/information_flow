{
  "dataset_revision": "fad2c6e8459f9e1c45d9315f4953d921437d70f8",
  "evaluation_time": 65.42470264434814,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.6007733691997309,
        "f1": 0.583544802750479,
        "f1_weighted": 0.6025073040909807,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.6007733691997309,
        "scores_per_experiment": [
          {
            "accuracy": 0.6318090114324143,
            "f1": 0.6154681340302561,
            "f1_weighted": 0.6342935706589595
          },
          {
            "accuracy": 0.624747814391392,
            "f1": 0.6054174684851323,
            "f1_weighted": 0.6302414477520758
          },
          {
            "accuracy": 0.5786819098856758,
            "f1": 0.5691435556656678,
            "f1_weighted": 0.5813778095164079
          },
          {
            "accuracy": 0.589778076664425,
            "f1": 0.5726747047570195,
            "f1_weighted": 0.5915453308699838
          },
          {
            "accuracy": 0.5954942837928715,
            "f1": 0.5729267171082406,
            "f1_weighted": 0.5906130474398354
          },
          {
            "accuracy": 0.5981842636180229,
            "f1": 0.57983267497665,
            "f1_weighted": 0.6047238836584305
          },
          {
            "accuracy": 0.6086079354404842,
            "f1": 0.5885223781990808,
            "f1_weighted": 0.6150953832533206
          },
          {
            "accuracy": 0.6197041022192333,
            "f1": 0.600799932548685,
            "f1_weighted": 0.6178797749028437
          },
          {
            "accuracy": 0.5867518493611298,
            "f1": 0.5683367572814263,
            "f1_weighted": 0.5845730188246873
          },
          {
            "accuracy": 0.5739744451916611,
            "f1": 0.5623257044526331,
            "f1_weighted": 0.574729774033262
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.5921790457452041,
        "f1": 0.5822856718062936,
        "f1_weighted": 0.5916708598112135,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5921790457452041,
        "scores_per_experiment": [
          {
            "accuracy": 0.6256763403836695,
            "f1": 0.6124883270937413,
            "f1_weighted": 0.6274062095260712
          },
          {
            "accuracy": 0.6069847515986228,
            "f1": 0.6086420697894581,
            "f1_weighted": 0.6099245416767605
          },
          {
            "accuracy": 0.5666502705361535,
            "f1": 0.5615527074640893,
            "f1_weighted": 0.5668790988718259
          },
          {
            "accuracy": 0.5759960649286768,
            "f1": 0.566015757360907,
            "f1_weighted": 0.5758989102796923
          },
          {
            "accuracy": 0.5966551893753075,
            "f1": 0.5807338020475442,
            "f1_weighted": 0.588240383624742
          },
          {
            "accuracy": 0.5887850467289719,
            "f1": 0.5804903454516016,
            "f1_weighted": 0.5910975234392952
          },
          {
            "accuracy": 0.5986227250368913,
            "f1": 0.5902690227585988,
            "f1_weighted": 0.6067568224176482
          },
          {
            "accuracy": 0.6094441711756026,
            "f1": 0.5903626641443895,
            "f1_weighted": 0.6037165754082495
          },
          {
            "accuracy": 0.5700934579439252,
            "f1": 0.561204751672256,
            "f1_weighted": 0.5659466255895513
          },
          {
            "accuracy": 0.5828824397442204,
            "f1": 0.5710972702803514,
            "f1_weighted": 0.5808419072783003
          }
        ]
      }
    ]
  },
  "task_name": "MassiveScenarioClassification"
}