{
  "dataset_revision": "4672e20407010da34463acc759c162ca9734bca6",
  "evaluation_time": 85.4723129272461,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.5713180901143241,
        "f1": 0.5439065975981693,
        "f1_weighted": 0.5730628715851473,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5713180901143241,
        "scores_per_experiment": [
          {
            "accuracy": 0.5827168796234028,
            "f1": 0.5588017134620537,
            "f1_weighted": 0.5853644273010283
          },
          {
            "accuracy": 0.5837256220578345,
            "f1": 0.5477369994418458,
            "f1_weighted": 0.5864659288448407
          },
          {
            "accuracy": 0.5669132481506388,
            "f1": 0.5352442794675514,
            "f1_weighted": 0.5638019170822862
          },
          {
            "accuracy": 0.5833893745796906,
            "f1": 0.5463022570845765,
            "f1_weighted": 0.5861187951971617
          },
          {
            "accuracy": 0.5857431069266981,
            "f1": 0.5452582764252905,
            "f1_weighted": 0.5827803584155383
          },
          {
            "accuracy": 0.5524546065904505,
            "f1": 0.537362548508193,
            "f1_weighted": 0.555360081446855
          },
          {
            "accuracy": 0.5521183591123067,
            "f1": 0.5391131282830379,
            "f1_weighted": 0.5532881818035832
          },
          {
            "accuracy": 0.5692669804976462,
            "f1": 0.541976297881966,
            "f1_weighted": 0.5749252317154644
          },
          {
            "accuracy": 0.5514458641560188,
            "f1": 0.5309572593373036,
            "f1_weighted": 0.5523744133791978
          },
          {
            "accuracy": 0.5854068594485541,
            "f1": 0.5563132160898756,
            "f1_weighted": 0.5901493806655179
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.5792916871618298,
        "f1": 0.5529104594678891,
        "f1_weighted": 0.5815642904354373,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5792916871618298,
        "scores_per_experiment": [
          {
            "accuracy": 0.5804230201672406,
            "f1": 0.5614360292754355,
            "f1_weighted": 0.5826346872758331
          },
          {
            "accuracy": 0.5991146089522873,
            "f1": 0.5614918534530007,
            "f1_weighted": 0.6009225449790742
          },
          {
            "accuracy": 0.5873093949827841,
            "f1": 0.5657734334522748,
            "f1_weighted": 0.5849286138970379
          },
          {
            "accuracy": 0.5799311362518446,
            "f1": 0.530908410878213,
            "f1_weighted": 0.5842772165647125
          },
          {
            "accuracy": 0.602557796360059,
            "f1": 0.5694907513541682,
            "f1_weighted": 0.6004213392702241
          },
          {
            "accuracy": 0.5632070831283817,
            "f1": 0.5546571143311797,
            "f1_weighted": 0.5677272310350471
          },
          {
            "accuracy": 0.5558288243974422,
            "f1": 0.5418577767235812,
            "f1_weighted": 0.557502874498418
          },
          {
            "accuracy": 0.558780127889818,
            "f1": 0.5293163710667358,
            "f1_weighted": 0.5606794266137348
          },
          {
            "accuracy": 0.5597638957206099,
            "f1": 0.5388143376176727,
            "f1_weighted": 0.5676259508204633
          },
          {
            "accuracy": 0.6060009837678307,
            "f1": 0.5753585165266286,
            "f1_weighted": 0.608923019399828
          }
        ]
      }
    ]
  },
  "task_name": "MassiveIntentClassification"
}