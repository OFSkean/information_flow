{
  "dataset_revision": "ae001d0e6b1228650b7bd1c2c65fb50ad11a8aba",
  "evaluation_time": 123.0686445236206,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.6694710442316462,
        "f1": 0.4622131780789235,
        "f1_weighted": 0.7097089208261265,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.6694710442316462,
        "scores_per_experiment": [
          {
            "accuracy": 0.6595987232102143,
            "f1": 0.45763846240976946,
            "f1_weighted": 0.6972501280087267
          },
          {
            "accuracy": 0.6696306429548564,
            "f1": 0.4492130678783917,
            "f1_weighted": 0.7085885429714549
          },
          {
            "accuracy": 0.6523027815777473,
            "f1": 0.45630578860235954,
            "f1_weighted": 0.6948435722110156
          },
          {
            "accuracy": 0.6757865937072504,
            "f1": 0.4609601386596955,
            "f1_weighted": 0.7153874283379122
          },
          {
            "accuracy": 0.6678066575467396,
            "f1": 0.4533863849458802,
            "f1_weighted": 0.7099739850336755
          },
          {
            "accuracy": 0.657546739626083,
            "f1": 0.4590721300726082,
            "f1_weighted": 0.7008958695970743
          },
          {
            "accuracy": 0.6673506611947104,
            "f1": 0.46687511968884565,
            "f1_weighted": 0.7068764668669401
          },
          {
            "accuracy": 0.7063383492932056,
            "f1": 0.5016960805033102,
            "f1_weighted": 0.7415395340889374
          },
          {
            "accuracy": 0.663702690378477,
            "f1": 0.45497723916083516,
            "f1_weighted": 0.7064490819834993
          },
          {
            "accuracy": 0.6746466028271774,
            "f1": 0.4620073688675386,
            "f1_weighted": 0.7152845991620288
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.6749888143176734,
        "f1": 0.45944519086913493,
        "f1_weighted": 0.7170528416575035,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.6749888143176734,
        "scores_per_experiment": [
          {
            "accuracy": 0.6577181208053692,
            "f1": 0.45041070591234394,
            "f1_weighted": 0.7008980193807942
          },
          {
            "accuracy": 0.6595078299776287,
            "f1": 0.44069135865813186,
            "f1_weighted": 0.7014240037163729
          },
          {
            "accuracy": 0.6724832214765101,
            "f1": 0.4513517165638051,
            "f1_weighted": 0.718361876131943
          },
          {
            "accuracy": 0.6769574944071588,
            "f1": 0.45182643605568584,
            "f1_weighted": 0.7176157937992738
          },
          {
            "accuracy": 0.6926174496644295,
            "f1": 0.48419982036527803,
            "f1_weighted": 0.7354203996400512
          },
          {
            "accuracy": 0.6747203579418345,
            "f1": 0.46327456064593475,
            "f1_weighted": 0.7197101394275016
          },
          {
            "accuracy": 0.6599552572706935,
            "f1": 0.448733768730121,
            "f1_weighted": 0.7007907937861242
          },
          {
            "accuracy": 0.7015659955257271,
            "f1": 0.4736640494170448,
            "f1_weighted": 0.7406606354585076
          },
          {
            "accuracy": 0.6675615212527964,
            "f1": 0.4461940170035005,
            "f1_weighted": 0.710304784376509
          },
          {
            "accuracy": 0.6868008948545862,
            "f1": 0.4841054753395032,
            "f1_weighted": 0.7253419708579569
          }
        ]
      }
    ]
  },
  "task_name": "MTOPIntentClassification"
}