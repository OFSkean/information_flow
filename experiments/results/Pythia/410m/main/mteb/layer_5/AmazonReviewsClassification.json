{
  "dataset_revision": "1399c76144fd37290681b995c656ef9b2e06e26d",
  "evaluation_time": 108.90480637550354,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.32796000000000003,
        "f1": 0.3268664627517796,
        "f1_weighted": 0.32686646275177966,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.32796000000000003,
        "scores_per_experiment": [
          {
            "accuracy": 0.3532,
            "f1": 0.3532693357664487,
            "f1_weighted": 0.3532693357664487
          },
          {
            "accuracy": 0.341,
            "f1": 0.3423899763790258,
            "f1_weighted": 0.34238997637902574
          },
          {
            "accuracy": 0.2966,
            "f1": 0.29695061782692034,
            "f1_weighted": 0.2969506178269204
          },
          {
            "accuracy": 0.3352,
            "f1": 0.33413957962750923,
            "f1_weighted": 0.33413957962750923
          },
          {
            "accuracy": 0.3768,
            "f1": 0.36896807684637095,
            "f1_weighted": 0.36896807684637095
          },
          {
            "accuracy": 0.2866,
            "f1": 0.28638536203110915,
            "f1_weighted": 0.2863853620311091
          },
          {
            "accuracy": 0.2824,
            "f1": 0.28352769292655616,
            "f1_weighted": 0.2835276929265561
          },
          {
            "accuracy": 0.3366,
            "f1": 0.334205895429147,
            "f1_weighted": 0.3342058954291471
          },
          {
            "accuracy": 0.3292,
            "f1": 0.3276200080467685,
            "f1_weighted": 0.32762000804676844
          },
          {
            "accuracy": 0.342,
            "f1": 0.3412080826379406,
            "f1_weighted": 0.3412080826379407
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.32964,
        "f1": 0.3282350798313197,
        "f1_weighted": 0.3282350798313197,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.32964,
        "scores_per_experiment": [
          {
            "accuracy": 0.3418,
            "f1": 0.34314887683723855,
            "f1_weighted": 0.34314887683723855
          },
          {
            "accuracy": 0.3358,
            "f1": 0.3398405264737653,
            "f1_weighted": 0.3398405264737652
          },
          {
            "accuracy": 0.3074,
            "f1": 0.30770199652971975,
            "f1_weighted": 0.3077019965297198
          },
          {
            "accuracy": 0.3282,
            "f1": 0.32650600724878254,
            "f1_weighted": 0.3265060072487826
          },
          {
            "accuracy": 0.3776,
            "f1": 0.3664745619191806,
            "f1_weighted": 0.36647456191918065
          },
          {
            "accuracy": 0.2908,
            "f1": 0.2878178436259436,
            "f1_weighted": 0.28781784362594365
          },
          {
            "accuracy": 0.2948,
            "f1": 0.2949496804481192,
            "f1_weighted": 0.2949496804481192
          },
          {
            "accuracy": 0.3304,
            "f1": 0.32782731895940864,
            "f1_weighted": 0.3278273189594087
          },
          {
            "accuracy": 0.3482,
            "f1": 0.34748726429370186,
            "f1_weighted": 0.3474872642937019
          },
          {
            "accuracy": 0.3414,
            "f1": 0.3405967219773369,
            "f1_weighted": 0.3405967219773369
          }
        ]
      }
    ]
  },
  "task_name": "AmazonReviewsClassification"
}