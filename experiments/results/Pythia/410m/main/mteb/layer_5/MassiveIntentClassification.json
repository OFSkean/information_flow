{
  "dataset_revision": "4672e20407010da34463acc759c162ca9734bca6",
  "evaluation_time": 82.87455582618713,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.5841291190316071,
        "f1": 0.54940098833493,
        "f1_weighted": 0.5872026873289939,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5841291190316071,
        "scores_per_experiment": [
          {
            "accuracy": 0.5938130464021519,
            "f1": 0.5689050187010736,
            "f1_weighted": 0.5962125195442958
          },
          {
            "accuracy": 0.597511768661735,
            "f1": 0.5577686020153723,
            "f1_weighted": 0.5992581785281785
          },
          {
            "accuracy": 0.5793544048419637,
            "f1": 0.5367711239454825,
            "f1_weighted": 0.5782164883553419
          },
          {
            "accuracy": 0.5917955615332885,
            "f1": 0.5407560976514119,
            "f1_weighted": 0.5963192683681977
          },
          {
            "accuracy": 0.5961667787491594,
            "f1": 0.5544209909847293,
            "f1_weighted": 0.5964571515161736
          },
          {
            "accuracy": 0.5618695359784802,
            "f1": 0.5443080573268452,
            "f1_weighted": 0.5643439453325876
          },
          {
            "accuracy": 0.5672494956287828,
            "f1": 0.5427539724945216,
            "f1_weighted": 0.5705049705802214
          },
          {
            "accuracy": 0.5746469401479489,
            "f1": 0.5336559225360223,
            "f1_weighted": 0.5818517153430863
          },
          {
            "accuracy": 0.582044384667115,
            "f1": 0.5531269766983247,
            "f1_weighted": 0.5878611875904386
          },
          {
            "accuracy": 0.5968392737054472,
            "f1": 0.5615431209955158,
            "f1_weighted": 0.6010014481314184
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.5953271028037382,
        "f1": 0.5651809832286765,
        "f1_weighted": 0.5966053134736414,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5953271028037382,
        "scores_per_experiment": [
          {
            "accuracy": 0.5902606984751598,
            "f1": 0.5672799424574256,
            "f1_weighted": 0.5924021834302976
          },
          {
            "accuracy": 0.6074766355140186,
            "f1": 0.57413349809959,
            "f1_weighted": 0.6081869756667684
          },
          {
            "accuracy": 0.617806197737334,
            "f1": 0.5807085720326455,
            "f1_weighted": 0.6172323330571057
          },
          {
            "accuracy": 0.5971470732907034,
            "f1": 0.5506470429511713,
            "f1_weighted": 0.5995593952922286
          },
          {
            "accuracy": 0.6055090998524348,
            "f1": 0.5795433762325747,
            "f1_weighted": 0.6033762894680532
          },
          {
            "accuracy": 0.5794392523364486,
            "f1": 0.5659057076535186,
            "f1_weighted": 0.5819726999440196
          },
          {
            "accuracy": 0.5651746187899656,
            "f1": 0.5429301133293086,
            "f1_weighted": 0.5677097511304829
          },
          {
            "accuracy": 0.5804230201672406,
            "f1": 0.5416647315269041,
            "f1_weighted": 0.5807274501602194
          },
          {
            "accuracy": 0.5838662075750123,
            "f1": 0.5597265409603303,
            "f1_weighted": 0.5885678289489075
          },
          {
            "accuracy": 0.6261682242990654,
            "f1": 0.5892703070432957,
            "f1_weighted": 0.6263182276383317
          }
        ]
      }
    ]
  },
  "task_name": "MassiveIntentClassification"
}