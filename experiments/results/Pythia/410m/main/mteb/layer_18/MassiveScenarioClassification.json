{
  "dataset_revision": "fad2c6e8459f9e1c45d9315f4953d921437d70f8",
  "evaluation_time": 67.90717554092407,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.6356758574310694,
        "f1": 0.6240927907314415,
        "f1_weighted": 0.637611623276711,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.6356758574310694,
        "scores_per_experiment": [
          {
            "accuracy": 0.6408876933422999,
            "f1": 0.6367421087374479,
            "f1_weighted": 0.6438873838872403
          },
          {
            "accuracy": 0.640551445864156,
            "f1": 0.63021416121,
            "f1_weighted": 0.6366360632376169
          },
          {
            "accuracy": 0.6139878950907868,
            "f1": 0.6142271129119258,
            "f1_weighted": 0.6186830105662032
          },
          {
            "accuracy": 0.6503026227303296,
            "f1": 0.6329333168858697,
            "f1_weighted": 0.6508911596308524
          },
          {
            "accuracy": 0.6684599865501009,
            "f1": 0.6441381244754578,
            "f1_weighted": 0.6694770485697228
          },
          {
            "accuracy": 0.6230665770006725,
            "f1": 0.6127664997191397,
            "f1_weighted": 0.6208296853512949
          },
          {
            "accuracy": 0.6334902488231339,
            "f1": 0.6165363352870621,
            "f1_weighted": 0.6366075407405775
          },
          {
            "accuracy": 0.6418964357767317,
            "f1": 0.627767989281431,
            "f1_weighted": 0.6461382385730459
          },
          {
            "accuracy": 0.6375252185608608,
            "f1": 0.62470388599843,
            "f1_weighted": 0.6406599075608614
          },
          {
            "accuracy": 0.6065904505716208,
            "f1": 0.6008983728076508,
            "f1_weighted": 0.6123061946496937
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.630841121495327,
        "f1": 0.6223935550213937,
        "f1_weighted": 0.6333798912997542,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.630841121495327,
        "scores_per_experiment": [
          {
            "accuracy": 0.631578947368421,
            "f1": 0.6263883364642459,
            "f1_weighted": 0.6338662610619692
          },
          {
            "accuracy": 0.632070831283817,
            "f1": 0.6306947601110857,
            "f1_weighted": 0.6304135529827141
          },
          {
            "accuracy": 0.6305951795376291,
            "f1": 0.6308885481268519,
            "f1_weighted": 0.6361797561735962
          },
          {
            "accuracy": 0.6266601082144614,
            "f1": 0.6118293903262746,
            "f1_weighted": 0.6274168396817239
          },
          {
            "accuracy": 0.661091982292179,
            "f1": 0.6404612604790612,
            "f1_weighted": 0.6634890891162176
          },
          {
            "accuracy": 0.6227250368912937,
            "f1": 0.6211718596385135,
            "f1_weighted": 0.6217299120513955
          },
          {
            "accuracy": 0.6251844564682735,
            "f1": 0.6127285121478395,
            "f1_weighted": 0.6272098248159658
          },
          {
            "accuracy": 0.6281357599606493,
            "f1": 0.6118106109221553,
            "f1_weighted": 0.632774531756248
          },
          {
            "accuracy": 0.6261682242990654,
            "f1": 0.6199460992843236,
            "f1_weighted": 0.6311206249697142
          },
          {
            "accuracy": 0.6242006886374816,
            "f1": 0.6180161727135861,
            "f1_weighted": 0.6295985203879977
          }
        ]
      }
    ]
  },
  "task_name": "MassiveScenarioClassification"
}