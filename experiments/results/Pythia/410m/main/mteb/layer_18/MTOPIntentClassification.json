{
  "dataset_revision": "ae001d0e6b1228650b7bd1c2c65fb50ad11a8aba",
  "evaluation_time": 130.72280168533325,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.6884176926584588,
        "f1": 0.47086199241923204,
        "f1_weighted": 0.7273055600737398,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.6884176926584588,
        "scores_per_experiment": [
          {
            "accuracy": 0.6735066119471044,
            "f1": 0.4533251296032201,
            "f1_weighted": 0.7144212285034478
          },
          {
            "accuracy": 0.6958504331965344,
            "f1": 0.47545188624701257,
            "f1_weighted": 0.7349604009650753
          },
          {
            "accuracy": 0.6985864113087096,
            "f1": 0.4684359067839426,
            "f1_weighted": 0.7373713337210506
          },
          {
            "accuracy": 0.6833105335157319,
            "f1": 0.47230734392805196,
            "f1_weighted": 0.7252309643827703
          },
          {
            "accuracy": 0.6935704514363885,
            "f1": 0.4785622287709008,
            "f1_weighted": 0.7294769116001291
          },
          {
            "accuracy": 0.6837665298677611,
            "f1": 0.47360642721382146,
            "f1_weighted": 0.7274146429806915
          },
          {
            "accuracy": 0.6835385316917465,
            "f1": 0.48841901531817,
            "f1_weighted": 0.7232324246177994
          },
          {
            "accuracy": 0.7088463292293662,
            "f1": 0.48677526098207197,
            "f1_weighted": 0.7449667692499015
          },
          {
            "accuracy": 0.6803465572275422,
            "f1": 0.4575308063116199,
            "f1_weighted": 0.7138551784125301
          },
          {
            "accuracy": 0.6828545371637027,
            "f1": 0.45420591903350943,
            "f1_weighted": 0.722125746304003
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.6886800894854586,
        "f1": 0.45616874955669295,
        "f1_weighted": 0.7293002459356752,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.6886800894854586,
        "scores_per_experiment": [
          {
            "accuracy": 0.6554809843400448,
            "f1": 0.4412251285342921,
            "f1_weighted": 0.6989413526730488
          },
          {
            "accuracy": 0.6993288590604027,
            "f1": 0.4583577015198083,
            "f1_weighted": 0.7381313136523501
          },
          {
            "accuracy": 0.6975391498881431,
            "f1": 0.4464662677932362,
            "f1_weighted": 0.7373077666870032
          },
          {
            "accuracy": 0.6885906040268457,
            "f1": 0.46534709153824877,
            "f1_weighted": 0.7317001735567881
          },
          {
            "accuracy": 0.6917225950782998,
            "f1": 0.46034695449865987,
            "f1_weighted": 0.7279089630909753
          },
          {
            "accuracy": 0.6876957494407159,
            "f1": 0.4641247688401448,
            "f1_weighted": 0.7358573476651347
          },
          {
            "accuracy": 0.6774049217002237,
            "f1": 0.44419833094305544,
            "f1_weighted": 0.7194427917915508
          },
          {
            "accuracy": 0.7176733780760627,
            "f1": 0.4804281586811259,
            "f1_weighted": 0.7577290717213946
          },
          {
            "accuracy": 0.6854586129753915,
            "f1": 0.4486844685553544,
            "f1_weighted": 0.7214212438808357
          },
          {
            "accuracy": 0.6859060402684564,
            "f1": 0.4525086246630039,
            "f1_weighted": 0.7245624346376703
          }
        ]
      }
    ]
  },
  "task_name": "MTOPIntentClassification"
}