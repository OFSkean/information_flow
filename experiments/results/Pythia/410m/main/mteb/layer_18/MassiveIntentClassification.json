{
  "dataset_revision": "4672e20407010da34463acc759c162ca9734bca6",
  "evaluation_time": 92.73527336120605,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.5856422326832548,
        "f1": 0.5547997651597332,
        "f1_weighted": 0.5891461029142148,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5856422326832548,
        "scores_per_experiment": [
          {
            "accuracy": 0.5958305312710155,
            "f1": 0.5735660240150434,
            "f1_weighted": 0.5991639718636369
          },
          {
            "accuracy": 0.5911230665770006,
            "f1": 0.5624572349380295,
            "f1_weighted": 0.5946880023991724
          },
          {
            "accuracy": 0.5854068594485541,
            "f1": 0.549620837568573,
            "f1_weighted": 0.5887868953937456
          },
          {
            "accuracy": 0.5965030262273033,
            "f1": 0.5579907840929252,
            "f1_weighted": 0.5979866409641914
          },
          {
            "accuracy": 0.5938130464021519,
            "f1": 0.5506272161286083,
            "f1_weighted": 0.5960664365066289
          },
          {
            "accuracy": 0.5517821116341628,
            "f1": 0.5382084483057278,
            "f1_weighted": 0.5536772174392538
          },
          {
            "accuracy": 0.5716207128446537,
            "f1": 0.5542867936165318,
            "f1_weighted": 0.5749646796934641
          },
          {
            "accuracy": 0.5894418291862811,
            "f1": 0.5507761439136762,
            "f1_weighted": 0.5971869582452943
          },
          {
            "accuracy": 0.5776731674512441,
            "f1": 0.5455191732686048,
            "f1_weighted": 0.5842689977600156
          },
          {
            "accuracy": 0.6032279757901816,
            "f1": 0.5649449957496125,
            "f1_weighted": 0.604671228876744
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.5949827840629611,
        "f1": 0.5598335591299641,
        "f1_weighted": 0.5977488431926168,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5949827840629611,
        "scores_per_experiment": [
          {
            "accuracy": 0.5986227250368913,
            "f1": 0.5690846074598713,
            "f1_weighted": 0.6009144283637415
          },
          {
            "accuracy": 0.6050172159370388,
            "f1": 0.5601620949404952,
            "f1_weighted": 0.6106217888682204
          },
          {
            "accuracy": 0.6089522872602066,
            "f1": 0.5716222228092017,
            "f1_weighted": 0.6093636546432608
          },
          {
            "accuracy": 0.5912444663059518,
            "f1": 0.5461544015450897,
            "f1_weighted": 0.5917532764818051
          },
          {
            "accuracy": 0.5892769306443679,
            "f1": 0.5509185402479652,
            "f1_weighted": 0.5933946955688625
          },
          {
            "accuracy": 0.573044761436301,
            "f1": 0.5529415284561339,
            "f1_weighted": 0.5750048511362604
          },
          {
            "accuracy": 0.5705853418593212,
            "f1": 0.551094598000682,
            "f1_weighted": 0.5746804906055745
          },
          {
            "accuracy": 0.5971470732907034,
            "f1": 0.5559883997280602,
            "f1_weighted": 0.5999136032584651
          },
          {
            "accuracy": 0.5818986719134285,
            "f1": 0.5509932928578714,
            "f1_weighted": 0.5868777302249877
          },
          {
            "accuracy": 0.6340383669454008,
            "f1": 0.5893759052542714,
            "f1_weighted": 0.6349639127749898
          }
        ]
      }
    ]
  },
  "task_name": "MassiveIntentClassification"
}