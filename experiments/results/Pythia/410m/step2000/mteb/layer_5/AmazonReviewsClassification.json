{
  "dataset_revision": "1399c76144fd37290681b995c656ef9b2e06e26d",
  "evaluation_time": 26.74876832962036,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.20",
  "scores": {
    "test": [
      {
        "accuracy": 0.32814,
        "f1": 0.326329569833481,
        "f1_weighted": 0.326329569833481,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.32814,
        "scores_per_experiment": [
          {
            "accuracy": 0.3324,
            "f1": 0.3330389345890607,
            "f1_weighted": 0.33303893458906075
          },
          {
            "accuracy": 0.34,
            "f1": 0.34285168959792794,
            "f1_weighted": 0.3428516895979279
          },
          {
            "accuracy": 0.2908,
            "f1": 0.2926251116854819,
            "f1_weighted": 0.2926251116854819
          },
          {
            "accuracy": 0.3224,
            "f1": 0.32190538241078326,
            "f1_weighted": 0.3219053824107832
          },
          {
            "accuracy": 0.3922,
            "f1": 0.38143476682225674,
            "f1_weighted": 0.3814347668222567
          },
          {
            "accuracy": 0.324,
            "f1": 0.3210061915273916,
            "f1_weighted": 0.3210061915273916
          },
          {
            "accuracy": 0.2824,
            "f1": 0.2865049563202636,
            "f1_weighted": 0.2865049563202637
          },
          {
            "accuracy": 0.3522,
            "f1": 0.34395222910292655,
            "f1_weighted": 0.34395222910292667
          },
          {
            "accuracy": 0.3154,
            "f1": 0.3124731778863007,
            "f1_weighted": 0.31247317788630063
          },
          {
            "accuracy": 0.3296,
            "f1": 0.327503258392417,
            "f1_weighted": 0.3275032583924169
          }
        ]
      }
    ]
  },
  "task_name": "AmazonReviewsClassification"
}