{
  "dataset_revision": "1399c76144fd37290681b995c656ef9b2e06e26d",
  "evaluation_time": 21.3908371925354,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.20",
  "scores": {
    "test": [
      {
        "accuracy": 0.31574,
        "f1": 0.31367650992402385,
        "f1_weighted": 0.31367650992402385,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.31574,
        "scores_per_experiment": [
          {
            "accuracy": 0.3348,
            "f1": 0.336842551591866,
            "f1_weighted": 0.336842551591866
          },
          {
            "accuracy": 0.316,
            "f1": 0.3205271242037953,
            "f1_weighted": 0.3205271242037953
          },
          {
            "accuracy": 0.3008,
            "f1": 0.2996695123688317,
            "f1_weighted": 0.2996695123688317
          },
          {
            "accuracy": 0.3154,
            "f1": 0.31911013019932744,
            "f1_weighted": 0.3191101301993275
          },
          {
            "accuracy": 0.3724,
            "f1": 0.36336444378994714,
            "f1_weighted": 0.3633644437899472
          },
          {
            "accuracy": 0.299,
            "f1": 0.29545642633443964,
            "f1_weighted": 0.29545642633443975
          },
          {
            "accuracy": 0.2628,
            "f1": 0.26503773199699804,
            "f1_weighted": 0.26503773199699804
          },
          {
            "accuracy": 0.344,
            "f1": 0.33277156521509677,
            "f1_weighted": 0.33277156521509677
          },
          {
            "accuracy": 0.2932,
            "f1": 0.28950592399161434,
            "f1_weighted": 0.2895059239916144
          },
          {
            "accuracy": 0.319,
            "f1": 0.3144796895483225,
            "f1_weighted": 0.3144796895483225
          }
        ]
      }
    ]
  },
  "task_name": "AmazonReviewsClassification"
}