{
  "dataset_revision": "1399c76144fd37290681b995c656ef9b2e06e26d",
  "evaluation_time": 60.88134479522705,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.20",
  "scores": {
    "test": [
      {
        "accuracy": 0.31286,
        "f1": 0.3113481351381122,
        "f1_weighted": 0.3113481351381122,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.31286,
        "scores_per_experiment": [
          {
            "accuracy": 0.3152,
            "f1": 0.3099337878377087,
            "f1_weighted": 0.3099337878377087
          },
          {
            "accuracy": 0.3174,
            "f1": 0.3173277467376461,
            "f1_weighted": 0.3173277467376461
          },
          {
            "accuracy": 0.2754,
            "f1": 0.2766613402112676,
            "f1_weighted": 0.27666134021126754
          },
          {
            "accuracy": 0.3042,
            "f1": 0.30382251761199913,
            "f1_weighted": 0.3038225176119991
          },
          {
            "accuracy": 0.3554,
            "f1": 0.35163313450640443,
            "f1_weighted": 0.3516331345064044
          },
          {
            "accuracy": 0.3294,
            "f1": 0.3246500475437032,
            "f1_weighted": 0.3246500475437032
          },
          {
            "accuracy": 0.2584,
            "f1": 0.2589147053486968,
            "f1_weighted": 0.25891470534869687
          },
          {
            "accuracy": 0.3404,
            "f1": 0.3346826202200296,
            "f1_weighted": 0.3346826202200296
          },
          {
            "accuracy": 0.3118,
            "f1": 0.31297933193507094,
            "f1_weighted": 0.3129793319350709
          },
          {
            "accuracy": 0.321,
            "f1": 0.32287611942859573,
            "f1_weighted": 0.3228761194285958
          }
        ]
      }
    ]
  },
  "task_name": "AmazonReviewsClassification"
}