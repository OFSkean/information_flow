{
  "dataset_revision": "1399c76144fd37290681b995c656ef9b2e06e26d",
  "evaluation_time": 14.400651931762695,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.20",
  "scores": {
    "test": [
      {
        "accuracy": 0.27946000000000004,
        "f1": 0.2762355242508697,
        "f1_weighted": 0.2762355242508696,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.27946000000000004,
        "scores_per_experiment": [
          {
            "accuracy": 0.3094,
            "f1": 0.31095743604854026,
            "f1_weighted": 0.31095743604854026
          },
          {
            "accuracy": 0.2832,
            "f1": 0.2826125300386025,
            "f1_weighted": 0.2826125300386025
          },
          {
            "accuracy": 0.272,
            "f1": 0.2695906539827185,
            "f1_weighted": 0.26959065398271853
          },
          {
            "accuracy": 0.2716,
            "f1": 0.26961711680975403,
            "f1_weighted": 0.26961711680975403
          },
          {
            "accuracy": 0.3184,
            "f1": 0.3100861014424476,
            "f1_weighted": 0.3100861014424476
          },
          {
            "accuracy": 0.246,
            "f1": 0.24383827735351665,
            "f1_weighted": 0.2438382773535166
          },
          {
            "accuracy": 0.2542,
            "f1": 0.25212110082778716,
            "f1_weighted": 0.25212110082778716
          },
          {
            "accuracy": 0.2894,
            "f1": 0.27877558918138173,
            "f1_weighted": 0.2787755891813817
          },
          {
            "accuracy": 0.2646,
            "f1": 0.26307193614818775,
            "f1_weighted": 0.2630719361481877
          },
          {
            "accuracy": 0.2858,
            "f1": 0.2816845006757607,
            "f1_weighted": 0.2816845006757607
          }
        ]
      }
    ]
  },
  "task_name": "AmazonReviewsClassification"
}