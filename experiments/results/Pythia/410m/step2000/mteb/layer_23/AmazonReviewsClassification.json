{
  "dataset_revision": "1399c76144fd37290681b995c656ef9b2e06e26d",
  "evaluation_time": 66.63235831260681,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.20",
  "scores": {
    "test": [
      {
        "accuracy": 0.30726,
        "f1": 0.3059069063562435,
        "f1_weighted": 0.3059069063562435,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.30726,
        "scores_per_experiment": [
          {
            "accuracy": 0.3122,
            "f1": 0.30748747299652074,
            "f1_weighted": 0.30748747299652074
          },
          {
            "accuracy": 0.3132,
            "f1": 0.3128317549702674,
            "f1_weighted": 0.31283175497026733
          },
          {
            "accuracy": 0.2724,
            "f1": 0.2736258548130339,
            "f1_weighted": 0.273625854813034
          },
          {
            "accuracy": 0.3008,
            "f1": 0.3016406432514218,
            "f1_weighted": 0.30164064325142187
          },
          {
            "accuracy": 0.35,
            "f1": 0.34489551250543604,
            "f1_weighted": 0.34489551250543604
          },
          {
            "accuracy": 0.3214,
            "f1": 0.31656798803083197,
            "f1_weighted": 0.31656798803083197
          },
          {
            "accuracy": 0.246,
            "f1": 0.24683012435169993,
            "f1_weighted": 0.24683012435169993
          },
          {
            "accuracy": 0.3364,
            "f1": 0.3316916285566167,
            "f1_weighted": 0.3316916285566167
          },
          {
            "accuracy": 0.3186,
            "f1": 0.31974801926694985,
            "f1_weighted": 0.3197480192669498
          },
          {
            "accuracy": 0.3016,
            "f1": 0.3037500648196566,
            "f1_weighted": 0.30375006481965666
          }
        ]
      }
    ]
  },
  "task_name": "AmazonReviewsClassification"
}