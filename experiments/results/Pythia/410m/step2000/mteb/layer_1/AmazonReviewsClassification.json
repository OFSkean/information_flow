{
  "dataset_revision": "1399c76144fd37290681b995c656ef9b2e06e26d",
  "evaluation_time": 17.02847123146057,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.20",
  "scores": {
    "test": [
      {
        "accuracy": 0.3134,
        "f1": 0.3108820546986754,
        "f1_weighted": 0.31088205469867547,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.3134,
        "scores_per_experiment": [
          {
            "accuracy": 0.3392,
            "f1": 0.33896014222271453,
            "f1_weighted": 0.33896014222271453
          },
          {
            "accuracy": 0.3036,
            "f1": 0.3095325446722554,
            "f1_weighted": 0.30953254467225544
          },
          {
            "accuracy": 0.3068,
            "f1": 0.30650406694609933,
            "f1_weighted": 0.3065040669460994
          },
          {
            "accuracy": 0.312,
            "f1": 0.31371023849584284,
            "f1_weighted": 0.3137102384958428
          },
          {
            "accuracy": 0.3814,
            "f1": 0.3713968038036469,
            "f1_weighted": 0.371396803803647
          },
          {
            "accuracy": 0.2814,
            "f1": 0.2793268118402847,
            "f1_weighted": 0.2793268118402847
          },
          {
            "accuracy": 0.241,
            "f1": 0.2409922338249777,
            "f1_weighted": 0.24099223382497767
          },
          {
            "accuracy": 0.3422,
            "f1": 0.32930201478539406,
            "f1_weighted": 0.32930201478539406
          },
          {
            "accuracy": 0.3024,
            "f1": 0.29879950015142726,
            "f1_weighted": 0.2987995001514272
          },
          {
            "accuracy": 0.324,
            "f1": 0.3202961902441117,
            "f1_weighted": 0.32029619024411177
          }
        ]
      }
    ]
  },
  "task_name": "AmazonReviewsClassification"
}