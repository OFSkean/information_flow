{
  "dataset_revision": "1399c76144fd37290681b995c656ef9b2e06e26d",
  "evaluation_time": 42.86309218406677,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.20",
  "scores": {
    "test": [
      {
        "accuracy": 0.32616,
        "f1": 0.32384420855187424,
        "f1_weighted": 0.32384420855187424,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.32616,
        "scores_per_experiment": [
          {
            "accuracy": 0.329,
            "f1": 0.3284051344114092,
            "f1_weighted": 0.3284051344114092
          },
          {
            "accuracy": 0.338,
            "f1": 0.3382718311358756,
            "f1_weighted": 0.33827183113587556
          },
          {
            "accuracy": 0.2936,
            "f1": 0.2958876490472992,
            "f1_weighted": 0.29588764904729925
          },
          {
            "accuracy": 0.3262,
            "f1": 0.32670422024107326,
            "f1_weighted": 0.32670422024107326
          },
          {
            "accuracy": 0.3778,
            "f1": 0.366237145781879,
            "f1_weighted": 0.366237145781879
          },
          {
            "accuracy": 0.3274,
            "f1": 0.3197735751787277,
            "f1_weighted": 0.3197735751787276
          },
          {
            "accuracy": 0.2806,
            "f1": 0.28377437309003173,
            "f1_weighted": 0.28377437309003173
          },
          {
            "accuracy": 0.348,
            "f1": 0.3362130624251582,
            "f1_weighted": 0.3362130624251582
          },
          {
            "accuracy": 0.3028,
            "f1": 0.3048190128828402,
            "f1_weighted": 0.30481901288284025
          },
          {
            "accuracy": 0.3382,
            "f1": 0.33835608132444817,
            "f1_weighted": 0.33835608132444817
          }
        ]
      }
    ]
  },
  "task_name": "AmazonReviewsClassification"
}