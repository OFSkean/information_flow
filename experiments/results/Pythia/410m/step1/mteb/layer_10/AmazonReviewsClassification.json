{
  "dataset_revision": "1399c76144fd37290681b995c656ef9b2e06e26d",
  "evaluation_time": 35.58932447433472,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.20",
  "scores": {
    "test": [
      {
        "accuracy": 0.27776,
        "f1": 0.2732029026868095,
        "f1_weighted": 0.2732029026868095,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.27776,
        "scores_per_experiment": [
          {
            "accuracy": 0.2828,
            "f1": 0.2813870057103631,
            "f1_weighted": 0.2813870057103631
          },
          {
            "accuracy": 0.301,
            "f1": 0.2977474176895699,
            "f1_weighted": 0.29774741768956986
          },
          {
            "accuracy": 0.259,
            "f1": 0.24990611900749773,
            "f1_weighted": 0.24990611900749768
          },
          {
            "accuracy": 0.2584,
            "f1": 0.2531269567627973,
            "f1_weighted": 0.2531269567627973
          },
          {
            "accuracy": 0.3314,
            "f1": 0.32281691724992523,
            "f1_weighted": 0.32281691724992523
          },
          {
            "accuracy": 0.2652,
            "f1": 0.26156698676017787,
            "f1_weighted": 0.2615669867601779
          },
          {
            "accuracy": 0.2444,
            "f1": 0.24184107670729832,
            "f1_weighted": 0.24184107670729837
          },
          {
            "accuracy": 0.3032,
            "f1": 0.3032404679574316,
            "f1_weighted": 0.3032404679574316
          },
          {
            "accuracy": 0.2704,
            "f1": 0.2647011215100892,
            "f1_weighted": 0.26470112151008923
          },
          {
            "accuracy": 0.2618,
            "f1": 0.25569495751294513,
            "f1_weighted": 0.2556949575129452
          }
        ]
      }
    ]
  },
  "task_name": "AmazonReviewsClassification"
}