{
  "dataset_revision": "1399c76144fd37290681b995c656ef9b2e06e26d",
  "evaluation_time": 44.975316286087036,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.20",
  "scores": {
    "test": [
      {
        "accuracy": 0.27852,
        "f1": 0.27409408214919984,
        "f1_weighted": 0.27409408214919984,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.27852,
        "scores_per_experiment": [
          {
            "accuracy": 0.2768,
            "f1": 0.2758564425809538,
            "f1_weighted": 0.2758564425809538
          },
          {
            "accuracy": 0.2958,
            "f1": 0.2916597005473277,
            "f1_weighted": 0.29165970054732765
          },
          {
            "accuracy": 0.2588,
            "f1": 0.2496599459424517,
            "f1_weighted": 0.24965994594245172
          },
          {
            "accuracy": 0.2606,
            "f1": 0.2551699825433072,
            "f1_weighted": 0.2551699825433072
          },
          {
            "accuracy": 0.3368,
            "f1": 0.3273092958067747,
            "f1_weighted": 0.3273092958067747
          },
          {
            "accuracy": 0.2626,
            "f1": 0.2588429903721264,
            "f1_weighted": 0.2588429903721263
          },
          {
            "accuracy": 0.2472,
            "f1": 0.24494866497069218,
            "f1_weighted": 0.24494866497069215
          },
          {
            "accuracy": 0.3062,
            "f1": 0.30665799942454514,
            "f1_weighted": 0.3066579994245451
          },
          {
            "accuracy": 0.2772,
            "f1": 0.2736196489948345,
            "f1_weighted": 0.2736196489948345
          },
          {
            "accuracy": 0.2632,
            "f1": 0.25721615030898476,
            "f1_weighted": 0.2572161503089848
          }
        ]
      }
    ]
  },
  "task_name": "AmazonReviewsClassification"
}