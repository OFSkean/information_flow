{
  "dataset_revision": "1399c76144fd37290681b995c656ef9b2e06e26d",
  "evaluation_time": 50.254974603652954,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.20",
  "scores": {
    "test": [
      {
        "accuracy": 0.28062,
        "f1": 0.27633527870510727,
        "f1_weighted": 0.27633527870510727,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.28062,
        "scores_per_experiment": [
          {
            "accuracy": 0.2808,
            "f1": 0.28122669583125964,
            "f1_weighted": 0.28122669583125964
          },
          {
            "accuracy": 0.3006,
            "f1": 0.2972930219606953,
            "f1_weighted": 0.2972930219606953
          },
          {
            "accuracy": 0.2688,
            "f1": 0.25836160667512187,
            "f1_weighted": 0.2583616066751219
          },
          {
            "accuracy": 0.2522,
            "f1": 0.24604369900232897,
            "f1_weighted": 0.24604369900232895
          },
          {
            "accuracy": 0.3328,
            "f1": 0.32298635976085,
            "f1_weighted": 0.32298635976085
          },
          {
            "accuracy": 0.27,
            "f1": 0.2649454897158001,
            "f1_weighted": 0.26494548971580006
          },
          {
            "accuracy": 0.2586,
            "f1": 0.2576750765323907,
            "f1_weighted": 0.25767507653239075
          },
          {
            "accuracy": 0.3044,
            "f1": 0.30570578808076787,
            "f1_weighted": 0.30570578808076787
          },
          {
            "accuracy": 0.2788,
            "f1": 0.27431821078176344,
            "f1_weighted": 0.2743182107817635
          },
          {
            "accuracy": 0.2592,
            "f1": 0.2547968387100942,
            "f1_weighted": 0.2547968387100942
          }
        ]
      }
    ]
  },
  "task_name": "AmazonReviewsClassification"
}