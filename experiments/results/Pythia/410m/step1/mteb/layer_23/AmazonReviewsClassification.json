{
  "dataset_revision": "1399c76144fd37290681b995c656ef9b2e06e26d",
  "evaluation_time": 67.14129495620728,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.20",
  "scores": {
    "test": [
      {
        "accuracy": 0.2802,
        "f1": 0.2754690162073597,
        "f1_weighted": 0.2754690162073597,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.2802,
        "scores_per_experiment": [
          {
            "accuracy": 0.2818,
            "f1": 0.2799424128582765,
            "f1_weighted": 0.2799424128582764
          },
          {
            "accuracy": 0.298,
            "f1": 0.296521449022879,
            "f1_weighted": 0.296521449022879
          },
          {
            "accuracy": 0.2608,
            "f1": 0.2482306338090751,
            "f1_weighted": 0.24823063380907512
          },
          {
            "accuracy": 0.251,
            "f1": 0.24427802244928457,
            "f1_weighted": 0.24427802244928454
          },
          {
            "accuracy": 0.3324,
            "f1": 0.32308224895996646,
            "f1_weighted": 0.32308224895996646
          },
          {
            "accuracy": 0.2628,
            "f1": 0.25796508993462614,
            "f1_weighted": 0.25796508993462614
          },
          {
            "accuracy": 0.2694,
            "f1": 0.26852651163326685,
            "f1_weighted": 0.26852651163326685
          },
          {
            "accuracy": 0.3046,
            "f1": 0.3055476689182242,
            "f1_weighted": 0.3055476689182242
          },
          {
            "accuracy": 0.2756,
            "f1": 0.2716879209627461,
            "f1_weighted": 0.2716879209627461
          },
          {
            "accuracy": 0.2656,
            "f1": 0.25890820352525223,
            "f1_weighted": 0.2589082035252522
          }
        ]
      }
    ]
  },
  "task_name": "AmazonReviewsClassification"
}