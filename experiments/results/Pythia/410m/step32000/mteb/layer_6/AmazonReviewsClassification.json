{
  "dataset_revision": "1399c76144fd37290681b995c656ef9b2e06e26d",
  "evaluation_time": 27.945398330688477,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.20",
  "scores": {
    "test": [
      {
        "accuracy": 0.34872000000000003,
        "f1": 0.3477261294074109,
        "f1_weighted": 0.34772612940741077,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.34872000000000003,
        "scores_per_experiment": [
          {
            "accuracy": 0.3662,
            "f1": 0.3690164911752448,
            "f1_weighted": 0.3690164911752448
          },
          {
            "accuracy": 0.3596,
            "f1": 0.36142247676045175,
            "f1_weighted": 0.3614224767604517
          },
          {
            "accuracy": 0.3272,
            "f1": 0.33010502376230355,
            "f1_weighted": 0.33010502376230355
          },
          {
            "accuracy": 0.355,
            "f1": 0.3517981299339201,
            "f1_weighted": 0.3517981299339202
          },
          {
            "accuracy": 0.4078,
            "f1": 0.3945672849132768,
            "f1_weighted": 0.39456728491327675
          },
          {
            "accuracy": 0.3032,
            "f1": 0.30316057050451944,
            "f1_weighted": 0.3031605705045194
          },
          {
            "accuracy": 0.2996,
            "f1": 0.2982651339648328,
            "f1_weighted": 0.29826513396483273
          },
          {
            "accuracy": 0.3674,
            "f1": 0.36319806492872153,
            "f1_weighted": 0.36319806492872153
          },
          {
            "accuracy": 0.3438,
            "f1": 0.3484055262703052,
            "f1_weighted": 0.34840552627030513
          },
          {
            "accuracy": 0.3574,
            "f1": 0.35732259186053233,
            "f1_weighted": 0.3573225918605323
          }
        ]
      }
    ]
  },
  "task_name": "AmazonReviewsClassification"
}