{
  "dataset_revision": "1399c76144fd37290681b995c656ef9b2e06e26d",
  "evaluation_time": 35.82506275177002,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.20",
  "scores": {
    "test": [
      {
        "accuracy": 0.35534,
        "f1": 0.35297548942506823,
        "f1_weighted": 0.35297548942506823,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.35534,
        "scores_per_experiment": [
          {
            "accuracy": 0.3652,
            "f1": 0.36763246362512436,
            "f1_weighted": 0.3676324636251243
          },
          {
            "accuracy": 0.3726,
            "f1": 0.37310664294831114,
            "f1_weighted": 0.37310664294831114
          },
          {
            "accuracy": 0.3454,
            "f1": 0.34156987303906083,
            "f1_weighted": 0.3415698730390609
          },
          {
            "accuracy": 0.3662,
            "f1": 0.36523437735368997,
            "f1_weighted": 0.36523437735368997
          },
          {
            "accuracy": 0.4106,
            "f1": 0.3889123728151569,
            "f1_weighted": 0.3889123728151569
          },
          {
            "accuracy": 0.3192,
            "f1": 0.31881429364327263,
            "f1_weighted": 0.3188142936432726
          },
          {
            "accuracy": 0.3108,
            "f1": 0.3124461722185548,
            "f1_weighted": 0.3124461722185548
          },
          {
            "accuracy": 0.3682,
            "f1": 0.3651515116823987,
            "f1_weighted": 0.3651515116823986
          },
          {
            "accuracy": 0.3444,
            "f1": 0.3460946017184493,
            "f1_weighted": 0.3460946017184493
          },
          {
            "accuracy": 0.3508,
            "f1": 0.35079258520666434,
            "f1_weighted": 0.3507925852066643
          }
        ]
      }
    ]
  },
  "task_name": "AmazonReviewsClassification"
}