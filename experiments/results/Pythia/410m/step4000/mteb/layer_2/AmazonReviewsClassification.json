{
  "dataset_revision": "1399c76144fd37290681b995c656ef9b2e06e26d",
  "evaluation_time": 23.417880535125732,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.20",
  "scores": {
    "test": [
      {
        "accuracy": 0.3260200000000001,
        "f1": 0.3249691475493819,
        "f1_weighted": 0.32496914754938194,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.3260200000000001,
        "scores_per_experiment": [
          {
            "accuracy": 0.3446,
            "f1": 0.34732002662804967,
            "f1_weighted": 0.3473200266280496
          },
          {
            "accuracy": 0.3222,
            "f1": 0.3279117381416903,
            "f1_weighted": 0.32791173814169033
          },
          {
            "accuracy": 0.2994,
            "f1": 0.29854888277860353,
            "f1_weighted": 0.29854888277860353
          },
          {
            "accuracy": 0.3236,
            "f1": 0.32584243850531214,
            "f1_weighted": 0.3258424385053122
          },
          {
            "accuracy": 0.3804,
            "f1": 0.37253214478489227,
            "f1_weighted": 0.37253214478489227
          },
          {
            "accuracy": 0.3104,
            "f1": 0.3076611451677057,
            "f1_weighted": 0.30766114516770565
          },
          {
            "accuracy": 0.2806,
            "f1": 0.2838763160142591,
            "f1_weighted": 0.28387631601425917
          },
          {
            "accuracy": 0.3536,
            "f1": 0.34730525857681316,
            "f1_weighted": 0.34730525857681316
          },
          {
            "accuracy": 0.3042,
            "f1": 0.3017515582901676,
            "f1_weighted": 0.3017515582901676
          },
          {
            "accuracy": 0.3412,
            "f1": 0.33694196660632575,
            "f1_weighted": 0.33694196660632575
          }
        ]
      }
    ]
  },
  "task_name": "AmazonReviewsClassification"
}