{
  "dataset_revision": "1399c76144fd37290681b995c656ef9b2e06e26d",
  "evaluation_time": 29.229211807250977,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.20",
  "scores": {
    "test": [
      {
        "accuracy": 0.34602,
        "f1": 0.3456270186546459,
        "f1_weighted": 0.34562701865464585,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.34602,
        "scores_per_experiment": [
          {
            "accuracy": 0.3598,
            "f1": 0.36423518457241544,
            "f1_weighted": 0.36423518457241544
          },
          {
            "accuracy": 0.3536,
            "f1": 0.35641930867261556,
            "f1_weighted": 0.35641930867261556
          },
          {
            "accuracy": 0.314,
            "f1": 0.3175703260411503,
            "f1_weighted": 0.3175703260411504
          },
          {
            "accuracy": 0.3418,
            "f1": 0.34116020117679524,
            "f1_weighted": 0.3411602011767952
          },
          {
            "accuracy": 0.3988,
            "f1": 0.39102435956347986,
            "f1_weighted": 0.3910243595634799
          },
          {
            "accuracy": 0.3292,
            "f1": 0.32652464302031037,
            "f1_weighted": 0.32652464302031037
          },
          {
            "accuracy": 0.3096,
            "f1": 0.3099580100364531,
            "f1_weighted": 0.3099580100364531
          },
          {
            "accuracy": 0.3686,
            "f1": 0.36087038599587107,
            "f1_weighted": 0.360870385995871
          },
          {
            "accuracy": 0.3268,
            "f1": 0.3309695112480722,
            "f1_weighted": 0.33096951124807217
          },
          {
            "accuracy": 0.358,
            "f1": 0.3575382562192959,
            "f1_weighted": 0.35753825621929586
          }
        ]
      }
    ]
  },
  "task_name": "AmazonReviewsClassification"
}