{
  "dataset_revision": "1399c76144fd37290681b995c656ef9b2e06e26d",
  "evaluation_time": 33.41943430900574,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.20",
  "scores": {
    "test": [
      {
        "accuracy": 0.33622,
        "f1": 0.3361009519958376,
        "f1_weighted": 0.33610095199583767,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.33622,
        "scores_per_experiment": [
          {
            "accuracy": 0.3448,
            "f1": 0.34970376997099256,
            "f1_weighted": 0.34970376997099256
          },
          {
            "accuracy": 0.349,
            "f1": 0.3515596101881268,
            "f1_weighted": 0.35155961018812687
          },
          {
            "accuracy": 0.3106,
            "f1": 0.3119562149148262,
            "f1_weighted": 0.3119562149148262
          },
          {
            "accuracy": 0.3432,
            "f1": 0.34380462376718113,
            "f1_weighted": 0.34380462376718113
          },
          {
            "accuracy": 0.3876,
            "f1": 0.37919294773041823,
            "f1_weighted": 0.3791929477304182
          },
          {
            "accuracy": 0.3048,
            "f1": 0.30361255987427693,
            "f1_weighted": 0.303612559874277
          },
          {
            "accuracy": 0.2904,
            "f1": 0.2916265313624963,
            "f1_weighted": 0.2916265313624963
          },
          {
            "accuracy": 0.362,
            "f1": 0.3538735831538288,
            "f1_weighted": 0.35387358315382883
          },
          {
            "accuracy": 0.3158,
            "f1": 0.3201922479020046,
            "f1_weighted": 0.3201922479020046
          },
          {
            "accuracy": 0.354,
            "f1": 0.35548743109422454,
            "f1_weighted": 0.3554874310942245
          }
        ]
      }
    ]
  },
  "task_name": "AmazonReviewsClassification"
}