{
  "dataset_revision": "1399c76144fd37290681b995c656ef9b2e06e26d",
  "evaluation_time": 48.397865295410156,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.20",
  "scores": {
    "test": [
      {
        "accuracy": 0.3432,
        "f1": 0.3411405685178063,
        "f1_weighted": 0.3411405685178063,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.3432,
        "scores_per_experiment": [
          {
            "accuracy": 0.3484,
            "f1": 0.35078321743913177,
            "f1_weighted": 0.3507832174391317
          },
          {
            "accuracy": 0.3522,
            "f1": 0.353426322397839,
            "f1_weighted": 0.35342632239783905
          },
          {
            "accuracy": 0.3382,
            "f1": 0.33254250644188854,
            "f1_weighted": 0.3325425064418885
          },
          {
            "accuracy": 0.341,
            "f1": 0.33795492292603435,
            "f1_weighted": 0.33795492292603435
          },
          {
            "accuracy": 0.3836,
            "f1": 0.37171860906451404,
            "f1_weighted": 0.37171860906451404
          },
          {
            "accuracy": 0.3294,
            "f1": 0.3295202373408467,
            "f1_weighted": 0.3295202373408466
          },
          {
            "accuracy": 0.2996,
            "f1": 0.3007798025686131,
            "f1_weighted": 0.3007798025686131
          },
          {
            "accuracy": 0.3644,
            "f1": 0.356969840498406,
            "f1_weighted": 0.356969840498406
          },
          {
            "accuracy": 0.3202,
            "f1": 0.32349705567197573,
            "f1_weighted": 0.32349705567197573
          },
          {
            "accuracy": 0.355,
            "f1": 0.35421317082881376,
            "f1_weighted": 0.35421317082881376
          }
        ]
      }
    ]
  },
  "task_name": "AmazonReviewsClassification"
}