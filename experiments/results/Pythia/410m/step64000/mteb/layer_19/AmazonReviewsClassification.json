{
  "dataset_revision": "1399c76144fd37290681b995c656ef9b2e06e26d",
  "evaluation_time": 58.435628175735474,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.20",
  "scores": {
    "test": [
      {
        "accuracy": 0.35978,
        "f1": 0.3577151067428294,
        "f1_weighted": 0.3577151067428294,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.35978,
        "scores_per_experiment": [
          {
            "accuracy": 0.38,
            "f1": 0.37540468045687947,
            "f1_weighted": 0.3754046804568796
          },
          {
            "accuracy": 0.3766,
            "f1": 0.3745674318859137,
            "f1_weighted": 0.3745674318859137
          },
          {
            "accuracy": 0.3476,
            "f1": 0.33817967999148,
            "f1_weighted": 0.33817967999148
          },
          {
            "accuracy": 0.3672,
            "f1": 0.36989638430826977,
            "f1_weighted": 0.36989638430826977
          },
          {
            "accuracy": 0.3754,
            "f1": 0.3659739073567795,
            "f1_weighted": 0.3659739073567795
          },
          {
            "accuracy": 0.3342,
            "f1": 0.335819077118681,
            "f1_weighted": 0.335819077118681
          },
          {
            "accuracy": 0.3444,
            "f1": 0.3449523883334573,
            "f1_weighted": 0.3449523883334573
          },
          {
            "accuracy": 0.3704,
            "f1": 0.3702293802826234,
            "f1_weighted": 0.3702293802826234
          },
          {
            "accuracy": 0.347,
            "f1": 0.3462289469410106,
            "f1_weighted": 0.3462289469410106
          },
          {
            "accuracy": 0.355,
            "f1": 0.35589919075319953,
            "f1_weighted": 0.3558991907531995
          }
        ]
      }
    ]
  },
  "task_name": "AmazonReviewsClassification"
}