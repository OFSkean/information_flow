{
  "dataset_revision": "1399c76144fd37290681b995c656ef9b2e06e26d",
  "evaluation_time": 20.601642370224,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.20",
  "scores": {
    "test": [
      {
        "accuracy": 0.32288,
        "f1": 0.32149793230635904,
        "f1_weighted": 0.32149793230635904,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.32288,
        "scores_per_experiment": [
          {
            "accuracy": 0.344,
            "f1": 0.3440692131995289,
            "f1_weighted": 0.3440692131995288
          },
          {
            "accuracy": 0.3244,
            "f1": 0.32726149612736977,
            "f1_weighted": 0.3272614961273697
          },
          {
            "accuracy": 0.3116,
            "f1": 0.311356587189056,
            "f1_weighted": 0.31135658718905596
          },
          {
            "accuracy": 0.3212,
            "f1": 0.31745077714313047,
            "f1_weighted": 0.31745077714313047
          },
          {
            "accuracy": 0.367,
            "f1": 0.36201696200854905,
            "f1_weighted": 0.36201696200854905
          },
          {
            "accuracy": 0.2834,
            "f1": 0.28337745756472343,
            "f1_weighted": 0.2833774575647235
          },
          {
            "accuracy": 0.2888,
            "f1": 0.28885425771713397,
            "f1_weighted": 0.28885425771713397
          },
          {
            "accuracy": 0.3376,
            "f1": 0.334981861627583,
            "f1_weighted": 0.33498186162758303
          },
          {
            "accuracy": 0.319,
            "f1": 0.3172469744414304,
            "f1_weighted": 0.3172469744414304
          },
          {
            "accuracy": 0.3318,
            "f1": 0.3283637360450854,
            "f1_weighted": 0.3283637360450854
          }
        ]
      }
    ]
  },
  "task_name": "AmazonReviewsClassification"
}