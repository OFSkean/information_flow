{
  "dataset_revision": "1399c76144fd37290681b995c656ef9b2e06e26d",
  "evaluation_time": 52.309558391571045,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.20",
  "scores": {
    "test": [
      {
        "accuracy": 0.36836,
        "f1": 0.3653886799426872,
        "f1_weighted": 0.36538867994268726,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.36836,
        "scores_per_experiment": [
          {
            "accuracy": 0.3836,
            "f1": 0.38149010356760404,
            "f1_weighted": 0.381490103567604
          },
          {
            "accuracy": 0.3808,
            "f1": 0.37782058899254,
            "f1_weighted": 0.37782058899254
          },
          {
            "accuracy": 0.357,
            "f1": 0.3472471455423641,
            "f1_weighted": 0.3472471455423641
          },
          {
            "accuracy": 0.3852,
            "f1": 0.38558151810999164,
            "f1_weighted": 0.38558151810999164
          },
          {
            "accuracy": 0.388,
            "f1": 0.37620500910200566,
            "f1_weighted": 0.37620500910200566
          },
          {
            "accuracy": 0.339,
            "f1": 0.33822356621617516,
            "f1_weighted": 0.33822356621617516
          },
          {
            "accuracy": 0.3536,
            "f1": 0.3557337458140794,
            "f1_weighted": 0.3557337458140795
          },
          {
            "accuracy": 0.37,
            "f1": 0.3682868141255914,
            "f1_weighted": 0.36828681412559133
          },
          {
            "accuracy": 0.3564,
            "f1": 0.35661987897741376,
            "f1_weighted": 0.3566198789774138
          },
          {
            "accuracy": 0.37,
            "f1": 0.3666784289791072,
            "f1_weighted": 0.36667842897910724
          }
        ]
      }
    ]
  },
  "task_name": "AmazonReviewsClassification"
}