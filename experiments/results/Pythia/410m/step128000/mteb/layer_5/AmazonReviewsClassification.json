{
  "dataset_revision": "1399c76144fd37290681b995c656ef9b2e06e26d",
  "evaluation_time": 27.052351713180542,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.20",
  "scores": {
    "test": [
      {
        "accuracy": 0.32863999999999993,
        "f1": 0.3279220189846431,
        "f1_weighted": 0.3279220189846432,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.32863999999999993,
        "scores_per_experiment": [
          {
            "accuracy": 0.3552,
            "f1": 0.3565193148672323,
            "f1_weighted": 0.3565193148672323
          },
          {
            "accuracy": 0.3412,
            "f1": 0.3418598786853341,
            "f1_weighted": 0.34185987868533413
          },
          {
            "accuracy": 0.2994,
            "f1": 0.29920377430934647,
            "f1_weighted": 0.2992037743093465
          },
          {
            "accuracy": 0.3346,
            "f1": 0.33488886192350165,
            "f1_weighted": 0.33488886192350176
          },
          {
            "accuracy": 0.3722,
            "f1": 0.36483789384370874,
            "f1_weighted": 0.36483789384370874
          },
          {
            "accuracy": 0.2888,
            "f1": 0.28869195115927804,
            "f1_weighted": 0.288691951159278
          },
          {
            "accuracy": 0.286,
            "f1": 0.28764346696206805,
            "f1_weighted": 0.28764346696206805
          },
          {
            "accuracy": 0.3416,
            "f1": 0.33958505746127926,
            "f1_weighted": 0.33958505746127926
          },
          {
            "accuracy": 0.3308,
            "f1": 0.32913398077670714,
            "f1_weighted": 0.3291339807767071
          },
          {
            "accuracy": 0.3366,
            "f1": 0.336856009857976,
            "f1_weighted": 0.336856009857976
          }
        ]
      }
    ]
  },
  "task_name": "AmazonReviewsClassification"
}