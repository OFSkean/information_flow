{
  "dataset_revision": "1399c76144fd37290681b995c656ef9b2e06e26d",
  "evaluation_time": 15.402609825134277,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.20",
  "scores": {
    "test": [
      {
        "accuracy": 0.30144000000000004,
        "f1": 0.29844050097546754,
        "f1_weighted": 0.29844050097546754,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.30144000000000004,
        "scores_per_experiment": [
          {
            "accuracy": 0.3246,
            "f1": 0.328698353942887,
            "f1_weighted": 0.32869835394288693
          },
          {
            "accuracy": 0.3034,
            "f1": 0.3019630215167055,
            "f1_weighted": 0.3019630215167055
          },
          {
            "accuracy": 0.302,
            "f1": 0.30059048290390433,
            "f1_weighted": 0.30059048290390433
          },
          {
            "accuracy": 0.2984,
            "f1": 0.29568820849928845,
            "f1_weighted": 0.29568820849928845
          },
          {
            "accuracy": 0.337,
            "f1": 0.3271295572433256,
            "f1_weighted": 0.32712955724332554
          },
          {
            "accuracy": 0.2578,
            "f1": 0.25611492807808833,
            "f1_weighted": 0.25611492807808833
          },
          {
            "accuracy": 0.2612,
            "f1": 0.2591577670890283,
            "f1_weighted": 0.2591577670890284
          },
          {
            "accuracy": 0.3214,
            "f1": 0.3133785715596761,
            "f1_weighted": 0.31337857155967597
          },
          {
            "accuracy": 0.2916,
            "f1": 0.28995101945170554,
            "f1_weighted": 0.2899510194517054
          },
          {
            "accuracy": 0.317,
            "f1": 0.3117330994700668,
            "f1_weighted": 0.3117330994700668
          }
        ]
      }
    ]
  },
  "task_name": "AmazonReviewsClassification"
}