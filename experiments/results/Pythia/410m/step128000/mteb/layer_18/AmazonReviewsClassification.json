{
  "dataset_revision": "1399c76144fd37290681b995c656ef9b2e06e26d",
  "evaluation_time": 54.283222675323486,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.20",
  "scores": {
    "test": [
      {
        "accuracy": 0.35514,
        "f1": 0.354785772063734,
        "f1_weighted": 0.354785772063734,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.35514,
        "scores_per_experiment": [
          {
            "accuracy": 0.379,
            "f1": 0.3791398628350053,
            "f1_weighted": 0.3791398628350053
          },
          {
            "accuracy": 0.3748,
            "f1": 0.3740255163526947,
            "f1_weighted": 0.37402551635269476
          },
          {
            "accuracy": 0.341,
            "f1": 0.3367025277339759,
            "f1_weighted": 0.33670252773397596
          },
          {
            "accuracy": 0.3566,
            "f1": 0.35917269214098,
            "f1_weighted": 0.3591726921409801
          },
          {
            "accuracy": 0.3786,
            "f1": 0.37161211139452954,
            "f1_weighted": 0.3716121113945296
          },
          {
            "accuracy": 0.3314,
            "f1": 0.33233348639902294,
            "f1_weighted": 0.3323334863990229
          },
          {
            "accuracy": 0.3216,
            "f1": 0.3219015848973613,
            "f1_weighted": 0.3219015848973613
          },
          {
            "accuracy": 0.3522,
            "f1": 0.35226988861438113,
            "f1_weighted": 0.35226988861438113
          },
          {
            "accuracy": 0.361,
            "f1": 0.3613294556916939,
            "f1_weighted": 0.36132945569169395
          },
          {
            "accuracy": 0.3552,
            "f1": 0.3593705945776953,
            "f1_weighted": 0.3593705945776954
          }
        ]
      }
    ]
  },
  "task_name": "AmazonReviewsClassification"
}