{
  "dataset_revision": "1399c76144fd37290681b995c656ef9b2e06e26d",
  "evaluation_time": 39.2166543006897,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.20",
  "scores": {
    "test": [
      {
        "accuracy": 0.37094,
        "f1": 0.3676118015490749,
        "f1_weighted": 0.3676118015490749,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.37094,
        "scores_per_experiment": [
          {
            "accuracy": 0.3816,
            "f1": 0.3840712924287163,
            "f1_weighted": 0.3840712924287163
          },
          {
            "accuracy": 0.4016,
            "f1": 0.39851580108045603,
            "f1_weighted": 0.3985158010804561
          },
          {
            "accuracy": 0.352,
            "f1": 0.34687742785711606,
            "f1_weighted": 0.34687742785711606
          },
          {
            "accuracy": 0.3756,
            "f1": 0.3753155678774452,
            "f1_weighted": 0.3753155678774452
          },
          {
            "accuracy": 0.4154,
            "f1": 0.39411715552822474,
            "f1_weighted": 0.3941171555282248
          },
          {
            "accuracy": 0.3324,
            "f1": 0.33223231092605554,
            "f1_weighted": 0.33223231092605554
          },
          {
            "accuracy": 0.3436,
            "f1": 0.34428136361227046,
            "f1_weighted": 0.3442813636122705
          },
          {
            "accuracy": 0.3758,
            "f1": 0.3716450810805706,
            "f1_weighted": 0.3716450810805706
          },
          {
            "accuracy": 0.3594,
            "f1": 0.35974590191163586,
            "f1_weighted": 0.35974590191163586
          },
          {
            "accuracy": 0.372,
            "f1": 0.36931611318825847,
            "f1_weighted": 0.3693161131882585
          }
        ]
      }
    ]
  },
  "task_name": "AmazonReviewsClassification"
}