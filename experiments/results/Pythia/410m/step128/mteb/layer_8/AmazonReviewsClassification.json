{
  "dataset_revision": "1399c76144fd37290681b995c656ef9b2e06e26d",
  "evaluation_time": 30.758994579315186,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.20",
  "scores": {
    "test": [
      {
        "accuracy": 0.27348,
        "f1": 0.27079840395158017,
        "f1_weighted": 0.27079840395158017,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.27348,
        "scores_per_experiment": [
          {
            "accuracy": 0.2824,
            "f1": 0.27938474162033683,
            "f1_weighted": 0.27938474162033683
          },
          {
            "accuracy": 0.2798,
            "f1": 0.27760050572325284,
            "f1_weighted": 0.27760050572325284
          },
          {
            "accuracy": 0.2502,
            "f1": 0.24685443988702085,
            "f1_weighted": 0.24685443988702083
          },
          {
            "accuracy": 0.2592,
            "f1": 0.2521538385885629,
            "f1_weighted": 0.2521538385885629
          },
          {
            "accuracy": 0.3022,
            "f1": 0.29754006177881837,
            "f1_weighted": 0.2975400617788184
          },
          {
            "accuracy": 0.275,
            "f1": 0.2741684398483459,
            "f1_weighted": 0.2741684398483459
          },
          {
            "accuracy": 0.239,
            "f1": 0.23718499437739715,
            "f1_weighted": 0.2371849943773972
          },
          {
            "accuracy": 0.3018,
            "f1": 0.3021813965479919,
            "f1_weighted": 0.302181396547992
          },
          {
            "accuracy": 0.2782,
            "f1": 0.2766315930737711,
            "f1_weighted": 0.2766315930737711
          },
          {
            "accuracy": 0.267,
            "f1": 0.2642840280703036,
            "f1_weighted": 0.2642840280703036
          }
        ]
      }
    ]
  },
  "task_name": "AmazonReviewsClassification"
}