{
  "dataset_revision": "1399c76144fd37290681b995c656ef9b2e06e26d",
  "evaluation_time": 53.34582781791687,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.20",
  "scores": {
    "test": [
      {
        "accuracy": 0.26952,
        "f1": 0.2668981438438962,
        "f1_weighted": 0.2668981438438962,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.26952,
        "scores_per_experiment": [
          {
            "accuracy": 0.2708,
            "f1": 0.26761462571682115,
            "f1_weighted": 0.26761462571682115
          },
          {
            "accuracy": 0.2844,
            "f1": 0.2810296737736056,
            "f1_weighted": 0.28102967377360555
          },
          {
            "accuracy": 0.249,
            "f1": 0.24655596753056108,
            "f1_weighted": 0.24655596753056105
          },
          {
            "accuracy": 0.2512,
            "f1": 0.2433318467791624,
            "f1_weighted": 0.24333184677916234
          },
          {
            "accuracy": 0.3008,
            "f1": 0.2989015465231547,
            "f1_weighted": 0.2989015465231547
          },
          {
            "accuracy": 0.2718,
            "f1": 0.26986833137516697,
            "f1_weighted": 0.269868331375167
          },
          {
            "accuracy": 0.2388,
            "f1": 0.23935376570755745,
            "f1_weighted": 0.2393537657075575
          },
          {
            "accuracy": 0.2998,
            "f1": 0.2980359105929781,
            "f1_weighted": 0.2980359105929781
          },
          {
            "accuracy": 0.2768,
            "f1": 0.2757967758078152,
            "f1_weighted": 0.2757967758078152
          },
          {
            "accuracy": 0.2518,
            "f1": 0.2484929946321396,
            "f1_weighted": 0.24849299463213956
          }
        ]
      }
    ]
  },
  "task_name": "AmazonReviewsClassification"
}