{
  "dataset_revision": "fad2c6e8459f9e1c45d9315f4953d921437d70f8",
  "evaluation_time": 31.204264402389526,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.45749831876260927,
        "f1": 0.4458028991797474,
        "f1_weighted": 0.4624247451978089,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.45749831876260927,
        "scores_per_experiment": [
          {
            "accuracy": 0.4989912575655683,
            "f1": 0.48833813121604436,
            "f1_weighted": 0.5052594417087685
          },
          {
            "accuracy": 0.476462676529926,
            "f1": 0.4570663706100507,
            "f1_weighted": 0.4826709756738641
          },
          {
            "accuracy": 0.4092131809011432,
            "f1": 0.40863307454326636,
            "f1_weighted": 0.4099691634635415
          },
          {
            "accuracy": 0.42232683254875586,
            "f1": 0.4132459251193231,
            "f1_weighted": 0.4334388927010986
          },
          {
            "accuracy": 0.44519166106254204,
            "f1": 0.43388032682621686,
            "f1_weighted": 0.4510983408818077
          },
          {
            "accuracy": 0.4603227975790182,
            "f1": 0.4449922012426919,
            "f1_weighted": 0.4693100684787449
          },
          {
            "accuracy": 0.4821788836583726,
            "f1": 0.46387357819843533,
            "f1_weighted": 0.48800254043025704
          },
          {
            "accuracy": 0.495965030262273,
            "f1": 0.477248455386869,
            "f1_weighted": 0.5002808189685569
          },
          {
            "accuracy": 0.4300605245460659,
            "f1": 0.4278855890312058,
            "f1_weighted": 0.4265817963009994
          },
          {
            "accuracy": 0.4542703429724277,
            "f1": 0.44286533962337093,
            "f1_weighted": 0.4576354133704509
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.44741760944417114,
        "f1": 0.4406427990702917,
        "f1_weighted": 0.4491635023473245,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.44741760944417114,
        "scores_per_experiment": [
          {
            "accuracy": 0.485489424495819,
            "f1": 0.47261109064074674,
            "f1_weighted": 0.4908515866611696
          },
          {
            "accuracy": 0.46974913920314804,
            "f1": 0.46226812502103265,
            "f1_weighted": 0.47091782555553685
          },
          {
            "accuracy": 0.40531234628627644,
            "f1": 0.40513532186015133,
            "f1_weighted": 0.4064649010561607
          },
          {
            "accuracy": 0.4171175602557796,
            "f1": 0.410203868709516,
            "f1_weighted": 0.4267908617726234
          },
          {
            "accuracy": 0.41564190850959176,
            "f1": 0.4071034132725396,
            "f1_weighted": 0.4139107041499032
          },
          {
            "accuracy": 0.4545007378258731,
            "f1": 0.4483395783723309,
            "f1_weighted": 0.4559291295268419
          },
          {
            "accuracy": 0.45548450565666504,
            "f1": 0.4478598754038861,
            "f1_weighted": 0.4584844059047536
          },
          {
            "accuracy": 0.4815543531726513,
            "f1": 0.4670695208657004,
            "f1_weighted": 0.48292799241452317
          },
          {
            "accuracy": 0.4328578455484506,
            "f1": 0.4316465857221743,
            "f1_weighted": 0.4287290542392774
          },
          {
            "accuracy": 0.45646827348745694,
            "f1": 0.4541906108348395,
            "f1_weighted": 0.4566285621924553
          }
        ]
      }
    ]
  },
  "task_name": "MassiveScenarioClassification"
}