{
  "dataset_revision": "d80d48c1eb48d3562165c59d59d0034df9fff0bf",
  "evaluation_time": 35.07613277435303,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.6625626994984041,
        "f1": 0.6617013552096076,
        "f1_weighted": 0.664202829327251,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.6625626994984041,
        "scores_per_experiment": [
          {
            "accuracy": 0.6149110807113544,
            "f1": 0.6143221863987655,
            "f1_weighted": 0.610869720880699
          },
          {
            "accuracy": 0.6616507067943457,
            "f1": 0.6671043711270483,
            "f1_weighted": 0.6626932793185699
          },
          {
            "accuracy": 0.686046511627907,
            "f1": 0.6757438364265828,
            "f1_weighted": 0.685914519214673
          },
          {
            "accuracy": 0.6643866849065208,
            "f1": 0.6639641178188247,
            "f1_weighted": 0.6660793788520671
          },
          {
            "accuracy": 0.6766985864113088,
            "f1": 0.6784721517703439,
            "f1_weighted": 0.683005102642461
          },
          {
            "accuracy": 0.6577747378020976,
            "f1": 0.6600559893191451,
            "f1_weighted": 0.6641598542506244
          },
          {
            "accuracy": 0.6600547195622435,
            "f1": 0.6487576586375859,
            "f1_weighted": 0.6526800702746599
          },
          {
            "accuracy": 0.6379388964888281,
            "f1": 0.641341751977197,
            "f1_weighted": 0.6462327430082275
          },
          {
            "accuracy": 0.6782945736434108,
            "f1": 0.6813783120267789,
            "f1_weighted": 0.6825727411692866
          },
          {
            "accuracy": 0.6878704970360238,
            "f1": 0.6858731765938028,
            "f1_weighted": 0.6878208836612428
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.6540939597315436,
        "f1": 0.6579565826671953,
        "f1_weighted": 0.654614490733467,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.6540939597315436,
        "scores_per_experiment": [
          {
            "accuracy": 0.6017897091722595,
            "f1": 0.6054405426050393,
            "f1_weighted": 0.5943629071239709
          },
          {
            "accuracy": 0.6644295302013423,
            "f1": 0.6704093552133276,
            "f1_weighted": 0.6653390173346575
          },
          {
            "accuracy": 0.6621923937360179,
            "f1": 0.6594726157476244,
            "f1_weighted": 0.6616813644029097
          },
          {
            "accuracy": 0.6447427293064877,
            "f1": 0.6507302605380965,
            "f1_weighted": 0.6459789798928764
          },
          {
            "accuracy": 0.6832214765100671,
            "f1": 0.6920385124687651,
            "f1_weighted": 0.6869715651296632
          },
          {
            "accuracy": 0.6510067114093959,
            "f1": 0.6560498529731316,
            "f1_weighted": 0.6543229666439706
          },
          {
            "accuracy": 0.643847874720358,
            "f1": 0.6397961246344432,
            "f1_weighted": 0.6386523423415936
          },
          {
            "accuracy": 0.6317673378076063,
            "f1": 0.6380956274770155,
            "f1_weighted": 0.6379473948763941
          },
          {
            "accuracy": 0.6751677852348993,
            "f1": 0.6799793832932634,
            "f1_weighted": 0.6775596157498921
          },
          {
            "accuracy": 0.6827740492170022,
            "f1": 0.6875535517212463,
            "f1_weighted": 0.6833287538387417
          }
        ]
      }
    ]
  },
  "task_name": "MTOPDomainClassification"
}