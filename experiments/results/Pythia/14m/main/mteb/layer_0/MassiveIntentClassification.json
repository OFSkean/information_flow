{
  "dataset_revision": "4672e20407010da34463acc759c162ca9734bca6",
  "evaluation_time": 37.3460419178009,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.44142568930733017,
        "f1": 0.4317064568413723,
        "f1_weighted": 0.44467560757306746,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.44142568930733017,
        "scores_per_experiment": [
          {
            "accuracy": 0.4546065904505716,
            "f1": 0.44677904477326913,
            "f1_weighted": 0.461694534571242
          },
          {
            "accuracy": 0.4593140551445864,
            "f1": 0.4495439404868623,
            "f1_weighted": 0.464667633057536
          },
          {
            "accuracy": 0.43140551445864156,
            "f1": 0.425112279412969,
            "f1_weighted": 0.4342770191249384
          },
          {
            "accuracy": 0.4576328177538668,
            "f1": 0.44288388181984456,
            "f1_weighted": 0.4588761428199558
          },
          {
            "accuracy": 0.45090786819098855,
            "f1": 0.4331030401288571,
            "f1_weighted": 0.44949068062647046
          },
          {
            "accuracy": 0.4327505043712172,
            "f1": 0.42051950680460687,
            "f1_weighted": 0.43707925247700924
          },
          {
            "accuracy": 0.4156018829858776,
            "f1": 0.41371161161296066,
            "f1_weighted": 0.41854118071525775
          },
          {
            "accuracy": 0.4371217215870881,
            "f1": 0.42695693388813105,
            "f1_weighted": 0.4414200251702324
          },
          {
            "accuracy": 0.4300605245460659,
            "f1": 0.4290932099591705,
            "f1_weighted": 0.43180425412617834
          },
          {
            "accuracy": 0.4448554135843981,
            "f1": 0.4293611195270526,
            "f1_weighted": 0.448905353041854
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.44476143630103293,
        "f1": 0.43296485016340336,
        "f1_weighted": 0.4482626555885762,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.44476143630103293,
        "scores_per_experiment": [
          {
            "accuracy": 0.44220363994097395,
            "f1": 0.4420010624016261,
            "f1_weighted": 0.4491140757339067
          },
          {
            "accuracy": 0.46876537137235613,
            "f1": 0.4415615574142787,
            "f1_weighted": 0.4735596339852721
          },
          {
            "accuracy": 0.43482538121003445,
            "f1": 0.43433089898041133,
            "f1_weighted": 0.43587500254122397
          },
          {
            "accuracy": 0.47122479094933595,
            "f1": 0.44501081192151065,
            "f1_weighted": 0.47409926752860904
          },
          {
            "accuracy": 0.46237088047220853,
            "f1": 0.4454141354537095,
            "f1_weighted": 0.46652054429441336
          },
          {
            "accuracy": 0.4323659616330546,
            "f1": 0.42902683598767766,
            "f1_weighted": 0.43445206762830163
          },
          {
            "accuracy": 0.4072798819478603,
            "f1": 0.40237979642977906,
            "f1_weighted": 0.4086540500657697
          },
          {
            "accuracy": 0.4377766847024102,
            "f1": 0.42142662222958466,
            "f1_weighted": 0.444578652000862
          },
          {
            "accuracy": 0.44171175602557794,
            "f1": 0.44138826951993154,
            "f1_weighted": 0.4402218564022305
          },
          {
            "accuracy": 0.44909001475651744,
            "f1": 0.42710851129552435,
            "f1_weighted": 0.4555514057051735
          }
        ]
      }
    ]
  },
  "task_name": "MassiveIntentClassification"
}