{
  "dataset_revision": "4672e20407010da34463acc759c162ca9734bca6",
  "evaluation_time": 39.426270484924316,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.31398789509078684,
        "f1": 0.2978543054415258,
        "f1_weighted": 0.3244634275762603,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.31398789509078684,
        "scores_per_experiment": [
          {
            "accuracy": 0.30060524546065903,
            "f1": 0.289792696064996,
            "f1_weighted": 0.3145898539398553
          },
          {
            "accuracy": 0.3103564223268325,
            "f1": 0.2885185260521832,
            "f1_weighted": 0.3250241882329569
          },
          {
            "accuracy": 0.3117014122394082,
            "f1": 0.30430467424629204,
            "f1_weighted": 0.3168989763952189
          },
          {
            "accuracy": 0.3288500336247478,
            "f1": 0.29949562000020513,
            "f1_weighted": 0.34183943807200234
          },
          {
            "accuracy": 0.32481506388702086,
            "f1": 0.2992251465936229,
            "f1_weighted": 0.33410000893977887
          },
          {
            "accuracy": 0.3231338264963013,
            "f1": 0.30856471436818056,
            "f1_weighted": 0.33215240332252316
          },
          {
            "accuracy": 0.3053127101546738,
            "f1": 0.286003675591701,
            "f1_weighted": 0.3132389847154469
          },
          {
            "accuracy": 0.30564895763281774,
            "f1": 0.29760688527464857,
            "f1_weighted": 0.3189232892579176
          },
          {
            "accuracy": 0.3110289172831204,
            "f1": 0.29865262789578323,
            "f1_weighted": 0.3203973602588437
          },
          {
            "accuracy": 0.3184263618022865,
            "f1": 0.3063784883276452,
            "f1_weighted": 0.32746977262805865
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.31790457452041315,
        "f1": 0.2974766256709169,
        "f1_weighted": 0.3281999004881705,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.31790457452041315,
        "scores_per_experiment": [
          {
            "accuracy": 0.3280865715691097,
            "f1": 0.31760445051879316,
            "f1_weighted": 0.3450136251289843
          },
          {
            "accuracy": 0.3044761436301033,
            "f1": 0.269655312754675,
            "f1_weighted": 0.32416000928686856
          },
          {
            "accuracy": 0.31037875061485487,
            "f1": 0.29563853808962576,
            "f1_weighted": 0.31742792360185973
          },
          {
            "accuracy": 0.3423512051155927,
            "f1": 0.30688494410758327,
            "f1_weighted": 0.35152673991470385
          },
          {
            "accuracy": 0.3354648303000492,
            "f1": 0.311185901036725,
            "f1_weighted": 0.3507554693299717
          },
          {
            "accuracy": 0.3325135268076734,
            "f1": 0.32081413921494484,
            "f1_weighted": 0.3410160966692792
          },
          {
            "accuracy": 0.2975897688145598,
            "f1": 0.2826356337940398,
            "f1_weighted": 0.30220166354226063
          },
          {
            "accuracy": 0.3034923757993114,
            "f1": 0.28251432828512574,
            "f1_weighted": 0.30937616059905126
          },
          {
            "accuracy": 0.3034923757993114,
            "f1": 0.29571957564289547,
            "f1_weighted": 0.30910151982387096
          },
          {
            "accuracy": 0.32120019675356615,
            "f1": 0.29211343326476086,
            "f1_weighted": 0.3314197969848549
          }
        ]
      }
    ]
  },
  "task_name": "MassiveIntentClassification"
}