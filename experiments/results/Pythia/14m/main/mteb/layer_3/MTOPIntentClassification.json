{
  "dataset_revision": "ae001d0e6b1228650b7bd1c2c65fb50ad11a8aba",
  "evaluation_time": 52.55710029602051,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.3973552211582307,
        "f1": 0.2400610537140977,
        "f1_weighted": 0.45520121821349707,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.3973552211582307,
        "scores_per_experiment": [
          {
            "accuracy": 0.359781121751026,
            "f1": 0.22941785422662994,
            "f1_weighted": 0.4105177444521385
          },
          {
            "accuracy": 0.40948472412220704,
            "f1": 0.24506496188463608,
            "f1_weighted": 0.46514442932542294
          },
          {
            "accuracy": 0.4046967624259006,
            "f1": 0.24994928195816732,
            "f1_weighted": 0.4608216739498039
          },
          {
            "accuracy": 0.38828089375284996,
            "f1": 0.23468833127117825,
            "f1_weighted": 0.4453839868884503
          },
          {
            "accuracy": 0.4046967624259006,
            "f1": 0.23958402523131497,
            "f1_weighted": 0.4698069095973813
          },
          {
            "accuracy": 0.42293661650706793,
            "f1": 0.23514435958662241,
            "f1_weighted": 0.48472348468597676
          },
          {
            "accuracy": 0.37733698130414955,
            "f1": 0.23109404021017255,
            "f1_weighted": 0.43227954103516486
          },
          {
            "accuracy": 0.42476060191518467,
            "f1": 0.25471902951273023,
            "f1_weighted": 0.4803230046717907
          },
          {
            "accuracy": 0.41358869129046966,
            "f1": 0.2503765624095207,
            "f1_weighted": 0.4714219045643059
          },
          {
            "accuracy": 0.3679890560875513,
            "f1": 0.23057209085000435,
            "f1_weighted": 0.4315895029645355
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.39937360178970915,
        "f1": 0.23721834940167824,
        "f1_weighted": 0.45711616084542195,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.39937360178970915,
        "scores_per_experiment": [
          {
            "accuracy": 0.36062639821029085,
            "f1": 0.2215085836314753,
            "f1_weighted": 0.41252315510695087
          },
          {
            "accuracy": 0.3946308724832215,
            "f1": 0.24150230699821842,
            "f1_weighted": 0.450301051846156
          },
          {
            "accuracy": 0.4134228187919463,
            "f1": 0.24141107750410326,
            "f1_weighted": 0.4738442688661078
          },
          {
            "accuracy": 0.3861297539149888,
            "f1": 0.23003150848973405,
            "f1_weighted": 0.43901739318770866
          },
          {
            "accuracy": 0.4228187919463087,
            "f1": 0.2559945088218973,
            "f1_weighted": 0.4836273501872252
          },
          {
            "accuracy": 0.4134228187919463,
            "f1": 0.2308769790269779,
            "f1_weighted": 0.47211239990997333
          },
          {
            "accuracy": 0.37762863534675617,
            "f1": 0.22906815465385288,
            "f1_weighted": 0.4332893736930864
          },
          {
            "accuracy": 0.4389261744966443,
            "f1": 0.259990443560018,
            "f1_weighted": 0.4949205346976729
          },
          {
            "accuracy": 0.4080536912751678,
            "f1": 0.22923484644929107,
            "f1_weighted": 0.46917198028301715
          },
          {
            "accuracy": 0.378076062639821,
            "f1": 0.23256508488121405,
            "f1_weighted": 0.4423541006763215
          }
        ]
      }
    ]
  },
  "task_name": "MTOPIntentClassification"
}