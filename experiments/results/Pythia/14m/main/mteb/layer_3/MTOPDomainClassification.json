{
  "dataset_revision": "d80d48c1eb48d3562165c59d59d0034df9fff0bf",
  "evaluation_time": 39.57374453544617,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.560077519379845,
        "f1": 0.553924826697502,
        "f1_weighted": 0.5638908390169559,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.560077519379845,
        "scores_per_experiment": [
          {
            "accuracy": 0.5266757865937073,
            "f1": 0.5181464525237188,
            "f1_weighted": 0.5254492662741135
          },
          {
            "accuracy": 0.5658914728682171,
            "f1": 0.5624271701686546,
            "f1_weighted": 0.5712481729210708
          },
          {
            "accuracy": 0.5756953944368445,
            "f1": 0.5596840113308809,
            "f1_weighted": 0.5785903707375275
          },
          {
            "accuracy": 0.5809393524851801,
            "f1": 0.5692545595337252,
            "f1_weighted": 0.5803350429247062
          },
          {
            "accuracy": 0.5941632466940264,
            "f1": 0.5923250574633937,
            "f1_weighted": 0.6026654067770972
          },
          {
            "accuracy": 0.5467396260829913,
            "f1": 0.5423318228878908,
            "f1_weighted": 0.5508940224929301
          },
          {
            "accuracy": 0.5186958504331965,
            "f1": 0.5163438544397784,
            "f1_weighted": 0.5235644019446061
          },
          {
            "accuracy": 0.5387596899224806,
            "f1": 0.5342216229832242,
            "f1_weighted": 0.5435717119459594
          },
          {
            "accuracy": 0.5896032831737346,
            "f1": 0.5860189482263464,
            "f1_weighted": 0.5979034806997345
          },
          {
            "accuracy": 0.5636114911080712,
            "f1": 0.5584947674174062,
            "f1_weighted": 0.5646865134518132
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.547606263982103,
        "f1": 0.5469970522112508,
        "f1_weighted": 0.549843362484244,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.547606263982103,
        "scores_per_experiment": [
          {
            "accuracy": 0.516331096196868,
            "f1": 0.5148626075542682,
            "f1_weighted": 0.5125283955720434
          },
          {
            "accuracy": 0.5503355704697986,
            "f1": 0.5536607326987134,
            "f1_weighted": 0.5528434359903115
          },
          {
            "accuracy": 0.5480984340044742,
            "f1": 0.5395464614336865,
            "f1_weighted": 0.5503141464450738
          },
          {
            "accuracy": 0.5686800894854586,
            "f1": 0.5669413251865211,
            "f1_weighted": 0.5659806891146894
          },
          {
            "accuracy": 0.5637583892617449,
            "f1": 0.5665538831586306,
            "f1_weighted": 0.5703760659878715
          },
          {
            "accuracy": 0.5467561521252796,
            "f1": 0.5455742760004441,
            "f1_weighted": 0.5494719481598642
          },
          {
            "accuracy": 0.5100671140939598,
            "f1": 0.5089176834940449,
            "f1_weighted": 0.5146032889891077
          },
          {
            "accuracy": 0.5275167785234899,
            "f1": 0.5301998099268128,
            "f1_weighted": 0.5306698510258094
          },
          {
            "accuracy": 0.5834451901565996,
            "f1": 0.5805489019824591,
            "f1_weighted": 0.5907280128466686
          },
          {
            "accuracy": 0.5610738255033557,
            "f1": 0.5631648406769272,
            "f1_weighted": 0.5609177907110009
          }
        ]
      }
    ]
  },
  "task_name": "MTOPDomainClassification"
}