{
  "dataset_revision": "d80d48c1eb48d3562165c59d59d0034df9fff0bf",
  "evaluation_time": 41.667856216430664,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.5765389876880984,
        "f1": 0.5727244614663004,
        "f1_weighted": 0.5792788792331152,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5765389876880984,
        "scores_per_experiment": [
          {
            "accuracy": 0.5319197446420428,
            "f1": 0.5351344274500227,
            "f1_weighted": 0.533511801283112
          },
          {
            "accuracy": 0.5946192430460556,
            "f1": 0.5943259587624993,
            "f1_weighted": 0.5931528824531074
          },
          {
            "accuracy": 0.5699954400364797,
            "f1": 0.5710008687384222,
            "f1_weighted": 0.5834028973268922
          },
          {
            "accuracy": 0.6101231190150479,
            "f1": 0.6011208213664315,
            "f1_weighted": 0.618534727561364
          },
          {
            "accuracy": 0.5859553123575011,
            "f1": 0.5798166515373542,
            "f1_weighted": 0.5872307210352294
          },
          {
            "accuracy": 0.54468764249886,
            "f1": 0.5442350276754105,
            "f1_weighted": 0.5434066259291478
          },
          {
            "accuracy": 0.5542635658914729,
            "f1": 0.5469755123964729,
            "f1_weighted": 0.5560853838345142
          },
          {
            "accuracy": 0.5444596443228454,
            "f1": 0.5481760942909101,
            "f1_weighted": 0.5461087765935155
          },
          {
            "accuracy": 0.6064751481988144,
            "f1": 0.5966508277214218,
            "f1_weighted": 0.609721752391931
          },
          {
            "accuracy": 0.622891016871865,
            "f1": 0.6098084247240592,
            "f1_weighted": 0.6216332239223367
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.5725279642058166,
        "f1": 0.5680414793228418,
        "f1_weighted": 0.5738282124716292,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5725279642058166,
        "scores_per_experiment": [
          {
            "accuracy": 0.5131991051454139,
            "f1": 0.514414678362821,
            "f1_weighted": 0.514962141794638
          },
          {
            "accuracy": 0.5874720357941835,
            "f1": 0.5876927703542442,
            "f1_weighted": 0.5855566411108916
          },
          {
            "accuracy": 0.5901565995525727,
            "f1": 0.5879358456906318,
            "f1_weighted": 0.6019255093105401
          },
          {
            "accuracy": 0.6246085011185682,
            "f1": 0.6177477438728631,
            "f1_weighted": 0.6286090853947184
          },
          {
            "accuracy": 0.5803131991051454,
            "f1": 0.5767233431766052,
            "f1_weighted": 0.5804525322433091
          },
          {
            "accuracy": 0.5243847874720358,
            "f1": 0.5243467184377798,
            "f1_weighted": 0.5203634237564887
          },
          {
            "accuracy": 0.5548098434004475,
            "f1": 0.5438275668478905,
            "f1_weighted": 0.56019453099744
          },
          {
            "accuracy": 0.5516778523489932,
            "f1": 0.5506026137268081,
            "f1_weighted": 0.5500262907334151
          },
          {
            "accuracy": 0.5892617449664429,
            "f1": 0.5829611200145667,
            "f1_weighted": 0.5913208751228546
          },
          {
            "accuracy": 0.6093959731543624,
            "f1": 0.5941623927442076,
            "f1_weighted": 0.6048710942519966
          }
        ]
      }
    ]
  },
  "task_name": "MTOPDomainClassification"
}