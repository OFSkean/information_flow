{
  "dataset_revision": "4672e20407010da34463acc759c162ca9734bca6",
  "evaluation_time": 41.60170650482178,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.34798251513113654,
        "f1": 0.3199033575466882,
        "f1_weighted": 0.3588735438006199,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.34798251513113654,
        "scores_per_experiment": [
          {
            "accuracy": 0.3352387357094822,
            "f1": 0.31270371640789474,
            "f1_weighted": 0.34383660075985395
          },
          {
            "accuracy": 0.3537323470073974,
            "f1": 0.3224866489330091,
            "f1_weighted": 0.36477645905794287
          },
          {
            "accuracy": 0.3439811701412239,
            "f1": 0.3176206817979059,
            "f1_weighted": 0.35111056231469606
          },
          {
            "accuracy": 0.36012104909213183,
            "f1": 0.328073157929702,
            "f1_weighted": 0.3735040523529904
          },
          {
            "accuracy": 0.34734364492266306,
            "f1": 0.31559217016542135,
            "f1_weighted": 0.3634631485742702
          },
          {
            "accuracy": 0.3486886348352387,
            "f1": 0.3200191174938782,
            "f1_weighted": 0.3594095490315627
          },
          {
            "accuracy": 0.3500336247478144,
            "f1": 0.3205914415309698,
            "f1_weighted": 0.3608080289350835
          },
          {
            "accuracy": 0.3446536650975118,
            "f1": 0.3160162681162155,
            "f1_weighted": 0.35804857394135203
          },
          {
            "accuracy": 0.34129119031607263,
            "f1": 0.321504243184838,
            "f1_weighted": 0.3509048583746389
          },
          {
            "accuracy": 0.3547410894418292,
            "f1": 0.32442612990704695,
            "f1_weighted": 0.36287360466380836
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.3517461878996557,
        "f1": 0.3187476601993358,
        "f1_weighted": 0.3633674869209292,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.3517461878996557,
        "scores_per_experiment": [
          {
            "accuracy": 0.34923757993113624,
            "f1": 0.32332908094330676,
            "f1_weighted": 0.3536553397358238
          },
          {
            "accuracy": 0.34481062469257256,
            "f1": 0.3024308432287618,
            "f1_weighted": 0.3581248617781686
          },
          {
            "accuracy": 0.3443187407771766,
            "f1": 0.3152717777229432,
            "f1_weighted": 0.35502223689443896
          },
          {
            "accuracy": 0.38121003443187407,
            "f1": 0.3323810926538841,
            "f1_weighted": 0.3991926113409667
          },
          {
            "accuracy": 0.3605509099852435,
            "f1": 0.3217208883295852,
            "f1_weighted": 0.3795084552039393
          },
          {
            "accuracy": 0.3600590260698475,
            "f1": 0.33883006258997056,
            "f1_weighted": 0.3697124905068769
          },
          {
            "accuracy": 0.3325135268076734,
            "f1": 0.30480391763529086,
            "f1_weighted": 0.33964546531095147
          },
          {
            "accuracy": 0.3497294638465322,
            "f1": 0.3149290146885794,
            "f1_weighted": 0.3627342038035791
          },
          {
            "accuracy": 0.34333497294638465,
            "f1": 0.3258405776196332,
            "f1_weighted": 0.3551247570783776
          },
          {
            "accuracy": 0.35169699950811606,
            "f1": 0.30793934658140226,
            "f1_weighted": 0.36095444755616896
          }
        ]
      }
    ]
  },
  "task_name": "MassiveIntentClassification"
}