{
  "dataset_revision": "fad2c6e8459f9e1c45d9315f4953d921437d70f8",
  "evaluation_time": 37.15421795845032,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.400168123739072,
        "f1": 0.36952043110185623,
        "f1_weighted": 0.41234219508498066,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.400168123739072,
        "scores_per_experiment": [
          {
            "accuracy": 0.42400806993947543,
            "f1": 0.3917407774570341,
            "f1_weighted": 0.4361580686341508
          },
          {
            "accuracy": 0.42165433759246806,
            "f1": 0.383054962113553,
            "f1_weighted": 0.4400137849821541
          },
          {
            "accuracy": 0.37995965030262274,
            "f1": 0.34681512147367993,
            "f1_weighted": 0.39214843580072617
          },
          {
            "accuracy": 0.3429724277067922,
            "f1": 0.3291683442042312,
            "f1_weighted": 0.35297324138065495
          },
          {
            "accuracy": 0.4418291862811029,
            "f1": 0.4044369579163467,
            "f1_weighted": 0.4499604575772691
          },
          {
            "accuracy": 0.3991257565568258,
            "f1": 0.36485509273364997,
            "f1_weighted": 0.41001267419874576
          },
          {
            "accuracy": 0.4250168123739072,
            "f1": 0.39219626682072595,
            "f1_weighted": 0.4343627202129777
          },
          {
            "accuracy": 0.4132481506388702,
            "f1": 0.3782361226807838,
            "f1_weighted": 0.42496222184637994
          },
          {
            "accuracy": 0.3685272360457297,
            "f1": 0.34645631224524315,
            "f1_weighted": 0.3794459220922951
          },
          {
            "accuracy": 0.3853396099529254,
            "f1": 0.3582443533733143,
            "f1_weighted": 0.4033844241244531
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.3922282341367437,
        "f1": 0.36835619248269647,
        "f1_weighted": 0.4005990833727179,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.3922282341367437,
        "scores_per_experiment": [
          {
            "accuracy": 0.40924741760944416,
            "f1": 0.383606719327461,
            "f1_weighted": 0.42068291695121296
          },
          {
            "accuracy": 0.4013772749631087,
            "f1": 0.3715341958586449,
            "f1_weighted": 0.41370365093245587
          },
          {
            "accuracy": 0.3807181505164781,
            "f1": 0.34842082123414636,
            "f1_weighted": 0.3868323609753552
          },
          {
            "accuracy": 0.3339891785538613,
            "f1": 0.3228058505302008,
            "f1_weighted": 0.34793578298183764
          },
          {
            "accuracy": 0.42105263157894735,
            "f1": 0.39594976250396435,
            "f1_weighted": 0.4252031970967345
          },
          {
            "accuracy": 0.396458435809149,
            "f1": 0.37442592113707984,
            "f1_weighted": 0.39668565854751403
          },
          {
            "accuracy": 0.4013772749631087,
            "f1": 0.37610537312094844,
            "f1_weighted": 0.40858840482834136
          },
          {
            "accuracy": 0.42006886374815544,
            "f1": 0.3970727699929316,
            "f1_weighted": 0.4288804304229188
          },
          {
            "accuracy": 0.3694048204623709,
            "f1": 0.3522485478197408,
            "f1_weighted": 0.3800045515199833
          },
          {
            "accuracy": 0.38858829316281357,
            "f1": 0.3613919633018469,
            "f1_weighted": 0.39747387947082435
          }
        ]
      }
    ]
  },
  "task_name": "MassiveScenarioClassification"
}