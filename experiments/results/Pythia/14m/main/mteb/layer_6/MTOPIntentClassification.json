{
  "dataset_revision": "ae001d0e6b1228650b7bd1c2c65fb50ad11a8aba",
  "evaluation_time": 55.63334631919861,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.42457820337437296,
        "f1": 0.2567549252785167,
        "f1_weighted": 0.4872619167132338,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.42457820337437296,
        "scores_per_experiment": [
          {
            "accuracy": 0.4160966712266302,
            "f1": 0.2693982313319433,
            "f1_weighted": 0.4756865526243807
          },
          {
            "accuracy": 0.4386684906520748,
            "f1": 0.25744081310461536,
            "f1_weighted": 0.5015738143412304
          },
          {
            "accuracy": 0.41951664386684906,
            "f1": 0.267520483844616,
            "f1_weighted": 0.48301149138543253
          },
          {
            "accuracy": 0.4265845873233014,
            "f1": 0.24710642397115076,
            "f1_weighted": 0.48717746359615716
          },
          {
            "accuracy": 0.4274965800273598,
            "f1": 0.25776264683939415,
            "f1_weighted": 0.4962370471494419
          },
          {
            "accuracy": 0.42248062015503873,
            "f1": 0.24575672068656226,
            "f1_weighted": 0.4814011608447074
          },
          {
            "accuracy": 0.4042407660738714,
            "f1": 0.2593599328186701,
            "f1_weighted": 0.46083492907983853
          },
          {
            "accuracy": 0.42795257637938894,
            "f1": 0.2529988545835231,
            "f1_weighted": 0.4880335171856792
          },
          {
            "accuracy": 0.42544459644322846,
            "f1": 0.2649350536808593,
            "f1_weighted": 0.4982732785095243
          },
          {
            "accuracy": 0.4373005015959872,
            "f1": 0.24527009192383226,
            "f1_weighted": 0.5003899124159464
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.4306935123042506,
        "f1": 0.2682248531342193,
        "f1_weighted": 0.49297563985480747,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.4306935123042506,
        "scores_per_experiment": [
          {
            "accuracy": 0.4008948545861298,
            "f1": 0.24978156677688051,
            "f1_weighted": 0.4621647923158035
          },
          {
            "accuracy": 0.44608501118568233,
            "f1": 0.29096231834430164,
            "f1_weighted": 0.5103406614708471
          },
          {
            "accuracy": 0.4344519015659955,
            "f1": 0.25757232801829005,
            "f1_weighted": 0.49779066820019496
          },
          {
            "accuracy": 0.44205816554809846,
            "f1": 0.26736416041369504,
            "f1_weighted": 0.4993501020804836
          },
          {
            "accuracy": 0.4344519015659955,
            "f1": 0.2676542286268243,
            "f1_weighted": 0.5008683150180748
          },
          {
            "accuracy": 0.42953020134228187,
            "f1": 0.27464555405381474,
            "f1_weighted": 0.4878766325451845
          },
          {
            "accuracy": 0.42058165548098436,
            "f1": 0.2712569402538368,
            "f1_weighted": 0.4756300485551149
          },
          {
            "accuracy": 0.4313199105145414,
            "f1": 0.27956329700253585,
            "f1_weighted": 0.49704790694018847
          },
          {
            "accuracy": 0.429082774049217,
            "f1": 0.256324385994674,
            "f1_weighted": 0.4954053759932418
          },
          {
            "accuracy": 0.43847874720357943,
            "f1": 0.2671237518573399,
            "f1_weighted": 0.5032818954289409
          }
        ]
      }
    ]
  },
  "task_name": "MTOPIntentClassification"
}