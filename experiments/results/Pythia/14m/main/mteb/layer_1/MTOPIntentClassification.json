{
  "dataset_revision": "ae001d0e6b1228650b7bd1c2c65fb50ad11a8aba",
  "evaluation_time": 54.73644757270813,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.48670770633834926,
        "f1": 0.3174568324402719,
        "f1_weighted": 0.5376459023658618,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.48670770633834926,
        "scores_per_experiment": [
          {
            "accuracy": 0.47036023711810304,
            "f1": 0.3161140204259737,
            "f1_weighted": 0.5151003936039357
          },
          {
            "accuracy": 0.4806201550387597,
            "f1": 0.3324150121507101,
            "f1_weighted": 0.528439005445755
          },
          {
            "accuracy": 0.4806201550387597,
            "f1": 0.3027793453999542,
            "f1_weighted": 0.5284236405565645
          },
          {
            "accuracy": 0.48723210214318285,
            "f1": 0.323014714315156,
            "f1_weighted": 0.5384151790210618
          },
          {
            "accuracy": 0.49612403100775193,
            "f1": 0.3246338073572637,
            "f1_weighted": 0.5517657036036678
          },
          {
            "accuracy": 0.4995440036479708,
            "f1": 0.3197070062576444,
            "f1_weighted": 0.5501321546332305
          },
          {
            "accuracy": 0.48084815321477425,
            "f1": 0.31773526573116667,
            "f1_weighted": 0.5270302797249307
          },
          {
            "accuracy": 0.5082079343365253,
            "f1": 0.3312165118821963,
            "f1_weighted": 0.5646609546160324
          },
          {
            "accuracy": 0.4968080255357957,
            "f1": 0.31545054843306736,
            "f1_weighted": 0.5540891334200707
          },
          {
            "accuracy": 0.46671226630186957,
            "f1": 0.29150209244958597,
            "f1_weighted": 0.5184025790333696
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.48908277404921696,
        "f1": 0.31254403068175557,
        "f1_weighted": 0.5413843204175628,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.48908277404921696,
        "scores_per_experiment": [
          {
            "accuracy": 0.4671140939597315,
            "f1": 0.3019246541040759,
            "f1_weighted": 0.5188543281452402
          },
          {
            "accuracy": 0.4841163310961969,
            "f1": 0.3274139583671217,
            "f1_weighted": 0.5314710852484319
          },
          {
            "accuracy": 0.49261744966442955,
            "f1": 0.3036615098332848,
            "f1_weighted": 0.5425089649371395
          },
          {
            "accuracy": 0.48053691275167787,
            "f1": 0.3242566341610906,
            "f1_weighted": 0.5351245083751794
          },
          {
            "accuracy": 0.5064876957494407,
            "f1": 0.30997233374853056,
            "f1_weighted": 0.5590189772048937
          },
          {
            "accuracy": 0.4961968680089485,
            "f1": 0.30176436681031155,
            "f1_weighted": 0.5479352861440312
          },
          {
            "accuracy": 0.4899328859060403,
            "f1": 0.33445834643917444,
            "f1_weighted": 0.5382703834558648
          },
          {
            "accuracy": 0.5234899328859061,
            "f1": 0.3254391585901355,
            "f1_weighted": 0.5778046984621322
          },
          {
            "accuracy": 0.4841163310961969,
            "f1": 0.29651044844334773,
            "f1_weighted": 0.5442127798735528
          },
          {
            "accuracy": 0.46621923937360177,
            "f1": 0.3000388963204825,
            "f1_weighted": 0.5186421923291622
          }
        ]
      }
    ]
  },
  "task_name": "MTOPIntentClassification"
}