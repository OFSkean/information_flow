{
  "dataset_revision": "d80d48c1eb48d3562165c59d59d0034df9fff0bf",
  "evaluation_time": 41.30509305000305,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.628545371637027,
        "f1": 0.6243441210073634,
        "f1_weighted": 0.6294695830560448,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.628545371637027,
        "scores_per_experiment": [
          {
            "accuracy": 0.5745554035567716,
            "f1": 0.5773236791967449,
            "f1_weighted": 0.5742087381628329
          },
          {
            "accuracy": 0.6265389876880985,
            "f1": 0.6289245842030695,
            "f1_weighted": 0.6287126826744482
          },
          {
            "accuracy": 0.6379388964888281,
            "f1": 0.6245241706231065,
            "f1_weighted": 0.6369893473277501
          },
          {
            "accuracy": 0.624031007751938,
            "f1": 0.6172114150113267,
            "f1_weighted": 0.6216221392480191
          },
          {
            "accuracy": 0.6413588691290469,
            "f1": 0.6382974668394438,
            "f1_weighted": 0.645009845561536
          },
          {
            "accuracy": 0.6354309165526676,
            "f1": 0.628327267202715,
            "f1_weighted": 0.6365439215589773
          },
          {
            "accuracy": 0.6078431372549019,
            "f1": 0.59413581158314,
            "f1_weighted": 0.6008128034253778
          },
          {
            "accuracy": 0.6238030095759234,
            "f1": 0.6233075606424076,
            "f1_weighted": 0.6333452472291502
          },
          {
            "accuracy": 0.6566347469220246,
            "f1": 0.6580228565789542,
            "f1_weighted": 0.6606767989789182
          },
          {
            "accuracy": 0.6573187414500684,
            "f1": 0.6533663981927247,
            "f1_weighted": 0.6567743063934387
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.6184340044742729,
        "f1": 0.6183085219233455,
        "f1_weighted": 0.6179033250626802,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.6184340044742729,
        "scores_per_experiment": [
          {
            "accuracy": 0.5686800894854586,
            "f1": 0.5765855060112082,
            "f1_weighted": 0.5639366670279129
          },
          {
            "accuracy": 0.6237136465324384,
            "f1": 0.6278307056002657,
            "f1_weighted": 0.6229347303994506
          },
          {
            "accuracy": 0.6170022371364653,
            "f1": 0.609529664681071,
            "f1_weighted": 0.615255767405171
          },
          {
            "accuracy": 0.614765100671141,
            "f1": 0.6156048047973877,
            "f1_weighted": 0.6098060488554752
          },
          {
            "accuracy": 0.6407158836689038,
            "f1": 0.6453159684842817,
            "f1_weighted": 0.6427276558611666
          },
          {
            "accuracy": 0.6205816554809843,
            "f1": 0.6151404320078148,
            "f1_weighted": 0.620960354037847
          },
          {
            "accuracy": 0.6,
            "f1": 0.5904183166405234,
            "f1_weighted": 0.5951603128048013
          },
          {
            "accuracy": 0.5991051454138703,
            "f1": 0.6017448948416253,
            "f1_weighted": 0.6060421036893533
          },
          {
            "accuracy": 0.661744966442953,
            "f1": 0.66288845847716,
            "f1_weighted": 0.6655276819957209
          },
          {
            "accuracy": 0.6380313199105145,
            "f1": 0.6380264676921176,
            "f1_weighted": 0.6366819285499022
          }
        ]
      }
    ]
  },
  "task_name": "MTOPDomainClassification"
}