{
  "dataset_revision": "4672e20407010da34463acc759c162ca9734bca6",
  "evaluation_time": 41.62753772735596,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.39246805648957633,
        "f1": 0.375744944325583,
        "f1_weighted": 0.39777524559743277,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.39246805648957633,
        "scores_per_experiment": [
          {
            "accuracy": 0.4004707464694015,
            "f1": 0.38333404430117624,
            "f1_weighted": 0.4058781126449695
          },
          {
            "accuracy": 0.394418291862811,
            "f1": 0.3726361193774427,
            "f1_weighted": 0.4020082360778971
          },
          {
            "accuracy": 0.3910558170813719,
            "f1": 0.3843756122921299,
            "f1_weighted": 0.39395432876088743
          },
          {
            "accuracy": 0.4152656355077337,
            "f1": 0.3844940451313328,
            "f1_weighted": 0.41906650074499585
          },
          {
            "accuracy": 0.39778076664425016,
            "f1": 0.38178370746107265,
            "f1_weighted": 0.40092983906855706
          },
          {
            "accuracy": 0.3792871553463349,
            "f1": 0.3621277932974565,
            "f1_weighted": 0.3858175208596871
          },
          {
            "accuracy": 0.38500336247478145,
            "f1": 0.3751988689568479,
            "f1_weighted": 0.38833208438323813
          },
          {
            "accuracy": 0.38702084734364495,
            "f1": 0.37053249562287455,
            "f1_weighted": 0.39597554559581144
          },
          {
            "accuracy": 0.3725622057834566,
            "f1": 0.36490440157235354,
            "f1_weighted": 0.37502958510331663
          },
          {
            "accuracy": 0.40181573638197715,
            "f1": 0.378062355243143,
            "f1_weighted": 0.4107607027349676
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.3999016232169208,
        "f1": 0.38409468104918393,
        "f1_weighted": 0.4048166981338107,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.3999016232169208,
        "scores_per_experiment": [
          {
            "accuracy": 0.39940973930152485,
            "f1": 0.38416277669030496,
            "f1_weighted": 0.40518723241884597
          },
          {
            "accuracy": 0.4023610427939006,
            "f1": 0.3720704643626503,
            "f1_weighted": 0.4103843498478895
          },
          {
            "accuracy": 0.3939990162321692,
            "f1": 0.38760206753670706,
            "f1_weighted": 0.3964197742785851
          },
          {
            "accuracy": 0.43728480078701426,
            "f1": 0.4035519625011623,
            "f1_weighted": 0.43991771436650234
          },
          {
            "accuracy": 0.4107230693556321,
            "f1": 0.393322476107442,
            "f1_weighted": 0.41825214807546385
          },
          {
            "accuracy": 0.4028529267092966,
            "f1": 0.39830730980481416,
            "f1_weighted": 0.40709209247560907
          },
          {
            "accuracy": 0.367929168716183,
            "f1": 0.36072323694621944,
            "f1_weighted": 0.3646799763619934
          },
          {
            "accuracy": 0.39498278406296117,
            "f1": 0.38033058127754865,
            "f1_weighted": 0.4037483887440033
          },
          {
            "accuracy": 0.3866207575012297,
            "f1": 0.38408511982790056,
            "f1_weighted": 0.39123722087991764
          },
          {
            "accuracy": 0.4028529267092966,
            "f1": 0.37679081543708975,
            "f1_weighted": 0.41124808388929684
          }
        ]
      }
    ]
  },
  "task_name": "MassiveIntentClassification"
}