{
  "dataset_revision": "fad2c6e8459f9e1c45d9315f4953d921437d70f8",
  "evaluation_time": 37.29755091667175,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.4275386684599865,
        "f1": 0.41181174451484903,
        "f1_weighted": 0.43199920747598475,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.4275386684599865,
        "scores_per_experiment": [
          {
            "accuracy": 0.46099529253530597,
            "f1": 0.4449499679134799,
            "f1_weighted": 0.4683802011968482
          },
          {
            "accuracy": 0.4613315400134499,
            "f1": 0.43743282440810194,
            "f1_weighted": 0.470167223428148
          },
          {
            "accuracy": 0.39878950907868194,
            "f1": 0.3889835170406893,
            "f1_weighted": 0.4036097804980705
          },
          {
            "accuracy": 0.4112306657700067,
            "f1": 0.3919652447917073,
            "f1_weighted": 0.4136977314256621
          },
          {
            "accuracy": 0.4011432414256893,
            "f1": 0.39165684569856946,
            "f1_weighted": 0.4074844596832056
          },
          {
            "accuracy": 0.4132481506388702,
            "f1": 0.39444676947029655,
            "f1_weighted": 0.415444126338537
          },
          {
            "accuracy": 0.4519166106254203,
            "f1": 0.43084380032171565,
            "f1_weighted": 0.45515032829570706
          },
          {
            "accuracy": 0.46234028244788167,
            "f1": 0.44435421812892617,
            "f1_weighted": 0.46688253980773603
          },
          {
            "accuracy": 0.40248823133826495,
            "f1": 0.38621021882877904,
            "f1_weighted": 0.4007056909628384
          },
          {
            "accuracy": 0.41190316072629457,
            "f1": 0.4072740385462251,
            "f1_weighted": 0.4184699931230949
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.41672405312346295,
        "f1": 0.40638869931408805,
        "f1_weighted": 0.4182599477494717,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.41672405312346295,
        "scores_per_experiment": [
          {
            "accuracy": 0.43580914904082635,
            "f1": 0.4245234778159053,
            "f1_weighted": 0.44302053100940225
          },
          {
            "accuracy": 0.43334972946384653,
            "f1": 0.4134410015632714,
            "f1_weighted": 0.43518136748328484
          },
          {
            "accuracy": 0.3880964092474176,
            "f1": 0.38164922722332156,
            "f1_weighted": 0.391439330296364
          },
          {
            "accuracy": 0.4028529267092966,
            "f1": 0.3860289666846896,
            "f1_weighted": 0.4056077214813181
          },
          {
            "accuracy": 0.4107230693556321,
            "f1": 0.40841896316082493,
            "f1_weighted": 0.41395622678055055
          },
          {
            "accuracy": 0.40531234628627644,
            "f1": 0.40032085650505317,
            "f1_weighted": 0.40156765991569393
          },
          {
            "accuracy": 0.42252828332513526,
            "f1": 0.41309016856608355,
            "f1_weighted": 0.42340208665867207
          },
          {
            "accuracy": 0.44220363994097395,
            "f1": 0.42239658075680087,
            "f1_weighted": 0.4431508990120863
          },
          {
            "accuracy": 0.4013772749631087,
            "f1": 0.38991892522668464,
            "f1_weighted": 0.4021302698017665
          },
          {
            "accuracy": 0.4249877029021151,
            "f1": 0.42409882563824564,
            "f1_weighted": 0.4231433850555791
          }
        ]
      }
    ]
  },
  "task_name": "MassiveScenarioClassification"
}