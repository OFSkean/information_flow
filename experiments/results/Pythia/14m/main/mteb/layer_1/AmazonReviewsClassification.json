{
  "dataset_revision": "1399c76144fd37290681b995c656ef9b2e06e26d",
  "evaluation_time": 53.27730417251587,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.28653999999999996,
        "f1": 0.2847055417785821,
        "f1_weighted": 0.2847055417785821,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.28653999999999996,
        "scores_per_experiment": [
          {
            "accuracy": 0.3052,
            "f1": 0.3075352251179865,
            "f1_weighted": 0.3075352251179865
          },
          {
            "accuracy": 0.2882,
            "f1": 0.29160575835203445,
            "f1_weighted": 0.2916057583520345
          },
          {
            "accuracy": 0.275,
            "f1": 0.27602554591564593,
            "f1_weighted": 0.27602554591564593
          },
          {
            "accuracy": 0.258,
            "f1": 0.25366095538226036,
            "f1_weighted": 0.25366095538226036
          },
          {
            "accuracy": 0.3172,
            "f1": 0.3138880954936014,
            "f1_weighted": 0.3138880954936014
          },
          {
            "accuracy": 0.2534,
            "f1": 0.2520917376209468,
            "f1_weighted": 0.2520917376209468
          },
          {
            "accuracy": 0.2562,
            "f1": 0.25573866756243174,
            "f1_weighted": 0.2557386675624318
          },
          {
            "accuracy": 0.3124,
            "f1": 0.30417188215914237,
            "f1_weighted": 0.3041718821591424
          },
          {
            "accuracy": 0.2882,
            "f1": 0.2833694659850575,
            "f1_weighted": 0.2833694659850576
          },
          {
            "accuracy": 0.3116,
            "f1": 0.30896808419671346,
            "f1_weighted": 0.3089680841967134
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.28672,
        "f1": 0.28438011714892586,
        "f1_weighted": 0.28438011714892586,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.28672,
        "scores_per_experiment": [
          {
            "accuracy": 0.3058,
            "f1": 0.30549032028517203,
            "f1_weighted": 0.30549032028517203
          },
          {
            "accuracy": 0.2856,
            "f1": 0.2902970114913396,
            "f1_weighted": 0.2902970114913396
          },
          {
            "accuracy": 0.2708,
            "f1": 0.2712389166206051,
            "f1_weighted": 0.2712389166206051
          },
          {
            "accuracy": 0.2552,
            "f1": 0.24978280978540207,
            "f1_weighted": 0.24978280978540207
          },
          {
            "accuracy": 0.3182,
            "f1": 0.3144468416260503,
            "f1_weighted": 0.31444684162605024
          },
          {
            "accuracy": 0.2468,
            "f1": 0.24475833423537013,
            "f1_weighted": 0.24475833423537016
          },
          {
            "accuracy": 0.2672,
            "f1": 0.26643326513396925,
            "f1_weighted": 0.26643326513396925
          },
          {
            "accuracy": 0.3076,
            "f1": 0.3025011612881953,
            "f1_weighted": 0.3025011612881953
          },
          {
            "accuracy": 0.2912,
            "f1": 0.28623776374937027,
            "f1_weighted": 0.28623776374937027
          },
          {
            "accuracy": 0.3188,
            "f1": 0.3126147472737845,
            "f1_weighted": 0.3126147472737845
          }
        ]
      }
    ]
  },
  "task_name": "AmazonReviewsClassification"
}