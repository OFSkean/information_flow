{
  "dataset_revision": "4672e20407010da34463acc759c162ca9734bca6",
  "evaluation_time": 43.664037227630615,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.36311365164761267,
        "f1": 0.3417559694095687,
        "f1_weighted": 0.37188943629319543,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.36311365164761267,
        "scores_per_experiment": [
          {
            "accuracy": 0.36415601882985876,
            "f1": 0.34274486542442484,
            "f1_weighted": 0.37152644370050697
          },
          {
            "accuracy": 0.3624747814391392,
            "f1": 0.3394121565898471,
            "f1_weighted": 0.37286441889469885
          },
          {
            "accuracy": 0.3550773369199731,
            "f1": 0.33456700556494734,
            "f1_weighted": 0.36134198855132404
          },
          {
            "accuracy": 0.37020847343644925,
            "f1": 0.34098116902315495,
            "f1_weighted": 0.3801926691169761
          },
          {
            "accuracy": 0.3769334229993275,
            "f1": 0.3481142535238251,
            "f1_weighted": 0.38532485683041
          },
          {
            "accuracy": 0.36348352387357097,
            "f1": 0.3396519086153216,
            "f1_weighted": 0.37472109363753686
          },
          {
            "accuracy": 0.355413584398117,
            "f1": 0.33830383782450274,
            "f1_weighted": 0.3639458083205259
          },
          {
            "accuracy": 0.34902488231338263,
            "f1": 0.33213844012307153,
            "f1_weighted": 0.361570959202039
          },
          {
            "accuracy": 0.35608607935440484,
            "f1": 0.34311673669562254,
            "f1_weighted": 0.3623324327013168
          },
          {
            "accuracy": 0.3782784129119032,
            "f1": 0.35852932071096977,
            "f1_weighted": 0.38507369197661995
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.36748647319232663,
        "f1": 0.35024161788036695,
        "f1_weighted": 0.3754763690286393,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.36748647319232663,
        "scores_per_experiment": [
          {
            "accuracy": 0.37383177570093457,
            "f1": 0.3621034673463128,
            "f1_weighted": 0.38055341566510587
          },
          {
            "accuracy": 0.3561239547466798,
            "f1": 0.326713084109385,
            "f1_weighted": 0.3720625590743049
          },
          {
            "accuracy": 0.367437284800787,
            "f1": 0.35188384194462013,
            "f1_weighted": 0.37528208437403826
          },
          {
            "accuracy": 0.3851451057550418,
            "f1": 0.36208747094462673,
            "f1_weighted": 0.395211289635817
          },
          {
            "accuracy": 0.38711264141662566,
            "f1": 0.36425217070399174,
            "f1_weighted": 0.3960709404351818
          },
          {
            "accuracy": 0.3743236596163306,
            "f1": 0.3579579154906322,
            "f1_weighted": 0.38205755544915204
          },
          {
            "accuracy": 0.34677816035415643,
            "f1": 0.3316807932719883,
            "f1_weighted": 0.35119251820711755
          },
          {
            "accuracy": 0.34185932120019674,
            "f1": 0.3278102221298106,
            "f1_weighted": 0.3494201969459197
          },
          {
            "accuracy": 0.35661583866207575,
            "f1": 0.3546187634156839,
            "f1_weighted": 0.358862953776046
          },
          {
            "accuracy": 0.3856369896704378,
            "f1": 0.36330844944661816,
            "f1_weighted": 0.3940501767237105
          }
        ]
      }
    ]
  },
  "task_name": "MassiveIntentClassification"
}