{
  "dataset_revision": "ae001d0e6b1228650b7bd1c2c65fb50ad11a8aba",
  "evaluation_time": 54.346532583236694,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.4509803921568628,
        "f1": 0.28267665482694465,
        "f1_weighted": 0.5018448997070527,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.4509803921568628,
        "scores_per_experiment": [
          {
            "accuracy": 0.41472868217054265,
            "f1": 0.2710740229640572,
            "f1_weighted": 0.46042520848212126
          },
          {
            "accuracy": 0.45006839945280436,
            "f1": 0.2851359167145558,
            "f1_weighted": 0.5016206045947068
          },
          {
            "accuracy": 0.4473324213406293,
            "f1": 0.2882582738113882,
            "f1_weighted": 0.4990766774915854
          },
          {
            "accuracy": 0.4751481988144095,
            "f1": 0.29390581166238405,
            "f1_weighted": 0.5267393079948824
          },
          {
            "accuracy": 0.444140446876425,
            "f1": 0.27817064211783743,
            "f1_weighted": 0.5001977015104955
          },
          {
            "accuracy": 0.4571363429092567,
            "f1": 0.2891521017471615,
            "f1_weighted": 0.5069818328754214
          },
          {
            "accuracy": 0.4409484724122207,
            "f1": 0.28419438111515455,
            "f1_weighted": 0.4848213439248213
          },
          {
            "accuracy": 0.47697218422252624,
            "f1": 0.2999227510666074,
            "f1_weighted": 0.5278036488672375
          },
          {
            "accuracy": 0.46602827177382583,
            "f1": 0.2723701308147221,
            "f1_weighted": 0.5213962677913878
          },
          {
            "accuracy": 0.4373005015959872,
            "f1": 0.2645825162555782,
            "f1_weighted": 0.4893864035378674
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.4492170022371365,
        "f1": 0.27357777634526725,
        "f1_weighted": 0.4997152879353871,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.4492170022371365,
        "scores_per_experiment": [
          {
            "accuracy": 0.4174496644295302,
            "f1": 0.2584254978572151,
            "f1_weighted": 0.46749779642484873
          },
          {
            "accuracy": 0.43266219239373604,
            "f1": 0.273588383683817,
            "f1_weighted": 0.47958587131008684
          },
          {
            "accuracy": 0.4621923937360179,
            "f1": 0.27979850319391114,
            "f1_weighted": 0.5155047734504055
          },
          {
            "accuracy": 0.4407158836689038,
            "f1": 0.26980217430394965,
            "f1_weighted": 0.4884145313851182
          },
          {
            "accuracy": 0.44742729306487694,
            "f1": 0.2641910768632449,
            "f1_weighted": 0.4983904307200044
          },
          {
            "accuracy": 0.44205816554809846,
            "f1": 0.26634205556165347,
            "f1_weighted": 0.49269386495696726
          },
          {
            "accuracy": 0.44161073825503355,
            "f1": 0.2734160264500822,
            "f1_weighted": 0.4910700128883114
          },
          {
            "accuracy": 0.4850111856823266,
            "f1": 0.29405504768133645,
            "f1_weighted": 0.538071141338929
          },
          {
            "accuracy": 0.4702460850111857,
            "f1": 0.2739735796053835,
            "f1_weighted": 0.5240894661363644
          },
          {
            "accuracy": 0.4527964205816555,
            "f1": 0.28218541825207866,
            "f1_weighted": 0.5018349907428363
          }
        ]
      }
    ]
  },
  "task_name": "MTOPIntentClassification"
}