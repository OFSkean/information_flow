{
  "dataset_revision": "fad2c6e8459f9e1c45d9315f4953d921437d70f8",
  "evaluation_time": 36.193931579589844,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.39673839946200407,
        "f1": 0.376552931053056,
        "f1_weighted": 0.40440341416037845,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.39673839946200407,
        "scores_per_experiment": [
          {
            "accuracy": 0.44317417619367855,
            "f1": 0.42243706264844527,
            "f1_weighted": 0.45090977832679924
          },
          {
            "accuracy": 0.4199731002017485,
            "f1": 0.3871636524019235,
            "f1_weighted": 0.4291107868703998
          },
          {
            "accuracy": 0.3695359784801614,
            "f1": 0.3493449234151893,
            "f1_weighted": 0.37683193773223245
          },
          {
            "accuracy": 0.3913920645595158,
            "f1": 0.3720448291719476,
            "f1_weighted": 0.396525662523387
          },
          {
            "accuracy": 0.37088096839273704,
            "f1": 0.35275181868909133,
            "f1_weighted": 0.3730442351501644
          },
          {
            "accuracy": 0.3887020847343645,
            "f1": 0.3762236863227733,
            "f1_weighted": 0.40076797558446825
          },
          {
            "accuracy": 0.39710827168796237,
            "f1": 0.3808298097660046,
            "f1_weighted": 0.40400932129778605
          },
          {
            "accuracy": 0.4260255548083389,
            "f1": 0.4037502988121109,
            "f1_weighted": 0.4385761944009061
          },
          {
            "accuracy": 0.37592468056489575,
            "f1": 0.35170501924886605,
            "f1_weighted": 0.3813892509989533
          },
          {
            "accuracy": 0.3846671149966375,
            "f1": 0.3692782100542082,
            "f1_weighted": 0.3928689987186881
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.38967043777668475,
        "f1": 0.3769640818462528,
        "f1_weighted": 0.3948281016728671,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.38967043777668475,
        "scores_per_experiment": [
          {
            "accuracy": 0.42597147073290703,
            "f1": 0.41147084900354813,
            "f1_weighted": 0.4304525474278684
          },
          {
            "accuracy": 0.40186915887850466,
            "f1": 0.3823854699001039,
            "f1_weighted": 0.4087196150216929
          },
          {
            "accuracy": 0.36694540088539107,
            "f1": 0.3540557835550651,
            "f1_weighted": 0.3734228107501636
          },
          {
            "accuracy": 0.3684210526315789,
            "f1": 0.3501422460579159,
            "f1_weighted": 0.3765777969893183
          },
          {
            "accuracy": 0.3777668470241023,
            "f1": 0.3733778745191632,
            "f1_weighted": 0.37499357254413035
          },
          {
            "accuracy": 0.38612887358583375,
            "f1": 0.381546079428329,
            "f1_weighted": 0.3961153260830224
          },
          {
            "accuracy": 0.37973438268568616,
            "f1": 0.3677856208440878,
            "f1_weighted": 0.38574949937106223
          },
          {
            "accuracy": 0.4062961141170684,
            "f1": 0.3861249336689271,
            "f1_weighted": 0.41289356110496084
          },
          {
            "accuracy": 0.37973438268568616,
            "f1": 0.36741319300849307,
            "f1_weighted": 0.3830176693799044
          },
          {
            "accuracy": 0.4038366945400885,
            "f1": 0.39533876847689414,
            "f1_weighted": 0.4063386180565468
          }
        ]
      }
    ]
  },
  "task_name": "MassiveScenarioClassification"
}