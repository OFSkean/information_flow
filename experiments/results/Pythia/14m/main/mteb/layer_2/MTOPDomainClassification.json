{
  "dataset_revision": "d80d48c1eb48d3562165c59d59d0034df9fff0bf",
  "evaluation_time": 39.4874324798584,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.5835157318741451,
        "f1": 0.5800102509094105,
        "f1_weighted": 0.5842945472635391,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5835157318741451,
        "scores_per_experiment": [
          {
            "accuracy": 0.5291837665298678,
            "f1": 0.527649458414964,
            "f1_weighted": 0.5254163839055196
          },
          {
            "accuracy": 0.5754673962608299,
            "f1": 0.5787903651734219,
            "f1_weighted": 0.5796202782513978
          },
          {
            "accuracy": 0.5772913816689467,
            "f1": 0.5614295192315639,
            "f1_weighted": 0.5803665813833936
          },
          {
            "accuracy": 0.6057911536707706,
            "f1": 0.5948381491408065,
            "f1_weighted": 0.5998267334264297
          },
          {
            "accuracy": 0.5823073415412676,
            "f1": 0.5865432675788621,
            "f1_weighted": 0.5880028223973977
          },
          {
            "accuracy": 0.5902872777017784,
            "f1": 0.5850802019368214,
            "f1_weighted": 0.590926856528369
          },
          {
            "accuracy": 0.5392156862745098,
            "f1": 0.5327396774093022,
            "f1_weighted": 0.5332089771162517
          },
          {
            "accuracy": 0.5782033743730051,
            "f1": 0.5789785555900197,
            "f1_weighted": 0.5839291596295367
          },
          {
            "accuracy": 0.6315549475604195,
            "f1": 0.6302566506955057,
            "f1_weighted": 0.6359819322015858
          },
          {
            "accuracy": 0.6258549931600548,
            "f1": 0.6237966639228373,
            "f1_weighted": 0.6256657477955099
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.5707382550335571,
        "f1": 0.5731263601229598,
        "f1_weighted": 0.5708113659668844,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5707382550335571,
        "scores_per_experiment": [
          {
            "accuracy": 0.5167785234899329,
            "f1": 0.522050262220946,
            "f1_weighted": 0.5087763800337759
          },
          {
            "accuracy": 0.5776286353467561,
            "f1": 0.5834722374697257,
            "f1_weighted": 0.5786812440280428
          },
          {
            "accuracy": 0.5539149888143177,
            "f1": 0.5488611760270157,
            "f1_weighted": 0.5592755159195055
          },
          {
            "accuracy": 0.574496644295302,
            "f1": 0.5755799512357137,
            "f1_weighted": 0.5709622534067639
          },
          {
            "accuracy": 0.5776286353467561,
            "f1": 0.5859909767105203,
            "f1_weighted": 0.5802733883320359
          },
          {
            "accuracy": 0.5807606263982102,
            "f1": 0.5792583347240111,
            "f1_weighted": 0.581207940732973
          },
          {
            "accuracy": 0.5360178970917227,
            "f1": 0.5351993531327548,
            "f1_weighted": 0.5323842704603591
          },
          {
            "accuracy": 0.5552572706935123,
            "f1": 0.5601414425088886,
            "f1_weighted": 0.5588349431668147
          },
          {
            "accuracy": 0.6299776286353468,
            "f1": 0.6303219230635597,
            "f1_weighted": 0.6334635263310804
          },
          {
            "accuracy": 0.6049217002237136,
            "f1": 0.6103879441364627,
            "f1_weighted": 0.6042541972574922
          }
        ]
      }
    ]
  },
  "task_name": "MTOPDomainClassification"
}