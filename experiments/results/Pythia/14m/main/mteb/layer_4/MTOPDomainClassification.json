{
  "dataset_revision": "d80d48c1eb48d3562165c59d59d0034df9fff0bf",
  "evaluation_time": 41.82020616531372,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.5961696306429549,
        "f1": 0.5922396519680327,
        "f1_weighted": 0.5994275999696278,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5961696306429549,
        "scores_per_experiment": [
          {
            "accuracy": 0.5715914272685818,
            "f1": 0.5689619913635693,
            "f1_weighted": 0.5697793942073105
          },
          {
            "accuracy": 0.6014591883264934,
            "f1": 0.6065789320714482,
            "f1_weighted": 0.6070516170911521
          },
          {
            "accuracy": 0.601687186502508,
            "f1": 0.595436754097749,
            "f1_weighted": 0.6110558120398442
          },
          {
            "accuracy": 0.5989512083903329,
            "f1": 0.5925501678347675,
            "f1_weighted": 0.6065910307022291
          },
          {
            "accuracy": 0.5823073415412676,
            "f1": 0.5791567502891377,
            "f1_weighted": 0.5841727113056221
          },
          {
            "accuracy": 0.6149110807113544,
            "f1": 0.6111794095947325,
            "f1_weighted": 0.6187825885721046
          },
          {
            "accuracy": 0.5718194254445964,
            "f1": 0.5662987806246801,
            "f1_weighted": 0.5738134194688788
          },
          {
            "accuracy": 0.5638394892840858,
            "f1": 0.561510129907223,
            "f1_weighted": 0.5657635727214686
          },
          {
            "accuracy": 0.6292749658002736,
            "f1": 0.6228184293323488,
            "f1_weighted": 0.6315662821628968
          },
          {
            "accuracy": 0.6258549931600548,
            "f1": 0.6179051745646703,
            "f1_weighted": 0.6256995714247712
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.5888590604026847,
        "f1": 0.5871807660834041,
        "f1_weighted": 0.5907798003303387,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5888590604026847,
        "scores_per_experiment": [
          {
            "accuracy": 0.570917225950783,
            "f1": 0.5698028103513324,
            "f1_weighted": 0.5687575062051468
          },
          {
            "accuracy": 0.5941834451901566,
            "f1": 0.6005061488697899,
            "f1_weighted": 0.5988185124683502
          },
          {
            "accuracy": 0.5919463087248322,
            "f1": 0.5876989117488488,
            "f1_weighted": 0.6009071222047114
          },
          {
            "accuracy": 0.5870246085011186,
            "f1": 0.5854210807747828,
            "f1_weighted": 0.5898305102976068
          },
          {
            "accuracy": 0.5923937360178971,
            "f1": 0.5911660933089319,
            "f1_weighted": 0.5928318718770598
          },
          {
            "accuracy": 0.6053691275167785,
            "f1": 0.6042628885187085,
            "f1_weighted": 0.6081078444385192
          },
          {
            "accuracy": 0.5583892617449664,
            "f1": 0.5540852543046697,
            "f1_weighted": 0.5619283375529256
          },
          {
            "accuracy": 0.5668903803131992,
            "f1": 0.5615106773117886,
            "f1_weighted": 0.5648142140530996
          },
          {
            "accuracy": 0.6281879194630873,
            "f1": 0.6211483942971532,
            "f1_weighted": 0.6284611633447102
          },
          {
            "accuracy": 0.5932885906040268,
            "f1": 0.5962054013480361,
            "f1_weighted": 0.5933409208612581
          }
        ]
      }
    ]
  },
  "task_name": "MTOPDomainClassification"
}