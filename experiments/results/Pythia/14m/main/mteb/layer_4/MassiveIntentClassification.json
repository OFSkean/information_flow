{
  "dataset_revision": "4672e20407010da34463acc759c162ca9734bca6",
  "evaluation_time": 39.36610507965088,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.34263618022864833,
        "f1": 0.32055904809024416,
        "f1_weighted": 0.3538694089645523,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.34263618022864833,
        "scores_per_experiment": [
          {
            "accuracy": 0.32817753866846,
            "f1": 0.31138934157856285,
            "f1_weighted": 0.3386615061645841
          },
          {
            "accuracy": 0.35137861466039005,
            "f1": 0.31464171289951903,
            "f1_weighted": 0.3651964607068911
          },
          {
            "accuracy": 0.3456624075319435,
            "f1": 0.3241021695305617,
            "f1_weighted": 0.35332960208284797
          },
          {
            "accuracy": 0.3530598520511096,
            "f1": 0.33035244265303604,
            "f1_weighted": 0.36529522428894756
          },
          {
            "accuracy": 0.3446536650975118,
            "f1": 0.3142263904208813,
            "f1_weighted": 0.35787675885710357
          },
          {
            "accuracy": 0.33927370544720914,
            "f1": 0.3167550044825783,
            "f1_weighted": 0.34852063644073594
          },
          {
            "accuracy": 0.33053127101546736,
            "f1": 0.3111808844121427,
            "f1_weighted": 0.3462984556877074
          },
          {
            "accuracy": 0.3446536650975118,
            "f1": 0.3191172604124641,
            "f1_weighted": 0.3584231951802118
          },
          {
            "accuracy": 0.3523873570948218,
            "f1": 0.34650630370093444,
            "f1_weighted": 0.3584951016140412
          },
          {
            "accuracy": 0.33658372562205785,
            "f1": 0.3173189708117616,
            "f1_weighted": 0.34659714862245217
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.34338416133792427,
        "f1": 0.3181130133080253,
        "f1_weighted": 0.3528962385743011,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.34338416133792427,
        "scores_per_experiment": [
          {
            "accuracy": 0.3393999016232169,
            "f1": 0.322479642556739,
            "f1_weighted": 0.3490062282287635
          },
          {
            "accuracy": 0.34481062469257256,
            "f1": 0.30757091188797075,
            "f1_weighted": 0.3600482249245449
          },
          {
            "accuracy": 0.3443187407771766,
            "f1": 0.3148539948257217,
            "f1_weighted": 0.3528723066004235
          },
          {
            "accuracy": 0.3703885882931628,
            "f1": 0.3475497086361023,
            "f1_weighted": 0.37849199065227335
          },
          {
            "accuracy": 0.3610427939006394,
            "f1": 0.3332398339684911,
            "f1_weighted": 0.3767483354421256
          },
          {
            "accuracy": 0.34825381210034434,
            "f1": 0.3257834163263604,
            "f1_weighted": 0.35753842371939293
          },
          {
            "accuracy": 0.31382193802262665,
            "f1": 0.29354863635358863,
            "f1_weighted": 0.3238214499063143
          },
          {
            "accuracy": 0.3354648303000492,
            "f1": 0.3069816519517624,
            "f1_weighted": 0.33779421840596935
          },
          {
            "accuracy": 0.3339891785538613,
            "f1": 0.3207153745524382,
            "f1_weighted": 0.3415276926652457
          },
          {
            "accuracy": 0.3423512051155927,
            "f1": 0.3084069620210786,
            "f1_weighted": 0.35111351519795764
          }
        ]
      }
    ]
  },
  "task_name": "MassiveIntentClassification"
}