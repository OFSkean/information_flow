{
  "dataset_revision": "ae001d0e6b1228650b7bd1c2c65fb50ad11a8aba",
  "evaluation_time": 56.58586597442627,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.4402416780665755,
        "f1": 0.26680364990072675,
        "f1_weighted": 0.49682722219998376,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.4402416780665755,
        "scores_per_experiment": [
          {
            "accuracy": 0.4386684906520748,
            "f1": 0.2696345527260876,
            "f1_weighted": 0.4928765032798332
          },
          {
            "accuracy": 0.41928864569083446,
            "f1": 0.26059980019431095,
            "f1_weighted": 0.47163783071656146
          },
          {
            "accuracy": 0.45531235750113996,
            "f1": 0.2837749051922281,
            "f1_weighted": 0.5140806315890668
          },
          {
            "accuracy": 0.4382124943000456,
            "f1": 0.26427109588289666,
            "f1_weighted": 0.49808317073620495
          },
          {
            "accuracy": 0.44596443228454175,
            "f1": 0.27273481445455017,
            "f1_weighted": 0.5047275643610504
          },
          {
            "accuracy": 0.43889648882808935,
            "f1": 0.2583389337309296,
            "f1_weighted": 0.4947322786816704
          },
          {
            "accuracy": 0.42339261285909713,
            "f1": 0.25988915006038676,
            "f1_weighted": 0.4703568720225152
          },
          {
            "accuracy": 0.4489284085727314,
            "f1": 0.27444589229444866,
            "f1_weighted": 0.5067315606130668
          },
          {
            "accuracy": 0.47332421340629277,
            "f1": 0.2857054942068667,
            "f1_weighted": 0.5333500307519389
          },
          {
            "accuracy": 0.42042863657090745,
            "f1": 0.23864186026456208,
            "f1_weighted": 0.48169577924792906
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.4440715883668903,
        "f1": 0.2625077158376386,
        "f1_weighted": 0.5012234437132416,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.4440715883668903,
        "scores_per_experiment": [
          {
            "accuracy": 0.42818791946308726,
            "f1": 0.25127604145928245,
            "f1_weighted": 0.48562170099405105
          },
          {
            "accuracy": 0.4223713646532439,
            "f1": 0.2611548578487755,
            "f1_weighted": 0.47650148854478275
          },
          {
            "accuracy": 0.45727069351230426,
            "f1": 0.26129271395348724,
            "f1_weighted": 0.5188445072270192
          },
          {
            "accuracy": 0.4451901565995526,
            "f1": 0.2540024436930001,
            "f1_weighted": 0.5063521284002471
          },
          {
            "accuracy": 0.458165548098434,
            "f1": 0.27357893364720803,
            "f1_weighted": 0.5138900969268977
          },
          {
            "accuracy": 0.4407158836689038,
            "f1": 0.26888660086449656,
            "f1_weighted": 0.4915903024002354
          },
          {
            "accuracy": 0.4268456375838926,
            "f1": 0.25743470341839747,
            "f1_weighted": 0.47477976996646415
          },
          {
            "accuracy": 0.4559284116331096,
            "f1": 0.276438928363051,
            "f1_weighted": 0.5136446140833517
          },
          {
            "accuracy": 0.47829977628635345,
            "f1": 0.27488467408480155,
            "f1_weighted": 0.5361422594411133
          },
          {
            "accuracy": 0.42774049217002236,
            "f1": 0.24612726104388538,
            "f1_weighted": 0.4948675691482537
          }
        ]
      }
    ]
  },
  "task_name": "MTOPIntentClassification"
}