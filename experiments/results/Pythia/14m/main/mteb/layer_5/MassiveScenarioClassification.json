{
  "dataset_revision": "fad2c6e8459f9e1c45d9315f4953d921437d70f8",
  "evaluation_time": 36.9404091835022,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.3972091459314055,
        "f1": 0.36853660344333966,
        "f1_weighted": 0.40693703889621247,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.3972091459314055,
        "scores_per_experiment": [
          {
            "accuracy": 0.4229993275050437,
            "f1": 0.3992955325000405,
            "f1_weighted": 0.43268723593773817
          },
          {
            "accuracy": 0.4092131809011432,
            "f1": 0.37876788278742174,
            "f1_weighted": 0.41977100224037645
          },
          {
            "accuracy": 0.3695359784801614,
            "f1": 0.3441841933405021,
            "f1_weighted": 0.3812744418459088
          },
          {
            "accuracy": 0.3722259583053127,
            "f1": 0.3494981865454438,
            "f1_weighted": 0.381125572467648
          },
          {
            "accuracy": 0.4159381304640215,
            "f1": 0.3761032390900927,
            "f1_weighted": 0.41992237700704876
          },
          {
            "accuracy": 0.39509078681909887,
            "f1": 0.3697888962856938,
            "f1_weighted": 0.40783991092509597
          },
          {
            "accuracy": 0.4078681909885676,
            "f1": 0.3765053615475188,
            "f1_weighted": 0.4189845202059138
          },
          {
            "accuracy": 0.4203093476798924,
            "f1": 0.3810506788357349,
            "f1_weighted": 0.4343494315789119
          },
          {
            "accuracy": 0.3668459986550101,
            "f1": 0.3475695976141511,
            "f1_weighted": 0.37309379209783916
          },
          {
            "accuracy": 0.39206455951580366,
            "f1": 0.3626024658867971,
            "f1_weighted": 0.4003221046556439
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.3803246433841613,
        "f1": 0.35981203815761553,
        "f1_weighted": 0.386905041876926,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.3803246433841613,
        "scores_per_experiment": [
          {
            "accuracy": 0.39940973930152485,
            "f1": 0.38037131477932395,
            "f1_weighted": 0.404859996603359
          },
          {
            "accuracy": 0.3792424987702902,
            "f1": 0.3552267843466202,
            "f1_weighted": 0.38247360911938905
          },
          {
            "accuracy": 0.35415641908509593,
            "f1": 0.33761825244502686,
            "f1_weighted": 0.3617764226460332
          },
          {
            "accuracy": 0.3438268568617806,
            "f1": 0.3267084551011838,
            "f1_weighted": 0.3532517649756358
          },
          {
            "accuracy": 0.40088539104771276,
            "f1": 0.36929950041765086,
            "f1_weighted": 0.4028728573237373
          },
          {
            "accuracy": 0.3851451057550418,
            "f1": 0.36828388376599924,
            "f1_weighted": 0.39109789944618745
          },
          {
            "accuracy": 0.382193802262666,
            "f1": 0.3619399884040363,
            "f1_weighted": 0.3918778986946488
          },
          {
            "accuracy": 0.4097393015248401,
            "f1": 0.38370611490551426,
            "f1_weighted": 0.4178681245092472
          },
          {
            "accuracy": 0.3610427939006394,
            "f1": 0.34727719242682276,
            "f1_weighted": 0.36721004693121634
          },
          {
            "accuracy": 0.38760452533202167,
            "f1": 0.3676888949839767,
            "f1_weighted": 0.3957617985198057
          }
        ]
      }
    ]
  },
  "task_name": "MassiveScenarioClassification"
}