{
  "dataset_revision": "ae001d0e6b1228650b7bd1c2c65fb50ad11a8aba",
  "evaluation_time": 57.57191586494446,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.4555859553123575,
        "f1": 0.27444365132295184,
        "f1_weighted": 0.5134856961940314,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.4555859553123575,
        "scores_per_experiment": [
          {
            "accuracy": 0.4482444140446876,
            "f1": 0.27458898834560075,
            "f1_weighted": 0.505761320661795
          },
          {
            "accuracy": 0.43091655266757867,
            "f1": 0.27394122712628394,
            "f1_weighted": 0.48453637133084804
          },
          {
            "accuracy": 0.4635202918376653,
            "f1": 0.2756378802592878,
            "f1_weighted": 0.5282661030503724
          },
          {
            "accuracy": 0.4532603739170087,
            "f1": 0.2808120613363482,
            "f1_weighted": 0.5148682213766946
          },
          {
            "accuracy": 0.4571363429092567,
            "f1": 0.2635637465636443,
            "f1_weighted": 0.5170735337293061
          },
          {
            "accuracy": 0.4587323301413589,
            "f1": 0.2785911956786092,
            "f1_weighted": 0.5156648724190518
          },
          {
            "accuracy": 0.43023255813953487,
            "f1": 0.2640819848904437,
            "f1_weighted": 0.4773564651946939
          },
          {
            "accuracy": 0.4737802097583219,
            "f1": 0.2754806309281301,
            "f1_weighted": 0.5327966843324993
          },
          {
            "accuracy": 0.4794801641586867,
            "f1": 0.2856234820349921,
            "f1_weighted": 0.5412397137098192
          },
          {
            "accuracy": 0.4605563155494756,
            "f1": 0.27211531606617856,
            "f1_weighted": 0.5172936761352347
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.461834451901566,
        "f1": 0.2761743954856574,
        "f1_weighted": 0.5207781054442179,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.461834451901566,
        "scores_per_experiment": [
          {
            "accuracy": 0.4483221476510067,
            "f1": 0.25708628102972203,
            "f1_weighted": 0.5075798009496547
          },
          {
            "accuracy": 0.4393736017897092,
            "f1": 0.2769645617028546,
            "f1_weighted": 0.4919172505147107
          },
          {
            "accuracy": 0.4818791946308725,
            "f1": 0.27535702782603305,
            "f1_weighted": 0.5497847116594706
          },
          {
            "accuracy": 0.46174496644295304,
            "f1": 0.27387565666233055,
            "f1_weighted": 0.5252229483611245
          },
          {
            "accuracy": 0.46621923937360177,
            "f1": 0.2644642032826845,
            "f1_weighted": 0.5237965146034734
          },
          {
            "accuracy": 0.454586129753915,
            "f1": 0.2659710284684568,
            "f1_weighted": 0.5084385337883784
          },
          {
            "accuracy": 0.43847874720357943,
            "f1": 0.27052721594860674,
            "f1_weighted": 0.49520279099982745
          },
          {
            "accuracy": 0.4885906040268456,
            "f1": 0.30929448860024283,
            "f1_weighted": 0.5468412219653079
          },
          {
            "accuracy": 0.47651006711409394,
            "f1": 0.28603450944086034,
            "f1_weighted": 0.5361919872908922
          },
          {
            "accuracy": 0.4626398210290828,
            "f1": 0.2821689818947825,
            "f1_weighted": 0.5228052943093383
          }
        ]
      }
    ]
  },
  "task_name": "MTOPIntentClassification"
}