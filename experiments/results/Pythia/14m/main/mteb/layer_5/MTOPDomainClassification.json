{
  "dataset_revision": "d80d48c1eb48d3562165c59d59d0034df9fff0bf",
  "evaluation_time": 42.05986928939819,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.5896488828089376,
        "f1": 0.5859676705291641,
        "f1_weighted": 0.592613071006628,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5896488828089376,
        "scores_per_experiment": [
          {
            "accuracy": 0.5200638394892841,
            "f1": 0.5235727700767919,
            "f1_weighted": 0.5215344988142052
          },
          {
            "accuracy": 0.5973552211582307,
            "f1": 0.602392058682831,
            "f1_weighted": 0.6026961255853794
          },
          {
            "accuracy": 0.6082991336069311,
            "f1": 0.598973169268644,
            "f1_weighted": 0.6168986743478247
          },
          {
            "accuracy": 0.61171910624715,
            "f1": 0.6018716828646977,
            "f1_weighted": 0.6147703641156449
          },
          {
            "accuracy": 0.5788873689010487,
            "f1": 0.5748525119251929,
            "f1_weighted": 0.5797144570164046
          },
          {
            "accuracy": 0.6064751481988144,
            "f1": 0.6027154176544481,
            "f1_weighted": 0.6076956977171437
          },
          {
            "accuracy": 0.5549475604195167,
            "f1": 0.5461069396978657,
            "f1_weighted": 0.5553325803902209
          },
          {
            "accuracy": 0.5430916552667578,
            "f1": 0.5461061934333707,
            "f1_weighted": 0.5459247123940392
          },
          {
            "accuracy": 0.6258549931600548,
            "f1": 0.6210677740717944,
            "f1_weighted": 0.6317747886221438
          },
          {
            "accuracy": 0.6497948016415869,
            "f1": 0.6420181876160044,
            "f1_weighted": 0.6497888110632747
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.5806711409395973,
        "f1": 0.5776208290398352,
        "f1_weighted": 0.582713412950099,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5806711409395973,
        "scores_per_experiment": [
          {
            "accuracy": 0.5212527964205816,
            "f1": 0.5225947438312674,
            "f1_weighted": 0.5236686880165303
          },
          {
            "accuracy": 0.5865771812080537,
            "f1": 0.5922524439912306,
            "f1_weighted": 0.5907296545888222
          },
          {
            "accuracy": 0.6058165548098434,
            "f1": 0.5967062624990962,
            "f1_weighted": 0.6124216834640311
          },
          {
            "accuracy": 0.6049217002237136,
            "f1": 0.597267806988704,
            "f1_weighted": 0.6049590841453399
          },
          {
            "accuracy": 0.5718120805369128,
            "f1": 0.5712620954504929,
            "f1_weighted": 0.5725023302850151
          },
          {
            "accuracy": 0.578076062639821,
            "f1": 0.5739572684937352,
            "f1_weighted": 0.5790099781784636
          },
          {
            "accuracy": 0.5445190156599553,
            "f1": 0.5358091095484039,
            "f1_weighted": 0.5475304996054645
          },
          {
            "accuracy": 0.5422818791946309,
            "f1": 0.5415992787456717,
            "f1_weighted": 0.5409224152807026
          },
          {
            "accuracy": 0.6219239373601789,
            "f1": 0.6183108291580353,
            "f1_weighted": 0.6254184025179446
          },
          {
            "accuracy": 0.6295302013422819,
            "f1": 0.6264484516917151,
            "f1_weighted": 0.6299713934186761
          }
        ]
      }
    ]
  },
  "task_name": "MTOPDomainClassification"
}