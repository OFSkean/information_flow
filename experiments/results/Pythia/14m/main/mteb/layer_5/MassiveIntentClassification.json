{
  "dataset_revision": "4672e20407010da34463acc759c162ca9734bca6",
  "evaluation_time": 40.29334759712219,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.3367518493611298,
        "f1": 0.31615842152759377,
        "f1_weighted": 0.3481594763617958,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.3367518493611298,
        "scores_per_experiment": [
          {
            "accuracy": 0.3301950235373235,
            "f1": 0.3101588069968705,
            "f1_weighted": 0.34111336129075587
          },
          {
            "accuracy": 0.3439811701412239,
            "f1": 0.3212981466062931,
            "f1_weighted": 0.35554339029263
          },
          {
            "accuracy": 0.34700739744451914,
            "f1": 0.32547635351418097,
            "f1_weighted": 0.35593544776693825
          },
          {
            "accuracy": 0.33422999327505043,
            "f1": 0.31460781832269874,
            "f1_weighted": 0.34572648082224844
          },
          {
            "accuracy": 0.3507061197041022,
            "f1": 0.3226128211393292,
            "f1_weighted": 0.36058740336098927
          },
          {
            "accuracy": 0.3382649630127774,
            "f1": 0.31330855972778815,
            "f1_weighted": 0.3515825443290575
          },
          {
            "accuracy": 0.3133826496301278,
            "f1": 0.2922848620397628,
            "f1_weighted": 0.32280296166314143
          },
          {
            "accuracy": 0.33254875588433086,
            "f1": 0.3080121823514873,
            "f1_weighted": 0.3496008836533281
          },
          {
            "accuracy": 0.3426361802286483,
            "f1": 0.3429648878735541,
            "f1_weighted": 0.3514126207124427
          },
          {
            "accuracy": 0.33456624075319435,
            "f1": 0.3108597767039733,
            "f1_weighted": 0.3472896697264262
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.3390555828824397,
        "f1": 0.3128189133353123,
        "f1_weighted": 0.3502360524600344,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.3390555828824397,
        "scores_per_experiment": [
          {
            "accuracy": 0.34923757993113624,
            "f1": 0.3206687694467201,
            "f1_weighted": 0.3596044967817954
          },
          {
            "accuracy": 0.3364485981308411,
            "f1": 0.306204620841732,
            "f1_weighted": 0.3468292029571504
          },
          {
            "accuracy": 0.3472700442695524,
            "f1": 0.32024875473400416,
            "f1_weighted": 0.3616057729154574
          },
          {
            "accuracy": 0.36153467781603543,
            "f1": 0.32275927422632755,
            "f1_weighted": 0.3764836922717256
          },
          {
            "accuracy": 0.3610427939006394,
            "f1": 0.3314443533978983,
            "f1_weighted": 0.37702576483261946
          },
          {
            "accuracy": 0.3502213477619282,
            "f1": 0.32741582838253797,
            "f1_weighted": 0.35935067403792
          },
          {
            "accuracy": 0.29168716182980814,
            "f1": 0.2735389586701006,
            "f1_weighted": 0.2949220404515069
          },
          {
            "accuracy": 0.33743236596163306,
            "f1": 0.3100821052124597,
            "f1_weighted": 0.3507876850798423
          },
          {
            "accuracy": 0.3246433841613379,
            "f1": 0.31755159472593086,
            "f1_weighted": 0.3313809339215284
          },
          {
            "accuracy": 0.33103787506148546,
            "f1": 0.29827487371541206,
            "f1_weighted": 0.34437026135079785
          }
        ]
      }
    ]
  },
  "task_name": "MassiveIntentClassification"
}