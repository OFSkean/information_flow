{
  "dataset_revision": "ae001d0e6b1228650b7bd1c2c65fb50ad11a8aba",
  "evaluation_time": 116.83768701553345,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.6407204742362062,
        "f1": 0.4177665870412969,
        "f1_weighted": 0.6846457607727967,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.6407204742362062,
        "scores_per_experiment": [
          {
            "accuracy": 0.6295029639762882,
            "f1": 0.4016735462237647,
            "f1_weighted": 0.6728486106379092
          },
          {
            "accuracy": 0.6541267669858641,
            "f1": 0.4173966537107701,
            "f1_weighted": 0.6992794269289434
          },
          {
            "accuracy": 0.646374829001368,
            "f1": 0.4244785854040721,
            "f1_weighted": 0.689493697532466
          },
          {
            "accuracy": 0.6424988600091199,
            "f1": 0.42919304068793107,
            "f1_weighted": 0.6842158048436446
          },
          {
            "accuracy": 0.6502507979936161,
            "f1": 0.4307187266647656,
            "f1_weighted": 0.6937065468279674
          },
          {
            "accuracy": 0.6434108527131783,
            "f1": 0.4232694843709029,
            "f1_weighted": 0.6914915466300149
          },
          {
            "accuracy": 0.6194710442316461,
            "f1": 0.40628586636458813,
            "f1_weighted": 0.6660016825920428
          },
          {
            "accuracy": 0.6582307341541268,
            "f1": 0.43594760667197313,
            "f1_weighted": 0.6989747523026577
          },
          {
            "accuracy": 0.6436388508891929,
            "f1": 0.41797593543615563,
            "f1_weighted": 0.6848637236227597
          },
          {
            "accuracy": 0.6196990424076607,
            "f1": 0.39072642487804565,
            "f1_weighted": 0.6655818158095594
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.6435794183445191,
        "f1": 0.4077489397770192,
        "f1_weighted": 0.6903951524695204,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.6435794183445191,
        "scores_per_experiment": [
          {
            "accuracy": 0.6237136465324384,
            "f1": 0.39815072523223705,
            "f1_weighted": 0.674377988083268
          },
          {
            "accuracy": 0.6644295302013423,
            "f1": 0.4123837311269665,
            "f1_weighted": 0.711662929869649
          },
          {
            "accuracy": 0.6456375838926175,
            "f1": 0.4076881592399148,
            "f1_weighted": 0.6912414615297228
          },
          {
            "accuracy": 0.6348993288590604,
            "f1": 0.3929939149206679,
            "f1_weighted": 0.682280432312504
          },
          {
            "accuracy": 0.6514541387024608,
            "f1": 0.42822430440889797,
            "f1_weighted": 0.6944487345676927
          },
          {
            "accuracy": 0.6384787472035794,
            "f1": 0.4115374190941519,
            "f1_weighted": 0.6871785563045703
          },
          {
            "accuracy": 0.640268456375839,
            "f1": 0.41450667511841505,
            "f1_weighted": 0.6870923499958496
          },
          {
            "accuracy": 0.6621923937360179,
            "f1": 0.41282027160531237,
            "f1_weighted": 0.709058295158987
          },
          {
            "accuracy": 0.6478747203579418,
            "f1": 0.3954184762742744,
            "f1_weighted": 0.6899084466287022
          },
          {
            "accuracy": 0.6268456375838927,
            "f1": 0.4037657207493533,
            "f1_weighted": 0.6767023302442595
          }
        ]
      }
    ]
  },
  "task_name": "MTOPIntentClassification"
}