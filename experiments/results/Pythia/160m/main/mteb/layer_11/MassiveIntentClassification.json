{
  "dataset_revision": "4672e20407010da34463acc759c162ca9734bca6",
  "evaluation_time": 80.93843364715576,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.5354068594485542,
        "f1": 0.5018888687551268,
        "f1_weighted": 0.5413938675240162,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5354068594485542,
        "scores_per_experiment": [
          {
            "accuracy": 0.539340954942838,
            "f1": 0.5125448991139662,
            "f1_weighted": 0.5473167778353945
          },
          {
            "accuracy": 0.5672494956287828,
            "f1": 0.524581050693305,
            "f1_weighted": 0.5734477900680295
          },
          {
            "accuracy": 0.5383322125084062,
            "f1": 0.49779419352817506,
            "f1_weighted": 0.5431321288670513
          },
          {
            "accuracy": 0.5484196368527237,
            "f1": 0.5093876820731914,
            "f1_weighted": 0.5552563255884428
          },
          {
            "accuracy": 0.5484196368527237,
            "f1": 0.501307872974985,
            "f1_weighted": 0.5526304221575191
          },
          {
            "accuracy": 0.5043712172158709,
            "f1": 0.48611327175416746,
            "f1_weighted": 0.5089917077382307
          },
          {
            "accuracy": 0.5168123739071957,
            "f1": 0.4982852751424312,
            "f1_weighted": 0.5246492838166705
          },
          {
            "accuracy": 0.5305985205110961,
            "f1": 0.49786614740722135,
            "f1_weighted": 0.5359544267392817
          },
          {
            "accuracy": 0.519502353732347,
            "f1": 0.4907733579581239,
            "f1_weighted": 0.5250572373296281
          },
          {
            "accuracy": 0.5410221923335575,
            "f1": 0.5002349369057032,
            "f1_weighted": 0.5475025750999142
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.545843580914904,
        "f1": 0.5143054024273679,
        "f1_weighted": 0.5505101401589839,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.545843580914904,
        "scores_per_experiment": [
          {
            "accuracy": 0.5391047712739794,
            "f1": 0.5141562178591571,
            "f1_weighted": 0.5413451086854926
          },
          {
            "accuracy": 0.559272011805214,
            "f1": 0.5199038531956532,
            "f1_weighted": 0.5656649941683932
          },
          {
            "accuracy": 0.5666502705361535,
            "f1": 0.5391269887265977,
            "f1_weighted": 0.5734952262551335
          },
          {
            "accuracy": 0.5705853418593212,
            "f1": 0.5337018656471577,
            "f1_weighted": 0.5765873911644913
          },
          {
            "accuracy": 0.5558288243974422,
            "f1": 0.5185981241836431,
            "f1_weighted": 0.5592165788654575
          },
          {
            "accuracy": 0.5238563698967044,
            "f1": 0.4985578374894989,
            "f1_weighted": 0.5268813634385263
          },
          {
            "accuracy": 0.5130349237579931,
            "f1": 0.4927971822695161,
            "f1_weighted": 0.5198996233157058
          },
          {
            "accuracy": 0.529758976881456,
            "f1": 0.49631144890576895,
            "f1_weighted": 0.5343632148625603
          },
          {
            "accuracy": 0.5218888342351206,
            "f1": 0.4957223793549597,
            "f1_weighted": 0.5236292626291003
          },
          {
            "accuracy": 0.5784554845056566,
            "f1": 0.5341781266417268,
            "f1_weighted": 0.5840186382049788
          }
        ]
      }
    ]
  },
  "task_name": "MassiveIntentClassification"
}