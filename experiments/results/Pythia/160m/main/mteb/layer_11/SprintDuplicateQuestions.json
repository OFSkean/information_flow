{
  "dataset_revision": "d66bd1f72af766a5cc4b0ca5e00c162f89e8cc46",
  "evaluation_time": 24.419283151626587,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "cosine_accuracy": 0.9906435643564356,
        "cosine_accuracy_threshold": 0.9717274904251099,
        "cosine_ap": 0.30665870045027427,
        "cosine_f1": 0.39271017048794826,
        "cosine_f1_threshold": 0.9587302803993225,
        "cosine_precision": 0.4764621968616263,
        "cosine_recall": 0.334,
        "dot_accuracy": 0.9901188118811881,
        "dot_accuracy_threshold": 29363.734375,
        "dot_ap": 0.03735423313762171,
        "dot_f1": 0.08915304606240712,
        "dot_f1_threshold": 26990.00390625,
        "dot_precision": 0.08832188420019627,
        "dot_recall": 0.09,
        "euclidean_accuracy": 0.9904851485148515,
        "euclidean_accuracy_threshold": 36.42470932006836,
        "euclidean_ap": 0.2832742585060518,
        "euclidean_f1": 0.3656267104542966,
        "euclidean_f1_threshold": 47.05649185180664,
        "euclidean_precision": 0.40386940749697703,
        "euclidean_recall": 0.334,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.30665870045027427,
        "manhattan_accuracy": 0.9905247524752475,
        "manhattan_accuracy_threshold": 792.933837890625,
        "manhattan_ap": 0.2994589576503243,
        "manhattan_f1": 0.38307107976125876,
        "manhattan_f1_threshold": 1026.9892578125,
        "manhattan_precision": 0.41874258600237246,
        "manhattan_recall": 0.353,
        "max_accuracy": 0.9906435643564356,
        "max_ap": 0.30665870045027427,
        "max_f1": 0.39271017048794826,
        "max_precision": 0.4764621968616263,
        "max_recall": 0.353,
        "similarity_accuracy": 0.9906435643564356,
        "similarity_accuracy_threshold": 0.9717274904251099,
        "similarity_ap": 0.30665870045027427,
        "similarity_f1": 0.39271017048794826,
        "similarity_f1_threshold": 0.9587302803993225,
        "similarity_precision": 0.4764621968616263,
        "similarity_recall": 0.334
      }
    ],
    "validation": [
      {
        "cosine_accuracy": 0.9905940594059406,
        "cosine_accuracy_threshold": 0.9670792818069458,
        "cosine_ap": 0.23855596920468702,
        "cosine_f1": 0.30674157303370786,
        "cosine_f1_threshold": 0.9564613103866577,
        "cosine_precision": 0.35,
        "cosine_recall": 0.273,
        "dot_accuracy": 0.9900990099009901,
        "dot_accuracy_threshold": 29582.78515625,
        "dot_ap": 0.03103589528888083,
        "dot_f1": 0.07918634217217581,
        "dot_f1_threshold": 26664.890625,
        "dot_precision": 0.06217912150598973,
        "dot_recall": 0.109,
        "euclidean_accuracy": 0.9905544554455445,
        "euclidean_accuracy_threshold": 38.814598083496094,
        "euclidean_ap": 0.21633152108426484,
        "euclidean_f1": 0.28125000000000006,
        "euclidean_f1_threshold": 48.15862274169922,
        "euclidean_precision": 0.29347826086956524,
        "euclidean_recall": 0.27,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.23855596920468702,
        "manhattan_accuracy": 0.9905247524752475,
        "manhattan_accuracy_threshold": 854.382568359375,
        "manhattan_ap": 0.23019818881283494,
        "manhattan_f1": 0.29523809523809524,
        "manhattan_f1_threshold": 1048.635498046875,
        "manhattan_precision": 0.31348314606741573,
        "manhattan_recall": 0.279,
        "max_accuracy": 0.9905940594059406,
        "max_ap": 0.23855596920468702,
        "max_f1": 0.30674157303370786,
        "max_precision": 0.35,
        "max_recall": 0.279,
        "similarity_accuracy": 0.9905940594059406,
        "similarity_accuracy_threshold": 0.9670792818069458,
        "similarity_ap": 0.23855596920468702,
        "similarity_f1": 0.30674157303370786,
        "similarity_f1_threshold": 0.9564613103866577,
        "similarity_precision": 0.35,
        "similarity_recall": 0.273
      }
    ]
  },
  "task_name": "SprintDuplicateQuestions"
}