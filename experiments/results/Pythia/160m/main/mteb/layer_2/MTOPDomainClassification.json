{
  "dataset_revision": "d80d48c1eb48d3562165c59d59d0034df9fff0bf",
  "evaluation_time": 42.29475235939026,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.762859097127223,
        "f1": 0.756413276013472,
        "f1_weighted": 0.7656732180408744,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.762859097127223,
        "scores_per_experiment": [
          {
            "accuracy": 0.7088463292293662,
            "f1": 0.709353168283465,
            "f1_weighted": 0.7123288155933752
          },
          {
            "accuracy": 0.7612859097127223,
            "f1": 0.7568721902301742,
            "f1_weighted": 0.7660396296904393
          },
          {
            "accuracy": 0.7637938896488828,
            "f1": 0.7540126716433193,
            "f1_weighted": 0.7726690920426522
          },
          {
            "accuracy": 0.782717738258094,
            "f1": 0.7720967022820122,
            "f1_weighted": 0.7865836597642429
          },
          {
            "accuracy": 0.8011855905152758,
            "f1": 0.7922700762112572,
            "f1_weighted": 0.8024557467121036
          },
          {
            "accuracy": 0.7802097583219334,
            "f1": 0.7751748250128425,
            "f1_weighted": 0.7798267061640175
          },
          {
            "accuracy": 0.7323301413588691,
            "f1": 0.7236357637811341,
            "f1_weighted": 0.7291682746021038
          },
          {
            "accuracy": 0.7211582307341541,
            "f1": 0.7179359706386047,
            "f1_weighted": 0.7268925180671393
          },
          {
            "accuracy": 0.774281805745554,
            "f1": 0.7717797021988028,
            "f1_weighted": 0.7779706144800567
          },
          {
            "accuracy": 0.802781577747378,
            "f1": 0.7910016898531079,
            "f1_weighted": 0.8027971232926125
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.7545861297539149,
        "f1": 0.7519664371090093,
        "f1_weighted": 0.7567658261738879,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.7545861297539149,
        "scores_per_experiment": [
          {
            "accuracy": 0.7051454138702461,
            "f1": 0.7077908177519704,
            "f1_weighted": 0.7076761904578741
          },
          {
            "accuracy": 0.7579418344519016,
            "f1": 0.7575414197747435,
            "f1_weighted": 0.7604132996275693
          },
          {
            "accuracy": 0.7628635346756152,
            "f1": 0.7580966399162725,
            "f1_weighted": 0.7715958249254941
          },
          {
            "accuracy": 0.7753914988814318,
            "f1": 0.775970517086492,
            "f1_weighted": 0.7807806487093306
          },
          {
            "accuracy": 0.789261744966443,
            "f1": 0.7881003581394896,
            "f1_weighted": 0.7915736647091391
          },
          {
            "accuracy": 0.7642058165548098,
            "f1": 0.7628075033299898,
            "f1_weighted": 0.7629030665269868
          },
          {
            "accuracy": 0.7239373601789709,
            "f1": 0.7125893488349141,
            "f1_weighted": 0.7199055465096637
          },
          {
            "accuracy": 0.7078299776286353,
            "f1": 0.7082413457257649,
            "f1_weighted": 0.711738418693255
          },
          {
            "accuracy": 0.7713646532438478,
            "f1": 0.7694789906726018,
            "f1_weighted": 0.7739435317435144
          },
          {
            "accuracy": 0.7879194630872484,
            "f1": 0.7790474298578548,
            "f1_weighted": 0.7871280698360525
          }
        ]
      }
    ]
  },
  "task_name": "MTOPDomainClassification"
}