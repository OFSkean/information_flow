{
  "dataset_revision": "4672e20407010da34463acc759c162ca9734bca6",
  "evaluation_time": 75.01151990890503,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.5410894418291863,
        "f1": 0.514623430110159,
        "f1_weighted": 0.5439104360249528,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5410894418291863,
        "scores_per_experiment": [
          {
            "accuracy": 0.531607262945528,
            "f1": 0.5095462646396463,
            "f1_weighted": 0.5329748317587384
          },
          {
            "accuracy": 0.5551445864156019,
            "f1": 0.5281849314353357,
            "f1_weighted": 0.5586868213496183
          },
          {
            "accuracy": 0.5427034297242771,
            "f1": 0.5157957883858804,
            "f1_weighted": 0.5407311247180918
          },
          {
            "accuracy": 0.5474108944182918,
            "f1": 0.5005074028949665,
            "f1_weighted": 0.550878726180647
          },
          {
            "accuracy": 0.5490921318090114,
            "f1": 0.518976970586363,
            "f1_weighted": 0.5518327621078919
          },
          {
            "accuracy": 0.5349697377269671,
            "f1": 0.5154255907238433,
            "f1_weighted": 0.5392185511179249
          },
          {
            "accuracy": 0.5346334902488231,
            "f1": 0.5204349721329404,
            "f1_weighted": 0.5365374503009448
          },
          {
            "accuracy": 0.5460659045057162,
            "f1": 0.5131630419676321,
            "f1_weighted": 0.5514284091387858
          },
          {
            "accuracy": 0.5289172831203766,
            "f1": 0.5072706205539554,
            "f1_weighted": 0.5330846070970275
          },
          {
            "accuracy": 0.5403496973772697,
            "f1": 0.5169287177810262,
            "f1_weighted": 0.5437310764798586
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.5518937530742745,
        "f1": 0.5279193110374476,
        "f1_weighted": 0.5535448377344633,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5518937530742745,
        "scores_per_experiment": [
          {
            "accuracy": 0.5386128873585834,
            "f1": 0.5263624098641867,
            "f1_weighted": 0.5358001526704057
          },
          {
            "accuracy": 0.5705853418593212,
            "f1": 0.5400000934259751,
            "f1_weighted": 0.5716066438751475
          },
          {
            "accuracy": 0.5676340383669454,
            "f1": 0.542511215423889,
            "f1_weighted": 0.5683833915528881
          },
          {
            "accuracy": 0.558288243974422,
            "f1": 0.5177163095989534,
            "f1_weighted": 0.5597814986563466
          },
          {
            "accuracy": 0.5696015740285293,
            "f1": 0.5471168181654656,
            "f1_weighted": 0.5702958193296098
          },
          {
            "accuracy": 0.544023610427939,
            "f1": 0.5334377432008832,
            "f1_weighted": 0.5499915455684017
          },
          {
            "accuracy": 0.5105755041810133,
            "f1": 0.49602356577152457,
            "f1_weighted": 0.5088194973876395
          },
          {
            "accuracy": 0.5469749139203148,
            "f1": 0.5131098724226462,
            "f1_weighted": 0.5496181518224065
          },
          {
            "accuracy": 0.5464830300049188,
            "f1": 0.5284611588517444,
            "f1_weighted": 0.5507526120966645
          },
          {
            "accuracy": 0.5661583866207575,
            "f1": 0.5344539236492084,
            "f1_weighted": 0.5703990643851231
          }
        ]
      }
    ]
  },
  "task_name": "MassiveIntentClassification"
}