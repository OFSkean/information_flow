{
  "dataset_revision": "ae001d0e6b1228650b7bd1c2c65fb50ad11a8aba",
  "evaluation_time": 106.98770356178284,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.6509575923392613,
        "f1": 0.44002447561479385,
        "f1_weighted": 0.6919970111247048,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.6509575923392613,
        "scores_per_experiment": [
          {
            "accuracy": 0.6523027815777473,
            "f1": 0.4423915066892413,
            "f1_weighted": 0.6886416444616449
          },
          {
            "accuracy": 0.6370269037847697,
            "f1": 0.41176816335612176,
            "f1_weighted": 0.6784525990517656
          },
          {
            "accuracy": 0.6404468764249887,
            "f1": 0.4370123691749903,
            "f1_weighted": 0.6839412468886893
          },
          {
            "accuracy": 0.6577747378020976,
            "f1": 0.45738523584466745,
            "f1_weighted": 0.6998169206209897
          },
          {
            "accuracy": 0.6568627450980392,
            "f1": 0.44531630594207294,
            "f1_weighted": 0.6983367349092777
          },
          {
            "accuracy": 0.6527587779297765,
            "f1": 0.44410145622553554,
            "f1_weighted": 0.6944948008133008
          },
          {
            "accuracy": 0.63406292749658,
            "f1": 0.437225223467506,
            "f1_weighted": 0.6724518406555245
          },
          {
            "accuracy": 0.6709986320109439,
            "f1": 0.4527701743754231,
            "f1_weighted": 0.7123599537185884
          },
          {
            "accuracy": 0.6495668034655723,
            "f1": 0.4428862827681043,
            "f1_weighted": 0.692156054605322
          },
          {
            "accuracy": 0.6577747378020976,
            "f1": 0.42938803830427646,
            "f1_weighted": 0.6993183155219452
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.6521700223713646,
        "f1": 0.4381238790472352,
        "f1_weighted": 0.6956364378607836,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.6521700223713646,
        "scores_per_experiment": [
          {
            "accuracy": 0.643847874720358,
            "f1": 0.42315486157460946,
            "f1_weighted": 0.6840050017150339
          },
          {
            "accuracy": 0.6353467561521253,
            "f1": 0.4232494609117622,
            "f1_weighted": 0.6776651674073222
          },
          {
            "accuracy": 0.6505592841163311,
            "f1": 0.4257472458433778,
            "f1_weighted": 0.6989079990419171
          },
          {
            "accuracy": 0.6487695749440716,
            "f1": 0.4417318693636531,
            "f1_weighted": 0.6953153915337591
          },
          {
            "accuracy": 0.6724832214765101,
            "f1": 0.46727791721813694,
            "f1_weighted": 0.7169807640045563
          },
          {
            "accuracy": 0.6572706935123043,
            "f1": 0.43242346585552577,
            "f1_weighted": 0.6999306838855611
          },
          {
            "accuracy": 0.6340044742729306,
            "f1": 0.4461039294314872,
            "f1_weighted": 0.6750400888208641
          },
          {
            "accuracy": 0.669351230425056,
            "f1": 0.44280964606824624,
            "f1_weighted": 0.7126743294487861
          },
          {
            "accuracy": 0.6420581655480985,
            "f1": 0.4262072264772006,
            "f1_weighted": 0.6865662419003946
          },
          {
            "accuracy": 0.6680089485458613,
            "f1": 0.45253316772835267,
            "f1_weighted": 0.7092787108496412
          }
        ]
      }
    ]
  },
  "task_name": "MTOPIntentClassification"
}