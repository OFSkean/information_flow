{
  "dataset_revision": "fad2c6e8459f9e1c45d9315f4953d921437d70f8",
  "evaluation_time": 39.49222159385681,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.5649293880295898,
        "f1": 0.5447595221995275,
        "f1_weighted": 0.5678592456385421,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5649293880295898,
        "scores_per_experiment": [
          {
            "accuracy": 0.5776731674512441,
            "f1": 0.5577298199124259,
            "f1_weighted": 0.5815810740820443
          },
          {
            "accuracy": 0.5981842636180229,
            "f1": 0.5732072371413457,
            "f1_weighted": 0.5993708604751521
          },
          {
            "accuracy": 0.5450571620712845,
            "f1": 0.5286413718275039,
            "f1_weighted": 0.5509305891273377
          },
          {
            "accuracy": 0.5642232683254875,
            "f1": 0.5458502923321913,
            "f1_weighted": 0.5685027182976079
          },
          {
            "accuracy": 0.5591795561533288,
            "f1": 0.5342626480943387,
            "f1_weighted": 0.5590807729185951
          },
          {
            "accuracy": 0.5642232683254875,
            "f1": 0.5425271267165889,
            "f1_weighted": 0.5686360460771137
          },
          {
            "accuracy": 0.5753194351042367,
            "f1": 0.5555343589124896,
            "f1_weighted": 0.5793150360292323
          },
          {
            "accuracy": 0.5850706119704102,
            "f1": 0.5679744631882025,
            "f1_weighted": 0.5859932455009244
          },
          {
            "accuracy": 0.5423671822461331,
            "f1": 0.5203465045528507,
            "f1_weighted": 0.5447255851753388
          },
          {
            "accuracy": 0.5379959650302623,
            "f1": 0.5215213993173374,
            "f1_weighted": 0.5404565287020734
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.5568617806197736,
        "f1": 0.5439156996363886,
        "f1_weighted": 0.5569586390952196,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5568617806197736,
        "scores_per_experiment": [
          {
            "accuracy": 0.5632070831283817,
            "f1": 0.549601154924724,
            "f1_weighted": 0.5661047172775826
          },
          {
            "accuracy": 0.5912444663059518,
            "f1": 0.5807680220418017,
            "f1_weighted": 0.5887470525260003
          },
          {
            "accuracy": 0.5459911460895229,
            "f1": 0.5383749222261931,
            "f1_weighted": 0.5476614332916755
          },
          {
            "accuracy": 0.5504181013280866,
            "f1": 0.5327385043834397,
            "f1_weighted": 0.5517052183146174
          },
          {
            "accuracy": 0.5459911460895229,
            "f1": 0.5281861551939268,
            "f1_weighted": 0.5428175740786103
          },
          {
            "accuracy": 0.5454992621741269,
            "f1": 0.5373039193478254,
            "f1_weighted": 0.5449973624746517
          },
          {
            "accuracy": 0.5450073782587309,
            "f1": 0.535873746872121,
            "f1_weighted": 0.55184940990993
          },
          {
            "accuracy": 0.5838662075750123,
            "f1": 0.5654506416404071,
            "f1_weighted": 0.5804601309159046
          },
          {
            "accuracy": 0.5459911460895229,
            "f1": 0.5326626565202416,
            "f1_weighted": 0.5437257845561955
          },
          {
            "accuracy": 0.5514018691588785,
            "f1": 0.538197273213206,
            "f1_weighted": 0.5515177076070283
          }
        ]
      }
    ]
  },
  "task_name": "MassiveScenarioClassification"
}