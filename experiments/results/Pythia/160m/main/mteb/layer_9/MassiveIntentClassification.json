{
  "dataset_revision": "4672e20407010da34463acc759c162ca9734bca6",
  "evaluation_time": 80.40079426765442,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.5306993947545393,
        "f1": 0.4973604288574096,
        "f1_weighted": 0.5360963723754941,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5306993947545393,
        "scores_per_experiment": [
          {
            "accuracy": 0.531607262945528,
            "f1": 0.5019451504012167,
            "f1_weighted": 0.536893624384308
          },
          {
            "accuracy": 0.5514458641560188,
            "f1": 0.5092419900756934,
            "f1_weighted": 0.5587171491475929
          },
          {
            "accuracy": 0.5242098184263618,
            "f1": 0.48720474336227093,
            "f1_weighted": 0.5283865449003318
          },
          {
            "accuracy": 0.558843308675185,
            "f1": 0.5150253413866424,
            "f1_weighted": 0.5651021034666021
          },
          {
            "accuracy": 0.5443846671149967,
            "f1": 0.5031237391905911,
            "f1_weighted": 0.5470017978270056
          },
          {
            "accuracy": 0.49932750504371215,
            "f1": 0.48383829393643124,
            "f1_weighted": 0.5001305606870653
          },
          {
            "accuracy": 0.5131136516476127,
            "f1": 0.4888245967801288,
            "f1_weighted": 0.5193728060766718
          },
          {
            "accuracy": 0.5279085406859448,
            "f1": 0.4889711893885151,
            "f1_weighted": 0.5369706333872752
          },
          {
            "accuracy": 0.5110961667787491,
            "f1": 0.4868431952966035,
            "f1_weighted": 0.5182324482532128
          },
          {
            "accuracy": 0.5450571620712845,
            "f1": 0.5085860487560031,
            "f1_weighted": 0.5501560556248765
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.5417609444171175,
        "f1": 0.5097372752648446,
        "f1_weighted": 0.5477890302448242,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5417609444171175,
        "scores_per_experiment": [
          {
            "accuracy": 0.5400885391047713,
            "f1": 0.5139961135490652,
            "f1_weighted": 0.5427466038555071
          },
          {
            "accuracy": 0.5573044761436301,
            "f1": 0.510272328003096,
            "f1_weighted": 0.5648588264914276
          },
          {
            "accuracy": 0.5533694048204624,
            "f1": 0.518448132866319,
            "f1_weighted": 0.5610507593536695
          },
          {
            "accuracy": 0.5607476635514018,
            "f1": 0.5165906909605742,
            "f1_weighted": 0.5674369370644075
          },
          {
            "accuracy": 0.558288243974422,
            "f1": 0.5211792489876552,
            "f1_weighted": 0.5615021073359094
          },
          {
            "accuracy": 0.515002459419577,
            "f1": 0.503036099377556,
            "f1_weighted": 0.5143982379612185
          },
          {
            "accuracy": 0.5031972454500738,
            "f1": 0.48263916694396225,
            "f1_weighted": 0.5113245498451567
          },
          {
            "accuracy": 0.5307427447122479,
            "f1": 0.5036967375833474,
            "f1_weighted": 0.5400595747875148
          },
          {
            "accuracy": 0.5213969503197246,
            "f1": 0.49196857934974036,
            "f1_weighted": 0.5303417021864844
          },
          {
            "accuracy": 0.5774717166748647,
            "f1": 0.5355456550271301,
            "f1_weighted": 0.5841710035669468
          }
        ]
      }
    ]
  },
  "task_name": "MassiveIntentClassification"
}