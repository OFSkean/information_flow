{
  "dataset_revision": "ae001d0e6b1228650b7bd1c2c65fb50ad11a8aba",
  "evaluation_time": 114.5217854976654,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.652485180118559,
        "f1": 0.4310681332561783,
        "f1_weighted": 0.6950394131232629,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.652485180118559,
        "scores_per_experiment": [
          {
            "accuracy": 0.6459188326493388,
            "f1": 0.42226399488733507,
            "f1_weighted": 0.6866697044523541
          },
          {
            "accuracy": 0.6568627450980392,
            "f1": 0.4243585975008166,
            "f1_weighted": 0.7016784552588166
          },
          {
            "accuracy": 0.6593707250341997,
            "f1": 0.42841736950698767,
            "f1_weighted": 0.7033070703787748
          },
          {
            "accuracy": 0.6511627906976745,
            "f1": 0.4412358477288769,
            "f1_weighted": 0.6940203769769243
          },
          {
            "accuracy": 0.6443228454172366,
            "f1": 0.43131395865678723,
            "f1_weighted": 0.6898741526129128
          },
          {
            "accuracy": 0.6580027359781122,
            "f1": 0.43519725764937556,
            "f1_weighted": 0.7029095006930229
          },
          {
            "accuracy": 0.6388508891928865,
            "f1": 0.4341600678458819,
            "f1_weighted": 0.6798428880493986
          },
          {
            "accuracy": 0.6696306429548564,
            "f1": 0.4497691099899684,
            "f1_weighted": 0.7098053219592155
          },
          {
            "accuracy": 0.6491108071135431,
            "f1": 0.4226983312309386,
            "f1_weighted": 0.688996430921654
          },
          {
            "accuracy": 0.6516187870497036,
            "f1": 0.42126679756481483,
            "f1_weighted": 0.6932902299295545
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.6522147651006711,
        "f1": 0.4147223618163968,
        "f1_weighted": 0.6978088029860036,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.6522147651006711,
        "scores_per_experiment": [
          {
            "accuracy": 0.6380313199105145,
            "f1": 0.4097552339538931,
            "f1_weighted": 0.6842776222672616
          },
          {
            "accuracy": 0.6635346756152125,
            "f1": 0.41393973477687956,
            "f1_weighted": 0.7105230236699932
          },
          {
            "accuracy": 0.6590604026845638,
            "f1": 0.4054359632415166,
            "f1_weighted": 0.7058090330934507
          },
          {
            "accuracy": 0.6550335570469799,
            "f1": 0.4181441368088817,
            "f1_weighted": 0.7053648480239528
          },
          {
            "accuracy": 0.6478747203579418,
            "f1": 0.43609966224548197,
            "f1_weighted": 0.6893233512063653
          },
          {
            "accuracy": 0.6590604026845638,
            "f1": 0.42615580657099067,
            "f1_weighted": 0.7079602570462915
          },
          {
            "accuracy": 0.6357941834451901,
            "f1": 0.40304940571887604,
            "f1_weighted": 0.6802169625264409
          },
          {
            "accuracy": 0.658165548098434,
            "f1": 0.4273525130806513,
            "f1_weighted": 0.7065676117118518
          },
          {
            "accuracy": 0.6456375838926175,
            "f1": 0.3930687430604409,
            "f1_weighted": 0.6848395645761555
          },
          {
            "accuracy": 0.6599552572706935,
            "f1": 0.4142224187063565,
            "f1_weighted": 0.7032057557382716
          }
        ]
      }
    ]
  },
  "task_name": "MTOPIntentClassification"
}