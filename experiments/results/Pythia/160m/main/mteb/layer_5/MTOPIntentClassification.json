{
  "dataset_revision": "ae001d0e6b1228650b7bd1c2c65fb50ad11a8aba",
  "evaluation_time": 106.40379691123962,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.6799361605107159,
        "f1": 0.46317270608318123,
        "f1_weighted": 0.7194846761147624,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.6799361605107159,
        "scores_per_experiment": [
          {
            "accuracy": 0.6792065663474692,
            "f1": 0.4541440135557327,
            "f1_weighted": 0.7144345811071539
          },
          {
            "accuracy": 0.6853625170998632,
            "f1": 0.4491475486645876,
            "f1_weighted": 0.7250510451915639
          },
          {
            "accuracy": 0.668718650250798,
            "f1": 0.46036288980516155,
            "f1_weighted": 0.7055940014663475
          },
          {
            "accuracy": 0.6819425444596443,
            "f1": 0.4699286046347022,
            "f1_weighted": 0.7214012464237157
          },
          {
            "accuracy": 0.6716826265389877,
            "f1": 0.4622668969355709,
            "f1_weighted": 0.71344725725438
          },
          {
            "accuracy": 0.6851345189238486,
            "f1": 0.4738293103174285,
            "f1_weighted": 0.7276696588240403
          },
          {
            "accuracy": 0.6730506155950753,
            "f1": 0.46658491370560273,
            "f1_weighted": 0.713523566862631
          },
          {
            "accuracy": 0.70109439124487,
            "f1": 0.4906880658115304,
            "f1_weighted": 0.7406952619259316
          },
          {
            "accuracy": 0.6773825809393524,
            "f1": 0.4596182346692885,
            "f1_weighted": 0.7161521675644129
          },
          {
            "accuracy": 0.6757865937072504,
            "f1": 0.4451565827322074,
            "f1_weighted": 0.7168779745274464
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.6783892617449665,
        "f1": 0.44112656784149296,
        "f1_weighted": 0.7190903902700739,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.6783892617449665,
        "scores_per_experiment": [
          {
            "accuracy": 0.6760626398210291,
            "f1": 0.4473969182139506,
            "f1_weighted": 0.713755149466368
          },
          {
            "accuracy": 0.6805369127516778,
            "f1": 0.43759611873829696,
            "f1_weighted": 0.724642939159757
          },
          {
            "accuracy": 0.6706935123042506,
            "f1": 0.43603757581207964,
            "f1_weighted": 0.7126439005818941
          },
          {
            "accuracy": 0.6733780760626398,
            "f1": 0.4328296283559585,
            "f1_weighted": 0.7183657473025458
          },
          {
            "accuracy": 0.669351230425056,
            "f1": 0.4501958943035763,
            "f1_weighted": 0.7099757075487152
          },
          {
            "accuracy": 0.6774049217002237,
            "f1": 0.4447109293977652,
            "f1_weighted": 0.7165607217835634
          },
          {
            "accuracy": 0.6720357941834452,
            "f1": 0.4459942662215189,
            "f1_weighted": 0.7123222818668516
          },
          {
            "accuracy": 0.6966442953020134,
            "f1": 0.4315117393571785,
            "f1_weighted": 0.7371048271036427
          },
          {
            "accuracy": 0.6778523489932886,
            "f1": 0.43294304923232657,
            "f1_weighted": 0.7154754798046009
          },
          {
            "accuracy": 0.6899328859060403,
            "f1": 0.4520495587822785,
            "f1_weighted": 0.7300571480828006
          }
        ]
      }
    ]
  },
  "task_name": "MTOPIntentClassification"
}