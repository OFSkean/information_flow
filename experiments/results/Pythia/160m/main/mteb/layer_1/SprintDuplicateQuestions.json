{
  "dataset_revision": "d66bd1f72af766a5cc4b0ca5e00c162f89e8cc46",
  "evaluation_time": 18.010775327682495,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "cosine_accuracy": 0.9913465346534653,
        "cosine_accuracy_threshold": 0.9535441994667053,
        "cosine_ap": 0.3642377626521041,
        "cosine_f1": 0.3959843837144451,
        "cosine_f1_threshold": 0.9316461682319641,
        "cosine_precision": 0.44766708701134933,
        "cosine_recall": 0.355,
        "dot_accuracy": 0.9903069306930693,
        "dot_accuracy_threshold": 16616.087890625,
        "dot_ap": 0.15464178615163282,
        "dot_f1": 0.22667829119442023,
        "dot_f1_threshold": 15392.955078125,
        "dot_precision": 0.2009273570324575,
        "dot_recall": 0.26,
        "euclidean_accuracy": 0.9912475247524752,
        "euclidean_accuracy_threshold": 38.37019348144531,
        "euclidean_ap": 0.33279100419781843,
        "euclidean_f1": 0.37300177619893427,
        "euclidean_f1_threshold": 45.58174133300781,
        "euclidean_precision": 0.45718432510885343,
        "euclidean_recall": 0.315,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.3642377626521041,
        "manhattan_accuracy": 0.9913069306930693,
        "manhattan_accuracy_threshold": 857.859375,
        "manhattan_ap": 0.33981067982082314,
        "manhattan_f1": 0.37889960294951786,
        "manhattan_f1_threshold": 1004.9689331054688,
        "manhattan_precision": 0.43774574049803405,
        "manhattan_recall": 0.334,
        "max_accuracy": 0.9913465346534653,
        "max_ap": 0.3642377626521041,
        "max_f1": 0.3959843837144451,
        "max_precision": 0.45718432510885343,
        "max_recall": 0.355,
        "similarity_accuracy": 0.9913465346534653,
        "similarity_accuracy_threshold": 0.9535441994667053,
        "similarity_ap": 0.3642377626521041,
        "similarity_f1": 0.3959843837144451,
        "similarity_f1_threshold": 0.9316461682319641,
        "similarity_precision": 0.44766708701134933,
        "similarity_recall": 0.355
      }
    ],
    "validation": [
      {
        "cosine_accuracy": 0.9908514851485148,
        "cosine_accuracy_threshold": 0.9513276815414429,
        "cosine_ap": 0.2689459442183573,
        "cosine_f1": 0.33074131674442714,
        "cosine_f1_threshold": 0.9279282093048096,
        "cosine_precision": 0.3433799784714747,
        "cosine_recall": 0.319,
        "dot_accuracy": 0.9901980198019802,
        "dot_accuracy_threshold": 16742.76171875,
        "dot_ap": 0.13093485778089242,
        "dot_f1": 0.21654749744637386,
        "dot_f1_threshold": 15510.482421875,
        "dot_precision": 0.22129436325678498,
        "dot_recall": 0.212,
        "euclidean_accuracy": 0.9908118811881188,
        "euclidean_accuracy_threshold": 39.422569274902344,
        "euclidean_ap": 0.23997171107492427,
        "euclidean_f1": 0.2984749455337691,
        "euclidean_f1_threshold": 47.291908264160156,
        "euclidean_precision": 0.3277511961722488,
        "euclidean_recall": 0.274,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.2689459442183573,
        "manhattan_accuracy": 0.9908316831683168,
        "manhattan_accuracy_threshold": 864.9345703125,
        "manhattan_ap": 0.24630669245070957,
        "manhattan_f1": 0.3045375218150087,
        "manhattan_f1_threshold": 1067.352783203125,
        "manhattan_precision": 0.27012383900928794,
        "manhattan_recall": 0.349,
        "max_accuracy": 0.9908514851485148,
        "max_ap": 0.2689459442183573,
        "max_f1": 0.33074131674442714,
        "max_precision": 0.3433799784714747,
        "max_recall": 0.349,
        "similarity_accuracy": 0.9908514851485148,
        "similarity_accuracy_threshold": 0.9513276815414429,
        "similarity_ap": 0.2689459442183573,
        "similarity_f1": 0.33074131674442714,
        "similarity_f1_threshold": 0.9279282093048096,
        "similarity_precision": 0.3433799784714747,
        "similarity_recall": 0.319
      }
    ]
  },
  "task_name": "SprintDuplicateQuestions"
}