{
  "dataset_revision": "fad2c6e8459f9e1c45d9315f4953d921437d70f8",
  "evaluation_time": 42.73366832733154,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.5714189643577673,
        "f1": 0.5503623261702258,
        "f1_weighted": 0.5728473812677628,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5714189643577673,
        "scores_per_experiment": [
          {
            "accuracy": 0.5810356422326832,
            "f1": 0.5592563716690466,
            "f1_weighted": 0.5862888695163184
          },
          {
            "accuracy": 0.5965030262273033,
            "f1": 0.5703156083247451,
            "f1_weighted": 0.6020588086841021
          },
          {
            "accuracy": 0.5501008742434432,
            "f1": 0.5352220004755661,
            "f1_weighted": 0.5525753440179029
          },
          {
            "accuracy": 0.5716207128446537,
            "f1": 0.5500682988628233,
            "f1_weighted": 0.5716693556678585
          },
          {
            "accuracy": 0.5655682582380632,
            "f1": 0.5410187674417973,
            "f1_weighted": 0.564519520630938
          },
          {
            "accuracy": 0.5736381977135171,
            "f1": 0.5486507747756398,
            "f1_weighted": 0.5778663556738258
          },
          {
            "accuracy": 0.5776731674512441,
            "f1": 0.5586099659453474,
            "f1_weighted": 0.5794559588569963
          },
          {
            "accuracy": 0.5961667787491594,
            "f1": 0.5771725225281562,
            "f1_weighted": 0.5946586871494866
          },
          {
            "accuracy": 0.551109616677875,
            "f1": 0.5296435644349529,
            "f1_weighted": 0.5495916807364905
          },
          {
            "accuracy": 0.550773369199731,
            "f1": 0.5336653872441826,
            "f1_weighted": 0.5497892317437083
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.5598622725036891,
        "f1": 0.5473116716931582,
        "f1_weighted": 0.5593406121323288,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5598622725036891,
        "scores_per_experiment": [
          {
            "accuracy": 0.5646827348745695,
            "f1": 0.5492909309542229,
            "f1_weighted": 0.5693195076594182
          },
          {
            "accuracy": 0.5818986719134285,
            "f1": 0.5751234046860261,
            "f1_weighted": 0.5833937175252848
          },
          {
            "accuracy": 0.5509099852434826,
            "f1": 0.5396734214409737,
            "f1_weighted": 0.5504173637823175
          },
          {
            "accuracy": 0.5504181013280866,
            "f1": 0.5292781596192148,
            "f1_weighted": 0.5481868726713746
          },
          {
            "accuracy": 0.5479586817511067,
            "f1": 0.530821011341819,
            "f1_weighted": 0.542328911227578
          },
          {
            "accuracy": 0.5514018691588785,
            "f1": 0.5436965955006513,
            "f1_weighted": 0.5524914768552667
          },
          {
            "accuracy": 0.5573044761436301,
            "f1": 0.5468866777596495,
            "f1_weighted": 0.5637777366416022
          },
          {
            "accuracy": 0.5799311362518446,
            "f1": 0.5590088290438969,
            "f1_weighted": 0.573627766381549
          },
          {
            "accuracy": 0.5686178061977374,
            "f1": 0.563382089440686,
            "f1_weighted": 0.5657575528260111
          },
          {
            "accuracy": 0.5454992621741269,
            "f1": 0.5359555971444414,
            "f1_weighted": 0.5441052157528853
          }
        ]
      }
    ]
  },
  "task_name": "MassiveScenarioClassification"
}