{
  "dataset_revision": "ae001d0e6b1228650b7bd1c2c65fb50ad11a8aba",
  "evaluation_time": 96.40342116355896,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.6528043775649794,
        "f1": 0.4427407430210014,
        "f1_weighted": 0.694895283530413,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.6528043775649794,
        "scores_per_experiment": [
          {
            "accuracy": 0.6523027815777473,
            "f1": 0.44139040748574915,
            "f1_weighted": 0.6900274778636396
          },
          {
            "accuracy": 0.6434108527131783,
            "f1": 0.44285932041471426,
            "f1_weighted": 0.68407176211634
          },
          {
            "accuracy": 0.6390788873689011,
            "f1": 0.43498283821724243,
            "f1_weighted": 0.684671118554694
          },
          {
            "accuracy": 0.666438668490652,
            "f1": 0.4560511947186467,
            "f1_weighted": 0.707176183133126
          },
          {
            "accuracy": 0.6570907432740538,
            "f1": 0.4449895614722476,
            "f1_weighted": 0.7001201348855155
          },
          {
            "accuracy": 0.6409028727770177,
            "f1": 0.4216405684302638,
            "f1_weighted": 0.683392789555251
          },
          {
            "accuracy": 0.6434108527131783,
            "f1": 0.44650287578584624,
            "f1_weighted": 0.6860107278515445
          },
          {
            "accuracy": 0.6778385772913816,
            "f1": 0.46209428392744956,
            "f1_weighted": 0.7190981603294678
          },
          {
            "accuracy": 0.6561787505699954,
            "f1": 0.4468003053918573,
            "f1_weighted": 0.6982280468633363
          },
          {
            "accuracy": 0.6513907888736891,
            "f1": 0.4300960743659966,
            "f1_weighted": 0.6961564341512143
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.6556152125279642,
        "f1": 0.4361458176638022,
        "f1_weighted": 0.7000401929689171,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.6556152125279642,
        "scores_per_experiment": [
          {
            "accuracy": 0.6425055928411633,
            "f1": 0.42516930961530414,
            "f1_weighted": 0.683978501301295
          },
          {
            "accuracy": 0.6420581655480985,
            "f1": 0.4237701064105455,
            "f1_weighted": 0.6832111014068053
          },
          {
            "accuracy": 0.6514541387024608,
            "f1": 0.42649168885602734,
            "f1_weighted": 0.6987315137585992
          },
          {
            "accuracy": 0.6747203579418345,
            "f1": 0.44397345455294057,
            "f1_weighted": 0.7183229667263135
          },
          {
            "accuracy": 0.6769574944071588,
            "f1": 0.4662967872844803,
            "f1_weighted": 0.7237576544283624
          },
          {
            "accuracy": 0.6541387024608502,
            "f1": 0.43100447121676505,
            "f1_weighted": 0.6963467774956895
          },
          {
            "accuracy": 0.6366890380313199,
            "f1": 0.43356738978046766,
            "f1_weighted": 0.6832261992609554
          },
          {
            "accuracy": 0.6782997762863535,
            "f1": 0.4566892527916161,
            "f1_weighted": 0.7221194965959818
          },
          {
            "accuracy": 0.6389261744966444,
            "f1": 0.4249410769848929,
            "f1_weighted": 0.6857274731660823
          },
          {
            "accuracy": 0.6604026845637584,
            "f1": 0.4295546391449825,
            "f1_weighted": 0.7049802455490869
          }
        ]
      }
    ]
  },
  "task_name": "MTOPIntentClassification"
}