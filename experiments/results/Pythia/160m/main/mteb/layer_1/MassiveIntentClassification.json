{
  "dataset_revision": "4672e20407010da34463acc759c162ca9734bca6",
  "evaluation_time": 71.6123833656311,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.5501008742434432,
        "f1": 0.5239787544204538,
        "f1_weighted": 0.5527970562227289,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5501008742434432,
        "scores_per_experiment": [
          {
            "accuracy": 0.5423671822461331,
            "f1": 0.5251222923652452,
            "f1_weighted": 0.5429809258550368
          },
          {
            "accuracy": 0.5706119704102219,
            "f1": 0.5423426425529276,
            "f1_weighted": 0.5750700801499486
          },
          {
            "accuracy": 0.5406859448554135,
            "f1": 0.5103409267464725,
            "f1_weighted": 0.5375167787066049
          },
          {
            "accuracy": 0.5605245460659045,
            "f1": 0.5222605753952538,
            "f1_weighted": 0.5623391324003795
          },
          {
            "accuracy": 0.5598520511096167,
            "f1": 0.5278356321193381,
            "f1_weighted": 0.5622081340444822
          },
          {
            "accuracy": 0.5443846671149967,
            "f1": 0.5235132246939668,
            "f1_weighted": 0.5496039354450559
          },
          {
            "accuracy": 0.5326160053799597,
            "f1": 0.523422625312837,
            "f1_weighted": 0.533230967250784
          },
          {
            "accuracy": 0.5571620712844654,
            "f1": 0.5216060355892763,
            "f1_weighted": 0.5643891456740602
          },
          {
            "accuracy": 0.5376597175521184,
            "f1": 0.5156368345951038,
            "f1_weighted": 0.5380331404083936
          },
          {
            "accuracy": 0.5551445864156019,
            "f1": 0.5277067548341164,
            "f1_weighted": 0.5625983222925444
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.5596163305459911,
        "f1": 0.5337588226752911,
        "f1_weighted": 0.5619545398656982,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5596163305459911,
        "scores_per_experiment": [
          {
            "accuracy": 0.5381210034431874,
            "f1": 0.5256080690392482,
            "f1_weighted": 0.5350931270598055
          },
          {
            "accuracy": 0.5799311362518446,
            "f1": 0.5419298023860946,
            "f1_weighted": 0.5824555879251203
          },
          {
            "accuracy": 0.5750122970978849,
            "f1": 0.5452506017895871,
            "f1_weighted": 0.5745810355634867
          },
          {
            "accuracy": 0.5671421544515495,
            "f1": 0.5254188054725654,
            "f1_weighted": 0.5690978417875152
          },
          {
            "accuracy": 0.5759960649286768,
            "f1": 0.5521687789298636,
            "f1_weighted": 0.5775522116826848
          },
          {
            "accuracy": 0.5459911460895229,
            "f1": 0.5279244877348022,
            "f1_weighted": 0.5510856877797482
          },
          {
            "accuracy": 0.529758976881456,
            "f1": 0.5212639984770041,
            "f1_weighted": 0.5313501483226865
          },
          {
            "accuracy": 0.5533694048204624,
            "f1": 0.5168717900722709,
            "f1_weighted": 0.5569132198493046
          },
          {
            "accuracy": 0.5528775209050664,
            "f1": 0.5380166965813706,
            "f1_weighted": 0.5579285079720891
          },
          {
            "accuracy": 0.5779636005902608,
            "f1": 0.5431351962701049,
            "f1_weighted": 0.583488030714541
          }
        ]
      }
    ]
  },
  "task_name": "MassiveIntentClassification"
}