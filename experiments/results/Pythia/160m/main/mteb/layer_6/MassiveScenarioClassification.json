{
  "dataset_revision": "fad2c6e8459f9e1c45d9315f4953d921437d70f8",
  "evaluation_time": 41.90041899681091,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.5838937457969066,
        "f1": 0.5669606613720816,
        "f1_weighted": 0.5870002628827584,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5838937457969066,
        "scores_per_experiment": [
          {
            "accuracy": 0.5901143241425689,
            "f1": 0.5757715331116036,
            "f1_weighted": 0.5943236450550601
          },
          {
            "accuracy": 0.6240753194351042,
            "f1": 0.6083298193413031,
            "f1_weighted": 0.6268651446190465
          },
          {
            "accuracy": 0.5877605917955615,
            "f1": 0.5790543261464589,
            "f1_weighted": 0.5908044678762466
          },
          {
            "accuracy": 0.5622057834566241,
            "f1": 0.5469804610868018,
            "f1_weighted": 0.5699421278550173
          },
          {
            "accuracy": 0.6119704102219233,
            "f1": 0.5742716530007493,
            "f1_weighted": 0.6095140107752874
          },
          {
            "accuracy": 0.5598520511096167,
            "f1": 0.5401144563022178,
            "f1_weighted": 0.5589771752613243
          },
          {
            "accuracy": 0.574310692669805,
            "f1": 0.5576074742526912,
            "f1_weighted": 0.5804153876358262
          },
          {
            "accuracy": 0.6042367182246133,
            "f1": 0.5864904264176791,
            "f1_weighted": 0.6077411264310035
          },
          {
            "accuracy": 0.5719569603227975,
            "f1": 0.5577158733005229,
            "f1_weighted": 0.5748653740944185
          },
          {
            "accuracy": 0.5524546065904505,
            "f1": 0.5432705907607879,
            "f1_weighted": 0.5565541692243533
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.5774225282833252,
        "f1": 0.5681638435203092,
        "f1_weighted": 0.5792432017052074,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5774225282833252,
        "scores_per_experiment": [
          {
            "accuracy": 0.5927201180521396,
            "f1": 0.5902335530933837,
            "f1_weighted": 0.5945285535201108
          },
          {
            "accuracy": 0.6064928676832267,
            "f1": 0.5998129742665981,
            "f1_weighted": 0.6088159713408824
          },
          {
            "accuracy": 0.5750122970978849,
            "f1": 0.5743614981084932,
            "f1_weighted": 0.576752866269471
          },
          {
            "accuracy": 0.5494343334972946,
            "f1": 0.5425598983997983,
            "f1_weighted": 0.5573949508309718
          },
          {
            "accuracy": 0.5996064928676832,
            "f1": 0.5687635674736908,
            "f1_weighted": 0.5948660534135612
          },
          {
            "accuracy": 0.559272011805214,
            "f1": 0.5480431054705437,
            "f1_weighted": 0.5543457234504843
          },
          {
            "accuracy": 0.5523856369896705,
            "f1": 0.5489334197685869,
            "f1_weighted": 0.5601590363153333
          },
          {
            "accuracy": 0.5961633054599115,
            "f1": 0.5801589129604957,
            "f1_weighted": 0.5985676557519694
          },
          {
            "accuracy": 0.5676340383669454,
            "f1": 0.5635561105051068,
            "f1_weighted": 0.5668289814372146
          },
          {
            "accuracy": 0.5755041810132808,
            "f1": 0.5652153951563963,
            "f1_weighted": 0.5801722247220751
          }
        ]
      }
    ]
  },
  "task_name": "MassiveScenarioClassification"
}