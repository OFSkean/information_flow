{
  "dataset_revision": "d66bd1f72af766a5cc4b0ca5e00c162f89e8cc46",
  "evaluation_time": 21.682581663131714,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "cosine_accuracy": 0.9905841584158416,
        "cosine_accuracy_threshold": 0.9605544805526733,
        "cosine_ap": 0.27148025659557135,
        "cosine_f1": 0.3390964863357501,
        "cosine_f1_threshold": 0.9476162791252136,
        "cosine_precision": 0.3833543505674653,
        "cosine_recall": 0.304,
        "dot_accuracy": 0.9901287128712871,
        "dot_accuracy_threshold": 20825.197265625,
        "dot_ap": 0.0676767337609773,
        "dot_f1": 0.12903225806451613,
        "dot_f1_threshold": 19964.90625,
        "dot_precision": 0.15602836879432624,
        "dot_recall": 0.11,
        "euclidean_accuracy": 0.9904257425742574,
        "euclidean_accuracy_threshold": 39.312225341796875,
        "euclidean_ap": 0.2523059933363687,
        "euclidean_f1": 0.3284552845528455,
        "euclidean_f1_threshold": 45.87149429321289,
        "euclidean_precision": 0.3585798816568047,
        "euclidean_recall": 0.303,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.27148025659557135,
        "manhattan_accuracy": 0.9904257425742574,
        "manhattan_accuracy_threshold": 860.5086059570312,
        "manhattan_ap": 0.25949829916889416,
        "manhattan_f1": 0.3357597816196542,
        "manhattan_f1_threshold": 1034.605224609375,
        "manhattan_precision": 0.3080133555926544,
        "manhattan_recall": 0.369,
        "max_accuracy": 0.9905841584158416,
        "max_ap": 0.27148025659557135,
        "max_f1": 0.3390964863357501,
        "max_precision": 0.3833543505674653,
        "max_recall": 0.369,
        "similarity_accuracy": 0.9905841584158416,
        "similarity_accuracy_threshold": 0.9605544805526733,
        "similarity_ap": 0.27148025659557135,
        "similarity_f1": 0.3390964863357501,
        "similarity_f1_threshold": 0.9476162791252136,
        "similarity_precision": 0.3833543505674653,
        "similarity_recall": 0.304
      }
    ],
    "validation": [
      {
        "cosine_accuracy": 0.9903366336633663,
        "cosine_accuracy_threshold": 0.9677197933197021,
        "cosine_ap": 0.21506482885063463,
        "cosine_f1": 0.30251256281407035,
        "cosine_f1_threshold": 0.9422897100448608,
        "cosine_precision": 0.30404040404040406,
        "cosine_recall": 0.301,
        "dot_accuracy": 0.9901188118811881,
        "dot_accuracy_threshold": 21235.484375,
        "dot_ap": 0.045116778385069764,
        "dot_f1": 0.09439166411277965,
        "dot_f1_threshold": 19532.74609375,
        "dot_precision": 0.06805125939019001,
        "dot_recall": 0.154,
        "euclidean_accuracy": 0.9903069306930693,
        "euclidean_accuracy_threshold": 35.50366973876953,
        "euclidean_ap": 0.1989842472897035,
        "euclidean_f1": 0.28278114792347175,
        "euclidean_f1_threshold": 48.23155212402344,
        "euclidean_precision": 0.2650918635170604,
        "euclidean_recall": 0.303,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.21506482885063463,
        "manhattan_accuracy": 0.9902970297029703,
        "manhattan_accuracy_threshold": 781.8743896484375,
        "manhattan_ap": 0.20300193224291757,
        "manhattan_f1": 0.2855851784907365,
        "manhattan_f1_threshold": 1061.81005859375,
        "manhattan_precision": 0.2605111294311624,
        "manhattan_recall": 0.316,
        "max_accuracy": 0.9903366336633663,
        "max_ap": 0.21506482885063463,
        "max_f1": 0.30251256281407035,
        "max_precision": 0.30404040404040406,
        "max_recall": 0.316,
        "similarity_accuracy": 0.9903366336633663,
        "similarity_accuracy_threshold": 0.9677197933197021,
        "similarity_ap": 0.21506482885063463,
        "similarity_f1": 0.30251256281407035,
        "similarity_f1_threshold": 0.9422897100448608,
        "similarity_precision": 0.30404040404040406,
        "similarity_recall": 0.301
      }
    ]
  },
  "task_name": "SprintDuplicateQuestions"
}