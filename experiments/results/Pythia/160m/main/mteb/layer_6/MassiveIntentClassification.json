{
  "dataset_revision": "4672e20407010da34463acc759c162ca9734bca6",
  "evaluation_time": 81.98447465896606,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.5472091459314055,
        "f1": 0.5159620441569485,
        "f1_weighted": 0.5509033822063653,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5472091459314055,
        "scores_per_experiment": [
          {
            "accuracy": 0.550773369199731,
            "f1": 0.5198785082847869,
            "f1_weighted": 0.5555278115247241
          },
          {
            "accuracy": 0.5554808338937458,
            "f1": 0.5206269782569382,
            "f1_weighted": 0.5580434418181835
          },
          {
            "accuracy": 0.5497646267652992,
            "f1": 0.5181854519807692,
            "f1_weighted": 0.5529042558556841
          },
          {
            "accuracy": 0.5571620712844654,
            "f1": 0.519303666403228,
            "f1_weighted": 0.5645906155116656
          },
          {
            "accuracy": 0.5581708137188971,
            "f1": 0.5194305713463768,
            "f1_weighted": 0.5598872642369693
          },
          {
            "accuracy": 0.5255548083389374,
            "f1": 0.5094803104898085,
            "f1_weighted": 0.5248136361940874
          },
          {
            "accuracy": 0.5346334902488231,
            "f1": 0.5096650916078975,
            "f1_weighted": 0.5356876854616058
          },
          {
            "accuracy": 0.550773369199731,
            "f1": 0.5106022014901018,
            "f1_weighted": 0.5585904126109623
          },
          {
            "accuracy": 0.5332885003362475,
            "f1": 0.5130824349704362,
            "f1_weighted": 0.5372611878908639
          },
          {
            "accuracy": 0.5564895763281775,
            "f1": 0.5193652267391416,
            "f1_weighted": 0.5617275109589063
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.5634038366945401,
        "f1": 0.5369821417399849,
        "f1_weighted": 0.5673438410413076,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5634038366945401,
        "scores_per_experiment": [
          {
            "accuracy": 0.558288243974422,
            "f1": 0.5338633893531668,
            "f1_weighted": 0.5615360833833932
          },
          {
            "accuracy": 0.5828824397442204,
            "f1": 0.5449124614044832,
            "f1_weighted": 0.5882998597607287
          },
          {
            "accuracy": 0.5902606984751598,
            "f1": 0.5600217310701985,
            "f1_weighted": 0.5934910619241002
          },
          {
            "accuracy": 0.5661583866207575,
            "f1": 0.5268500526527746,
            "f1_weighted": 0.5732956846383537
          },
          {
            "accuracy": 0.5784554845056566,
            "f1": 0.5495527408333275,
            "f1_weighted": 0.5803323169242424
          },
          {
            "accuracy": 0.5430398425971471,
            "f1": 0.534710929760079,
            "f1_weighted": 0.5420845415885982
          },
          {
            "accuracy": 0.528775209050664,
            "f1": 0.5184686543029525,
            "f1_weighted": 0.5303346899170771
          },
          {
            "accuracy": 0.5504181013280866,
            "f1": 0.5243575023101869,
            "f1_weighted": 0.5576048122111269
          },
          {
            "accuracy": 0.5430398425971471,
            "f1": 0.5216717906209761,
            "f1_weighted": 0.5500730202477132
          },
          {
            "accuracy": 0.5927201180521396,
            "f1": 0.5554121650917035,
            "f1_weighted": 0.5963863398177428
          }
        ]
      }
    ]
  },
  "task_name": "MassiveIntentClassification"
}