{
  "dataset_revision": "ae001d0e6b1228650b7bd1c2c65fb50ad11a8aba",
  "evaluation_time": 117.88904404640198,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.608390332877337,
        "f1": 0.396184529337606,
        "f1_weighted": 0.656715275650302,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.608390332877337,
        "scores_per_experiment": [
          {
            "accuracy": 0.595531235750114,
            "f1": 0.38643550007058475,
            "f1_weighted": 0.6407718703544154
          },
          {
            "accuracy": 0.615595075239398,
            "f1": 0.39472026100856383,
            "f1_weighted": 0.6668638079328515
          },
          {
            "accuracy": 0.6181030551755586,
            "f1": 0.4106734793079747,
            "f1_weighted": 0.6642196666057615
          },
          {
            "accuracy": 0.6258549931600548,
            "f1": 0.41571025134987927,
            "f1_weighted": 0.6713500079991516
          },
          {
            "accuracy": 0.6114911080711354,
            "f1": 0.3995330117328813,
            "f1_weighted": 0.6621349367399104
          },
          {
            "accuracy": 0.5889192886456909,
            "f1": 0.3879122008847923,
            "f1_weighted": 0.640775338598379
          },
          {
            "accuracy": 0.5953032375740994,
            "f1": 0.3937869433615409,
            "f1_weighted": 0.6408015057239714
          },
          {
            "accuracy": 0.6199270405836753,
            "f1": 0.40417998885232637,
            "f1_weighted": 0.6667575564721836
          },
          {
            "accuracy": 0.6010031919744642,
            "f1": 0.38359210002258015,
            "f1_weighted": 0.6511468006885038
          },
          {
            "accuracy": 0.6121751025991792,
            "f1": 0.3853015567849366,
            "f1_weighted": 0.6623312653878911
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.6099776286353468,
        "f1": 0.3794335750305482,
        "f1_weighted": 0.6610925942118651,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.6099776286353468,
        "scores_per_experiment": [
          {
            "accuracy": 0.5870246085011186,
            "f1": 0.3627618179008496,
            "f1_weighted": 0.6402989926095569
          },
          {
            "accuracy": 0.6223713646532438,
            "f1": 0.39729075594756696,
            "f1_weighted": 0.675225846171278
          },
          {
            "accuracy": 0.6241610738255033,
            "f1": 0.3831900722848613,
            "f1_weighted": 0.6774005804622409
          },
          {
            "accuracy": 0.6353467561521253,
            "f1": 0.386793846661328,
            "f1_weighted": 0.6859208053251786
          },
          {
            "accuracy": 0.6116331096196868,
            "f1": 0.3884642130067583,
            "f1_weighted": 0.6583770530193958
          },
          {
            "accuracy": 0.5749440715883669,
            "f1": 0.35733704244686376,
            "f1_weighted": 0.6292308599558403
          },
          {
            "accuracy": 0.6008948545861298,
            "f1": 0.3876035130457576,
            "f1_weighted": 0.652744397717596
          },
          {
            "accuracy": 0.6210290827740492,
            "f1": 0.3915800577523918,
            "f1_weighted": 0.6687705227605415
          },
          {
            "accuracy": 0.6089485458612975,
            "f1": 0.36044543877908447,
            "f1_weighted": 0.6580407631979539
          },
          {
            "accuracy": 0.6134228187919463,
            "f1": 0.3788689924800205,
            "f1_weighted": 0.6649161208990696
          }
        ]
      }
    ]
  },
  "task_name": "MTOPIntentClassification"
}