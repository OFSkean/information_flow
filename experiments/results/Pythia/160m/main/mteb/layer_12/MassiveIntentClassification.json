{
  "dataset_revision": "4672e20407010da34463acc759c162ca9734bca6",
  "evaluation_time": 83.25861048698425,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.5012777404169468,
        "f1": 0.46937449397838105,
        "f1_weighted": 0.51021787993523,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5012777404169468,
        "scores_per_experiment": [
          {
            "accuracy": 0.5026899798251513,
            "f1": 0.48384846840002055,
            "f1_weighted": 0.5125598910646368
          },
          {
            "accuracy": 0.5299260255548084,
            "f1": 0.48455000463039866,
            "f1_weighted": 0.53617901944532
          },
          {
            "accuracy": 0.4751176866173504,
            "f1": 0.44684365538880727,
            "f1_weighted": 0.48557864703516695
          },
          {
            "accuracy": 0.5184936112979153,
            "f1": 0.4802465805703309,
            "f1_weighted": 0.5294378148039443
          },
          {
            "accuracy": 0.503698722259583,
            "f1": 0.45982142357319206,
            "f1_weighted": 0.5094442750750204
          },
          {
            "accuracy": 0.4754539340954943,
            "f1": 0.4561955707696373,
            "f1_weighted": 0.48031220840890465
          },
          {
            "accuracy": 0.5,
            "f1": 0.4840422248681227,
            "f1_weighted": 0.5088338112594478
          },
          {
            "accuracy": 0.4949562878278413,
            "f1": 0.46383597848966346,
            "f1_weighted": 0.5096387484238758
          },
          {
            "accuracy": 0.5097511768661735,
            "f1": 0.4667421280730545,
            "f1_weighted": 0.5140843653863796
          },
          {
            "accuracy": 0.5026899798251513,
            "f1": 0.467618905020583,
            "f1_weighted": 0.5161100184496037
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.5083620265617315,
        "f1": 0.4758267954591113,
        "f1_weighted": 0.5160727172847647,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5083620265617315,
        "scores_per_experiment": [
          {
            "accuracy": 0.49483521888834237,
            "f1": 0.46976169869607637,
            "f1_weighted": 0.498949738282015
          },
          {
            "accuracy": 0.5327102803738317,
            "f1": 0.48741243206233364,
            "f1_weighted": 0.5395685351706354
          },
          {
            "accuracy": 0.514018691588785,
            "f1": 0.4773346564770904,
            "f1_weighted": 0.5262747924334452
          },
          {
            "accuracy": 0.5248401377274963,
            "f1": 0.487218393460095,
            "f1_weighted": 0.5348074330768313
          },
          {
            "accuracy": 0.499754058042302,
            "f1": 0.46707404362357535,
            "f1_weighted": 0.503182601361844
          },
          {
            "accuracy": 0.48450565666502704,
            "f1": 0.4599916725977701,
            "f1_weighted": 0.49007986665863146
          },
          {
            "accuracy": 0.4825381210034432,
            "f1": 0.46936118598985405,
            "f1_weighted": 0.4887973050353599
          },
          {
            "accuracy": 0.5130349237579931,
            "f1": 0.47207391364479495,
            "f1_weighted": 0.5231966835754633
          },
          {
            "accuracy": 0.5154943433349729,
            "f1": 0.48962291565415744,
            "f1_weighted": 0.5217462069712265
          },
          {
            "accuracy": 0.5218888342351206,
            "f1": 0.47841704238536564,
            "f1_weighted": 0.5341240102821947
          }
        ]
      }
    ]
  },
  "task_name": "MassiveIntentClassification"
}