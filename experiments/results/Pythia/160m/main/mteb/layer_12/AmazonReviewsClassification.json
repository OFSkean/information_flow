{
  "dataset_revision": "1399c76144fd37290681b995c656ef9b2e06e26d",
  "evaluation_time": 146.81103372573853,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.32633999999999996,
        "f1": 0.32768314223355777,
        "f1_weighted": 0.3276831422335577,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.32633999999999996,
        "scores_per_experiment": [
          {
            "accuracy": 0.3494,
            "f1": 0.35145775570912413,
            "f1_weighted": 0.3514577557091241
          },
          {
            "accuracy": 0.3184,
            "f1": 0.3180198088262942,
            "f1_weighted": 0.3180198088262942
          },
          {
            "accuracy": 0.2994,
            "f1": 0.3009847302202001,
            "f1_weighted": 0.3009847302202001
          },
          {
            "accuracy": 0.3226,
            "f1": 0.32811523493442174,
            "f1_weighted": 0.32811523493442163
          },
          {
            "accuracy": 0.3626,
            "f1": 0.3593033671571854,
            "f1_weighted": 0.3593033671571854
          },
          {
            "accuracy": 0.3192,
            "f1": 0.31810690653623447,
            "f1_weighted": 0.31810690653623447
          },
          {
            "accuracy": 0.293,
            "f1": 0.2916011319338834,
            "f1_weighted": 0.2916011319338833
          },
          {
            "accuracy": 0.3562,
            "f1": 0.3565577434667113,
            "f1_weighted": 0.3565577434667113
          },
          {
            "accuracy": 0.326,
            "f1": 0.327295280853196,
            "f1_weighted": 0.3272952808531959
          },
          {
            "accuracy": 0.3166,
            "f1": 0.32538946269832686,
            "f1_weighted": 0.3253894626983268
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.32624,
        "f1": 0.3273124818284786,
        "f1_weighted": 0.3273124818284786,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.32624,
        "scores_per_experiment": [
          {
            "accuracy": 0.3386,
            "f1": 0.3402960858101225,
            "f1_weighted": 0.34029608581012244
          },
          {
            "accuracy": 0.3198,
            "f1": 0.319247110833731,
            "f1_weighted": 0.3192471108337309
          },
          {
            "accuracy": 0.3048,
            "f1": 0.3068667061506739,
            "f1_weighted": 0.30686670615067396
          },
          {
            "accuracy": 0.3124,
            "f1": 0.31679117743290963,
            "f1_weighted": 0.3167911774329096
          },
          {
            "accuracy": 0.3622,
            "f1": 0.3570588236571283,
            "f1_weighted": 0.3570588236571283
          },
          {
            "accuracy": 0.326,
            "f1": 0.32300157810093555,
            "f1_weighted": 0.32300157810093555
          },
          {
            "accuracy": 0.2968,
            "f1": 0.2974807522537051,
            "f1_weighted": 0.2974807522537051
          },
          {
            "accuracy": 0.3546,
            "f1": 0.35665134618459077,
            "f1_weighted": 0.35665134618459077
          },
          {
            "accuracy": 0.3316,
            "f1": 0.33316832827746945,
            "f1_weighted": 0.3331683282774695
          },
          {
            "accuracy": 0.3156,
            "f1": 0.3225629095835202,
            "f1_weighted": 0.32256290958352013
          }
        ]
      }
    ]
  },
  "task_name": "AmazonReviewsClassification"
}