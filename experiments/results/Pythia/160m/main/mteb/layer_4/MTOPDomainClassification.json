{
  "dataset_revision": "d80d48c1eb48d3562165c59d59d0034df9fff0bf",
  "evaluation_time": 42.22705674171448,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.7943000455996352,
        "f1": 0.7879621310147906,
        "f1_weighted": 0.7952773915741788,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.7943000455996352,
        "scores_per_experiment": [
          {
            "accuracy": 0.7489740082079344,
            "f1": 0.7438131767329601,
            "f1_weighted": 0.74803566746698
          },
          {
            "accuracy": 0.8046055631554948,
            "f1": 0.7992940884565514,
            "f1_weighted": 0.8081336645231703
          },
          {
            "accuracy": 0.8071135430916553,
            "f1": 0.7948941063214657,
            "f1_weighted": 0.8092800509704579
          },
          {
            "accuracy": 0.808937528499772,
            "f1": 0.799651830712699,
            "f1_weighted": 0.8102690984780093
          },
          {
            "accuracy": 0.8123575011399908,
            "f1": 0.8067211223719302,
            "f1_weighted": 0.813950866755459
          },
          {
            "accuracy": 0.8018695850433196,
            "f1": 0.7977099790115371,
            "f1_weighted": 0.8027395469827957
          },
          {
            "accuracy": 0.7772457820337437,
            "f1": 0.77134940553003,
            "f1_weighted": 0.7735388467165543
          },
          {
            "accuracy": 0.7724578203374373,
            "f1": 0.7704974087625319,
            "f1_weighted": 0.7767937604695996
          },
          {
            "accuracy": 0.8025535795713634,
            "f1": 0.7992584761761375,
            "f1_weighted": 0.8034003625292803
          },
          {
            "accuracy": 0.8068855449156407,
            "f1": 0.7964317160720633,
            "f1_weighted": 0.8066320508494815
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.7857718120805368,
        "f1": 0.7832456431962468,
        "f1_weighted": 0.7863949306202339,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.7857718120805368,
        "scores_per_experiment": [
          {
            "accuracy": 0.7494407158836689,
            "f1": 0.7496983513822845,
            "f1_weighted": 0.7474396612504027
          },
          {
            "accuracy": 0.7941834451901566,
            "f1": 0.7932487582281506,
            "f1_weighted": 0.7973975859548161
          },
          {
            "accuracy": 0.7950782997762863,
            "f1": 0.7866823174155908,
            "f1_weighted": 0.7969261309671883
          },
          {
            "accuracy": 0.8058165548098434,
            "f1": 0.8028278272369866,
            "f1_weighted": 0.8078063408714338
          },
          {
            "accuracy": 0.8049217002237137,
            "f1": 0.8066891870193927,
            "f1_weighted": 0.8053450121833681
          },
          {
            "accuracy": 0.7865771812080536,
            "f1": 0.7859603216130734,
            "f1_weighted": 0.7866620307074521
          },
          {
            "accuracy": 0.7704697986577181,
            "f1": 0.7639198880370621,
            "f1_weighted": 0.7678157628544692
          },
          {
            "accuracy": 0.7628635346756152,
            "f1": 0.7641588996015262,
            "f1_weighted": 0.7664693895431336
          },
          {
            "accuracy": 0.7955257270693512,
            "f1": 0.7927090347436354,
            "f1_weighted": 0.7960645314171154
          },
          {
            "accuracy": 0.792841163310962,
            "f1": 0.7865618466847661,
            "f1_weighted": 0.79202286045296
          }
        ]
      }
    ]
  },
  "task_name": "MTOPDomainClassification"
}