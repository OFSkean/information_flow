{
  "dataset_revision": "d66bd1f72af766a5cc4b0ca5e00c162f89e8cc46",
  "evaluation_time": 16.356919765472412,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "cosine_accuracy": 0.9923465346534653,
        "cosine_accuracy_threshold": 0.9191771149635315,
        "cosine_ap": 0.5052043957907689,
        "cosine_f1": 0.5138211382113821,
        "cosine_f1_threshold": 0.9051278829574585,
        "cosine_precision": 0.5609467455621302,
        "cosine_recall": 0.474,
        "dot_accuracy": 0.9908811881188119,
        "dot_accuracy_threshold": 10757.0166015625,
        "dot_ap": 0.31089946527783924,
        "dot_f1": 0.35283363802559414,
        "dot_f1_threshold": 10063.845703125,
        "dot_precision": 0.32491582491582494,
        "dot_recall": 0.386,
        "euclidean_accuracy": 0.9921089108910891,
        "euclidean_accuracy_threshold": 40.30738067626953,
        "euclidean_ap": 0.46099343119598074,
        "euclidean_f1": 0.49461077844311374,
        "euclidean_f1_threshold": 43.642242431640625,
        "euclidean_precision": 0.6164179104477612,
        "euclidean_recall": 0.413,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5052043957907689,
        "manhattan_accuracy": 0.9924059405940594,
        "manhattan_accuracy_threshold": 841.0315551757812,
        "manhattan_ap": 0.4995381794895676,
        "manhattan_f1": 0.516548463356974,
        "manhattan_f1_threshold": 949.8804931640625,
        "manhattan_precision": 0.6315028901734104,
        "manhattan_recall": 0.437,
        "max_accuracy": 0.9924059405940594,
        "max_ap": 0.5052043957907689,
        "max_f1": 0.516548463356974,
        "max_precision": 0.6315028901734104,
        "max_recall": 0.474,
        "similarity_accuracy": 0.9923465346534653,
        "similarity_accuracy_threshold": 0.9191771149635315,
        "similarity_ap": 0.5052043957907689,
        "similarity_f1": 0.5138211382113821,
        "similarity_f1_threshold": 0.9051278829574585,
        "similarity_precision": 0.5609467455621302,
        "similarity_recall": 0.474
      }
    ],
    "validation": [
      {
        "cosine_accuracy": 0.9918118811881188,
        "cosine_accuracy_threshold": 0.9215741157531738,
        "cosine_ap": 0.430529677952704,
        "cosine_f1": 0.47249190938511326,
        "cosine_f1_threshold": 0.9038717746734619,
        "cosine_precision": 0.5128805620608899,
        "cosine_recall": 0.438,
        "dot_accuracy": 0.9904851485148515,
        "dot_accuracy_threshold": 10684.140625,
        "dot_ap": 0.2817548405568539,
        "dot_f1": 0.34679802955665023,
        "dot_f1_threshold": 10065.013671875,
        "dot_precision": 0.341747572815534,
        "dot_recall": 0.352,
        "euclidean_accuracy": 0.9916633663366337,
        "euclidean_accuracy_threshold": 39.73961639404297,
        "euclidean_ap": 0.38857292111131714,
        "euclidean_f1": 0.4371859296482412,
        "euclidean_f1_threshold": 46.054542541503906,
        "euclidean_precision": 0.4393939393939394,
        "euclidean_recall": 0.435,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.430529677952704,
        "manhattan_accuracy": 0.9918316831683168,
        "manhattan_accuracy_threshold": 898.287109375,
        "manhattan_ap": 0.4239114309352909,
        "manhattan_f1": 0.4663726571113561,
        "manhattan_f1_threshold": 975.756591796875,
        "manhattan_precision": 0.5196560196560197,
        "manhattan_recall": 0.423,
        "max_accuracy": 0.9918316831683168,
        "max_ap": 0.430529677952704,
        "max_f1": 0.47249190938511326,
        "max_precision": 0.5196560196560197,
        "max_recall": 0.438,
        "similarity_accuracy": 0.9918118811881188,
        "similarity_accuracy_threshold": 0.9215741157531738,
        "similarity_ap": 0.430529677952704,
        "similarity_f1": 0.47249190938511326,
        "similarity_f1_threshold": 0.9038717746734619,
        "similarity_precision": 0.5128805620608899,
        "similarity_recall": 0.438
      }
    ]
  },
  "task_name": "SprintDuplicateQuestions"
}