{
  "dataset_revision": "fad2c6e8459f9e1c45d9315f4953d921437d70f8",
  "evaluation_time": 35.889081716537476,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.5964694014794889,
        "f1": 0.5816282882538815,
        "f1_weighted": 0.5975552215895034,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5964694014794889,
        "scores_per_experiment": [
          {
            "accuracy": 0.621049092131809,
            "f1": 0.6056939176210742,
            "f1_weighted": 0.6255468513607235
          },
          {
            "accuracy": 0.6119704102219233,
            "f1": 0.5971196713467585,
            "f1_weighted": 0.6162105888995666
          },
          {
            "accuracy": 0.5746469401479489,
            "f1": 0.565732372926768,
            "f1_weighted": 0.5738874271458018
          },
          {
            "accuracy": 0.5796906523201076,
            "f1": 0.5626488713404622,
            "f1_weighted": 0.5793500467283816
          },
          {
            "accuracy": 0.5988567585743106,
            "f1": 0.5748149921323937,
            "f1_weighted": 0.5973159792662833
          },
          {
            "accuracy": 0.5917955615332885,
            "f1": 0.5720567285662933,
            "f1_weighted": 0.5963870178974604
          },
          {
            "accuracy": 0.5928043039677202,
            "f1": 0.5760595044019104,
            "f1_weighted": 0.5957700318518822
          },
          {
            "accuracy": 0.6160053799596503,
            "f1": 0.6007745191570892,
            "f1_weighted": 0.6144994356436756
          },
          {
            "accuracy": 0.5857431069266981,
            "f1": 0.5762533763705876,
            "f1_weighted": 0.5835724728735767
          },
          {
            "accuracy": 0.5921318090114324,
            "f1": 0.5851289286754777,
            "f1_weighted": 0.5930123642276822
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.5852926709296604,
        "f1": 0.5776593580569893,
        "f1_weighted": 0.5838597899058702,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5852926709296604,
        "scores_per_experiment": [
          {
            "accuracy": 0.6114117068371864,
            "f1": 0.6028443814257908,
            "f1_weighted": 0.6134296567396659
          },
          {
            "accuracy": 0.5902606984751598,
            "f1": 0.5906555810142395,
            "f1_weighted": 0.5923718788938145
          },
          {
            "accuracy": 0.5681259222823414,
            "f1": 0.5606823265352175,
            "f1_weighted": 0.5682303590896414
          },
          {
            "accuracy": 0.5646827348745695,
            "f1": 0.552802405704647,
            "f1_weighted": 0.5596596075046223
          },
          {
            "accuracy": 0.5823905558288244,
            "f1": 0.5705937528714407,
            "f1_weighted": 0.5775475740725293
          },
          {
            "accuracy": 0.5799311362518446,
            "f1": 0.5758189686188934,
            "f1_weighted": 0.5796821879564257
          },
          {
            "accuracy": 0.5700934579439252,
            "f1": 0.5617844292168297,
            "f1_weighted": 0.5750477085719072
          },
          {
            "accuracy": 0.6060009837678307,
            "f1": 0.5894127113052234,
            "f1_weighted": 0.5982410958809481
          },
          {
            "accuracy": 0.5843580914904083,
            "f1": 0.5823160246167238,
            "f1_weighted": 0.5798918831999639
          },
          {
            "accuracy": 0.5956714215445155,
            "f1": 0.5896829992608884,
            "f1_weighted": 0.5944959471491843
          }
        ]
      }
    ]
  },
  "task_name": "MassiveScenarioClassification"
}