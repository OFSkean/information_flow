{
  "dataset_revision": "ae001d0e6b1228650b7bd1c2c65fb50ad11a8aba",
  "evaluation_time": 81.38697695732117,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.6505015959872321,
        "f1": 0.45469270724459826,
        "f1_weighted": 0.6942861431270096,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.6505015959872321,
        "scores_per_experiment": [
          {
            "accuracy": 0.6502507979936161,
            "f1": 0.45101893379241864,
            "f1_weighted": 0.6900952542770211
          },
          {
            "accuracy": 0.6479708162334701,
            "f1": 0.4451649730915174,
            "f1_weighted": 0.6908908728740029
          },
          {
            "accuracy": 0.6361149110807114,
            "f1": 0.44998485645892095,
            "f1_weighted": 0.6807905653650205
          },
          {
            "accuracy": 0.6582307341541268,
            "f1": 0.46339019110303103,
            "f1_weighted": 0.6996833532726531
          },
          {
            "accuracy": 0.6436388508891929,
            "f1": 0.4457791307289608,
            "f1_weighted": 0.6910286922708633
          },
          {
            "accuracy": 0.6395348837209303,
            "f1": 0.45900746770655293,
            "f1_weighted": 0.6849842152010892
          },
          {
            "accuracy": 0.6466028271773826,
            "f1": 0.4604679642114198,
            "f1_weighted": 0.6921571836868726
          },
          {
            "accuracy": 0.6885544915640675,
            "f1": 0.4887690231809482,
            "f1_weighted": 0.7285556240546338
          },
          {
            "accuracy": 0.6438668490652075,
            "f1": 0.44286698560168913,
            "f1_weighted": 0.6894330350853239
          },
          {
            "accuracy": 0.6502507979936161,
            "f1": 0.44047754657052457,
            "f1_weighted": 0.6952426351826154
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.6519910514541387,
        "f1": 0.44033689100215057,
        "f1_weighted": 0.6953451973253318,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.6519910514541387,
        "scores_per_experiment": [
          {
            "accuracy": 0.6510067114093959,
            "f1": 0.4413406715032276,
            "f1_weighted": 0.6896419441104622
          },
          {
            "accuracy": 0.6447427293064877,
            "f1": 0.44153437168547033,
            "f1_weighted": 0.6870005380000989
          },
          {
            "accuracy": 0.6447427293064877,
            "f1": 0.4185446139310987,
            "f1_weighted": 0.6896697844225133
          },
          {
            "accuracy": 0.654586129753915,
            "f1": 0.44290805121383825,
            "f1_weighted": 0.6948392637406805
          },
          {
            "accuracy": 0.665324384787472,
            "f1": 0.4549901481826412,
            "f1_weighted": 0.7133587665053415
          },
          {
            "accuracy": 0.640268456375839,
            "f1": 0.4304457167062773,
            "f1_weighted": 0.6831453053049594
          },
          {
            "accuracy": 0.6460850111856823,
            "f1": 0.4394522074847376,
            "f1_weighted": 0.6913793362957381
          },
          {
            "accuracy": 0.6711409395973155,
            "f1": 0.4534288780737206,
            "f1_weighted": 0.7137959680309907
          },
          {
            "accuracy": 0.647427293064877,
            "f1": 0.44094894780103644,
            "f1_weighted": 0.6940647234784991
          },
          {
            "accuracy": 0.654586129753915,
            "f1": 0.439775303439458,
            "f1_weighted": 0.6965563433640345
          }
        ]
      }
    ]
  },
  "task_name": "MTOPIntentClassification"
}