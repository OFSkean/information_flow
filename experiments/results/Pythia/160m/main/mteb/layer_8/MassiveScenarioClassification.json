{
  "dataset_revision": "fad2c6e8459f9e1c45d9315f4953d921437d70f8",
  "evaluation_time": 44.16717529296875,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.5957296570275722,
        "f1": 0.5794116652331681,
        "f1_weighted": 0.599471443250927,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5957296570275722,
        "scores_per_experiment": [
          {
            "accuracy": 0.6055817081371889,
            "f1": 0.5904102625721479,
            "f1_weighted": 0.6111268049811114
          },
          {
            "accuracy": 0.6223940820443846,
            "f1": 0.6063453965068809,
            "f1_weighted": 0.6251231951341952
          },
          {
            "accuracy": 0.5894418291862811,
            "f1": 0.5902750575358319,
            "f1_weighted": 0.5952343301895692
          },
          {
            "accuracy": 0.5998655010087425,
            "f1": 0.5803710677501086,
            "f1_weighted": 0.6037495939487043
          },
          {
            "accuracy": 0.6200403496973773,
            "f1": 0.5845986875116894,
            "f1_weighted": 0.6194924029714242
          },
          {
            "accuracy": 0.566577000672495,
            "f1": 0.5489031505752333,
            "f1_weighted": 0.5664563335250905
          },
          {
            "accuracy": 0.5803631472763954,
            "f1": 0.5657507048456034,
            "f1_weighted": 0.5849206116892953
          },
          {
            "accuracy": 0.6055817081371889,
            "f1": 0.5903819032715689,
            "f1_weighted": 0.6102096303166454
          },
          {
            "accuracy": 0.5988567585743106,
            "f1": 0.580755891382671,
            "f1_weighted": 0.6032846514150368
          },
          {
            "accuracy": 0.5685944855413584,
            "f1": 0.5563245303799459,
            "f1_weighted": 0.5751168783381985
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.5878504672897196,
        "f1": 0.5776015660844449,
        "f1_weighted": 0.5909491627134754,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5878504672897196,
        "scores_per_experiment": [
          {
            "accuracy": 0.5961633054599115,
            "f1": 0.5874310415815611,
            "f1_weighted": 0.6009748164644425
          },
          {
            "accuracy": 0.6035415641908509,
            "f1": 0.5901171171342212,
            "f1_weighted": 0.6061061517463244
          },
          {
            "accuracy": 0.5912444663059518,
            "f1": 0.5932328064149498,
            "f1_weighted": 0.5950674164808444
          },
          {
            "accuracy": 0.5843580914904083,
            "f1": 0.5722716678372258,
            "f1_weighted": 0.5890648127313646
          },
          {
            "accuracy": 0.6114117068371864,
            "f1": 0.5879185429996205,
            "f1_weighted": 0.608386145947466
          },
          {
            "accuracy": 0.5504181013280866,
            "f1": 0.5430593916997888,
            "f1_weighted": 0.5467769231128391
          },
          {
            "accuracy": 0.5641908509591737,
            "f1": 0.5568772929593151,
            "f1_weighted": 0.5712159304882946
          },
          {
            "accuracy": 0.5986227250368913,
            "f1": 0.5879438643916104,
            "f1_weighted": 0.6020048655771517
          },
          {
            "accuracy": 0.58780127889818,
            "f1": 0.5782441581635427,
            "f1_weighted": 0.5918265058214609
          },
          {
            "accuracy": 0.5907525823905558,
            "f1": 0.5789197776626132,
            "f1_weighted": 0.5980680587645659
          }
        ]
      }
    ]
  },
  "task_name": "MassiveScenarioClassification"
}