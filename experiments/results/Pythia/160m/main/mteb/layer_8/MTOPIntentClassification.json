{
  "dataset_revision": "ae001d0e6b1228650b7bd1c2c65fb50ad11a8aba",
  "evaluation_time": 112.62641406059265,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.6664614683082536,
        "f1": 0.4483421210931887,
        "f1_weighted": 0.7079033910576629,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.6664614683082536,
        "scores_per_experiment": [
          {
            "accuracy": 0.6600547195622435,
            "f1": 0.44070577058692934,
            "f1_weighted": 0.6985500871437541
          },
          {
            "accuracy": 0.663702690378477,
            "f1": 0.4461731316118258,
            "f1_weighted": 0.7052413152838469
          },
          {
            "accuracy": 0.6696306429548564,
            "f1": 0.44445742690917767,
            "f1_weighted": 0.7114049752771219
          },
          {
            "accuracy": 0.6659826721386229,
            "f1": 0.4523389833791861,
            "f1_weighted": 0.709724735005387
          },
          {
            "accuracy": 0.6671226630186958,
            "f1": 0.45073317777792155,
            "f1_weighted": 0.7111282637537432
          },
          {
            "accuracy": 0.6796625626994984,
            "f1": 0.46193066775744396,
            "f1_weighted": 0.7208002830363107
          },
          {
            "accuracy": 0.6529867761057911,
            "f1": 0.45182906141092827,
            "f1_weighted": 0.6943777460074158
          },
          {
            "accuracy": 0.6874145006839946,
            "f1": 0.4729375741069937,
            "f1_weighted": 0.7268636776414678
          },
          {
            "accuracy": 0.6623347013223895,
            "f1": 0.43395673451045125,
            "f1_weighted": 0.7006337500991396
          },
          {
            "accuracy": 0.6557227542179662,
            "f1": 0.42835868288102935,
            "f1_weighted": 0.7003090773284422
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.663937360178971,
        "f1": 0.42508900922201553,
        "f1_weighted": 0.7090038585952617,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.663937360178971,
        "scores_per_experiment": [
          {
            "accuracy": 0.6478747203579418,
            "f1": 0.40395695256066555,
            "f1_weighted": 0.6917680822298926
          },
          {
            "accuracy": 0.6662192393736018,
            "f1": 0.4339820969547,
            "f1_weighted": 0.7116000923216357
          },
          {
            "accuracy": 0.6635346756152125,
            "f1": 0.42030250605955605,
            "f1_weighted": 0.7085435651479475
          },
          {
            "accuracy": 0.6680089485458613,
            "f1": 0.43019983821332297,
            "f1_weighted": 0.7187480941591325
          },
          {
            "accuracy": 0.6626398210290828,
            "f1": 0.44349088814601045,
            "f1_weighted": 0.709336437395899
          },
          {
            "accuracy": 0.6675615212527964,
            "f1": 0.42402164145690624,
            "f1_weighted": 0.7123979201344839
          },
          {
            "accuracy": 0.6527964205816554,
            "f1": 0.41838374373551823,
            "f1_weighted": 0.694790057757497
          },
          {
            "accuracy": 0.680089485458613,
            "f1": 0.42630039313395135,
            "f1_weighted": 0.7260640960922257
          },
          {
            "accuracy": 0.6662192393736018,
            "f1": 0.4185654081916669,
            "f1_weighted": 0.7082132957971167
          },
          {
            "accuracy": 0.6644295302013423,
            "f1": 0.43168662376785705,
            "f1_weighted": 0.7085769449167874
          }
        ]
      }
    ]
  },
  "task_name": "MTOPIntentClassification"
}