{
  "dataset_revision": "d80d48c1eb48d3562165c59d59d0034df9fff0bf",
  "evaluation_time": 48.59435558319092,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.8014591883264934,
        "f1": 0.7971069101343566,
        "f1_weighted": 0.8026596140172693,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.8014591883264934,
        "scores_per_experiment": [
          {
            "accuracy": 0.7640218878248974,
            "f1": 0.7604299607692511,
            "f1_weighted": 0.7640905108915972
          },
          {
            "accuracy": 0.8264933880528956,
            "f1": 0.8225855337702139,
            "f1_weighted": 0.8290918431506703
          },
          {
            "accuracy": 0.8050615595075239,
            "f1": 0.7937997459477347,
            "f1_weighted": 0.806620521665645
          },
          {
            "accuracy": 0.8219334245326038,
            "f1": 0.8128982365658974,
            "f1_weighted": 0.8235561891659754
          },
          {
            "accuracy": 0.8185134518923849,
            "f1": 0.814160348239276,
            "f1_weighted": 0.8193884300174205
          },
          {
            "accuracy": 0.8055175558595531,
            "f1": 0.8033546505784055,
            "f1_weighted": 0.805678702655621
          },
          {
            "accuracy": 0.7906976744186046,
            "f1": 0.7867750447812877,
            "f1_weighted": 0.7878850739150386
          },
          {
            "accuracy": 0.7822617419060648,
            "f1": 0.7809673531663442,
            "f1_weighted": 0.7873903375107812
          },
          {
            "accuracy": 0.8080255357957137,
            "f1": 0.8103133780611492,
            "f1_weighted": 0.8093732164577842
          },
          {
            "accuracy": 0.7920656634746922,
            "f1": 0.785784849464006,
            "f1_weighted": 0.7935213147421593
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.799463087248322,
        "f1": 0.7985243873345971,
        "f1_weighted": 0.800193070363094,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.799463087248322,
        "scores_per_experiment": [
          {
            "accuracy": 0.7731543624161074,
            "f1": 0.7781415800827785,
            "f1_weighted": 0.7727094442407682
          },
          {
            "accuracy": 0.8299776286353467,
            "f1": 0.8280243705895707,
            "f1_weighted": 0.8318244334881058
          },
          {
            "accuracy": 0.8017897091722596,
            "f1": 0.7927953133784853,
            "f1_weighted": 0.8024966111018373
          },
          {
            "accuracy": 0.8246085011185682,
            "f1": 0.8214566476696565,
            "f1_weighted": 0.8260831557185293
          },
          {
            "accuracy": 0.814317673378076,
            "f1": 0.8178627163141257,
            "f1_weighted": 0.8159305433863355
          },
          {
            "accuracy": 0.7968680089485458,
            "f1": 0.7965966794244536,
            "f1_weighted": 0.7970851200342282
          },
          {
            "accuracy": 0.7901565995525727,
            "f1": 0.7844236909711497,
            "f1_weighted": 0.7878293976088508
          },
          {
            "accuracy": 0.7821029082774049,
            "f1": 0.7845225427129193,
            "f1_weighted": 0.7862397721751607
          },
          {
            "accuracy": 0.8,
            "f1": 0.8014604837594383,
            "f1_weighted": 0.8003842809195411
          },
          {
            "accuracy": 0.78165548098434,
            "f1": 0.779959848443393,
            "f1_weighted": 0.7813479449575822
          }
        ]
      }
    ]
  },
  "task_name": "MTOPDomainClassification"
}