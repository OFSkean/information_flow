{
  "dataset_revision": "fad2c6e8459f9e1c45d9315f4953d921437d70f8",
  "evaluation_time": 42.622071981430054,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.5801613987895091,
        "f1": 0.5630461396988393,
        "f1_weighted": 0.5838329833712069,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5801613987895091,
        "scores_per_experiment": [
          {
            "accuracy": 0.5988567585743106,
            "f1": 0.5814218827614615,
            "f1_weighted": 0.6043201436423414
          },
          {
            "accuracy": 0.6099529253530599,
            "f1": 0.5930194141981582,
            "f1_weighted": 0.6115896108106783
          },
          {
            "accuracy": 0.5709482178883658,
            "f1": 0.5660679152143625,
            "f1_weighted": 0.5746332841370329
          },
          {
            "accuracy": 0.582044384667115,
            "f1": 0.5705880206529046,
            "f1_weighted": 0.58721422992749
          },
          {
            "accuracy": 0.5995292535305985,
            "f1": 0.5589925706344498,
            "f1_weighted": 0.5993334740086095
          },
          {
            "accuracy": 0.5521183591123067,
            "f1": 0.5390859789626901,
            "f1_weighted": 0.5511226744865132
          },
          {
            "accuracy": 0.5675857431069267,
            "f1": 0.5506676814940357,
            "f1_weighted": 0.5749238445582211
          },
          {
            "accuracy": 0.5944855413584398,
            "f1": 0.5737584582220578,
            "f1_weighted": 0.5997111015821629
          },
          {
            "accuracy": 0.5706119704102219,
            "f1": 0.555370426506323,
            "f1_weighted": 0.5746617048045404
          },
          {
            "accuracy": 0.5554808338937458,
            "f1": 0.5414890483419509,
            "f1_weighted": 0.5608197657544796
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.5745696015740285,
        "f1": 0.5655658067911251,
        "f1_weighted": 0.5771165425103983,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5745696015740285,
        "scores_per_experiment": [
          {
            "accuracy": 0.5892769306443679,
            "f1": 0.5819840836714149,
            "f1_weighted": 0.5925958714976698
          },
          {
            "accuracy": 0.5907525823905558,
            "f1": 0.5870213054126517,
            "f1_weighted": 0.5900101288449746
          },
          {
            "accuracy": 0.559272011805214,
            "f1": 0.5626162441906932,
            "f1_weighted": 0.5622833520917933
          },
          {
            "accuracy": 0.5696015740285293,
            "f1": 0.5638076239051116,
            "f1_weighted": 0.5756677293479162
          },
          {
            "accuracy": 0.5917363502213477,
            "f1": 0.5654022713601533,
            "f1_weighted": 0.589621957474409
          },
          {
            "accuracy": 0.544515494343335,
            "f1": 0.5350666197203138,
            "f1_weighted": 0.5401691237843262
          },
          {
            "accuracy": 0.5523856369896705,
            "f1": 0.5436087626206476,
            "f1_weighted": 0.5614784564121672
          },
          {
            "accuracy": 0.5907525823905558,
            "f1": 0.5765042802949372,
            "f1_weighted": 0.594700442196376
          },
          {
            "accuracy": 0.5725528775209051,
            "f1": 0.5667542387228995,
            "f1_weighted": 0.5746567834794333
          },
          {
            "accuracy": 0.5848499754058042,
            "f1": 0.5728926380124273,
            "f1_weighted": 0.5899815799749168
          }
        ]
      }
    ]
  },
  "task_name": "MassiveScenarioClassification"
}