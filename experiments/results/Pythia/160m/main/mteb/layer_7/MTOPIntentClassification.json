{
  "dataset_revision": "ae001d0e6b1228650b7bd1c2c65fb50ad11a8aba",
  "evaluation_time": 110.69359278678894,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.6673050615595074,
        "f1": 0.4531242160680732,
        "f1_weighted": 0.7076453108844635,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.6673050615595074,
        "scores_per_experiment": [
          {
            "accuracy": 0.6584587323301414,
            "f1": 0.4486671837784712,
            "f1_weighted": 0.6954814624369152
          },
          {
            "accuracy": 0.6630186958504332,
            "f1": 0.4419347581124067,
            "f1_weighted": 0.7060735443647068
          },
          {
            "accuracy": 0.6593707250341997,
            "f1": 0.44763136431910716,
            "f1_weighted": 0.6983959012882263
          },
          {
            "accuracy": 0.6700866393068855,
            "f1": 0.4557149310842887,
            "f1_weighted": 0.7108560481950047
          },
          {
            "accuracy": 0.6659826721386229,
            "f1": 0.46185510393489493,
            "f1_weighted": 0.7074912697369043
          },
          {
            "accuracy": 0.673734610123119,
            "f1": 0.45280142975705173,
            "f1_weighted": 0.7165492452751838
          },
          {
            "accuracy": 0.6568627450980392,
            "f1": 0.4607367059591543,
            "f1_weighted": 0.6957643621763993
          },
          {
            "accuracy": 0.6894664842681258,
            "f1": 0.47034113298122904,
            "f1_weighted": 0.7280311075008746
          },
          {
            "accuracy": 0.666438668490652,
            "f1": 0.4442608578572575,
            "f1_weighted": 0.7059590084200229
          },
          {
            "accuracy": 0.6696306429548564,
            "f1": 0.44729869289687046,
            "f1_weighted": 0.7118511594503969
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.6667561521252796,
        "f1": 0.4301388109841081,
        "f1_weighted": 0.7102942540394473,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.6667561521252796,
        "scores_per_experiment": [
          {
            "accuracy": 0.654586129753915,
            "f1": 0.41397626588514674,
            "f1_weighted": 0.6942519137942422
          },
          {
            "accuracy": 0.6684563758389261,
            "f1": 0.42431563864196337,
            "f1_weighted": 0.7137314986742778
          },
          {
            "accuracy": 0.6630872483221476,
            "f1": 0.4253761804871686,
            "f1_weighted": 0.70913965510226
          },
          {
            "accuracy": 0.6684563758389261,
            "f1": 0.4353365180515619,
            "f1_weighted": 0.7152781753563985
          },
          {
            "accuracy": 0.6626398210290828,
            "f1": 0.43316718017454076,
            "f1_weighted": 0.7070379004866271
          },
          {
            "accuracy": 0.665324384787472,
            "f1": 0.43749536737942063,
            "f1_weighted": 0.7105428672653046
          },
          {
            "accuracy": 0.6536912751677852,
            "f1": 0.44101166965328037,
            "f1_weighted": 0.6957914003352031
          },
          {
            "accuracy": 0.6832214765100671,
            "f1": 0.4282150226908098,
            "f1_weighted": 0.7253155088007088
          },
          {
            "accuracy": 0.672930648769575,
            "f1": 0.4335086787624812,
            "f1_weighted": 0.7118335819836843
          },
          {
            "accuracy": 0.6751677852348993,
            "f1": 0.4289855881147084,
            "f1_weighted": 0.7200200385957675
          }
        ]
      }
    ]
  },
  "task_name": "MTOPIntentClassification"
}