{
  "dataset_revision": "ae001d0e6b1228650b7bd1c2c65fb50ad11a8aba",
  "evaluation_time": 102.43560123443604,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.6772457820337437,
        "f1": 0.46341289123892554,
        "f1_weighted": 0.7170976627409844,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.6772457820337437,
        "scores_per_experiment": [
          {
            "accuracy": 0.6707706338349293,
            "f1": 0.466894243490867,
            "f1_weighted": 0.7075931932143212
          },
          {
            "accuracy": 0.6698586411308709,
            "f1": 0.43985637835169594,
            "f1_weighted": 0.711040586297654
          },
          {
            "accuracy": 0.6709986320109439,
            "f1": 0.46076137440162007,
            "f1_weighted": 0.7098015542480492
          },
          {
            "accuracy": 0.6796625626994984,
            "f1": 0.4758690903405355,
            "f1_weighted": 0.7210141236711789
          },
          {
            "accuracy": 0.6762425900592796,
            "f1": 0.47874842374685983,
            "f1_weighted": 0.7170023960999026
          },
          {
            "accuracy": 0.6771545827633378,
            "f1": 0.46162323544676304,
            "f1_weighted": 0.7183027271050451
          },
          {
            "accuracy": 0.6714546283629731,
            "f1": 0.4674726999156502,
            "f1_weighted": 0.7106467841131702
          },
          {
            "accuracy": 0.6876424988600092,
            "f1": 0.47083138018781734,
            "f1_weighted": 0.7297474193715929
          },
          {
            "accuracy": 0.6766985864113088,
            "f1": 0.4542563485586104,
            "f1_weighted": 0.7162960724439694
          },
          {
            "accuracy": 0.6919744642042863,
            "f1": 0.45781573794883507,
            "f1_weighted": 0.7295317708449589
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.6759284116331096,
        "f1": 0.44979219441060775,
        "f1_weighted": 0.7171981228322855,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.6759284116331096,
        "scores_per_experiment": [
          {
            "accuracy": 0.6590604026845638,
            "f1": 0.4274967449688114,
            "f1_weighted": 0.700251796062362
          },
          {
            "accuracy": 0.6760626398210291,
            "f1": 0.458580147629021,
            "f1_weighted": 0.7144138651934558
          },
          {
            "accuracy": 0.6671140939597315,
            "f1": 0.42597334475210974,
            "f1_weighted": 0.7125763878319936
          },
          {
            "accuracy": 0.6774049217002237,
            "f1": 0.45310778891774894,
            "f1_weighted": 0.7207490266072604
          },
          {
            "accuracy": 0.6827740492170022,
            "f1": 0.4682916747876221,
            "f1_weighted": 0.7242836396826612
          },
          {
            "accuracy": 0.6666666666666666,
            "f1": 0.44023084439015064,
            "f1_weighted": 0.707867068548346
          },
          {
            "accuracy": 0.6671140939597315,
            "f1": 0.4546302409964374,
            "f1_weighted": 0.7094145110250815
          },
          {
            "accuracy": 0.6899328859060403,
            "f1": 0.464544176109358,
            "f1_weighted": 0.732445392594104
          },
          {
            "accuracy": 0.6724832214765101,
            "f1": 0.43690489742421545,
            "f1_weighted": 0.7103769003730419
          },
          {
            "accuracy": 0.7006711409395974,
            "f1": 0.4681620841306034,
            "f1_weighted": 0.7396026404045476
          }
        ]
      }
    ]
  },
  "task_name": "MTOPIntentClassification"
}