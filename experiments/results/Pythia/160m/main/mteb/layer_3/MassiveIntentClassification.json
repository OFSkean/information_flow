{
  "dataset_revision": "4672e20407010da34463acc759c162ca9734bca6",
  "evaluation_time": 73.2833023071289,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.5593813046402152,
        "f1": 0.5322850776698507,
        "f1_weighted": 0.5609289489929737,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5593813046402152,
        "scores_per_experiment": [
          {
            "accuracy": 0.5611970410221924,
            "f1": 0.538954240597507,
            "f1_weighted": 0.5627689772243811
          },
          {
            "accuracy": 0.5830531271015468,
            "f1": 0.5508318612469055,
            "f1_weighted": 0.5837404738678574
          },
          {
            "accuracy": 0.5544720914593141,
            "f1": 0.5247719222055788,
            "f1_weighted": 0.5536904226167735
          },
          {
            "accuracy": 0.5830531271015468,
            "f1": 0.5385722497110684,
            "f1_weighted": 0.5855699361787722
          },
          {
            "accuracy": 0.5655682582380632,
            "f1": 0.5357244192543476,
            "f1_weighted": 0.5660739841790922
          },
          {
            "accuracy": 0.5379959650302623,
            "f1": 0.5257071974563864,
            "f1_weighted": 0.5387883193847239
          },
          {
            "accuracy": 0.5464021519838601,
            "f1": 0.5311967187771015,
            "f1_weighted": 0.5490723577846353
          },
          {
            "accuracy": 0.5558170813718897,
            "f1": 0.5214573899747609,
            "f1_weighted": 0.5592971790938116
          },
          {
            "accuracy": 0.543039677202421,
            "f1": 0.5220826292428241,
            "f1_weighted": 0.5449566646131376
          },
          {
            "accuracy": 0.5632145258910558,
            "f1": 0.5335521482320258,
            "f1_weighted": 0.5653311749865516
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.5733890801770782,
        "f1": 0.5446554960998147,
        "f1_weighted": 0.5764396326249817,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5733890801770782,
        "scores_per_experiment": [
          {
            "accuracy": 0.5622233152975897,
            "f1": 0.5477729227133471,
            "f1_weighted": 0.5669343136316113
          },
          {
            "accuracy": 0.6005902606984752,
            "f1": 0.5583553495977944,
            "f1_weighted": 0.6026174230613637
          },
          {
            "accuracy": 0.5897688145597639,
            "f1": 0.5473042104729495,
            "f1_weighted": 0.5900443803221727
          },
          {
            "accuracy": 0.5755041810132808,
            "f1": 0.5360666100263134,
            "f1_weighted": 0.5779202674392669
          },
          {
            "accuracy": 0.5814067879980325,
            "f1": 0.5547994258148642,
            "f1_weighted": 0.5830517746345616
          },
          {
            "accuracy": 0.5622233152975897,
            "f1": 0.5515338755971348,
            "f1_weighted": 0.5678499773253112
          },
          {
            "accuracy": 0.5425479586817511,
            "f1": 0.5214020680901517,
            "f1_weighted": 0.5445227449801971
          },
          {
            "accuracy": 0.5641908509591737,
            "f1": 0.5282038487495649,
            "f1_weighted": 0.5663206916233128
          },
          {
            "accuracy": 0.5607476635514018,
            "f1": 0.5402580422307075,
            "f1_weighted": 0.5657478220361504
          },
          {
            "accuracy": 0.5946876537137236,
            "f1": 0.5608586077053197,
            "f1_weighted": 0.599386931195869
          }
        ]
      }
    ]
  },
  "task_name": "MassiveIntentClassification"
}