{
  "dataset_revision": "fad2c6e8459f9e1c45d9315f4953d921437d70f8",
  "evaluation_time": 39.068588972091675,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.5962676529926025,
        "f1": 0.5772798122876017,
        "f1_weighted": 0.5983964529759378,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5962676529926025,
        "scores_per_experiment": [
          {
            "accuracy": 0.6139878950907868,
            "f1": 0.5967635447395243,
            "f1_weighted": 0.618129222157162
          },
          {
            "accuracy": 0.6260928043039677,
            "f1": 0.6083919596230394,
            "f1_weighted": 0.6281049088865621
          },
          {
            "accuracy": 0.5813718897108272,
            "f1": 0.5707659283966667,
            "f1_weighted": 0.5849026316006718
          },
          {
            "accuracy": 0.5971755211835911,
            "f1": 0.5809114397155892,
            "f1_weighted": 0.6037939059039612
          },
          {
            "accuracy": 0.6116341627437795,
            "f1": 0.5826372922175943,
            "f1_weighted": 0.609799820413415
          },
          {
            "accuracy": 0.5803631472763954,
            "f1": 0.5568660859024627,
            "f1_weighted": 0.5816623024565531
          },
          {
            "accuracy": 0.5981842636180229,
            "f1": 0.5766191077523374,
            "f1_weighted": 0.6028921234208394
          },
          {
            "accuracy": 0.6260928043039677,
            "f1": 0.6025940097385903,
            "f1_weighted": 0.6263874523921542
          },
          {
            "accuracy": 0.5776731674512441,
            "f1": 0.5583283608119274,
            "f1_weighted": 0.5769208262936623
          },
          {
            "accuracy": 0.5501008742434432,
            "f1": 0.5389203939782864,
            "f1_weighted": 0.551371336234396
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.5864240039350713,
        "f1": 0.5749625660959048,
        "f1_weighted": 0.5872736425822128,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5864240039350713,
        "scores_per_experiment": [
          {
            "accuracy": 0.6060009837678307,
            "f1": 0.5959147531367717,
            "f1_weighted": 0.6082442926252266
          },
          {
            "accuracy": 0.6089522872602066,
            "f1": 0.6029639698458078,
            "f1_weighted": 0.6081322737416897
          },
          {
            "accuracy": 0.5715691096901131,
            "f1": 0.5637427538875944,
            "f1_weighted": 0.5731843238032844
          },
          {
            "accuracy": 0.5715691096901131,
            "f1": 0.5596444099117659,
            "f1_weighted": 0.5791355443254473
          },
          {
            "accuracy": 0.6192818494835219,
            "f1": 0.600167632912227,
            "f1_weighted": 0.6161363814912519
          },
          {
            "accuracy": 0.5548450565666503,
            "f1": 0.5448342444790358,
            "f1_weighted": 0.5486670230472848
          },
          {
            "accuracy": 0.5720609936055091,
            "f1": 0.5640938427848465,
            "f1_weighted": 0.5809186895207675
          },
          {
            "accuracy": 0.6163305459911461,
            "f1": 0.5952071701395645,
            "f1_weighted": 0.6142704753570041
          },
          {
            "accuracy": 0.5764879488440728,
            "f1": 0.5694897882323321,
            "f1_weighted": 0.5738961759943185
          },
          {
            "accuracy": 0.5671421544515495,
            "f1": 0.5535670956291021,
            "f1_weighted": 0.5701512459158536
          }
        ]
      }
    ]
  },
  "task_name": "MassiveScenarioClassification"
}