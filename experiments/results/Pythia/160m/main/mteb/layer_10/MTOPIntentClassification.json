{
  "dataset_revision": "ae001d0e6b1228650b7bd1c2c65fb50ad11a8aba",
  "evaluation_time": 113.97646117210388,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.6471044231646147,
        "f1": 0.4245684629173462,
        "f1_weighted": 0.6909856192941763,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.6471044231646147,
        "scores_per_experiment": [
          {
            "accuracy": 0.6390788873689011,
            "f1": 0.41313828745189723,
            "f1_weighted": 0.6825659753716771
          },
          {
            "accuracy": 0.6536707706338349,
            "f1": 0.42596215931828113,
            "f1_weighted": 0.6967079592889844
          },
          {
            "accuracy": 0.6543547651618787,
            "f1": 0.42461001794921194,
            "f1_weighted": 0.6972782186378319
          },
          {
            "accuracy": 0.646374829001368,
            "f1": 0.4291157377332721,
            "f1_weighted": 0.6878268354677846
          },
          {
            "accuracy": 0.6422708618331053,
            "f1": 0.4213960145462518,
            "f1_weighted": 0.6876388447536881
          },
          {
            "accuracy": 0.6534427724578203,
            "f1": 0.4276813431021091,
            "f1_weighted": 0.7020560586026153
          },
          {
            "accuracy": 0.6411308709530323,
            "f1": 0.41870318347632884,
            "f1_weighted": 0.6848348916401951
          },
          {
            "accuracy": 0.6632466940264478,
            "f1": 0.44929072444610973,
            "f1_weighted": 0.7048058186022004
          },
          {
            "accuracy": 0.6397628818969449,
            "f1": 0.41851996701905775,
            "f1_weighted": 0.6837070768469914
          },
          {
            "accuracy": 0.6377108983128135,
            "f1": 0.41726719413094254,
            "f1_weighted": 0.6824345137297956
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.6450111856823266,
        "f1": 0.4063801840611764,
        "f1_weighted": 0.6914956415461713,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.6450111856823266,
        "scores_per_experiment": [
          {
            "accuracy": 0.6286353467561522,
            "f1": 0.4005466655956178,
            "f1_weighted": 0.6764190790599933
          },
          {
            "accuracy": 0.6612975391498881,
            "f1": 0.41814905867455754,
            "f1_weighted": 0.7077003808729065
          },
          {
            "accuracy": 0.6505592841163311,
            "f1": 0.40625833706457865,
            "f1_weighted": 0.6995151770647311
          },
          {
            "accuracy": 0.6384787472035794,
            "f1": 0.4015618122812002,
            "f1_weighted": 0.686229269378618
          },
          {
            "accuracy": 0.6451901565995526,
            "f1": 0.4279921182182994,
            "f1_weighted": 0.6876826755022385
          },
          {
            "accuracy": 0.6514541387024608,
            "f1": 0.4113049499262385,
            "f1_weighted": 0.699964645473252
          },
          {
            "accuracy": 0.6371364653243848,
            "f1": 0.3966068048390379,
            "f1_weighted": 0.6833006715190328
          },
          {
            "accuracy": 0.6510067114093959,
            "f1": 0.41320745981272733,
            "f1_weighted": 0.6994451085513405
          },
          {
            "accuracy": 0.6407158836689038,
            "f1": 0.39444670755597344,
            "f1_weighted": 0.6817800746340287
          },
          {
            "accuracy": 0.6456375838926175,
            "f1": 0.39372792664353395,
            "f1_weighted": 0.6929193334055712
          }
        ]
      }
    ]
  },
  "task_name": "MTOPIntentClassification"
}