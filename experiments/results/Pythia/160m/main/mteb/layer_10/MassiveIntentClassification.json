{
  "dataset_revision": "4672e20407010da34463acc759c162ca9734bca6",
  "evaluation_time": 80.38057398796082,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.5370208473436449,
        "f1": 0.5051095905771973,
        "f1_weighted": 0.5424755790798734,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5370208473436449,
        "scores_per_experiment": [
          {
            "accuracy": 0.5440484196368527,
            "f1": 0.5159159764357214,
            "f1_weighted": 0.5492867662161899
          },
          {
            "accuracy": 0.5561533288500337,
            "f1": 0.5211058006980799,
            "f1_weighted": 0.5620840345113957
          },
          {
            "accuracy": 0.5336247478143914,
            "f1": 0.5019274840271966,
            "f1_weighted": 0.5358747471504977
          },
          {
            "accuracy": 0.5561533288500337,
            "f1": 0.5134048491724781,
            "f1_weighted": 0.5640363569517675
          },
          {
            "accuracy": 0.5453934095494284,
            "f1": 0.5040259672826208,
            "f1_weighted": 0.5500863899221254
          },
          {
            "accuracy": 0.5084061869535978,
            "f1": 0.4914707430750884,
            "f1_weighted": 0.5112750889281279
          },
          {
            "accuracy": 0.519838601210491,
            "f1": 0.49741189678106096,
            "f1_weighted": 0.5261662934676123
          },
          {
            "accuracy": 0.5413584398117014,
            "f1": 0.5051116516174666,
            "f1_weighted": 0.54963643987084
          },
          {
            "accuracy": 0.5188298587760591,
            "f1": 0.4924686380424628,
            "f1_weighted": 0.5238026328002557
          },
          {
            "accuracy": 0.5464021519838601,
            "f1": 0.5082528986397972,
            "f1_weighted": 0.5525070409799215
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.5484013772749632,
        "f1": 0.5160898926941347,
        "f1_weighted": 0.5538838125416801,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5484013772749632,
        "scores_per_experiment": [
          {
            "accuracy": 0.5376291195277915,
            "f1": 0.5178125684148982,
            "f1_weighted": 0.5407625578978213
          },
          {
            "accuracy": 0.5499262174126907,
            "f1": 0.511791562598265,
            "f1_weighted": 0.5574753302719854
          },
          {
            "accuracy": 0.5710772257747172,
            "f1": 0.5335925243516585,
            "f1_weighted": 0.5783859604357493
          },
          {
            "accuracy": 0.5671421544515495,
            "f1": 0.5263711135454271,
            "f1_weighted": 0.5734335637323625
          },
          {
            "accuracy": 0.5602557796360059,
            "f1": 0.5222108138669069,
            "f1_weighted": 0.5637752103679635
          },
          {
            "accuracy": 0.528775209050664,
            "f1": 0.5087745919224288,
            "f1_weighted": 0.5297665281112837
          },
          {
            "accuracy": 0.5100836202656173,
            "f1": 0.4883772920803659,
            "f1_weighted": 0.5163630092973966
          },
          {
            "accuracy": 0.5484505656665027,
            "f1": 0.5106309998945857,
            "f1_weighted": 0.5550124541030981
          },
          {
            "accuracy": 0.5346778160354156,
            "f1": 0.5070455752345625,
            "f1_weighted": 0.5408263848830653
          },
          {
            "accuracy": 0.5759960649286768,
            "f1": 0.5342918850322483,
            "f1_weighted": 0.5830371263160753
          }
        ]
      }
    ]
  },
  "task_name": "MassiveIntentClassification"
}