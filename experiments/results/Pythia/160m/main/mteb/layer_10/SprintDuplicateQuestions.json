{
  "dataset_revision": "d66bd1f72af766a5cc4b0ca5e00c162f89e8cc46",
  "evaluation_time": 23.735335111618042,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "cosine_accuracy": 0.9906039603960396,
        "cosine_accuracy_threshold": 0.9679988622665405,
        "cosine_ap": 0.2860194326641907,
        "cosine_f1": 0.37228123419322207,
        "cosine_f1_threshold": 0.9540395140647888,
        "cosine_precision": 0.3766632548618219,
        "cosine_recall": 0.368,
        "dot_accuracy": 0.9901287128712871,
        "dot_accuracy_threshold": 24916.3203125,
        "dot_ap": 0.04974820421569473,
        "dot_f1": 0.10948509485094853,
        "dot_f1_threshold": 23701.4921875,
        "dot_precision": 0.11952662721893491,
        "dot_recall": 0.101,
        "euclidean_accuracy": 0.9904950495049505,
        "euclidean_accuracy_threshold": 38.1910514831543,
        "euclidean_ap": 0.2692670868857542,
        "euclidean_f1": 0.36270491803278687,
        "euclidean_f1_threshold": 46.386043548583984,
        "euclidean_precision": 0.37184873949579833,
        "euclidean_recall": 0.354,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.28683898556484033,
        "manhattan_accuracy": 0.9905544554455445,
        "manhattan_accuracy_threshold": 803.0848388671875,
        "manhattan_ap": 0.28683898556484033,
        "manhattan_f1": 0.37513171759747105,
        "manhattan_f1_threshold": 1002.667236328125,
        "manhattan_precision": 0.39643652561247217,
        "manhattan_recall": 0.356,
        "max_accuracy": 0.9906039603960396,
        "max_ap": 0.28683898556484033,
        "max_f1": 0.37513171759747105,
        "max_precision": 0.39643652561247217,
        "max_recall": 0.368,
        "similarity_accuracy": 0.9906039603960396,
        "similarity_accuracy_threshold": 0.9679988622665405,
        "similarity_ap": 0.2860194326641907,
        "similarity_f1": 0.37228123419322207,
        "similarity_f1_threshold": 0.9540395140647888,
        "similarity_precision": 0.3766632548618219,
        "similarity_recall": 0.368
      }
    ],
    "validation": [
      {
        "cosine_accuracy": 0.9904554455445544,
        "cosine_accuracy_threshold": 0.9710636734962463,
        "cosine_ap": 0.21529567458062307,
        "cosine_f1": 0.3049946865037194,
        "cosine_f1_threshold": 0.9526069164276123,
        "cosine_precision": 0.3253968253968254,
        "cosine_recall": 0.287,
        "dot_accuracy": 0.9901089108910891,
        "dot_accuracy_threshold": 25496.09375,
        "dot_ap": 0.04068243987814243,
        "dot_f1": 0.08857808857808858,
        "dot_f1_threshold": 23351.59375,
        "dot_precision": 0.06640039940089865,
        "dot_recall": 0.133,
        "euclidean_accuracy": 0.9904257425742574,
        "euclidean_accuracy_threshold": 36.851802825927734,
        "euclidean_ap": 0.20150615306025732,
        "euclidean_f1": 0.28507295173961844,
        "euclidean_f1_threshold": 46.795345306396484,
        "euclidean_precision": 0.3248081841432225,
        "euclidean_recall": 0.254,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.21529567458062307,
        "manhattan_accuracy": 0.9904554455445544,
        "manhattan_accuracy_threshold": 808.2242431640625,
        "manhattan_ap": 0.21284044014434786,
        "manhattan_f1": 0.2869905080960357,
        "manhattan_f1_threshold": 1016.8673095703125,
        "manhattan_precision": 0.324905183312263,
        "manhattan_recall": 0.257,
        "max_accuracy": 0.9904554455445544,
        "max_ap": 0.21529567458062307,
        "max_f1": 0.3049946865037194,
        "max_precision": 0.3253968253968254,
        "max_recall": 0.287,
        "similarity_accuracy": 0.9904554455445544,
        "similarity_accuracy_threshold": 0.9710636734962463,
        "similarity_ap": 0.21529567458062307,
        "similarity_f1": 0.3049946865037194,
        "similarity_f1_threshold": 0.9526069164276123,
        "similarity_precision": 0.3253968253968254,
        "similarity_recall": 0.287
      }
    ]
  },
  "task_name": "SprintDuplicateQuestions"
}