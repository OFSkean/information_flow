{
  "dataset_revision": "ae001d0e6b1228650b7bd1c2c65fb50ad11a8aba",
  "evaluation_time": 75.6750955581665,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.6379616963064294,
        "f1": 0.44279748496023325,
        "f1_weighted": 0.6812333621916263,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.6379616963064294,
        "scores_per_experiment": [
          {
            "accuracy": 0.6406748746010031,
            "f1": 0.43871700151769755,
            "f1_weighted": 0.6829415913382498
          },
          {
            "accuracy": 0.6274509803921569,
            "f1": 0.4280914346878765,
            "f1_weighted": 0.6712286536803935
          },
          {
            "accuracy": 0.624031007751938,
            "f1": 0.4354041589613085,
            "f1_weighted": 0.6676234737803717
          },
          {
            "accuracy": 0.6438668490652075,
            "f1": 0.44034979843170075,
            "f1_weighted": 0.6838235012961285
          },
          {
            "accuracy": 0.6329229366165071,
            "f1": 0.44889028676700604,
            "f1_weighted": 0.6809435022387281
          },
          {
            "accuracy": 0.6326949384404925,
            "f1": 0.43829026722216474,
            "f1_weighted": 0.6773566912997161
          },
          {
            "accuracy": 0.6429548563611491,
            "f1": 0.45096571219428977,
            "f1_weighted": 0.6853414703566513
          },
          {
            "accuracy": 0.6689466484268126,
            "f1": 0.4701202692248802,
            "f1_weighted": 0.7081581446618626
          },
          {
            "accuracy": 0.6272229822161423,
            "f1": 0.43152278995846605,
            "f1_weighted": 0.6715055949334013
          },
          {
            "accuracy": 0.6388508891928865,
            "f1": 0.44562313063694164,
            "f1_weighted": 0.683410998330761
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.640268456375839,
        "f1": 0.4235790252466046,
        "f1_weighted": 0.6848547384070923,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.640268456375839,
        "scores_per_experiment": [
          {
            "accuracy": 0.632662192393736,
            "f1": 0.42241202584295934,
            "f1_weighted": 0.6738432742191565
          },
          {
            "accuracy": 0.6322147651006711,
            "f1": 0.4167051206770785,
            "f1_weighted": 0.6748560283544136
          },
          {
            "accuracy": 0.6196868008948546,
            "f1": 0.4037761142519224,
            "f1_weighted": 0.6655080209805153
          },
          {
            "accuracy": 0.6281879194630873,
            "f1": 0.41859299049567433,
            "f1_weighted": 0.6741755061624799
          },
          {
            "accuracy": 0.6626398210290828,
            "f1": 0.4428121173156125,
            "f1_weighted": 0.7076701919546714
          },
          {
            "accuracy": 0.6469798657718121,
            "f1": 0.43470969289592226,
            "f1_weighted": 0.6912111060039784
          },
          {
            "accuracy": 0.6456375838926175,
            "f1": 0.4140013660555909,
            "f1_weighted": 0.6902038986775394
          },
          {
            "accuracy": 0.6657718120805369,
            "f1": 0.44656429239397855,
            "f1_weighted": 0.7061111846766024
          },
          {
            "accuracy": 0.6196868008948546,
            "f1": 0.4111330519196554,
            "f1_weighted": 0.6717368399628676
          },
          {
            "accuracy": 0.6492170022371365,
            "f1": 0.4250834806176514,
            "f1_weighted": 0.6932313330786988
          }
        ]
      }
    ]
  },
  "task_name": "MTOPIntentClassification"
}