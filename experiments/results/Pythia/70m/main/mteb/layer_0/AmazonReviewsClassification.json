{
  "dataset_revision": "1399c76144fd37290681b995c656ef9b2e06e26d",
  "evaluation_time": 47.741326570510864,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.30400000000000005,
        "f1": 0.3005304887004231,
        "f1_weighted": 0.3005304887004231,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.30400000000000005,
        "scores_per_experiment": [
          {
            "accuracy": 0.3298,
            "f1": 0.3303050410341476,
            "f1_weighted": 0.3303050410341476
          },
          {
            "accuracy": 0.3108,
            "f1": 0.3131974688296178,
            "f1_weighted": 0.3131974688296178
          },
          {
            "accuracy": 0.2966,
            "f1": 0.29666412788178764,
            "f1_weighted": 0.29666412788178764
          },
          {
            "accuracy": 0.2994,
            "f1": 0.2964310839152955,
            "f1_weighted": 0.2964310839152955
          },
          {
            "accuracy": 0.335,
            "f1": 0.32931673303073083,
            "f1_weighted": 0.32931673303073083
          },
          {
            "accuracy": 0.2612,
            "f1": 0.25923581857896727,
            "f1_weighted": 0.2592358185789673
          },
          {
            "accuracy": 0.2508,
            "f1": 0.24228681523265455,
            "f1_weighted": 0.24228681523265458
          },
          {
            "accuracy": 0.3274,
            "f1": 0.3194454352296343,
            "f1_weighted": 0.3194454352296343
          },
          {
            "accuracy": 0.3104,
            "f1": 0.30635480710320717,
            "f1_weighted": 0.30635480710320717
          },
          {
            "accuracy": 0.3186,
            "f1": 0.3120675561681881,
            "f1_weighted": 0.3120675561681881
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.29806,
        "f1": 0.29434753462829777,
        "f1_weighted": 0.29434753462829777,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.29806,
        "scores_per_experiment": [
          {
            "accuracy": 0.3134,
            "f1": 0.3152342843305562,
            "f1_weighted": 0.3152342843305562
          },
          {
            "accuracy": 0.3012,
            "f1": 0.30564494998510716,
            "f1_weighted": 0.30564494998510716
          },
          {
            "accuracy": 0.2926,
            "f1": 0.29289290845762617,
            "f1_weighted": 0.2928929084576261
          },
          {
            "accuracy": 0.2878,
            "f1": 0.28367415309229227,
            "f1_weighted": 0.28367415309229227
          },
          {
            "accuracy": 0.3352,
            "f1": 0.32611542254966497,
            "f1_weighted": 0.3261154225496649
          },
          {
            "accuracy": 0.267,
            "f1": 0.26435233830931176,
            "f1_weighted": 0.26435233830931176
          },
          {
            "accuracy": 0.2572,
            "f1": 0.24825010712254386,
            "f1_weighted": 0.24825010712254383
          },
          {
            "accuracy": 0.3088,
            "f1": 0.3002666842643696,
            "f1_weighted": 0.3002666842643695
          },
          {
            "accuracy": 0.284,
            "f1": 0.28083701822252916,
            "f1_weighted": 0.28083701822252916
          },
          {
            "accuracy": 0.3334,
            "f1": 0.3262074799489766,
            "f1_weighted": 0.32620747994897653
          }
        ]
      }
    ]
  },
  "task_name": "AmazonReviewsClassification"
}