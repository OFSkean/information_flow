{
  "dataset_revision": "1399c76144fd37290681b995c656ef9b2e06e26d",
  "evaluation_time": 76.00444602966309,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.3211,
        "f1": 0.32069412603415737,
        "f1_weighted": 0.32069412603415737,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.3211,
        "scores_per_experiment": [
          {
            "accuracy": 0.3496,
            "f1": 0.3448262422338539,
            "f1_weighted": 0.34482624223385394
          },
          {
            "accuracy": 0.331,
            "f1": 0.32898861281386627,
            "f1_weighted": 0.32898861281386627
          },
          {
            "accuracy": 0.3052,
            "f1": 0.30461297826718603,
            "f1_weighted": 0.30461297826718603
          },
          {
            "accuracy": 0.3112,
            "f1": 0.3118122230039745,
            "f1_weighted": 0.3118122230039745
          },
          {
            "accuracy": 0.3434,
            "f1": 0.3423938951358392,
            "f1_weighted": 0.3423938951358391
          },
          {
            "accuracy": 0.307,
            "f1": 0.30346706379886945,
            "f1_weighted": 0.30346706379886945
          },
          {
            "accuracy": 0.267,
            "f1": 0.26796500344023466,
            "f1_weighted": 0.2679650034402346
          },
          {
            "accuracy": 0.3366,
            "f1": 0.3359464077474977,
            "f1_weighted": 0.3359464077474976
          },
          {
            "accuracy": 0.3202,
            "f1": 0.3216167565754697,
            "f1_weighted": 0.32161675657546973
          },
          {
            "accuracy": 0.3398,
            "f1": 0.34531207732478303,
            "f1_weighted": 0.3453120773247831
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.32130000000000003,
        "f1": 0.3204891701734659,
        "f1_weighted": 0.3204891701734659,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.32130000000000003,
        "scores_per_experiment": [
          {
            "accuracy": 0.3324,
            "f1": 0.3289998252743998,
            "f1_weighted": 0.3289998252743998
          },
          {
            "accuracy": 0.3442,
            "f1": 0.3427737457796144,
            "f1_weighted": 0.3427737457796144
          },
          {
            "accuracy": 0.3194,
            "f1": 0.31874474649468076,
            "f1_weighted": 0.31874474649468076
          },
          {
            "accuracy": 0.3052,
            "f1": 0.3036211659856151,
            "f1_weighted": 0.3036211659856151
          },
          {
            "accuracy": 0.3376,
            "f1": 0.3359432894085539,
            "f1_weighted": 0.33594328940855384
          },
          {
            "accuracy": 0.3022,
            "f1": 0.29681507946594715,
            "f1_weighted": 0.29681507946594715
          },
          {
            "accuracy": 0.2748,
            "f1": 0.2750468563805014,
            "f1_weighted": 0.27504685638050147
          },
          {
            "accuracy": 0.3198,
            "f1": 0.31989531911582797,
            "f1_weighted": 0.31989531911582797
          },
          {
            "accuracy": 0.3318,
            "f1": 0.33316180869238965,
            "f1_weighted": 0.3331618086923896
          },
          {
            "accuracy": 0.3456,
            "f1": 0.3498898651371293,
            "f1_weighted": 0.3498898651371293
          }
        ]
      }
    ]
  },
  "task_name": "AmazonReviewsClassification"
}