{
  "dataset_revision": "ae001d0e6b1228650b7bd1c2c65fb50ad11a8aba",
  "evaluation_time": 92.25463271141052,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.6029411764705883,
        "f1": 0.3873349261391058,
        "f1_weighted": 0.6499996007526707,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.6029411764705883,
        "scores_per_experiment": [
          {
            "accuracy": 0.5836753305973552,
            "f1": 0.38004192432559186,
            "f1_weighted": 0.6308925110938084
          },
          {
            "accuracy": 0.5925672594619243,
            "f1": 0.37834525240544503,
            "f1_weighted": 0.6436492864216358
          },
          {
            "accuracy": 0.6057911536707706,
            "f1": 0.38711497050014004,
            "f1_weighted": 0.6543234001202388
          },
          {
            "accuracy": 0.6098951208390333,
            "f1": 0.39096437217056174,
            "f1_weighted": 0.6531249353384145
          },
          {
            "accuracy": 0.6101231190150479,
            "f1": 0.3890600237441506,
            "f1_weighted": 0.6585336790660541
          },
          {
            "accuracy": 0.6062471500227998,
            "f1": 0.39112475060120383,
            "f1_weighted": 0.6559969268595153
          },
          {
            "accuracy": 0.6057911536707706,
            "f1": 0.3998462036950829,
            "f1_weighted": 0.6503384330919547
          },
          {
            "accuracy": 0.6130870953032376,
            "f1": 0.3959823617036679,
            "f1_weighted": 0.6561928912508889
          },
          {
            "accuracy": 0.6012311901504788,
            "f1": 0.38604262797654576,
            "f1_weighted": 0.6475251682576887
          },
          {
            "accuracy": 0.6010031919744642,
            "f1": 0.3748267742686688,
            "f1_weighted": 0.64941877602651
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.6081879194630873,
        "f1": 0.38019879577595667,
        "f1_weighted": 0.6582401325666488,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.6081879194630873,
        "scores_per_experiment": [
          {
            "accuracy": 0.5910514541387024,
            "f1": 0.3626191964815784,
            "f1_weighted": 0.6413787699400508
          },
          {
            "accuracy": 0.6076062639821029,
            "f1": 0.3785594107261782,
            "f1_weighted": 0.6611429812556249
          },
          {
            "accuracy": 0.6143176733780761,
            "f1": 0.36685904691024335,
            "f1_weighted": 0.6656334315522907
          },
          {
            "accuracy": 0.6067114093959731,
            "f1": 0.3793004197976555,
            "f1_weighted": 0.6588042520476572
          },
          {
            "accuracy": 0.6277404921700224,
            "f1": 0.40537529383633153,
            "f1_weighted": 0.6747627937114787
          },
          {
            "accuracy": 0.6,
            "f1": 0.3721739778988906,
            "f1_weighted": 0.6503912818266429
          },
          {
            "accuracy": 0.6076062639821029,
            "f1": 0.38820816639896655,
            "f1_weighted": 0.656136415752984
          },
          {
            "accuracy": 0.6219239373601789,
            "f1": 0.3947282882990276,
            "f1_weighted": 0.6712444036737063
          },
          {
            "accuracy": 0.6058165548098434,
            "f1": 0.36742590532569674,
            "f1_weighted": 0.653126387098694
          },
          {
            "accuracy": 0.5991051454138703,
            "f1": 0.3867382520849982,
            "f1_weighted": 0.6497806088073591
          }
        ]
      }
    ]
  },
  "task_name": "MTOPIntentClassification"
}