{
  "dataset_revision": "fad2c6e8459f9e1c45d9315f4953d921437d70f8",
  "evaluation_time": 39.59386110305786,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.5271015467383994,
        "f1": 0.5061880301206572,
        "f1_weighted": 0.5322426896275738,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5271015467383994,
        "scores_per_experiment": [
          {
            "accuracy": 0.5484196368527237,
            "f1": 0.5233005260713948,
            "f1_weighted": 0.553897034656579
          },
          {
            "accuracy": 0.5161398789509078,
            "f1": 0.49492853589278657,
            "f1_weighted": 0.5255406951235959
          },
          {
            "accuracy": 0.5205110961667787,
            "f1": 0.5009822182982343,
            "f1_weighted": 0.5289960233080379
          },
          {
            "accuracy": 0.5336247478143914,
            "f1": 0.5088452337132938,
            "f1_weighted": 0.5467634307737168
          },
          {
            "accuracy": 0.5487558843308675,
            "f1": 0.5198832213405432,
            "f1_weighted": 0.5475892956527688
          },
          {
            "accuracy": 0.5033624747814391,
            "f1": 0.48588278339004454,
            "f1_weighted": 0.5063547678592166
          },
          {
            "accuracy": 0.5232010759919301,
            "f1": 0.5052839424280626,
            "f1_weighted": 0.5255846719441896
          },
          {
            "accuracy": 0.5413584398117014,
            "f1": 0.5215102048241158,
            "f1_weighted": 0.5491965734091315
          },
          {
            "accuracy": 0.5295897780766644,
            "f1": 0.5107959475281777,
            "f1_weighted": 0.5295213953202618
          },
          {
            "accuracy": 0.5060524546065904,
            "f1": 0.4904676877199185,
            "f1_weighted": 0.5089830082282412
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.5209542547958682,
        "f1": 0.5040615851879184,
        "f1_weighted": 0.5253648366839186,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5209542547958682,
        "scores_per_experiment": [
          {
            "accuracy": 0.5386128873585834,
            "f1": 0.5194226005145475,
            "f1_weighted": 0.5431074044926167
          },
          {
            "accuracy": 0.5209050664043285,
            "f1": 0.5027682969059701,
            "f1_weighted": 0.5279786889951626
          },
          {
            "accuracy": 0.515002459419577,
            "f1": 0.49765411132011433,
            "f1_weighted": 0.5242065245815544
          },
          {
            "accuracy": 0.5036891293654697,
            "f1": 0.49144903878369495,
            "f1_weighted": 0.5144758053660633
          },
          {
            "accuracy": 0.5307427447122479,
            "f1": 0.5023797771057787,
            "f1_weighted": 0.5297111200480314
          },
          {
            "accuracy": 0.5027053615346778,
            "f1": 0.49348895076447835,
            "f1_weighted": 0.5012039887199082
          },
          {
            "accuracy": 0.5184456468273487,
            "f1": 0.5048835126288576,
            "f1_weighted": 0.5245751832582082
          },
          {
            "accuracy": 0.5312346286276439,
            "f1": 0.5142184946887415,
            "f1_weighted": 0.5381139062437036
          },
          {
            "accuracy": 0.5258239055582883,
            "f1": 0.5052162561519918,
            "f1_weighted": 0.5253230332731124
          },
          {
            "accuracy": 0.5223807181505165,
            "f1": 0.5091348130150087,
            "f1_weighted": 0.5249527118608251
          }
        ]
      }
    ]
  },
  "task_name": "MassiveScenarioClassification"
}