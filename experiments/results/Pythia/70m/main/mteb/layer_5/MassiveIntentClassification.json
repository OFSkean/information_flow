{
  "dataset_revision": "4672e20407010da34463acc759c162ca9734bca6",
  "evaluation_time": 72.08064270019531,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.47067921990585065,
        "f1": 0.441055328257537,
        "f1_weighted": 0.477483089675857,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.47067921990585065,
        "scores_per_experiment": [
          {
            "accuracy": 0.4690652320107599,
            "f1": 0.4417779581812683,
            "f1_weighted": 0.4749851264499479
          },
          {
            "accuracy": 0.4673839946200404,
            "f1": 0.42905582233733214,
            "f1_weighted": 0.47542276928821625
          },
          {
            "accuracy": 0.47780766644250167,
            "f1": 0.4454167932453103,
            "f1_weighted": 0.4818469829948932
          },
          {
            "accuracy": 0.4912575655682582,
            "f1": 0.45562267277330726,
            "f1_weighted": 0.5016915436425106
          },
          {
            "accuracy": 0.4811701412239408,
            "f1": 0.4450744740654154,
            "f1_weighted": 0.4877125950902442
          },
          {
            "accuracy": 0.4586415601882986,
            "f1": 0.442961439471512,
            "f1_weighted": 0.4639850145594649
          },
          {
            "accuracy": 0.4515803631472764,
            "f1": 0.4276538879645652,
            "f1_weighted": 0.4573408424532735
          },
          {
            "accuracy": 0.4636852723604573,
            "f1": 0.43189035539074777,
            "f1_weighted": 0.47513791053899795
          },
          {
            "accuracy": 0.46099529253530597,
            "f1": 0.4403079196847639,
            "f1_weighted": 0.4679942953824
          },
          {
            "accuracy": 0.4852051109616678,
            "f1": 0.4507919594611485,
            "f1_weighted": 0.4887138163586212
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.4850959173635022,
        "f1": 0.44943497730808424,
        "f1_weighted": 0.49273113735314766,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.4850959173635022,
        "scores_per_experiment": [
          {
            "accuracy": 0.47614363010329563,
            "f1": 0.44598994545075943,
            "f1_weighted": 0.47773691570902166
          },
          {
            "accuracy": 0.4963108706345303,
            "f1": 0.45286156105985026,
            "f1_weighted": 0.5037656775011883
          },
          {
            "accuracy": 0.49434333497294636,
            "f1": 0.4487015121836608,
            "f1_weighted": 0.5059271434437129
          },
          {
            "accuracy": 0.5012297097884899,
            "f1": 0.469328881264507,
            "f1_weighted": 0.5101785616254517
          },
          {
            "accuracy": 0.4899163797343827,
            "f1": 0.45283129240135306,
            "f1_weighted": 0.49905262237669734
          },
          {
            "accuracy": 0.4751598622725037,
            "f1": 0.44934581771757237,
            "f1_weighted": 0.48179123779918986
          },
          {
            "accuracy": 0.4574520413182489,
            "f1": 0.4267330882649119,
            "f1_weighted": 0.46149973839553177
          },
          {
            "accuracy": 0.4746679783571077,
            "f1": 0.43562315662323786,
            "f1_weighted": 0.48512247503349437
          },
          {
            "accuracy": 0.4731923266109198,
            "f1": 0.4485157427946522,
            "f1_weighted": 0.4831337312825678
          },
          {
            "accuracy": 0.5125430398425972,
            "f1": 0.4644187753203382,
            "f1_weighted": 0.5191032703646206
          }
        ]
      }
    ]
  },
  "task_name": "MassiveIntentClassification"
}