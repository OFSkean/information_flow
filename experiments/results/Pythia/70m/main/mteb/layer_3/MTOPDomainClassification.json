{
  "dataset_revision": "d80d48c1eb48d3562165c59d59d0034df9fff0bf",
  "evaluation_time": 42.86121153831482,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.6950980392156862,
        "f1": 0.6861430633260147,
        "f1_weighted": 0.6962493518681099,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.6950980392156862,
        "scores_per_experiment": [
          {
            "accuracy": 0.6682626538987688,
            "f1": 0.6614797835104405,
            "f1_weighted": 0.668919857488683
          },
          {
            "accuracy": 0.6992704058367533,
            "f1": 0.6909418392512584,
            "f1_weighted": 0.7033474776455694
          },
          {
            "accuracy": 0.713406292749658,
            "f1": 0.7012372878703953,
            "f1_weighted": 0.7164856858031
          },
          {
            "accuracy": 0.7229822161422709,
            "f1": 0.7074025468800467,
            "f1_weighted": 0.7217331761841905
          },
          {
            "accuracy": 0.6901504787961696,
            "f1": 0.6825526916412703,
            "f1_weighted": 0.6942992479245664
          },
          {
            "accuracy": 0.7129502963976289,
            "f1": 0.7027449795143194,
            "f1_weighted": 0.7131204138188273
          },
          {
            "accuracy": 0.6650706794345645,
            "f1": 0.657773602813645,
            "f1_weighted": 0.6619438860162093
          },
          {
            "accuracy": 0.6666666666666666,
            "f1": 0.665150134232828,
            "f1_weighted": 0.6716954268514266
          },
          {
            "accuracy": 0.698358413132695,
            "f1": 0.6913433891400504,
            "f1_weighted": 0.6987735762479665
          },
          {
            "accuracy": 0.7138622891016871,
            "f1": 0.7008043784058926,
            "f1_weighted": 0.7121747707005602
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.6794183445190156,
        "f1": 0.6738524144456128,
        "f1_weighted": 0.6788406900415407,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.6794183445190156,
        "scores_per_experiment": [
          {
            "accuracy": 0.6514541387024608,
            "f1": 0.6498274451377992,
            "f1_weighted": 0.6488158220507505
          },
          {
            "accuracy": 0.6903803131991052,
            "f1": 0.685435296579265,
            "f1_weighted": 0.6919156175524588
          },
          {
            "accuracy": 0.6921700223713646,
            "f1": 0.682799512620547,
            "f1_weighted": 0.6939597942291336
          },
          {
            "accuracy": 0.7181208053691275,
            "f1": 0.708759211355384,
            "f1_weighted": 0.7164652804630065
          },
          {
            "accuracy": 0.6809843400447427,
            "f1": 0.6789552417686958,
            "f1_weighted": 0.6821781405259355
          },
          {
            "accuracy": 0.6912751677852349,
            "f1": 0.6840313823524653,
            "f1_weighted": 0.6900283584194343
          },
          {
            "accuracy": 0.643847874720358,
            "f1": 0.6348429398005596,
            "f1_weighted": 0.6409901574881947
          },
          {
            "accuracy": 0.6505592841163311,
            "f1": 0.650831765864341,
            "f1_weighted": 0.6522902232766612
          },
          {
            "accuracy": 0.6809843400447427,
            "f1": 0.6781439937448553,
            "f1_weighted": 0.6807824208672019
          },
          {
            "accuracy": 0.694407158836689,
            "f1": 0.6848973552322158,
            "f1_weighted": 0.6909810855426298
          }
        ]
      }
    ]
  },
  "task_name": "MTOPDomainClassification"
}