{
  "dataset_revision": "fad2c6e8459f9e1c45d9315f4953d921437d70f8",
  "evaluation_time": 38.50249409675598,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.4694014794889038,
        "f1": 0.4500673416037329,
        "f1_weighted": 0.47518301490636394,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.4694014794889038,
        "scores_per_experiment": [
          {
            "accuracy": 0.45729657027572296,
            "f1": 0.4497621516066855,
            "f1_weighted": 0.45565188696660175
          },
          {
            "accuracy": 0.48386012104909215,
            "f1": 0.4582869857212926,
            "f1_weighted": 0.4895496146800591
          },
          {
            "accuracy": 0.46805648957632817,
            "f1": 0.4449874560281939,
            "f1_weighted": 0.4778016104490999
          },
          {
            "accuracy": 0.45023537323470075,
            "f1": 0.43599222414120375,
            "f1_weighted": 0.46424906415243516
          },
          {
            "accuracy": 0.5006724949562879,
            "f1": 0.46731146598015727,
            "f1_weighted": 0.49770951526759866
          },
          {
            "accuracy": 0.44317417619367855,
            "f1": 0.42874772537132816,
            "f1_weighted": 0.4544303847970951
          },
          {
            "accuracy": 0.4663752521856086,
            "f1": 0.45180955721770893,
            "f1_weighted": 0.47554066949928475
          },
          {
            "accuracy": 0.4852051109616678,
            "f1": 0.4627749500843125,
            "f1_weighted": 0.49135736130271734
          },
          {
            "accuracy": 0.4828513786146604,
            "f1": 0.46053263437433206,
            "f1_weighted": 0.4832721837352614
          },
          {
            "accuracy": 0.4562878278412912,
            "f1": 0.44046826551211493,
            "f1_weighted": 0.462267858213487
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.4530250860796852,
        "f1": 0.4398167161131976,
        "f1_weighted": 0.45670904024035225,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.4530250860796852,
        "scores_per_experiment": [
          {
            "accuracy": 0.43580914904082635,
            "f1": 0.42554511759163133,
            "f1_weighted": 0.4339369777787807
          },
          {
            "accuracy": 0.47712739793408754,
            "f1": 0.4595596254505126,
            "f1_weighted": 0.48068686350573087
          },
          {
            "accuracy": 0.4441711756025578,
            "f1": 0.4243865840720232,
            "f1_weighted": 0.4548825152148626
          },
          {
            "accuracy": 0.4254795868175111,
            "f1": 0.4134088403176729,
            "f1_weighted": 0.4374923525821801
          },
          {
            "accuracy": 0.4756517461878997,
            "f1": 0.44979234085712666,
            "f1_weighted": 0.4711440446266657
          },
          {
            "accuracy": 0.4377766847024102,
            "f1": 0.4345760137115762,
            "f1_weighted": 0.44268029751169186
          },
          {
            "accuracy": 0.4451549434333497,
            "f1": 0.44036763621134667,
            "f1_weighted": 0.45245800850395607
          },
          {
            "accuracy": 0.46138711264141663,
            "f1": 0.4469173074985329,
            "f1_weighted": 0.46471329652256516
          },
          {
            "accuracy": 0.47417609444171177,
            "f1": 0.4541109039795122,
            "f1_weighted": 0.4723572585892411
          },
          {
            "accuracy": 0.4535169699950812,
            "f1": 0.4495027914420414,
            "f1_weighted": 0.4567387875678483
          }
        ]
      }
    ]
  },
  "task_name": "MassiveScenarioClassification"
}