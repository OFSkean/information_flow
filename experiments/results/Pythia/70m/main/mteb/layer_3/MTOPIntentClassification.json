{
  "dataset_revision": "ae001d0e6b1228650b7bd1c2c65fb50ad11a8aba",
  "evaluation_time": 86.3400936126709,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.5817145462836296,
        "f1": 0.3746536389264889,
        "f1_weighted": 0.6329480409941255,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5817145462836296,
        "scores_per_experiment": [
          {
            "accuracy": 0.5813953488372093,
            "f1": 0.3698846087819729,
            "f1_weighted": 0.6351249615716222
          },
          {
            "accuracy": 0.5786593707250342,
            "f1": 0.3841957534773028,
            "f1_weighted": 0.6302242145488102
          },
          {
            "accuracy": 0.5884632922936617,
            "f1": 0.38449795623674266,
            "f1_weighted": 0.6409230381678827
          },
          {
            "accuracy": 0.5946192430460556,
            "f1": 0.3680214918350538,
            "f1_weighted": 0.6443774263662315
          },
          {
            "accuracy": 0.5877792977656179,
            "f1": 0.3883152217605434,
            "f1_weighted": 0.6419710897877888
          },
          {
            "accuracy": 0.5864113087095303,
            "f1": 0.36284219083491576,
            "f1_weighted": 0.6372842609232859
          },
          {
            "accuracy": 0.5713634290925672,
            "f1": 0.363545686237815,
            "f1_weighted": 0.6181146001801032
          },
          {
            "accuracy": 0.5816233470132239,
            "f1": 0.39472946515111107,
            "f1_weighted": 0.6300781027977327
          },
          {
            "accuracy": 0.5686274509803921,
            "f1": 0.37140259403143344,
            "f1_weighted": 0.6241954494550197
          },
          {
            "accuracy": 0.5782033743730051,
            "f1": 0.359101420917998,
            "f1_weighted": 0.6271872661427779
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.5832214765100672,
        "f1": 0.3739159585296228,
        "f1_weighted": 0.6368743261982243,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5832214765100672,
        "scores_per_experiment": [
          {
            "accuracy": 0.5574944071588367,
            "f1": 0.35212060379608023,
            "f1_weighted": 0.6139490365511201
          },
          {
            "accuracy": 0.5888143176733781,
            "f1": 0.3812282252844449,
            "f1_weighted": 0.6434521869346963
          },
          {
            "accuracy": 0.5977628635346757,
            "f1": 0.38815427182991574,
            "f1_weighted": 0.6510697151320278
          },
          {
            "accuracy": 0.5977628635346757,
            "f1": 0.38518506331258984,
            "f1_weighted": 0.6511953184523447
          },
          {
            "accuracy": 0.6044742729306488,
            "f1": 0.3867915359694412,
            "f1_weighted": 0.6569105432841046
          },
          {
            "accuracy": 0.5803131991051454,
            "f1": 0.37388407392455186,
            "f1_weighted": 0.6360894646622459
          },
          {
            "accuracy": 0.5731543624161074,
            "f1": 0.36727942556084525,
            "f1_weighted": 0.623247353829481
          },
          {
            "accuracy": 0.5816554809843401,
            "f1": 0.37230663381959955,
            "f1_weighted": 0.6354781603001138
          },
          {
            "accuracy": 0.5619686800894854,
            "f1": 0.3551918882063154,
            "f1_weighted": 0.6153722192374133
          },
          {
            "accuracy": 0.5888143176733781,
            "f1": 0.3770178635924437,
            "f1_weighted": 0.6419792635986962
          }
        ]
      }
    ]
  },
  "task_name": "MTOPIntentClassification"
}