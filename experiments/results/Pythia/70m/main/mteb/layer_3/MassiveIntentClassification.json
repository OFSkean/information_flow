{
  "dataset_revision": "4672e20407010da34463acc759c162ca9734bca6",
  "evaluation_time": 69.6197338104248,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.4312037659717552,
        "f1": 0.40129545848916043,
        "f1_weighted": 0.4406443487177437,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.4312037659717552,
        "scores_per_experiment": [
          {
            "accuracy": 0.43140551445864156,
            "f1": 0.40322598863465775,
            "f1_weighted": 0.43804365104513926
          },
          {
            "accuracy": 0.43342299932750505,
            "f1": 0.39653669312103584,
            "f1_weighted": 0.4434270385561852
          },
          {
            "accuracy": 0.41425689307330194,
            "f1": 0.38925488132330177,
            "f1_weighted": 0.41914445918103876
          },
          {
            "accuracy": 0.4505716207128447,
            "f1": 0.4159060920328283,
            "f1_weighted": 0.46241856229412553
          },
          {
            "accuracy": 0.433759246805649,
            "f1": 0.39617337067628305,
            "f1_weighted": 0.4426467043106182
          },
          {
            "accuracy": 0.41728312037659715,
            "f1": 0.40364102841331395,
            "f1_weighted": 0.4268252887965838
          },
          {
            "accuracy": 0.4266980497646268,
            "f1": 0.39137906140010137,
            "f1_weighted": 0.43731462143799166
          },
          {
            "accuracy": 0.42535305985205113,
            "f1": 0.3911518191752712,
            "f1_weighted": 0.44446867531681705
          },
          {
            "accuracy": 0.4280430396772024,
            "f1": 0.4063878565437981,
            "f1_weighted": 0.439865036283426
          },
          {
            "accuracy": 0.4512441156691325,
            "f1": 0.4192977935710129,
            "f1_weighted": 0.4522894499555118
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.4303492375799311,
        "f1": 0.40120922445863016,
        "f1_weighted": 0.43797689212284496,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.4303492375799311,
        "scores_per_experiment": [
          {
            "accuracy": 0.42006886374815544,
            "f1": 0.39779810100582735,
            "f1_weighted": 0.42327355427456975
          },
          {
            "accuracy": 0.43876045253320217,
            "f1": 0.3904655953877911,
            "f1_weighted": 0.4517226681794084
          },
          {
            "accuracy": 0.4254795868175111,
            "f1": 0.39370670094986,
            "f1_weighted": 0.4327485643059188
          },
          {
            "accuracy": 0.4535169699950812,
            "f1": 0.4176347005311268,
            "f1_weighted": 0.46538091376310514
          },
          {
            "accuracy": 0.4436792916871618,
            "f1": 0.4130389657472314,
            "f1_weighted": 0.45141395873414164
          },
          {
            "accuracy": 0.42597147073290703,
            "f1": 0.408575604075539,
            "f1_weighted": 0.43166337063769056
          },
          {
            "accuracy": 0.4023610427939006,
            "f1": 0.3877148555616469,
            "f1_weighted": 0.4050446109985604
          },
          {
            "accuracy": 0.4146581406787998,
            "f1": 0.38710716734742,
            "f1_weighted": 0.42706947708960386
          },
          {
            "accuracy": 0.411214953271028,
            "f1": 0.3897996267683305,
            "f1_weighted": 0.4182016805574016
          },
          {
            "accuracy": 0.4677816035415642,
            "f1": 0.4262509272115286,
            "f1_weighted": 0.4732501226880489
          }
        ]
      }
    ]
  },
  "task_name": "MassiveIntentClassification"
}