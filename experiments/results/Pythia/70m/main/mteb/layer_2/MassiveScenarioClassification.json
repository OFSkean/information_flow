{
  "dataset_revision": "fad2c6e8459f9e1c45d9315f4953d921437d70f8",
  "evaluation_time": 37.83312654495239,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.5189307330195023,
        "f1": 0.5021813990044264,
        "f1_weighted": 0.5227339128062307,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5189307330195023,
        "scores_per_experiment": [
          {
            "accuracy": 0.5464021519838601,
            "f1": 0.533970077403523,
            "f1_weighted": 0.5493924928410168
          },
          {
            "accuracy": 0.5265635507733692,
            "f1": 0.5077899405258779,
            "f1_weighted": 0.5307642729543169
          },
          {
            "accuracy": 0.49932750504371215,
            "f1": 0.4889421750300163,
            "f1_weighted": 0.505594015705417
          },
          {
            "accuracy": 0.4996637525218561,
            "f1": 0.4856082101587954,
            "f1_weighted": 0.5083480851013131
          },
          {
            "accuracy": 0.5373234700739744,
            "f1": 0.5141361833188166,
            "f1_weighted": 0.5322997477948754
          },
          {
            "accuracy": 0.5043712172158709,
            "f1": 0.48597090503801216,
            "f1_weighted": 0.5126510603254948
          },
          {
            "accuracy": 0.5285810356422327,
            "f1": 0.5110226432712488,
            "f1_weighted": 0.5325583930369759
          },
          {
            "accuracy": 0.5211835911230666,
            "f1": 0.5060856679503815,
            "f1_weighted": 0.526587523077304
          },
          {
            "accuracy": 0.5279085406859448,
            "f1": 0.5053645358032466,
            "f1_weighted": 0.5304759123139329
          },
          {
            "accuracy": 0.4979825151311365,
            "f1": 0.4829236515443467,
            "f1_weighted": 0.4986676249116605
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.5116576487948844,
        "f1": 0.5003723875719421,
        "f1_weighted": 0.5134594380596174,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5116576487948844,
        "scores_per_experiment": [
          {
            "accuracy": 0.5272995573044762,
            "f1": 0.5133341554710569,
            "f1_weighted": 0.5281004495499904
          },
          {
            "accuracy": 0.5164781111657649,
            "f1": 0.5122436519937603,
            "f1_weighted": 0.5199583892593341
          },
          {
            "accuracy": 0.49827840629611414,
            "f1": 0.495391622154052,
            "f1_weighted": 0.500763591295803
          },
          {
            "accuracy": 0.4815543531726513,
            "f1": 0.4673882991229957,
            "f1_weighted": 0.48963700058412796
          },
          {
            "accuracy": 0.5277914412198721,
            "f1": 0.5104562349910335,
            "f1_weighted": 0.5216399226574014
          },
          {
            "accuracy": 0.5017215937038859,
            "f1": 0.4910441486980536,
            "f1_weighted": 0.50137126775386
          },
          {
            "accuracy": 0.514510575504181,
            "f1": 0.5034014918332982,
            "f1_weighted": 0.522369534877232
          },
          {
            "accuracy": 0.515002459419577,
            "f1": 0.5035927852566713,
            "f1_weighted": 0.5164655683328538
          },
          {
            "accuracy": 0.5253320216428923,
            "f1": 0.5099042754430881,
            "f1_weighted": 0.5253549002040085
          },
          {
            "accuracy": 0.5086079685194295,
            "f1": 0.49696721075541156,
            "f1_weighted": 0.5089337560815614
          }
        ]
      }
    ]
  },
  "task_name": "MassiveScenarioClassification"
}