{
  "dataset_revision": "4672e20407010da34463acc759c162ca9734bca6",
  "evaluation_time": 68.57372665405273,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.49566240753194357,
        "f1": 0.4632941131050455,
        "f1_weighted": 0.5011095247466567,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.49566240753194357,
        "scores_per_experiment": [
          {
            "accuracy": 0.4996637525218561,
            "f1": 0.47358334516328476,
            "f1_weighted": 0.5019676188056688
          },
          {
            "accuracy": 0.5087424344317417,
            "f1": 0.4730595784313048,
            "f1_weighted": 0.5121166337599409
          },
          {
            "accuracy": 0.49260255548083387,
            "f1": 0.45772173661529275,
            "f1_weighted": 0.4979780055373587
          },
          {
            "accuracy": 0.5090786819098857,
            "f1": 0.46697027345204173,
            "f1_weighted": 0.5167113671926661
          },
          {
            "accuracy": 0.4979825151311365,
            "f1": 0.4539501901404668,
            "f1_weighted": 0.503727314410744
          },
          {
            "accuracy": 0.47881640887693344,
            "f1": 0.4504911481616623,
            "f1_weighted": 0.4829901567289971
          },
          {
            "accuracy": 0.4835238735709482,
            "f1": 0.45253722012556863,
            "f1_weighted": 0.48927967358073576
          },
          {
            "accuracy": 0.49562878278412914,
            "f1": 0.45970448797884494,
            "f1_weighted": 0.5080040352205408
          },
          {
            "accuracy": 0.4828513786146604,
            "f1": 0.4633126685254597,
            "f1_weighted": 0.48739527842863795
          },
          {
            "accuracy": 0.50773369199731,
            "f1": 0.48161048245652754,
            "f1_weighted": 0.5109251638012768
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.5045253320216428,
        "f1": 0.47404496684465275,
        "f1_weighted": 0.5090115002071899,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5045253320216428,
        "scores_per_experiment": [
          {
            "accuracy": 0.5086079685194295,
            "f1": 0.47735275154544676,
            "f1_weighted": 0.5095892159473889
          },
          {
            "accuracy": 0.5194294146581406,
            "f1": 0.47634978317551124,
            "f1_weighted": 0.5227054975197591
          },
          {
            "accuracy": 0.500245941957698,
            "f1": 0.46331493405157465,
            "f1_weighted": 0.5041553678028522
          },
          {
            "accuracy": 0.5031972454500738,
            "f1": 0.47374436383101776,
            "f1_weighted": 0.5071198647866213
          },
          {
            "accuracy": 0.5135268076733891,
            "f1": 0.4783142063997392,
            "f1_weighted": 0.5208758368661836
          },
          {
            "accuracy": 0.5027053615346778,
            "f1": 0.4798155361044246,
            "f1_weighted": 0.5083247769626557
          },
          {
            "accuracy": 0.48303000491883913,
            "f1": 0.46226235993191833,
            "f1_weighted": 0.4826817525365466
          },
          {
            "accuracy": 0.4938514510575504,
            "f1": 0.45906146674648546,
            "f1_weighted": 0.5027294556665304
          },
          {
            "accuracy": 0.48499754058042305,
            "f1": 0.46482899830200025,
            "f1_weighted": 0.49022424985308616
          },
          {
            "accuracy": 0.5356615838662075,
            "f1": 0.5054052683584092,
            "f1_weighted": 0.5417089841302749
          }
        ]
      }
    ]
  },
  "task_name": "MassiveIntentClassification"
}