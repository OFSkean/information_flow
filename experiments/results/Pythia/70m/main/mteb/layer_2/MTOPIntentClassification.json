{
  "dataset_revision": "ae001d0e6b1228650b7bd1c2c65fb50ad11a8aba",
  "evaluation_time": 86.19792199134827,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.6186274509803922,
        "f1": 0.39554234152599105,
        "f1_weighted": 0.6624997627292085,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.6186274509803922,
        "scores_per_experiment": [
          {
            "accuracy": 0.61171910624715,
            "f1": 0.3865045253576348,
            "f1_weighted": 0.6551172711046743
          },
          {
            "accuracy": 0.6108071135430917,
            "f1": 0.3863369972875605,
            "f1_weighted": 0.6538484465849799
          },
          {
            "accuracy": 0.6108071135430917,
            "f1": 0.39837505958864217,
            "f1_weighted": 0.6539508518558179
          },
          {
            "accuracy": 0.6308709530323757,
            "f1": 0.39951916562441236,
            "f1_weighted": 0.6733475973616437
          },
          {
            "accuracy": 0.6235750113999088,
            "f1": 0.40884094214703914,
            "f1_weighted": 0.667463049618888
          },
          {
            "accuracy": 0.6256269949840402,
            "f1": 0.39498082525146155,
            "f1_weighted": 0.6724417066145246
          },
          {
            "accuracy": 0.5946192430460556,
            "f1": 0.3785837364239883,
            "f1_weighted": 0.6374470965401162
          },
          {
            "accuracy": 0.6347469220246238,
            "f1": 0.4146114276369128,
            "f1_weighted": 0.6797966923104785
          },
          {
            "accuracy": 0.6151390788873689,
            "f1": 0.4002325952269432,
            "f1_weighted": 0.6597026122559951
          },
          {
            "accuracy": 0.6283629730962152,
            "f1": 0.38743814071531624,
            "f1_weighted": 0.671882303044967
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.6246085011185682,
        "f1": 0.39451269190132043,
        "f1_weighted": 0.6717263764702561,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.6246085011185682,
        "scores_per_experiment": [
          {
            "accuracy": 0.6085011185682326,
            "f1": 0.36923658437614393,
            "f1_weighted": 0.6536578675008872
          },
          {
            "accuracy": 0.6058165548098434,
            "f1": 0.3904755887024724,
            "f1_weighted": 0.6531318909185359
          },
          {
            "accuracy": 0.6192393736017897,
            "f1": 0.3973016439522686,
            "f1_weighted": 0.6662886223497603
          },
          {
            "accuracy": 0.6371364653243848,
            "f1": 0.39223731537639994,
            "f1_weighted": 0.6827821028358525
          },
          {
            "accuracy": 0.6456375838926175,
            "f1": 0.41738157414011257,
            "f1_weighted": 0.6927083118217199
          },
          {
            "accuracy": 0.6178970917225951,
            "f1": 0.38341150795069934,
            "f1_weighted": 0.6678673160920219
          },
          {
            "accuracy": 0.6004474272930649,
            "f1": 0.37858024223773545,
            "f1_weighted": 0.650087581873831
          },
          {
            "accuracy": 0.6577181208053692,
            "f1": 0.41328648443390725,
            "f1_weighted": 0.704980787856258
          },
          {
            "accuracy": 0.6210290827740492,
            "f1": 0.39458996974370447,
            "f1_weighted": 0.6668134858720497
          },
          {
            "accuracy": 0.632662192393736,
            "f1": 0.40862600809976046,
            "f1_weighted": 0.6789457975816445
          }
        ]
      }
    ]
  },
  "task_name": "MTOPIntentClassification"
}