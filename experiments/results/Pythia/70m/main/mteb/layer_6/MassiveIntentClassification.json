{
  "dataset_revision": "4672e20407010da34463acc759c162ca9734bca6",
  "evaluation_time": 70.7867476940155,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.4500336247478144,
        "f1": 0.41575220515185657,
        "f1_weighted": 0.4620302314338872,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.4500336247478144,
        "scores_per_experiment": [
          {
            "accuracy": 0.4596503026227303,
            "f1": 0.41724327062473965,
            "f1_weighted": 0.4687405444746368
          },
          {
            "accuracy": 0.4539340954942838,
            "f1": 0.41845196975686405,
            "f1_weighted": 0.46149621032897814
          },
          {
            "accuracy": 0.4398117014122394,
            "f1": 0.402497631298381,
            "f1_weighted": 0.45207316383598056
          },
          {
            "accuracy": 0.4791526563550773,
            "f1": 0.440391745895464,
            "f1_weighted": 0.49426271444314934
          },
          {
            "accuracy": 0.4576328177538668,
            "f1": 0.4188284611189857,
            "f1_weighted": 0.4744498744850299
          },
          {
            "accuracy": 0.44821788836583726,
            "f1": 0.4190480112155402,
            "f1_weighted": 0.4587919041143237
          },
          {
            "accuracy": 0.4398117014122394,
            "f1": 0.41943084847652407,
            "f1_weighted": 0.4504922945500313
          },
          {
            "accuracy": 0.4344317417619368,
            "f1": 0.4012019152191744,
            "f1_weighted": 0.4542017015300276
          },
          {
            "accuracy": 0.43813046402151984,
            "f1": 0.4128612233780966,
            "f1_weighted": 0.4504046014359634
          },
          {
            "accuracy": 0.4495628782784129,
            "f1": 0.40756697453479596,
            "f1_weighted": 0.4553893051407507
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.45799311362518447,
        "f1": 0.4225380167887506,
        "f1_weighted": 0.4696323620183346,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.45799311362518447,
        "scores_per_experiment": [
          {
            "accuracy": 0.47270044269552386,
            "f1": 0.4310318888038151,
            "f1_weighted": 0.4765770733358048
          },
          {
            "accuracy": 0.46876537137235613,
            "f1": 0.41729828374916406,
            "f1_weighted": 0.47292245249029463
          },
          {
            "accuracy": 0.4756517461878997,
            "f1": 0.4389159381981926,
            "f1_weighted": 0.4909575323534955
          },
          {
            "accuracy": 0.48499754058042305,
            "f1": 0.44678986327986325,
            "f1_weighted": 0.4995668681659278
          },
          {
            "accuracy": 0.46040334481062467,
            "f1": 0.428040510190997,
            "f1_weighted": 0.47494570214098925
          },
          {
            "accuracy": 0.46138711264141663,
            "f1": 0.43514228623193485,
            "f1_weighted": 0.47230385914028183
          },
          {
            "accuracy": 0.4318740777176586,
            "f1": 0.4099462921781559,
            "f1_weighted": 0.4423630699682344
          },
          {
            "accuracy": 0.4303984259714707,
            "f1": 0.39309206031840865,
            "f1_weighted": 0.4483338452926496
          },
          {
            "accuracy": 0.441219872110182,
            "f1": 0.42083918126095793,
            "f1_weighted": 0.4539920936399259
          },
          {
            "accuracy": 0.4525332021642892,
            "f1": 0.40428386367601693,
            "f1_weighted": 0.46436112365574256
          }
        ]
      }
    ]
  },
  "task_name": "MassiveIntentClassification"
}