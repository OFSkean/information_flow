{
  "dataset_revision": "ae001d0e6b1228650b7bd1c2c65fb50ad11a8aba",
  "evaluation_time": 88.25250458717346,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.574327405380757,
        "f1": 0.35850970511221697,
        "f1_weighted": 0.6271392151800782,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.574327405380757,
        "scores_per_experiment": [
          {
            "accuracy": 0.5565435476516188,
            "f1": 0.35464897994935396,
            "f1_weighted": 0.6047753166645358
          },
          {
            "accuracy": 0.5930232558139535,
            "f1": 0.369916066998592,
            "f1_weighted": 0.64698584683422
          },
          {
            "accuracy": 0.5647514819881441,
            "f1": 0.3629597502025753,
            "f1_weighted": 0.6186434833955176
          },
          {
            "accuracy": 0.5852713178294574,
            "f1": 0.36729034700946467,
            "f1_weighted": 0.6374112101230286
          },
          {
            "accuracy": 0.5800273597811217,
            "f1": 0.3571092456595051,
            "f1_weighted": 0.6316573372670103
          },
          {
            "accuracy": 0.5592795257637939,
            "f1": 0.35301058155330195,
            "f1_weighted": 0.6174534304303498
          },
          {
            "accuracy": 0.562015503875969,
            "f1": 0.358286246147078,
            "f1_weighted": 0.6137743683056667
          },
          {
            "accuracy": 0.5802553579571363,
            "f1": 0.3596943211461171,
            "f1_weighted": 0.6305884907791915
          },
          {
            "accuracy": 0.5706794345645235,
            "f1": 0.34351019522351106,
            "f1_weighted": 0.6271389862921112
          },
          {
            "accuracy": 0.5914272685818514,
            "f1": 0.35867131723267087,
            "f1_weighted": 0.6429636817091509
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.5821476510067114,
        "f1": 0.3579095905803488,
        "f1_weighted": 0.6374655763856165,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5821476510067114,
        "scores_per_experiment": [
          {
            "accuracy": 0.567337807606264,
            "f1": 0.3460264816881703,
            "f1_weighted": 0.619170305588699
          },
          {
            "accuracy": 0.6098434004474272,
            "f1": 0.36663392104128323,
            "f1_weighted": 0.6657145600577545
          },
          {
            "accuracy": 0.5807606263982102,
            "f1": 0.35238907990107876,
            "f1_weighted": 0.6379590733962017
          },
          {
            "accuracy": 0.5838926174496645,
            "f1": 0.3550112327346225,
            "f1_weighted": 0.6385521341261001
          },
          {
            "accuracy": 0.5941834451901566,
            "f1": 0.37670127855258717,
            "f1_weighted": 0.6474212122447555
          },
          {
            "accuracy": 0.5668903803131992,
            "f1": 0.36505802765559686,
            "f1_weighted": 0.6268601212135949
          },
          {
            "accuracy": 0.5691275167785235,
            "f1": 0.3618053769653642,
            "f1_weighted": 0.6261317843737102
          },
          {
            "accuracy": 0.5901565995525727,
            "f1": 0.3534669606782696,
            "f1_weighted": 0.6424719924281241
          },
          {
            "accuracy": 0.5731543624161074,
            "f1": 0.34412890735057383,
            "f1_weighted": 0.6288314916258271
          },
          {
            "accuracy": 0.5861297539149888,
            "f1": 0.3578746392359419,
            "f1_weighted": 0.6415430888013983
          }
        ]
      }
    ]
  },
  "task_name": "MTOPIntentClassification"
}