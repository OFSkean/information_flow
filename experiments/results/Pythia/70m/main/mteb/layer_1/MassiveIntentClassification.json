{
  "dataset_revision": "4672e20407010da34463acc759c162ca9734bca6",
  "evaluation_time": 69.42429971694946,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.4944182918628111,
        "f1": 0.46258163282016584,
        "f1_weighted": 0.49937868663863716,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.4944182918628111,
        "scores_per_experiment": [
          {
            "accuracy": 0.48924008069939473,
            "f1": 0.46631823341243966,
            "f1_weighted": 0.4946842699583282
          },
          {
            "accuracy": 0.5097511768661735,
            "f1": 0.4704358819606339,
            "f1_weighted": 0.5166294876043598
          },
          {
            "accuracy": 0.48856758574310694,
            "f1": 0.45732397277051107,
            "f1_weighted": 0.4916634725732681
          },
          {
            "accuracy": 0.5100874243443174,
            "f1": 0.46252402379539564,
            "f1_weighted": 0.5144552276838217
          },
          {
            "accuracy": 0.5026899798251513,
            "f1": 0.46307700791512263,
            "f1_weighted": 0.5058488331322833
          },
          {
            "accuracy": 0.4852051109616678,
            "f1": 0.4618500024258669,
            "f1_weighted": 0.48979655022221713
          },
          {
            "accuracy": 0.48755884330867516,
            "f1": 0.4566649365943247,
            "f1_weighted": 0.48904607434452496
          },
          {
            "accuracy": 0.49394754539340957,
            "f1": 0.455431095372002,
            "f1_weighted": 0.5029276272885643
          },
          {
            "accuracy": 0.4741089441829186,
            "f1": 0.4560509303236303,
            "f1_weighted": 0.47633683229942886
          },
          {
            "accuracy": 0.5030262273032953,
            "f1": 0.4761402436317315,
            "f1_weighted": 0.5123984912795744
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.502065912444663,
        "f1": 0.47040853401385635,
        "f1_weighted": 0.50755620792995,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.502065912444663,
        "scores_per_experiment": [
          {
            "accuracy": 0.49483521888834237,
            "f1": 0.47891485025050146,
            "f1_weighted": 0.4964732058849961
          },
          {
            "accuracy": 0.5081160846040335,
            "f1": 0.4596852595056183,
            "f1_weighted": 0.5147659861778103
          },
          {
            "accuracy": 0.5110673880964093,
            "f1": 0.4740963787095713,
            "f1_weighted": 0.5157082579914668
          },
          {
            "accuracy": 0.5027053615346778,
            "f1": 0.46085719228310923,
            "f1_weighted": 0.5079433930873748
          },
          {
            "accuracy": 0.5174618789965568,
            "f1": 0.48391399789283085,
            "f1_weighted": 0.5242862948795828
          },
          {
            "accuracy": 0.4972946384653222,
            "f1": 0.4728926958985002,
            "f1_weighted": 0.5048513768928193
          },
          {
            "accuracy": 0.4899163797343827,
            "f1": 0.46843725010336096,
            "f1_weighted": 0.490295109324304
          },
          {
            "accuracy": 0.49680275454992623,
            "f1": 0.45328212335322005,
            "f1_weighted": 0.502381505214791
          },
          {
            "accuracy": 0.48106246925725527,
            "f1": 0.46788255729399186,
            "f1_weighted": 0.48710724749984224
          },
          {
            "accuracy": 0.5213969503197246,
            "f1": 0.48412303484785946,
            "f1_weighted": 0.5317497023465142
          }
        ]
      }
    ]
  },
  "task_name": "MassiveIntentClassification"
}