{
  "dataset_revision": "fad2c6e8459f9e1c45d9315f4953d921437d70f8",
  "evaluation_time": 38.0856511592865,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.5062205783456625,
        "f1": 0.4881064275653116,
        "f1_weighted": 0.5087752593826317,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5062205783456625,
        "scores_per_experiment": [
          {
            "accuracy": 0.5289172831203766,
            "f1": 0.5125095782079878,
            "f1_weighted": 0.5305906207478414
          },
          {
            "accuracy": 0.5302622730329523,
            "f1": 0.5098683697657393,
            "f1_weighted": 0.5358981248241088
          },
          {
            "accuracy": 0.488231338264963,
            "f1": 0.47266950840158284,
            "f1_weighted": 0.49698768675844707
          },
          {
            "accuracy": 0.49260255548083387,
            "f1": 0.47703003196895577,
            "f1_weighted": 0.50006821735125
          },
          {
            "accuracy": 0.5144586415601883,
            "f1": 0.48462046672365056,
            "f1_weighted": 0.5098800863284499
          },
          {
            "accuracy": 0.4979825151311365,
            "f1": 0.48182329431099913,
            "f1_weighted": 0.501815709442453
          },
          {
            "accuracy": 0.5161398789509078,
            "f1": 0.4940482691118079,
            "f1_weighted": 0.5213794827507795
          },
          {
            "accuracy": 0.5252185608607935,
            "f1": 0.5073048601409387,
            "f1_weighted": 0.5248580330465934
          },
          {
            "accuracy": 0.4949562878278413,
            "f1": 0.47575591746811546,
            "f1_weighted": 0.49658688669229284
          },
          {
            "accuracy": 0.4734364492266308,
            "f1": 0.4654339795533376,
            "f1_weighted": 0.4696877458841014
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.4966551893753074,
        "f1": 0.4851718488967133,
        "f1_weighted": 0.4965810007383154,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.4966551893753074,
        "scores_per_experiment": [
          {
            "accuracy": 0.5051647811116576,
            "f1": 0.5004449890661258,
            "f1_weighted": 0.5072743186872187
          },
          {
            "accuracy": 0.5125430398425972,
            "f1": 0.5035274403942139,
            "f1_weighted": 0.5117059755689243
          },
          {
            "accuracy": 0.4840137727496311,
            "f1": 0.47489772899300753,
            "f1_weighted": 0.48844202783385937
          },
          {
            "accuracy": 0.4717166748647319,
            "f1": 0.4544020345885646,
            "f1_weighted": 0.47627384733988504
          },
          {
            "accuracy": 0.49778652238071813,
            "f1": 0.4780604235834127,
            "f1_weighted": 0.4918284284528798
          },
          {
            "accuracy": 0.4894244958189867,
            "f1": 0.4809545749938391,
            "f1_weighted": 0.48447817065907456
          },
          {
            "accuracy": 0.49483521888834237,
            "f1": 0.48003552860715487,
            "f1_weighted": 0.5002999819226502
          },
          {
            "accuracy": 0.5253320216428923,
            "f1": 0.5132797983175253,
            "f1_weighted": 0.5234014389248036
          },
          {
            "accuracy": 0.5012297097884899,
            "f1": 0.48577951861731605,
            "f1_weighted": 0.5006021738367407
          },
          {
            "accuracy": 0.48450565666502704,
            "f1": 0.4803364518059725,
            "f1_weighted": 0.48150364415711777
          }
        ]
      }
    ]
  },
  "task_name": "MassiveScenarioClassification"
}