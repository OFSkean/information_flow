{
  "dataset_revision": "ae001d0e6b1228650b7bd1c2c65fb50ad11a8aba",
  "evaluation_time": 87.84820175170898,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.5811217510259917,
        "f1": 0.3805102382107625,
        "f1_weighted": 0.6314002188547772,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5811217510259917,
        "scores_per_experiment": [
          {
            "accuracy": 0.5715914272685818,
            "f1": 0.37575388252000325,
            "f1_weighted": 0.6223069528164175
          },
          {
            "accuracy": 0.5845873233014136,
            "f1": 0.3709536527289826,
            "f1_weighted": 0.633302823233677
          },
          {
            "accuracy": 0.5766073871409029,
            "f1": 0.3841176221222141,
            "f1_weighted": 0.6265299430168733
          },
          {
            "accuracy": 0.5941632466940264,
            "f1": 0.38540032299439175,
            "f1_weighted": 0.6401221446914693
          },
          {
            "accuracy": 0.6082991336069311,
            "f1": 0.3881409013932104,
            "f1_weighted": 0.6615707752340304
          },
          {
            "accuracy": 0.5959872321021432,
            "f1": 0.3899709883039632,
            "f1_weighted": 0.64492086274088
          },
          {
            "accuracy": 0.5754673962608299,
            "f1": 0.3890373032989077,
            "f1_weighted": 0.6232619101759264
          },
          {
            "accuracy": 0.5827633378932968,
            "f1": 0.398189378174133,
            "f1_weighted": 0.6304487680281156
          },
          {
            "accuracy": 0.5802553579571363,
            "f1": 0.3884465276656814,
            "f1_weighted": 0.6336987920939305
          },
          {
            "accuracy": 0.5414956680346558,
            "f1": 0.33509180290613766,
            "f1_weighted": 0.5978392165164519
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.5914541387024609,
        "f1": 0.36797567918436214,
        "f1_weighted": 0.6443918637719932,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5914541387024609,
        "scores_per_experiment": [
          {
            "accuracy": 0.574496644295302,
            "f1": 0.34266682250411473,
            "f1_weighted": 0.6289172463214686
          },
          {
            "accuracy": 0.5986577181208054,
            "f1": 0.36191656135300065,
            "f1_weighted": 0.6524869603680552
          },
          {
            "accuracy": 0.6008948545861298,
            "f1": 0.3784615754274055,
            "f1_weighted": 0.6536516917098709
          },
          {
            "accuracy": 0.6049217002237136,
            "f1": 0.3683637805976098,
            "f1_weighted": 0.6558630843849693
          },
          {
            "accuracy": 0.6357941834451901,
            "f1": 0.39150351187483584,
            "f1_weighted": 0.688579162131864
          },
          {
            "accuracy": 0.5919463087248322,
            "f1": 0.3741287440593445,
            "f1_weighted": 0.6449424620620091
          },
          {
            "accuracy": 0.585682326621924,
            "f1": 0.38112857522173926,
            "f1_weighted": 0.6357403417515517
          },
          {
            "accuracy": 0.5932885906040268,
            "f1": 0.3782543467201194,
            "f1_weighted": 0.6440192684156242
          },
          {
            "accuracy": 0.574496644295302,
            "f1": 0.3550445047924324,
            "f1_weighted": 0.6290308673618366
          },
          {
            "accuracy": 0.5543624161073826,
            "f1": 0.34828836929302004,
            "f1_weighted": 0.6106875532126826
          }
        ]
      }
    ]
  },
  "task_name": "MTOPIntentClassification"
}