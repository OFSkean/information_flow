{
  "dataset_revision": "4672e20407010da34463acc759c162ca9734bca6",
  "evaluation_time": 71.53833985328674,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.44377942165433765,
        "f1": 0.4130028122708425,
        "f1_weighted": 0.4522812883169509,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.44377942165433765,
        "scores_per_experiment": [
          {
            "accuracy": 0.44788164088769333,
            "f1": 0.4181508277948456,
            "f1_weighted": 0.45925867076324306
          },
          {
            "accuracy": 0.44014794889038333,
            "f1": 0.41326349493061076,
            "f1_weighted": 0.4461423301042537
          },
          {
            "accuracy": 0.4425016812373907,
            "f1": 0.40955636803555306,
            "f1_weighted": 0.45117236140870026
          },
          {
            "accuracy": 0.4613315400134499,
            "f1": 0.42176916123731495,
            "f1_weighted": 0.4747193714063972
          },
          {
            "accuracy": 0.4556153328850034,
            "f1": 0.41396526569477304,
            "f1_weighted": 0.46072104681956083
          },
          {
            "accuracy": 0.4445191661062542,
            "f1": 0.41673900435870476,
            "f1_weighted": 0.4532882294850804
          },
          {
            "accuracy": 0.41862811028917285,
            "f1": 0.39756072858798613,
            "f1_weighted": 0.42110389863887765
          },
          {
            "accuracy": 0.4182918628110289,
            "f1": 0.3890022212025278,
            "f1_weighted": 0.43679435028842695
          },
          {
            "accuracy": 0.44014794889038333,
            "f1": 0.41239627592096834,
            "f1_weighted": 0.4462279460187335
          },
          {
            "accuracy": 0.468728984532616,
            "f1": 0.43762477494514024,
            "f1_weighted": 0.4733846782362361
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.45209050664043293,
        "f1": 0.4185151866820032,
        "f1_weighted": 0.45927883599653346,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.45209050664043293,
        "scores_per_experiment": [
          {
            "accuracy": 0.455976389572061,
            "f1": 0.42742021147312353,
            "f1_weighted": 0.4611942295400906
          },
          {
            "accuracy": 0.4426955238563699,
            "f1": 0.4041378777254147,
            "f1_weighted": 0.44972479972069984
          },
          {
            "accuracy": 0.4574520413182489,
            "f1": 0.42677925001142913,
            "f1_weighted": 0.4655456162487527
          },
          {
            "accuracy": 0.4682734874569602,
            "f1": 0.4367178949876959,
            "f1_weighted": 0.4764883072356481
          },
          {
            "accuracy": 0.4663059517953763,
            "f1": 0.43249562791913615,
            "f1_weighted": 0.475365593644015
          },
          {
            "accuracy": 0.4648303000491884,
            "f1": 0.4258742320297323,
            "f1_weighted": 0.4724058002534255
          },
          {
            "accuracy": 0.4181013280865716,
            "f1": 0.4010803388986574,
            "f1_weighted": 0.4158730167056263
          },
          {
            "accuracy": 0.426463354648303,
            "f1": 0.3892322036655495,
            "f1_weighted": 0.4419410986282255
          },
          {
            "accuracy": 0.4466305951795376,
            "f1": 0.41463261232060133,
            "f1_weighted": 0.4538360477138816
          },
          {
            "accuracy": 0.47417609444171177,
            "f1": 0.42678161778869167,
            "f1_weighted": 0.4804138502749689
          }
        ]
      }
    ]
  },
  "task_name": "MassiveIntentClassification"
}