{
  "dataset_revision": "fad2c6e8459f9e1c45d9315f4953d921437d70f8",
  "evaluation_time": 38.94676375389099,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.5118022864828513,
        "f1": 0.49262513658514395,
        "f1_weighted": 0.5164715366450171,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5118022864828513,
        "scores_per_experiment": [
          {
            "accuracy": 0.5238735709482178,
            "f1": 0.504260343787791,
            "f1_weighted": 0.5271893751639303
          },
          {
            "accuracy": 0.5104236718224613,
            "f1": 0.4887166395609588,
            "f1_weighted": 0.5156233027533766
          },
          {
            "accuracy": 0.503698722259583,
            "f1": 0.4858274771710673,
            "f1_weighted": 0.5113910125037309
          },
          {
            "accuracy": 0.5171486213853396,
            "f1": 0.49896380274333,
            "f1_weighted": 0.5326486314871437
          },
          {
            "accuracy": 0.5366509751176867,
            "f1": 0.5076211948395114,
            "f1_weighted": 0.5332398092668982
          },
          {
            "accuracy": 0.49428379287155344,
            "f1": 0.4796779878661397,
            "f1_weighted": 0.5005269816262597
          },
          {
            "accuracy": 0.49865501008742436,
            "f1": 0.4772489428480755,
            "f1_weighted": 0.5030802949829107
          },
          {
            "accuracy": 0.5228648285137861,
            "f1": 0.5050009961833652,
            "f1_weighted": 0.5278098183554542
          },
          {
            "accuracy": 0.5023537323470074,
            "f1": 0.48244049946360384,
            "f1_weighted": 0.5012869930761296
          },
          {
            "accuracy": 0.508069939475454,
            "f1": 0.4964934813875964,
            "f1_weighted": 0.5119191472343377
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.5002459419576979,
        "f1": 0.4859063124966422,
        "f1_weighted": 0.5036240625443457,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5002459419576979,
        "scores_per_experiment": [
          {
            "accuracy": 0.5164781111657649,
            "f1": 0.5012182774563514,
            "f1_weighted": 0.5200136123554406
          },
          {
            "accuracy": 0.49335956714215445,
            "f1": 0.4793772079849611,
            "f1_weighted": 0.4985774185358109
          },
          {
            "accuracy": 0.49335956714215445,
            "f1": 0.47870832095628973,
            "f1_weighted": 0.49988739119758024
          },
          {
            "accuracy": 0.4938514510575504,
            "f1": 0.4813326600184319,
            "f1_weighted": 0.505920087504227
          },
          {
            "accuracy": 0.5189375307427447,
            "f1": 0.4892383579047553,
            "f1_weighted": 0.5175007826532005
          },
          {
            "accuracy": 0.4938514510575504,
            "f1": 0.48825243275387603,
            "f1_weighted": 0.493995312845422
          },
          {
            "accuracy": 0.4717166748647319,
            "f1": 0.4716711021490682,
            "f1_weighted": 0.47479435624194355
          },
          {
            "accuracy": 0.5110673880964093,
            "f1": 0.49055958148873935,
            "f1_weighted": 0.5160077830390213
          },
          {
            "accuracy": 0.5031972454500738,
            "f1": 0.48437735142770777,
            "f1_weighted": 0.5008520410022997
          },
          {
            "accuracy": 0.5066404328578455,
            "f1": 0.49432783282624104,
            "f1_weighted": 0.5086918400685104
          }
        ]
      }
    ]
  },
  "task_name": "MassiveScenarioClassification"
}