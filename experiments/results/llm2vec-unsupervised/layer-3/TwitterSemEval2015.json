{"dataset_revision": "70970daeab8776df92f5ea462b6173c0b46fd2d1", "evaluation_time": 201.1978440284729, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"cosine": {"accuracy": 0.8151636168564106, "accuracy_threshold": 0.840399980545044, "ap": 0.5825710963120083, "f1": 0.5583266291230893, "f1_threshold": 0.7856987118721008, "precision": 0.494603950315618, "recall": 0.6408970976253299}, "dot": {"accuracy": 0.7810693210943553, "accuracy_threshold": 1.2805461883544922, "ap": 0.4055620209596636, "f1": 0.44528077646825787, "f1_threshold": 1.0410243272781372, "precision": 0.35642936419850957, "recall": 0.5931398416886543}, "euclidean": {"accuracy": 0.8117065029504679, "accuracy_threshold": 0.6698037981987, "ap": 0.5566225652396022, "f1": 0.5461972846329449, "f1_threshold": 0.7478693723678589, "precision": 0.5014339289653651, "recall": 0.5997361477572559}, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.5825710963120083, "manhattan": {"accuracy": 0.8118257137748107, "accuracy_threshold": 33.0860595703125, "ap": 0.5581274761228425, "f1": 0.5481063279228418, "f1_threshold": 36.88291931152344, "precision": 0.49448217317487264, "recall": 0.6147757255936676}, "max": {"accuracy": 0.8151636168564106, "ap": 0.5825710963120083, "f1": 0.5583266291230893}, "similarity": {"accuracy": 0.8151636168564106, "accuracy_threshold": 0.840399980545044, "ap": 0.5825710963120083, "f1": 0.5583266291230893, "f1_threshold": 0.7856987118721008, "precision": 0.494603950315618, "recall": 0.6408970976253299}}]}, "task_name": "TwitterSemEval2015", "avg_layerwise_entropy": 0.05945021878730748, "num_samples": 64}