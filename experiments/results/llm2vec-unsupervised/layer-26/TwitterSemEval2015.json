{"dataset_revision": "70970daeab8776df92f5ea462b6173c0b46fd2d1", "evaluation_time": 194.97686886787415, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"cosine": {"accuracy": 0.8396018358466949, "accuracy_threshold": 0.821361243724823, "ap": 0.6657715720863042, "f1": 0.6242463117382938, "f1_threshold": 0.8025673627853394, "precision": 0.6074906367041198, "recall": 0.6419525065963061}, "dot": {"accuracy": 0.7855397270072123, "accuracy_threshold": 239.27279663085938, "ap": 0.41915628075285605, "f1": 0.43595845954132406, "f1_threshold": 205.00369262695312, "precision": 0.36945361202786947, "recall": 0.5316622691292876}, "euclidean": {"accuracy": 0.8375752518328664, "accuracy_threshold": 9.357830047607422, "ap": 0.6585105407637663, "f1": 0.6216149679960611, "f1_threshold": 10.215221405029297, "precision": 0.5826026765113059, "recall": 0.6662269129287599}, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.6657715720863042, "manhattan": {"accuracy": 0.838350122191095, "accuracy_threshold": 461.7752380371094, "ap": 0.660940674754732, "f1": 0.6242688238954575, "f1_threshold": 509.43341064453125, "precision": 0.5908127208480566, "recall": 0.6617414248021108}, "max": {"accuracy": 0.8396018358466949, "ap": 0.6657715720863042, "f1": 0.6242688238954575}, "similarity": {"accuracy": 0.8396018358466949, "accuracy_threshold": 0.821361243724823, "ap": 0.6657715720863042, "f1": 0.6242463117382938, "f1_threshold": 0.8025673627853394, "precision": 0.6074906367041198, "recall": 0.6419525065963061}}]}, "task_name": "TwitterSemEval2015", "avg_layerwise_entropy": 0.7993061749106772, "num_samples": 64}