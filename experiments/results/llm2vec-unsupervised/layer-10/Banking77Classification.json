{"dataset_revision": "0fd18e25b25c072e09e0d92ab615fda904d66300", "evaluation_time": 159.81648659706116, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"accuracy": 0.719090909090909, "f1": 0.717420075820978, "f1_weighted": 0.717420075820978, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.719090909090909, "scores_per_experiment": [{"accuracy": 0.6967532467532468, "f1": 0.6941734614458734, "f1_weighted": 0.6941734614458736}, {"accuracy": 0.7318181818181818, "f1": 0.7310404685709292, "f1_weighted": 0.7310404685709293}, {"accuracy": 0.724025974025974, "f1": 0.720593679770692, "f1_weighted": 0.720593679770692}, {"accuracy": 0.7090909090909091, "f1": 0.7062087738141689, "f1_weighted": 0.7062087738141689}, {"accuracy": 0.7217532467532467, "f1": 0.7225623879612194, "f1_weighted": 0.7225623879612193}, {"accuracy": 0.7311688311688311, "f1": 0.730384690917254, "f1_weighted": 0.7303846909172538}, {"accuracy": 0.7077922077922078, "f1": 0.7039533680741292, "f1_weighted": 0.7039533680741292}, {"accuracy": 0.7217532467532467, "f1": 0.7216804185494136, "f1_weighted": 0.7216804185494134}, {"accuracy": 0.711038961038961, "f1": 0.7103213189761297, "f1_weighted": 0.7103213189761298}, {"accuracy": 0.7357142857142858, "f1": 0.7332821901299709, "f1_weighted": 0.7332821901299709}]}]}, "task_name": "Banking77Classification", "avg_layerwise_entropy": 0.13365797357540843, "num_samples": 64}