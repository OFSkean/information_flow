{
  "dataset_revision": "70970daeab8776df92f5ea462b6173c0b46fd2d1",
  "evaluation_time": 195.57904815673828,
  "kg_co2_emissions": null,
  "mteb_version": "1.12.49",
  "scores": {
    "test": [
      {
        "cosine": {
          "accuracy": 0.8284556237706384,
          "accuracy_threshold": 0.8560903668403625,
          "ap": 0.6264865103956018,
          "f1": 0.5937423612808603,
          "f1_threshold": 0.8236279487609863,
          "precision": 0.5530510018214936,
          "recall": 0.6408970976253299
        },
        "dot": {
          "accuracy": 0.779042737080527,
          "accuracy_threshold": 13.772985458374023,
          "ap": 0.39126400910336395,
          "f1": 0.44161933098909023,
          "f1_threshold": 11.324464797973633,
          "precision": 0.3354335022599644,
          "recall": 0.6461741424802111
        },
        "euclidean": {
          "accuracy": 0.8282768075341241,
          "accuracy_threshold": 2.0617854595184326,
          "ap": 0.623047396332394,
          "f1": 0.5954482117974919,
          "f1_threshold": 2.2787084579467773,
          "precision": 0.5317295727913729,
          "recall": 0.6765171503957783
        },
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.6265144929600723,
        "manhattan": {
          "accuracy": 0.8292900995410383,
          "accuracy_threshold": 101.59512329101562,
          "ap": 0.6265144929600723,
          "f1": 0.5975567190226876,
          "f1_threshold": 112.53196716308594,
          "precision": 0.5344432882414152,
          "recall": 0.6775725593667546
        },
        "max": {
          "accuracy": 0.8292900995410383,
          "ap": 0.6265144929600723,
          "f1": 0.5975567190226876
        },
        "similarity": {
          "accuracy": 0.8284556237706384,
          "accuracy_threshold": 0.8560903668403625,
          "ap": 0.6264865103956018,
          "f1": 0.5937423612808603,
          "f1_threshold": 0.8236279487609863,
          "precision": 0.5530510018214936,
          "recall": 0.6408970976253299
        }
      }
    ]
  },
  "task_name": "TwitterSemEval2015"
}