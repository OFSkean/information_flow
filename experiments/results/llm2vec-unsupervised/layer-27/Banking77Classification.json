{"dataset_revision": "0fd18e25b25c072e09e0d92ab615fda904d66300", "evaluation_time": 167.61770153045654, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"accuracy": 0.8375974025974026, "f1": 0.8369330469235235, "f1_weighted": 0.8369330469235237, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.8375974025974026, "scores_per_experiment": [{"accuracy": 0.8389610389610389, "f1": 0.8390599321270652, "f1_weighted": 0.8390599321270652}, {"accuracy": 0.8418831168831169, "f1": 0.8421240891083895, "f1_weighted": 0.8421240891083894}, {"accuracy": 0.8321428571428572, "f1": 0.8305824586666134, "f1_weighted": 0.8305824586666134}, {"accuracy": 0.8350649350649351, "f1": 0.8334527259311459, "f1_weighted": 0.833452725931146}, {"accuracy": 0.8311688311688312, "f1": 0.8305891132335802, "f1_weighted": 0.8305891132335804}, {"accuracy": 0.8435064935064935, "f1": 0.8439444350305133, "f1_weighted": 0.8439444350305136}, {"accuracy": 0.839935064935065, "f1": 0.8391447543283088, "f1_weighted": 0.8391447543283088}, {"accuracy": 0.8353896103896103, "f1": 0.8337421961762691, "f1_weighted": 0.833742196176269}, {"accuracy": 0.85, "f1": 0.8493265809931779, "f1_weighted": 0.849326580993178}, {"accuracy": 0.827922077922078, "f1": 0.8273641836401725, "f1_weighted": 0.8273641836401724}]}]}, "task_name": "Banking77Classification", "avg_layerwise_entropy": 0.6778430437058435, "num_samples": 64}