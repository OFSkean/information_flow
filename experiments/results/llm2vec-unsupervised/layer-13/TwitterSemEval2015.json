{
  "dataset_revision": "70970daeab8776df92f5ea462b6173c0b46fd2d1",
  "evaluation_time": 195.4169602394104,
  "kg_co2_emissions": null,
  "mteb_version": "1.12.49",
  "scores": {
    "test": [
      {
        "cosine": {
          "accuracy": 0.8319723430887525,
          "accuracy_threshold": 0.8786147832870483,
          "ap": 0.6370676204613999,
          "f1": 0.600801402454295,
          "f1_threshold": 0.86236572265625,
          "precision": 0.5717349857006673,
          "recall": 0.632981530343008
        },
        "dot": {
          "accuracy": 0.780175239911784,
          "accuracy_threshold": 24.406164169311523,
          "ap": 0.38124826379829085,
          "f1": 0.4237818506928923,
          "f1_threshold": 20.24554443359375,
          "precision": 0.3204868154158215,
          "recall": 0.6253298153034301
        },
        "euclidean": {
          "accuracy": 0.8298265482505811,
          "accuracy_threshold": 2.422797203063965,
          "ap": 0.634201908621721,
          "f1": 0.6005516549648947,
          "f1_threshold": 2.582756996154785,
          "precision": 0.5721452460582895,
          "recall": 0.6319261213720316
        },
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.6371678010260623,
        "manhattan": {
          "accuracy": 0.8303033915479525,
          "accuracy_threshold": 116.09268951416016,
          "ap": 0.6371678010260623,
          "f1": 0.6017505470459519,
          "f1_threshold": 127.68973541259766,
          "precision": 0.5579350766456267,
          "recall": 0.6530343007915568
        },
        "max": {
          "accuracy": 0.8319723430887525,
          "ap": 0.6371678010260623,
          "f1": 0.6017505470459519
        },
        "similarity": {
          "accuracy": 0.8319723430887525,
          "accuracy_threshold": 0.8786147832870483,
          "ap": 0.6370676204613999,
          "f1": 0.600801402454295,
          "f1_threshold": 0.86236572265625,
          "precision": 0.5717349857006673,
          "recall": 0.632981530343008
        }
      }
    ]
  },
  "task_name": "TwitterSemEval2015"
}