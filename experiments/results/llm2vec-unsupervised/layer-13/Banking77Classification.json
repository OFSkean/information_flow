{"dataset_revision": "0fd18e25b25c072e09e0d92ab615fda904d66300", "evaluation_time": 171.42016625404358, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"accuracy": 0.7040584415584417, "f1": 0.7022768986136358, "f1_weighted": 0.7022768986136358, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.7040584415584417, "scores_per_experiment": [{"accuracy": 0.7188311688311688, "f1": 0.716776985534182, "f1_weighted": 0.7167769855341818}, {"accuracy": 0.7227272727272728, "f1": 0.7209374804611902, "f1_weighted": 0.7209374804611904}, {"accuracy": 0.7077922077922078, "f1": 0.7052787548352323, "f1_weighted": 0.7052787548352322}, {"accuracy": 0.702922077922078, "f1": 0.7018919258727888, "f1_weighted": 0.7018919258727888}, {"accuracy": 0.6941558441558442, "f1": 0.6899294317934997, "f1_weighted": 0.6899294317934996}, {"accuracy": 0.7003246753246753, "f1": 0.6988253416321246, "f1_weighted": 0.6988253416321246}, {"accuracy": 0.6896103896103896, "f1": 0.6858727380565873, "f1_weighted": 0.6858727380565874}, {"accuracy": 0.7003246753246753, "f1": 0.6993838061691384, "f1_weighted": 0.6993838061691383}, {"accuracy": 0.7051948051948052, "f1": 0.7046446381743435, "f1_weighted": 0.7046446381743438}, {"accuracy": 0.6987012987012987, "f1": 0.6992278836072717, "f1_weighted": 0.6992278836072717}]}]}, "task_name": "Banking77Classification", "avg_layerwise_entropy": 0.1690264407892445, "num_samples": 64}