{"dataset_revision": "0fd18e25b25c072e09e0d92ab615fda904d66300", "evaluation_time": 133.58735156059265, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"accuracy": 0.8411363636363636, "f1": 0.8405748436599735, "f1_weighted": 0.8405748436599738, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.8411363636363636, "scores_per_experiment": [{"accuracy": 0.8396103896103896, "f1": 0.837620223921697, "f1_weighted": 0.837620223921697}, {"accuracy": 0.8396103896103896, "f1": 0.8394515778756143, "f1_weighted": 0.8394515778756145}, {"accuracy": 0.8506493506493507, "f1": 0.849816580640461, "f1_weighted": 0.8498165806404611}, {"accuracy": 0.8412337662337662, "f1": 0.8420785041063039, "f1_weighted": 0.842078504106304}, {"accuracy": 0.8451298701298702, "f1": 0.8447240015323605, "f1_weighted": 0.8447240015323607}, {"accuracy": 0.8353896103896103, "f1": 0.8345693254111225, "f1_weighted": 0.8345693254111222}, {"accuracy": 0.8448051948051948, "f1": 0.8442728605677191, "f1_weighted": 0.8442728605677196}, {"accuracy": 0.8324675324675325, "f1": 0.8307680627898842, "f1_weighted": 0.8307680627898844}, {"accuracy": 0.8438311688311688, "f1": 0.8445470680547543, "f1_weighted": 0.8445470680547543}, {"accuracy": 0.8386363636363636, "f1": 0.8379002316998194, "f1_weighted": 0.8379002316998198}]}]}, "task_name": "Banking77Classification", "avg_layerwise_entropy": 0.8991184089881673, "num_samples": 64}