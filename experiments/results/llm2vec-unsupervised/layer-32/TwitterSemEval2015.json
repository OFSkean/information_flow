{
  "dataset_revision": "70970daeab8776df92f5ea462b6173c0b46fd2d1",
  "evaluation_time": 198.65933632850647,
  "kg_co2_emissions": null,
  "mteb_version": "1.12.49",
  "scores": {
    "test": [
      {
        "cosine": {
          "accuracy": 0.841151576563152,
          "accuracy_threshold": 0.6945993900299072,
          "ap": 0.6619260188539045,
          "f1": 0.6187788609543355,
          "f1_threshold": 0.6544541120529175,
          "precision": 0.6020968547179231,
          "recall": 0.6364116094986807
        },
        "dot": {
          "accuracy": 0.8165345413363534,
          "accuracy_threshold": 7118.34375,
          "ap": 0.5850170221675131,
          "f1": 0.5575430307121161,
          "f1_threshold": 6103.7646484375,
          "precision": 0.4859776426750343,
          "recall": 0.6538258575197889
        },
        "euclidean": {
          "accuracy": 0.8357870894677236,
          "accuracy_threshold": 74.91444396972656,
          "ap": 0.645661781040293,
          "f1": 0.6074131076054893,
          "f1_threshold": 81.63571166992188,
          "precision": 0.5909658098327926,
          "recall": 0.6248021108179419
        },
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.6619260188539045,
        "manhattan": {
          "accuracy": 0.8361447219407522,
          "accuracy_threshold": 3808.0263671875,
          "ap": 0.650699599689371,
          "f1": 0.6136363636363636,
          "f1_threshold": 4148.7958984375,
          "precision": 0.5945076694705591,
          "recall": 0.6340369393139842
        },
        "max": {
          "accuracy": 0.841151576563152,
          "ap": 0.6619260188539045,
          "f1": 0.6187788609543355
        },
        "similarity": {
          "accuracy": 0.841151576563152,
          "accuracy_threshold": 0.6945993900299072,
          "ap": 0.6619260188539045,
          "f1": 0.6187788609543355,
          "f1_threshold": 0.6544541120529175,
          "precision": 0.6020968547179231,
          "recall": 0.6364116094986807
        }
      }
    ]
  },
  "task_name": "TwitterSemEval2015"
}