{"dataset_revision": "0fd18e25b25c072e09e0d92ab615fda904d66300", "evaluation_time": 172.3337585926056, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"accuracy": 0.7091233766233767, "f1": 0.7065466012727886, "f1_weighted": 0.7065466012727886, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.7091233766233767, "scores_per_experiment": [{"accuracy": 0.7107142857142857, "f1": 0.7059246978529701, "f1_weighted": 0.7059246978529704}, {"accuracy": 0.7146103896103896, "f1": 0.7117391740504635, "f1_weighted": 0.7117391740504634}, {"accuracy": 0.7042207792207792, "f1": 0.7005971956541576, "f1_weighted": 0.7005971956541578}, {"accuracy": 0.714935064935065, "f1": 0.7133490557238267, "f1_weighted": 0.7133490557238266}, {"accuracy": 0.714935064935065, "f1": 0.7120062294885581, "f1_weighted": 0.712006229488558}, {"accuracy": 0.7256493506493507, "f1": 0.7236959145208843, "f1_weighted": 0.7236959145208843}, {"accuracy": 0.7084415584415584, "f1": 0.7066211385930968, "f1_weighted": 0.7066211385930969}, {"accuracy": 0.7061688311688312, "f1": 0.7031587036302694, "f1_weighted": 0.7031587036302697}, {"accuracy": 0.7038961038961039, "f1": 0.7014777901786658, "f1_weighted": 0.7014777901786661}, {"accuracy": 0.6876623376623376, "f1": 0.686896113034993, "f1_weighted": 0.6868961130349929}]}]}, "task_name": "Banking77Classification", "avg_layerwise_entropy": 0.18603546653301128, "num_samples": 64}