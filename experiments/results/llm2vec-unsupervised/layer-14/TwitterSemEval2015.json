{"dataset_revision": "70970daeab8776df92f5ea462b6173c0b46fd2d1", "evaluation_time": 189.14002132415771, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"cosine": {"accuracy": 0.8337605054538952, "accuracy_threshold": 0.8698508143424988, "ap": 0.6451218989916987, "f1": 0.6018472291562657, "f1_threshold": 0.850208044052124, "precision": 0.5710563713879678, "recall": 0.6361477572559366}, "dot": {"accuracy": 0.7811885319186982, "accuracy_threshold": 23.824562072753906, "ap": 0.4143893128566746, "f1": 0.451761102603369, "f1_threshold": 19.495174407958984, "precision": 0.333375188347564, "recall": 0.7005277044854882}, "euclidean": {"accuracy": 0.8307206294331525, "accuracy_threshold": 2.4335341453552246, "ap": 0.6406873076191556, "f1": 0.6042060193930234, "f1_threshold": 2.6726326942443848, "precision": 0.5779330281859792, "recall": 0.632981530343008}, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.6456190873986787, "manhattan": {"accuracy": 0.8331048459200096, "accuracy_threshold": 119.0174560546875, "ap": 0.6456190873986787, "f1": 0.6100905562742561, "f1_threshold": 129.28756713867188, "precision": 0.5984771573604061, "recall": 0.6221635883905013}, "max": {"accuracy": 0.8337605054538952, "ap": 0.6456190873986787, "f1": 0.6100905562742561}, "similarity": {"accuracy": 0.8337605054538952, "accuracy_threshold": 0.8698508143424988, "ap": 0.6451218989916987, "f1": 0.6018472291562657, "f1_threshold": 0.850208044052124, "precision": 0.5710563713879678, "recall": 0.6361477572559366}}]}, "task_name": "TwitterSemEval2015", "avg_layerwise_entropy": 0.3257155538044028, "num_samples": 64}