{"dataset_revision": "0fd18e25b25c072e09e0d92ab615fda904d66300", "evaluation_time": 175.02053380012512, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"accuracy": 0.7801298701298701, "f1": 0.7790843895609941, "f1_weighted": 0.7790843895609941, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.7801298701298701, "scores_per_experiment": [{"accuracy": 0.7788961038961039, "f1": 0.7767802062939853, "f1_weighted": 0.7767802062939853}, {"accuracy": 0.7847402597402597, "f1": 0.7825745465092369, "f1_weighted": 0.782574546509237}, {"accuracy": 0.7863636363636364, "f1": 0.7872233348030774, "f1_weighted": 0.7872233348030774}, {"accuracy": 0.7772727272727272, "f1": 0.7761456636701818, "f1_weighted": 0.7761456636701819}, {"accuracy": 0.7805194805194805, "f1": 0.7808262859919193, "f1_weighted": 0.7808262859919195}, {"accuracy": 0.7834415584415585, "f1": 0.7831959645960787, "f1_weighted": 0.7831959645960785}, {"accuracy": 0.7857142857142857, "f1": 0.7839886189659089, "f1_weighted": 0.783988618965909}, {"accuracy": 0.7724025974025974, "f1": 0.7699204920021452, "f1_weighted": 0.7699204920021453}, {"accuracy": 0.772077922077922, "f1": 0.7719207755790288, "f1_weighted": 0.7719207755790288}, {"accuracy": 0.7798701298701298, "f1": 0.7782680071983786, "f1_weighted": 0.7782680071983789}]}]}, "task_name": "Banking77Classification", "avg_layerwise_entropy": 0.2560124053072393, "num_samples": 64}