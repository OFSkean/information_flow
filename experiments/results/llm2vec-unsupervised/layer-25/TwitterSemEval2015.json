{"dataset_revision": "70970daeab8776df92f5ea462b6173c0b46fd2d1", "evaluation_time": 191.93115425109863, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"cosine": {"accuracy": 0.841568814448352, "accuracy_threshold": 0.8213714361190796, "ap": 0.671200920851794, "f1": 0.6281957633308984, "f1_threshold": 0.792373538017273, "precision": 0.5831826401446655, "recall": 0.6807387862796834}, "dot": {"accuracy": 0.7901889491565834, "accuracy_threshold": 196.01730346679688, "ap": 0.4530327289168923, "f1": 0.4596467695059244, "f1_threshold": 167.8694610595703, "precision": 0.39875872769588827, "recall": 0.5424802110817942}, "euclidean": {"accuracy": 0.8374560410085236, "accuracy_threshold": 8.34298324584961, "ap": 0.66112444160384, "f1": 0.625823288181931, "f1_threshold": 9.264406204223633, "precision": 0.5914963589382194, "recall": 0.6643799472295514}, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.671200920851794, "manhattan": {"accuracy": 0.8390057817249806, "accuracy_threshold": 420.18353271484375, "ap": 0.6629548504572524, "f1": 0.6296755016101065, "f1_threshold": 465.02081298828125, "precision": 0.5933706816059757, "recall": 0.670712401055409}, "max": {"accuracy": 0.841568814448352, "ap": 0.671200920851794, "f1": 0.6296755016101065}, "similarity": {"accuracy": 0.841568814448352, "accuracy_threshold": 0.8213714361190796, "ap": 0.671200920851794, "f1": 0.6281957633308984, "f1_threshold": 0.792373538017273, "precision": 0.5831826401446655, "recall": 0.6807387862796834}}]}, "task_name": "TwitterSemEval2015", "avg_layerwise_entropy": 0.7791670010644184, "num_samples": 64}