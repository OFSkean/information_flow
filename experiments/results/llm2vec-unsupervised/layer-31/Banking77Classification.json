{"dataset_revision": "0fd18e25b25c072e09e0d92ab615fda904d66300", "evaluation_time": 173.37940192222595, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"accuracy": 0.8412012987012988, "f1": 0.8406251950505791, "f1_weighted": 0.8406251950505791, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.8412012987012988, "scores_per_experiment": [{"accuracy": 0.8295454545454546, "f1": 0.8289338433628302, "f1_weighted": 0.8289338433628304}, {"accuracy": 0.8503246753246754, "f1": 0.850349667375345, "f1_weighted": 0.8503496673753449}, {"accuracy": 0.8321428571428572, "f1": 0.8308321867905831, "f1_weighted": 0.8308321867905833}, {"accuracy": 0.8409090909090909, "f1": 0.8403623531224215, "f1_weighted": 0.8403623531224216}, {"accuracy": 0.8340909090909091, "f1": 0.8339962276380734, "f1_weighted": 0.8339962276380737}, {"accuracy": 0.85, "f1": 0.8480052520930453, "f1_weighted": 0.8480052520930452}, {"accuracy": 0.8470779220779221, "f1": 0.8454908088040853, "f1_weighted": 0.8454908088040853}, {"accuracy": 0.837987012987013, "f1": 0.8389475014985655, "f1_weighted": 0.8389475014985653}, {"accuracy": 0.8470779220779221, "f1": 0.8465671218121726, "f1_weighted": 0.8465671218121725}, {"accuracy": 0.8428571428571429, "f1": 0.8427669880086683, "f1_weighted": 0.8427669880086684}]}]}, "task_name": "Banking77Classification", "avg_layerwise_entropy": 0.8053950852120721, "num_samples": 64}