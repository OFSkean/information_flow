{"dataset_revision": "70970daeab8776df92f5ea462b6173c0b46fd2d1", "evaluation_time": 201.77266907691956, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"cosine": {"accuracy": 0.8149848006198963, "accuracy_threshold": 0.8260072469711304, "ap": 0.5811141723379146, "f1": 0.5578168362627197, "f1_threshold": 0.7787865400314331, "precision": 0.4965006175380815, "recall": 0.6364116094986807}, "dot": {"accuracy": 0.7823806401621267, "accuracy_threshold": 2.0570545196533203, "ap": 0.4154232030063583, "f1": 0.45683773361020463, "f1_threshold": 1.7215056419372559, "precision": 0.36533291159259845, "recall": 0.6094986807387863}, "euclidean": {"accuracy": 0.8120641354234964, "accuracy_threshold": 0.8749409914016724, "ap": 0.5658038648896278, "f1": 0.555293562838953, "f1_threshold": 0.9860960841178894, "precision": 0.5019181585677749, "recall": 0.6213720316622692}, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.5811141723379146, "manhattan": {"accuracy": 0.8126005841330393, "accuracy_threshold": 43.87498092651367, "ap": 0.5666903124044571, "f1": 0.5550842600022927, "f1_threshold": 48.967262268066406, "precision": 0.4907764038110683, "recall": 0.6387862796833773}, "max": {"accuracy": 0.8149848006198963, "ap": 0.5811141723379146, "f1": 0.5578168362627197}, "similarity": {"accuracy": 0.8149848006198963, "accuracy_threshold": 0.8260072469711304, "ap": 0.5811141723379146, "f1": 0.5578168362627197, "f1_threshold": 0.7787865400314331, "precision": 0.4965006175380815, "recall": 0.6364116094986807}}]}, "task_name": "TwitterSemEval2015", "avg_layerwise_entropy": 0.08523394986993167, "num_samples": 64}