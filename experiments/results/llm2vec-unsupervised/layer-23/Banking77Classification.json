{"dataset_revision": "0fd18e25b25c072e09e0d92ab615fda904d66300", "evaluation_time": 164.76771354675293, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"accuracy": 0.8260389610389611, "f1": 0.8250664401488139, "f1_weighted": 0.8250664401488141, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.8260389610389611, "scores_per_experiment": [{"accuracy": 0.8204545454545454, "f1": 0.8186276024921968, "f1_weighted": 0.8186276024921971}, {"accuracy": 0.8243506493506494, "f1": 0.8241041417835517, "f1_weighted": 0.8241041417835518}, {"accuracy": 0.8357142857142857, "f1": 0.8359117470546987, "f1_weighted": 0.835911747054699}, {"accuracy": 0.8084415584415584, "f1": 0.8091585766733149, "f1_weighted": 0.8091585766733153}, {"accuracy": 0.8402597402597403, "f1": 0.8398807152359453, "f1_weighted": 0.8398807152359454}, {"accuracy": 0.8311688311688312, "f1": 0.8300136746894013, "f1_weighted": 0.8300136746894015}, {"accuracy": 0.8178571428571428, "f1": 0.8154509276914512, "f1_weighted": 0.8154509276914514}, {"accuracy": 0.8272727272727273, "f1": 0.825600613542425, "f1_weighted": 0.825600613542425}, {"accuracy": 0.824025974025974, "f1": 0.8222052919681808, "f1_weighted": 0.8222052919681805}, {"accuracy": 0.8308441558441558, "f1": 0.8297111103569733, "f1_weighted": 0.8297111103569734}]}]}, "task_name": "Banking77Classification", "avg_layerwise_entropy": 0.5557830997574712, "num_samples": 64}