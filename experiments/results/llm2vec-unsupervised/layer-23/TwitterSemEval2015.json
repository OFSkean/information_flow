{"dataset_revision": "70970daeab8776df92f5ea462b6173c0b46fd2d1", "evaluation_time": 201.73671078681946, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"cosine": {"accuracy": 0.840734338677952, "accuracy_threshold": 0.8168509006500244, "ap": 0.6731517367771548, "f1": 0.6297303982191441, "f1_threshold": 0.7799516916275024, "precision": 0.5926443202979516, "recall": 0.6717678100263852}, "dot": {"accuracy": 0.794897776718126, "accuracy_threshold": 119.98033142089844, "ap": 0.4873348520461461, "f1": 0.49570135746606336, "f1_threshold": 105.08773040771484, "precision": 0.43386138613861386, "recall": 0.5781002638522428}, "euclidean": {"accuracy": 0.8382905167789235, "accuracy_threshold": 7.099316596984863, "ap": 0.667050571716867, "f1": 0.6339120797931289, "f1_threshold": 7.716497421264648, "precision": 0.5943200184714846, "recall": 0.679155672823219}, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.6731517367771548, "manhattan": {"accuracy": 0.8400786791440663, "accuracy_threshold": 353.03961181640625, "ap": 0.669515346267551, "f1": 0.6345439290423108, "f1_threshold": 389.085205078125, "precision": 0.581374917636723, "recall": 0.6984168865435356}, "max": {"accuracy": 0.840734338677952, "ap": 0.6731517367771548, "f1": 0.6345439290423108}, "similarity": {"accuracy": 0.840734338677952, "accuracy_threshold": 0.8168509006500244, "ap": 0.6731517367771548, "f1": 0.6297303982191441, "f1_threshold": 0.7799516916275024, "precision": 0.5926443202979516, "recall": 0.6717678100263852}}]}, "task_name": "TwitterSemEval2015", "avg_layerwise_entropy": 0.7341660791063284, "num_samples": 64}