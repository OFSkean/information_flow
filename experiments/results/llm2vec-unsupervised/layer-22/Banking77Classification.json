{"dataset_revision": "0fd18e25b25c072e09e0d92ab615fda904d66300", "evaluation_time": 159.08863186836243, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"accuracy": 0.8242532467532466, "f1": 0.8235297050247169, "f1_weighted": 0.8235297050247169, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.8242532467532466, "scores_per_experiment": [{"accuracy": 0.8301948051948052, "f1": 0.8306108367569951, "f1_weighted": 0.8306108367569954}, {"accuracy": 0.827922077922078, "f1": 0.8264229883539037, "f1_weighted": 0.8264229883539036}, {"accuracy": 0.8227272727272728, "f1": 0.8221392551127141, "f1_weighted": 0.8221392551127141}, {"accuracy": 0.8331168831168831, "f1": 0.8331719975034548, "f1_weighted": 0.8331719975034549}, {"accuracy": 0.8256493506493506, "f1": 0.8255920612871226, "f1_weighted": 0.8255920612871227}, {"accuracy": 0.8191558441558442, "f1": 0.8168888150159348, "f1_weighted": 0.816888815015935}, {"accuracy": 0.8168831168831169, "f1": 0.8169711156846322, "f1_weighted": 0.8169711156846322}, {"accuracy": 0.8142857142857143, "f1": 0.8130620175921152, "f1_weighted": 0.813062017592115}, {"accuracy": 0.8272727272727273, "f1": 0.8254885042242889, "f1_weighted": 0.825488504224289}, {"accuracy": 0.8253246753246753, "f1": 0.8249494587160078, "f1_weighted": 0.8249494587160078}]}]}, "task_name": "Banking77Classification", "avg_layerwise_entropy": 0.5204932722021178, "num_samples": 64}