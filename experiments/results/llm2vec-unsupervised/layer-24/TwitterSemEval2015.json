{"dataset_revision": "70970daeab8776df92f5ea462b6173c0b46fd2d1", "evaluation_time": 190.5123426914215, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"cosine": {"accuracy": 0.841568814448352, "accuracy_threshold": 0.8229202032089233, "ap": 0.6731669966498851, "f1": 0.6274607525541989, "f1_threshold": 0.7911778092384338, "precision": 0.5944287063267233, "recall": 0.6643799472295514}, "dot": {"accuracy": 0.790546581629612, "accuracy_threshold": 155.82891845703125, "ap": 0.45788984756888007, "f1": 0.4692653673163419, "f1_threshold": 135.39443969726562, "precision": 0.39491708723864455, "recall": 0.5781002638522428}, "euclidean": {"accuracy": 0.838350122191095, "accuracy_threshold": 7.775617599487305, "ap": 0.6644796914240704, "f1": 0.627867664815262, "f1_threshold": 8.575990676879883, "precision": 0.578806767586821, "recall": 0.6860158311345647}, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.6731669966498851, "manhattan": {"accuracy": 0.8399594683197235, "accuracy_threshold": 379.8191223144531, "ap": 0.6671614653331905, "f1": 0.6316181953765847, "f1_threshold": 424.333740234375, "precision": 0.5970394736842105, "recall": 0.6704485488126649}, "max": {"accuracy": 0.841568814448352, "ap": 0.6731669966498851, "f1": 0.6316181953765847}, "similarity": {"accuracy": 0.841568814448352, "accuracy_threshold": 0.8229202032089233, "ap": 0.6731669966498851, "f1": 0.6274607525541989, "f1_threshold": 0.7911778092384338, "precision": 0.5944287063267233, "recall": 0.6643799472295514}}]}, "task_name": "TwitterSemEval2015", "avg_layerwise_entropy": 0.7575549166813211, "num_samples": 64}