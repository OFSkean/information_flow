{"dataset_revision": "0fd18e25b25c072e09e0d92ab615fda904d66300", "evaluation_time": 149.48278975486755, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"accuracy": 0.6821428571428572, "f1": 0.6775852224069794, "f1_weighted": 0.6775852224069794, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.6821428571428572, "scores_per_experiment": [{"accuracy": 0.6957792207792208, "f1": 0.6910300215954992, "f1_weighted": 0.6910300215954992}, {"accuracy": 0.6772727272727272, "f1": 0.6716818877908631, "f1_weighted": 0.6716818877908632}, {"accuracy": 0.6766233766233766, "f1": 0.6680358896584734, "f1_weighted": 0.6680358896584736}, {"accuracy": 0.685064935064935, "f1": 0.6802798163564766, "f1_weighted": 0.6802798163564767}, {"accuracy": 0.6766233766233766, "f1": 0.6734640246743967, "f1_weighted": 0.6734640246743968}, {"accuracy": 0.6762987012987013, "f1": 0.6724476907839395, "f1_weighted": 0.6724476907839395}, {"accuracy": 0.6863636363636364, "f1": 0.6855292286095581, "f1_weighted": 0.6855292286095582}, {"accuracy": 0.7022727272727273, "f1": 0.6983675558923916, "f1_weighted": 0.6983675558923917}, {"accuracy": 0.6659090909090909, "f1": 0.660309385622325, "f1_weighted": 0.660309385622325}, {"accuracy": 0.6792207792207792, "f1": 0.6747067230858704, "f1_weighted": 0.6747067230858704}]}]}, "task_name": "Banking77Classification", "avg_layerwise_entropy": 0.08045606276444181, "num_samples": 64}