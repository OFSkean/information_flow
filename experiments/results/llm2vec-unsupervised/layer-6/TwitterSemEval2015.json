{"dataset_revision": "70970daeab8776df92f5ea462b6173c0b46fd2d1", "evaluation_time": 185.89744591712952, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"cosine": {"accuracy": 0.822375871729153, "accuracy_threshold": 0.8310821652412415, "ap": 0.6032293237771091, "f1": 0.571464299112167, "f1_threshold": 0.7865727543830872, "precision": 0.5431423817447112, "recall": 0.6029023746701847}, "dot": {"accuracy": 0.7854801215950409, "accuracy_threshold": 4.532971382141113, "ap": 0.4521504961440905, "f1": 0.4716961606126462, "f1_threshold": 3.889984369277954, "precision": 0.38804699472160736, "recall": 0.6013192612137204}, "euclidean": {"accuracy": 0.8183227037014961, "accuracy_threshold": 1.2947161197662354, "ap": 0.5863546344873196, "f1": 0.5596430126251632, "f1_threshold": 1.520462989807129, "precision": 0.47628751389403484, "recall": 0.6783641160949868}, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.6032293237771091, "manhattan": {"accuracy": 0.818739941586696, "accuracy_threshold": 64.79632568359375, "ap": 0.5867926628006392, "f1": 0.5639266706091071, "f1_threshold": 73.4688720703125, "precision": 0.5110396570203645, "recall": 0.629023746701847}, "max": {"accuracy": 0.822375871729153, "ap": 0.6032293237771091, "f1": 0.571464299112167}, "similarity": {"accuracy": 0.822375871729153, "accuracy_threshold": 0.8310821652412415, "ap": 0.6032293237771091, "f1": 0.571464299112167, "f1_threshold": 0.7865727543830872, "precision": 0.5431423817447112, "recall": 0.6029023746701847}}]}, "task_name": "TwitterSemEval2015", "avg_layerwise_entropy": 0.1447929875148181, "num_samples": 64}