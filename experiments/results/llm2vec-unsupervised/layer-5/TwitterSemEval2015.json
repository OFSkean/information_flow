{"dataset_revision": "70970daeab8776df92f5ea462b6173c0b46fd2d1", "evaluation_time": 202.59790420532227, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"cosine": {"accuracy": 0.8215413959587531, "accuracy_threshold": 0.8180252909660339, "ap": 0.6007601097418886, "f1": 0.5731467561003816, "f1_threshold": 0.776476263999939, "precision": 0.5101914762198888, "recall": 0.6538258575197889}, "dot": {"accuracy": 0.7860165703045836, "accuracy_threshold": 3.4331045150756836, "ap": 0.4554447639229573, "f1": 0.48195784208645936, "f1_threshold": 2.864077091217041, "precision": 0.3642992168512017, "recall": 0.7118733509234828}, "euclidean": {"accuracy": 0.8162961196876677, "accuracy_threshold": 1.1371850967407227, "ap": 0.5807359311769292, "f1": 0.5616326530612246, "f1_threshold": 1.2997578382492065, "precision": 0.503239289446186, "recall": 0.6353562005277045}, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.6007601097418886, "manhattan": {"accuracy": 0.8168325683972104, "accuracy_threshold": 55.96603012084961, "ap": 0.5792706803652639, "f1": 0.5601620204908266, "f1_threshold": 63.320335388183594, "precision": 0.510642919200695, "recall": 0.6203166226912928}, "max": {"accuracy": 0.8215413959587531, "ap": 0.6007601097418886, "f1": 0.5731467561003816}, "similarity": {"accuracy": 0.8215413959587531, "accuracy_threshold": 0.8180252909660339, "ap": 0.6007601097418886, "f1": 0.5731467561003816, "f1_threshold": 0.776476263999939, "precision": 0.5101914762198888, "recall": 0.6538258575197889}}]}, "task_name": "TwitterSemEval2015", "avg_layerwise_entropy": 0.11852207918277137, "num_samples": 64}