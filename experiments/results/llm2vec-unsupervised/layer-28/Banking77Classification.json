{"dataset_revision": "0fd18e25b25c072e09e0d92ab615fda904d66300", "evaluation_time": 176.29780459403992, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"accuracy": 0.842012987012987, "f1": 0.8411751991165259, "f1_weighted": 0.841175199116526, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.842012987012987, "scores_per_experiment": [{"accuracy": 0.8363636363636363, "f1": 0.8352730299084654, "f1_weighted": 0.8352730299084654}, {"accuracy": 0.8461038961038961, "f1": 0.845374549045069, "f1_weighted": 0.8453745490450691}, {"accuracy": 0.8444805194805195, "f1": 0.8439687177306022, "f1_weighted": 0.8439687177306024}, {"accuracy": 0.8350649350649351, "f1": 0.8328651057572061, "f1_weighted": 0.8328651057572061}, {"accuracy": 0.8477272727272728, "f1": 0.8465503332989276, "f1_weighted": 0.8465503332989277}, {"accuracy": 0.8253246753246753, "f1": 0.8252845747276073, "f1_weighted": 0.8252845747276074}, {"accuracy": 0.8389610389610389, "f1": 0.8381480217243383, "f1_weighted": 0.8381480217243382}, {"accuracy": 0.8487012987012987, "f1": 0.8489194101984902, "f1_weighted": 0.8489194101984905}, {"accuracy": 0.8636363636363636, "f1": 0.8620288513959727, "f1_weighted": 0.8620288513959729}, {"accuracy": 0.8337662337662337, "f1": 0.8333393973785803, "f1_weighted": 0.8333393973785804}]}]}, "task_name": "Banking77Classification", "avg_layerwise_entropy": 0.708160358748986, "num_samples": 64}