{"dataset_revision": "70970daeab8776df92f5ea462b6173c0b46fd2d1", "evaluation_time": 192.1824712753296, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"cosine": {"accuracy": 0.8363831435894379, "accuracy_threshold": 0.8468302488327026, "ap": 0.6514421954451977, "f1": 0.613021957101155, "f1_threshold": 0.8130651712417603, "precision": 0.5906089508437271, "recall": 0.637203166226913}, "dot": {"accuracy": 0.781724980628241, "accuracy_threshold": 355.5838623046875, "ap": 0.38278874302786253, "f1": 0.4079556806829534, "f1_threshold": 296.40667724609375, "precision": 0.3110372524581083, "recall": 0.5926121372031662}, "euclidean": {"accuracy": 0.8345353758121238, "accuracy_threshold": 10.821651458740234, "ap": 0.643751711708736, "f1": 0.6105690026612597, "f1_threshold": 11.823907852172852, "precision": 0.5874177029992684, "recall": 0.6356200527704485}, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.6514421954451977, "manhattan": {"accuracy": 0.8344161649877809, "accuracy_threshold": 554.8028564453125, "ap": 0.6454295439236593, "f1": 0.612108773730118, "f1_threshold": 592.164794921875, "precision": 0.5956065901148277, "recall": 0.6295514511873351}, "max": {"accuracy": 0.8363831435894379, "ap": 0.6514421954451977, "f1": 0.613021957101155}, "similarity": {"accuracy": 0.8363831435894379, "accuracy_threshold": 0.8468302488327026, "ap": 0.6514421954451977, "f1": 0.613021957101155, "f1_threshold": 0.8130651712417603, "precision": 0.5906089508437271, "recall": 0.637203166226913}}]}, "task_name": "TwitterSemEval2015", "avg_layerwise_entropy": 0.8345168646987344, "num_samples": 64}