{"dataset_revision": "0fd18e25b25c072e09e0d92ab615fda904d66300", "evaluation_time": 152.25369119644165, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"accuracy": 0.701461038961039, "f1": 0.700325494319818, "f1_weighted": 0.700325494319818, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.701461038961039, "scores_per_experiment": [{"accuracy": 0.6974025974025974, "f1": 0.6963677672142697, "f1_weighted": 0.6963677672142697}, {"accuracy": 0.7074675324675325, "f1": 0.7061122141814602, "f1_weighted": 0.7061122141814602}, {"accuracy": 0.7074675324675325, "f1": 0.7071388848785467, "f1_weighted": 0.7071388848785466}, {"accuracy": 0.7162337662337662, "f1": 0.7145377423312878, "f1_weighted": 0.7145377423312879}, {"accuracy": 0.6993506493506494, "f1": 0.6990171989251924, "f1_weighted": 0.6990171989251925}, {"accuracy": 0.6954545454545454, "f1": 0.6945597991030112, "f1_weighted": 0.6945597991030114}, {"accuracy": 0.6941558441558442, "f1": 0.6930567114232293, "f1_weighted": 0.6930567114232293}, {"accuracy": 0.687012987012987, "f1": 0.684814094461221, "f1_weighted": 0.684814094461221}, {"accuracy": 0.7090909090909091, "f1": 0.7081830494428522, "f1_weighted": 0.7081830494428522}, {"accuracy": 0.700974025974026, "f1": 0.6994674812371096, "f1_weighted": 0.6994674812371096}]}]}, "task_name": "Banking77Classification", "avg_layerwise_entropy": 0.1461844365916279, "num_samples": 64}