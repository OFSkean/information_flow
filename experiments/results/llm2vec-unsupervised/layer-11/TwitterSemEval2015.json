{"dataset_revision": "70970daeab8776df92f5ea462b6173c0b46fd2d1", "evaluation_time": 189.8270754814148, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"cosine": {"accuracy": 0.8314358943792096, "accuracy_threshold": 0.8483173847198486, "ap": 0.634230031181273, "f1": 0.5982989064398542, "f1_threshold": 0.8198229074478149, "precision": 0.5545045045045045, "recall": 0.6496042216358839}, "dot": {"accuracy": 0.7836919592298981, "accuracy_threshold": 13.867656707763672, "ap": 0.43881187183619935, "f1": 0.47150356205474314, "f1_threshold": 11.89771842956543, "precision": 0.3656586216923524, "recall": 0.6635883905013192}, "euclidean": {"accuracy": 0.8292900995410383, "accuracy_threshold": 2.113985538482666, "ap": 0.6269885582090462, "f1": 0.596597079763485, "f1_threshold": 2.327761173248291, "precision": 0.5496997998665777, "recall": 0.6522427440633245}, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.634230031181273, "manhattan": {"accuracy": 0.8307206294331525, "accuracy_threshold": 103.69992065429688, "ap": 0.6292602814225768, "f1": 0.596073765615705, "f1_threshold": 115.56965637207031, "precision": 0.542795232936078, "recall": 0.6609498680738787}, "max": {"accuracy": 0.8314358943792096, "ap": 0.634230031181273, "f1": 0.5982989064398542}, "similarity": {"accuracy": 0.8314358943792096, "accuracy_threshold": 0.8483173847198486, "ap": 0.634230031181273, "f1": 0.5982989064398542, "f1_threshold": 0.8198229074478149, "precision": 0.5545045045045045, "recall": 0.6496042216358839}}]}, "task_name": "TwitterSemEval2015", "avg_layerwise_entropy": 0.26286804453522516, "num_samples": 64}