{"dataset_revision": "70970daeab8776df92f5ea462b6173c0b46fd2d1", "evaluation_time": 206.01933312416077, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"cosine": {"accuracy": 0.8122429516600107, "accuracy_threshold": 0.84569251537323, "ap": 0.5630491951502508, "f1": 0.5443190007206342, "f1_threshold": 0.8054617643356323, "precision": 0.49955908289241624, "recall": 0.5978891820580475}, "dot": {"accuracy": 0.7777314180127556, "accuracy_threshold": 0.7285550832748413, "ap": 0.3799393876341933, "f1": 0.43302360003289203, "f1_threshold": 0.5649192333221436, "precision": 0.31453828694301755, "recall": 0.6947229551451187}, "euclidean": {"accuracy": 0.808487810693211, "accuracy_threshold": 0.4760613739490509, "ap": 0.5425502899930632, "f1": 0.5321413367390377, "f1_threshold": 0.5517181158065796, "precision": 0.4459507670353193, "recall": 0.6596306068601583}, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.5630491951502508, "manhattan": {"accuracy": 0.8080109673958396, "accuracy_threshold": 22.62981414794922, "ap": 0.542409094135119, "f1": 0.5372954764196343, "f1_threshold": 25.501508712768555, "precision": 0.4938080495356037, "recall": 0.5891820580474934}, "max": {"accuracy": 0.8122429516600107, "ap": 0.5630491951502508, "f1": 0.5443190007206342}, "similarity": {"accuracy": 0.8122429516600107, "accuracy_threshold": 0.84569251537323, "ap": 0.5630491951502508, "f1": 0.5443190007206342, "f1_threshold": 0.8054617643356323, "precision": 0.49955908289241624, "recall": 0.5978891820580475}}]}, "task_name": "TwitterSemEval2015", "avg_layerwise_entropy": 0.03708624456909314, "num_samples": 64}