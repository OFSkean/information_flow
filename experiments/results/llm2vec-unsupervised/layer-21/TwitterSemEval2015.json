{"dataset_revision": "70970daeab8776df92f5ea462b6173c0b46fd2d1", "evaluation_time": 191.9649064540863, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"cosine": {"accuracy": 0.8424628956309232, "accuracy_threshold": 0.7956799268722534, "ap": 0.6770626022095905, "f1": 0.6300437530384053, "f1_threshold": 0.7609729766845703, "precision": 0.5840468679585399, "recall": 0.6839050131926121}, "dot": {"accuracy": 0.795732252488526, "accuracy_threshold": 66.53691101074219, "ap": 0.5031450511645627, "f1": 0.5113812154696131, "f1_threshold": 58.89558410644531, "precision": 0.43992395437262355, "recall": 0.6105540897097625}, "euclidean": {"accuracy": 0.8382309113667521, "accuracy_threshold": 5.40363883972168, "ap": 0.6707579444153874, "f1": 0.6348122866894198, "f1_threshold": 6.008416175842285, "precision": 0.6093181266682844, "recall": 0.662532981530343}, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.6770626022095905, "manhattan": {"accuracy": 0.8401382845562377, "accuracy_threshold": 280.52398681640625, "ap": 0.6741573306612136, "f1": 0.6385393535652603, "f1_threshold": 302.1686706542969, "precision": 0.5996292863762743, "recall": 0.6828496042216359}, "max": {"accuracy": 0.8424628956309232, "ap": 0.6770626022095905, "f1": 0.6385393535652603}, "similarity": {"accuracy": 0.8424628956309232, "accuracy_threshold": 0.7956799268722534, "ap": 0.6770626022095905, "f1": 0.6300437530384053, "f1_threshold": 0.7609729766845703, "precision": 0.5840468679585399, "recall": 0.6839050131926121}}]}, "task_name": "TwitterSemEval2015", "avg_layerwise_entropy": 0.6630990218285416, "num_samples": 64}