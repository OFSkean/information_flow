{"dataset_revision": "0fd18e25b25c072e09e0d92ab615fda904d66300", "evaluation_time": 172.57328867912292, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"accuracy": 0.814025974025974, "f1": 0.8131519233843496, "f1_weighted": 0.8131519233843498, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.814025974025974, "scores_per_experiment": [{"accuracy": 0.8142857142857143, "f1": 0.8123980483685188, "f1_weighted": 0.8123980483685188}, {"accuracy": 0.8181818181818182, "f1": 0.8171491129969601, "f1_weighted": 0.8171491129969602}, {"accuracy": 0.826948051948052, "f1": 0.8244279737313209, "f1_weighted": 0.8244279737313212}, {"accuracy": 0.7954545454545454, "f1": 0.7942583999664605, "f1_weighted": 0.7942583999664606}, {"accuracy": 0.8136363636363636, "f1": 0.812671546689963, "f1_weighted": 0.8126715466899631}, {"accuracy": 0.8142857142857143, "f1": 0.8134832707823059, "f1_weighted": 0.8134832707823061}, {"accuracy": 0.8204545454545454, "f1": 0.8205049008910543, "f1_weighted": 0.8205049008910542}, {"accuracy": 0.8126623376623376, "f1": 0.8125625209013465, "f1_weighted": 0.8125625209013467}, {"accuracy": 0.8152597402597402, "f1": 0.8148041178531862, "f1_weighted": 0.8148041178531862}, {"accuracy": 0.8090909090909091, "f1": 0.8092593416623809, "f1_weighted": 0.8092593416623808}]}]}, "task_name": "Banking77Classification", "avg_layerwise_entropy": 0.4646911612691771, "num_samples": 64}