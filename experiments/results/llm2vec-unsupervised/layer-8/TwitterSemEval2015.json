{"dataset_revision": "70970daeab8776df92f5ea462b6173c0b46fd2d1", "evaluation_time": 203.9122908115387, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"cosine": {"accuracy": 0.8257137748107528, "accuracy_threshold": 0.8320965766906738, "ap": 0.6214441035787238, "f1": 0.5891223733003709, "f1_threshold": 0.7908032536506653, "precision": 0.554186046511628, "recall": 0.6287598944591029}, "dot": {"accuracy": 0.7776718126005842, "accuracy_threshold": 10.201261520385742, "ap": 0.3935053963926425, "f1": 0.4535042735042734, "f1_threshold": 7.666097640991211, "precision": 0.33539823008849556, "recall": 0.7}, "euclidean": {"accuracy": 0.8257137748107528, "accuracy_threshold": 1.902091145515442, "ap": 0.61653539464213, "f1": 0.58996688335582, "f1_threshold": 2.0861377716064453, "precision": 0.5512262204904882, "recall": 0.6345646437994723}, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.6214441035787238, "manhattan": {"accuracy": 0.8266674614054956, "accuracy_threshold": 94.62606811523438, "ap": 0.6196130282378565, "f1": 0.5923971699244512, "f1_threshold": 104.19422912597656, "precision": 0.5429764783468894, "recall": 0.6517150395778364}, "max": {"accuracy": 0.8266674614054956, "ap": 0.6214441035787238, "f1": 0.5923971699244512}, "similarity": {"accuracy": 0.8257137748107528, "accuracy_threshold": 0.8320965766906738, "ap": 0.6214441035787238, "f1": 0.5891223733003709, "f1_threshold": 0.7908032536506653, "precision": 0.554186046511628, "recall": 0.6287598944591029}}]}, "task_name": "TwitterSemEval2015", "avg_layerwise_entropy": 0.20642381784025662, "num_samples": 64}