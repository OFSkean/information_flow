{"dataset_revision": "0fd18e25b25c072e09e0d92ab615fda904d66300", "evaluation_time": 141.3970444202423, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"accuracy": 0.7056818181818182, "f1": 0.703754710398311, "f1_weighted": 0.7037547103983108, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.7056818181818182, "scores_per_experiment": [{"accuracy": 0.7097402597402598, "f1": 0.7064766658028988, "f1_weighted": 0.7064766658028987}, {"accuracy": 0.7113636363636363, "f1": 0.7080816777484352, "f1_weighted": 0.7080816777484353}, {"accuracy": 0.7136363636363636, "f1": 0.7143366207983681, "f1_weighted": 0.7143366207983677}, {"accuracy": 0.7087662337662337, "f1": 0.707575757710029, "f1_weighted": 0.7075757577100291}, {"accuracy": 0.7074675324675325, "f1": 0.7072938974325453, "f1_weighted": 0.7072938974325454}, {"accuracy": 0.6967532467532468, "f1": 0.6950983194253162, "f1_weighted": 0.6950983194253162}, {"accuracy": 0.7071428571428572, "f1": 0.7039612222900833, "f1_weighted": 0.7039612222900835}, {"accuracy": 0.6974025974025974, "f1": 0.6941246540479994, "f1_weighted": 0.6941246540479994}, {"accuracy": 0.7006493506493506, "f1": 0.699303525246509, "f1_weighted": 0.6993035252465092}, {"accuracy": 0.7038961038961039, "f1": 0.7012947634809243, "f1_weighted": 0.7012947634809241}]}]}, "task_name": "Banking77Classification", "avg_layerwise_entropy": 0.11207252532591544, "num_samples": 64}