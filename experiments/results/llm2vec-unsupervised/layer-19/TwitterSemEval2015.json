{"dataset_revision": "70970daeab8776df92f5ea462b6173c0b46fd2d1", "evaluation_time": 190.85307121276855, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"cosine": {"accuracy": 0.8410323657388091, "accuracy_threshold": 0.8060582876205444, "ap": 0.6784829160708913, "f1": 0.6329780864567118, "f1_threshold": 0.7731341123580933, "precision": 0.5794781846086384, "recall": 0.6973614775725594}, "dot": {"accuracy": 0.7935268522381832, "accuracy_threshold": 49.48680114746094, "ap": 0.4922054815953861, "f1": 0.5051516043567854, "f1_threshold": 43.104888916015625, "precision": 0.4021246680206218, "recall": 0.679155672823219}, "euclidean": {"accuracy": 0.8388269654884664, "accuracy_threshold": 4.496792793273926, "ap": 0.6726971162506405, "f1": 0.6367713004484306, "f1_threshold": 5.019186019897461, "precision": 0.6031146767343086, "recall": 0.6744063324538259}, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.6784829160708913, "manhattan": {"accuracy": 0.8400786791440663, "accuracy_threshold": 229.47360229492188, "ap": 0.6760388464038257, "f1": 0.6409644606304157, "f1_threshold": 248.5560302734375, "precision": 0.6115504433261443, "recall": 0.6733509234828496}, "max": {"accuracy": 0.8410323657388091, "ap": 0.6784829160708913, "f1": 0.6409644606304157}, "similarity": {"accuracy": 0.8410323657388091, "accuracy_threshold": 0.8060582876205444, "ap": 0.6784829160708913, "f1": 0.6329780864567118, "f1_threshold": 0.7731341123580933, "precision": 0.5794781846086384, "recall": 0.6973614775725594}}]}, "task_name": "TwitterSemEval2015", "avg_layerwise_entropy": 0.574648247765541, "num_samples": 64}