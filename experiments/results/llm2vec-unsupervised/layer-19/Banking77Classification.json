{"dataset_revision": "0fd18e25b25c072e09e0d92ab615fda904d66300", "evaluation_time": 186.54615664482117, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"accuracy": 0.8000974025974026, "f1": 0.7991401950802481, "f1_weighted": 0.7991401950802484, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.8000974025974026, "scores_per_experiment": [{"accuracy": 0.8097402597402598, "f1": 0.8082150652122294, "f1_weighted": 0.8082150652122295}, {"accuracy": 0.8051948051948052, "f1": 0.8047134593309925, "f1_weighted": 0.8047134593309925}, {"accuracy": 0.7977272727272727, "f1": 0.7970789598114675, "f1_weighted": 0.7970789598114675}, {"accuracy": 0.7954545454545454, "f1": 0.7921828514527947, "f1_weighted": 0.7921828514527948}, {"accuracy": 0.7844155844155845, "f1": 0.7841108383895787, "f1_weighted": 0.7841108383895788}, {"accuracy": 0.810064935064935, "f1": 0.8091176124760936, "f1_weighted": 0.8091176124760937}, {"accuracy": 0.7915584415584416, "f1": 0.7913601953105946, "f1_weighted": 0.7913601953105946}, {"accuracy": 0.8016233766233766, "f1": 0.8003323718625546, "f1_weighted": 0.8003323718625548}, {"accuracy": 0.799025974025974, "f1": 0.7981702493062557, "f1_weighted": 0.7981702493062556}, {"accuracy": 0.8061688311688312, "f1": 0.8061203476499217, "f1_weighted": 0.8061203476499219}]}]}, "task_name": "Banking77Classification", "avg_layerwise_entropy": 0.3730226207523538, "num_samples": 64}