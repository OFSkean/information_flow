{"dataset_revision": "0fd18e25b25c072e09e0d92ab615fda904d66300", "evaluation_time": 180.46206259727478, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"accuracy": 0.7828571428571428, "f1": 0.7816640788714593, "f1_weighted": 0.7816640788714594, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.7828571428571428, "scores_per_experiment": [{"accuracy": 0.7704545454545455, "f1": 0.7691937637270327, "f1_weighted": 0.7691937637270326}, {"accuracy": 0.7840909090909091, "f1": 0.7825553340259909, "f1_weighted": 0.782555334025991}, {"accuracy": 0.7821428571428571, "f1": 0.7825499703201155, "f1_weighted": 0.7825499703201156}, {"accuracy": 0.7681818181818182, "f1": 0.7659665752849976, "f1_weighted": 0.7659665752849975}, {"accuracy": 0.7941558441558442, "f1": 0.7918964968031204, "f1_weighted": 0.7918964968031206}, {"accuracy": 0.7837662337662338, "f1": 0.783042474800835, "f1_weighted": 0.7830424748008351}, {"accuracy": 0.7762987012987013, "f1": 0.774669122062292, "f1_weighted": 0.7746691220622919}, {"accuracy": 0.7892857142857143, "f1": 0.787544936422932, "f1_weighted": 0.7875449364229321}, {"accuracy": 0.7811688311688312, "f1": 0.7812831205936736, "f1_weighted": 0.7812831205936737}, {"accuracy": 0.799025974025974, "f1": 0.7979389946736025, "f1_weighted": 0.7979389946736029}]}]}, "task_name": "Banking77Classification", "avg_layerwise_entropy": 0.2891672542223562, "num_samples": 64}