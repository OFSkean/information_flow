{"dataset_revision": "70970daeab8776df92f5ea462b6173c0b46fd2d1", "evaluation_time": 195.56046748161316, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"cosine": {"accuracy": 0.8400786791440663, "accuracy_threshold": 0.8452949523925781, "ap": 0.6713052584137825, "f1": 0.624380574826561, "f1_threshold": 0.8138415813446045, "precision": 0.5885100420364315, "recall": 0.6649076517150396}, "dot": {"accuracy": 0.7852416999463552, "accuracy_threshold": 33.84766387939453, "ap": 0.4217122398121521, "f1": 0.4472113603622144, "f1_threshold": 28.78000831604004, "precision": 0.3665654520917679, "recall": 0.5733509234828496}, "euclidean": {"accuracy": 0.836263932765095, "accuracy_threshold": 3.2738089561462402, "ap": 0.6571343890061904, "f1": 0.617738524692126, "f1_threshold": 3.6344175338745117, "precision": 0.5843727935984938, "recall": 0.6551451187335092}, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.6713052584137825, "manhattan": {"accuracy": 0.8371580139476664, "accuracy_threshold": 161.9615936279297, "ap": 0.6612344841243071, "f1": 0.6203438395415473, "f1_threshold": 180.23370361328125, "precision": 0.5665067597034452, "recall": 0.6854881266490765}, "max": {"accuracy": 0.8400786791440663, "ap": 0.6713052584137825, "f1": 0.624380574826561}, "similarity": {"accuracy": 0.8400786791440663, "accuracy_threshold": 0.8452949523925781, "ap": 0.6713052584137825, "f1": 0.624380574826561, "f1_threshold": 0.8138415813446045, "precision": 0.5885100420364315, "recall": 0.6649076517150396}}]}, "task_name": "TwitterSemEval2015", "avg_layerwise_entropy": 0.4655515784302042, "num_samples": 64}