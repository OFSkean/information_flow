{"dataset_revision": "70970daeab8776df92f5ea462b6173c0b46fd2d1", "evaluation_time": 202.41164207458496, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"cosine": {"accuracy": 0.8268462776420099, "accuracy_threshold": 0.8388967514038086, "ap": 0.6245766105143647, "f1": 0.5941070354780517, "f1_threshold": 0.802705705165863, "precision": 0.5458563535911602, "recall": 0.6517150395778364}, "dot": {"accuracy": 0.7832151159325267, "accuracy_threshold": 11.333150863647461, "ap": 0.42293137908908046, "f1": 0.45824326979760266, "f1_threshold": 9.261734962463379, "precision": 0.36505948653725734, "recall": 0.6153034300791557}, "euclidean": {"accuracy": 0.8269654884663528, "accuracy_threshold": 1.9396038055419922, "ap": 0.6179173421660074, "f1": 0.5918975604371344, "f1_threshold": 2.213383674621582, "precision": 0.5088252040235339, "recall": 0.7073878627968337}, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.6245766105143647, "manhattan": {"accuracy": 0.8283364129462956, "accuracy_threshold": 96.94392395019531, "ap": 0.6205987284842951, "f1": 0.5952571886814068, "f1_threshold": 108.73321533203125, "precision": 0.5260174124316663, "recall": 0.6854881266490765}, "max": {"accuracy": 0.8283364129462956, "ap": 0.6245766105143647, "f1": 0.5952571886814068}, "similarity": {"accuracy": 0.8268462776420099, "accuracy_threshold": 0.8388967514038086, "ap": 0.6245766105143647, "f1": 0.5941070354780517, "f1_threshold": 0.802705705165863, "precision": 0.5458563535911602, "recall": 0.6517150395778364}}]}, "task_name": "TwitterSemEval2015", "avg_layerwise_entropy": 0.22015554789720856, "num_samples": 64}