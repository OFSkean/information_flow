{"dataset_revision": "70970daeab8776df92f5ea462b6173c0b46fd2d1", "evaluation_time": 186.07830691337585, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"cosine": {"accuracy": 0.836263932765095, "accuracy_threshold": 0.8495432138442993, "ap": 0.649336664784963, "f1": 0.6106947796265717, "f1_threshold": 0.8149404525756836, "precision": 0.588782757776145, "recall": 0.6343007915567282}, "dot": {"accuracy": 0.7823210347499553, "accuracy_threshold": 435.4417724609375, "ap": 0.3822639435033426, "f1": 0.40269083434551056, "f1_threshold": 362.2921142578125, "precision": 0.3117315203240272, "recall": 0.5686015831134564}, "euclidean": {"accuracy": 0.8328068188591524, "accuracy_threshold": 11.902351379394531, "ap": 0.6402195917257832, "f1": 0.6065099912027146, "f1_threshold": 12.981050491333008, "precision": 0.5790736741060715, "recall": 0.6366754617414248}, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.649336664784963, "manhattan": {"accuracy": 0.8329856350956667, "accuracy_threshold": 612.641357421875, "ap": 0.6422435474980119, "f1": 0.6100693552530182, "f1_threshold": 649.28125, "precision": 0.5943443443443444, "recall": 0.6266490765171504}, "max": {"accuracy": 0.836263932765095, "ap": 0.649336664784963, "f1": 0.6106947796265717}, "similarity": {"accuracy": 0.836263932765095, "accuracy_threshold": 0.8495432138442993, "ap": 0.649336664784963, "f1": 0.6106947796265717, "f1_threshold": 0.8149404525756836, "precision": 0.588782757776145, "recall": 0.6343007915567282}}]}, "task_name": "TwitterSemEval2015", "avg_layerwise_entropy": 0.8500298155191075, "num_samples": 64}