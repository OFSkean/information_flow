{"dataset_revision": "0fd18e25b25c072e09e0d92ab615fda904d66300", "evaluation_time": 175.45071649551392, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"accuracy": 0.843051948051948, "f1": 0.8420363503043763, "f1_weighted": 0.8420363503043763, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.843051948051948, "scores_per_experiment": [{"accuracy": 0.8512987012987013, "f1": 0.849941527350124, "f1_weighted": 0.8499415273501237}, {"accuracy": 0.8396103896103896, "f1": 0.8379143619368676, "f1_weighted": 0.8379143619368675}, {"accuracy": 0.8389610389610389, "f1": 0.8383425213518362, "f1_weighted": 0.8383425213518363}, {"accuracy": 0.8314935064935065, "f1": 0.8294785524414305, "f1_weighted": 0.8294785524414306}, {"accuracy": 0.8512987012987013, "f1": 0.849851233915547, "f1_weighted": 0.849851233915547}, {"accuracy": 0.8464285714285714, "f1": 0.8455762623989963, "f1_weighted": 0.8455762623989965}, {"accuracy": 0.8418831168831169, "f1": 0.8416627377104992, "f1_weighted": 0.841662737710499}, {"accuracy": 0.836038961038961, "f1": 0.8363477042179189, "f1_weighted": 0.836347704217919}, {"accuracy": 0.8496753246753247, "f1": 0.8491808087436553, "f1_weighted": 0.8491808087436558}, {"accuracy": 0.8438311688311688, "f1": 0.842067792976888, "f1_weighted": 0.8420677929768879}]}]}, "task_name": "Banking77Classification", "avg_layerwise_entropy": 0.7372166352461689, "num_samples": 64}