{"dataset_revision": "0fd18e25b25c072e09e0d92ab615fda904d66300", "evaluation_time": 184.27462124824524, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"accuracy": 0.796525974025974, "f1": 0.7956147152447111, "f1_weighted": 0.7956147152447112, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.796525974025974, "scores_per_experiment": [{"accuracy": 0.7954545454545454, "f1": 0.7946696409310947, "f1_weighted": 0.7946696409310948}, {"accuracy": 0.7964285714285714, "f1": 0.7971169925133504, "f1_weighted": 0.7971169925133503}, {"accuracy": 0.7837662337662338, "f1": 0.7833198710079747, "f1_weighted": 0.7833198710079748}, {"accuracy": 0.8055194805194805, "f1": 0.8043611501814701, "f1_weighted": 0.8043611501814704}, {"accuracy": 0.8012987012987013, "f1": 0.8015887868582835, "f1_weighted": 0.8015887868582836}, {"accuracy": 0.7941558441558442, "f1": 0.7929972271707219, "f1_weighted": 0.7929972271707221}, {"accuracy": 0.8058441558441558, "f1": 0.8043208029495296, "f1_weighted": 0.8043208029495297}, {"accuracy": 0.800974025974026, "f1": 0.7997746931628884, "f1_weighted": 0.7997746931628889}, {"accuracy": 0.7951298701298701, "f1": 0.7935028978932044, "f1_weighted": 0.7935028978932043}, {"accuracy": 0.7866883116883117, "f1": 0.7844950897785932, "f1_weighted": 0.7844950897785931}]}]}, "task_name": "Banking77Classification", "avg_layerwise_entropy": 0.3331838087549944, "num_samples": 64}