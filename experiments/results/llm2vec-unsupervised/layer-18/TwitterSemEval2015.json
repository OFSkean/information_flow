{"dataset_revision": "70970daeab8776df92f5ea462b6173c0b46fd2d1", "evaluation_time": 230.69443464279175, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"cosine": {"accuracy": 0.8403767062049234, "accuracy_threshold": 0.821123480796814, "ap": 0.676280872511721, "f1": 0.6302290836653387, "f1_threshold": 0.7898015975952148, "precision": 0.5966525223950967, "recall": 0.6678100263852242}, "dot": {"accuracy": 0.7916194790486977, "accuracy_threshold": 42.928924560546875, "ap": 0.47150215745871704, "f1": 0.48500235072872583, "f1_threshold": 35.991371154785156, "precision": 0.37677136596055516, "recall": 0.6804749340369393}, "euclidean": {"accuracy": 0.8390653871371521, "accuracy_threshold": 4.131599426269531, "ap": 0.6706581262063366, "f1": 0.6334061665452779, "f1_threshold": 4.496501922607422, "precision": 0.5865557553956835, "recall": 0.6883905013192612}, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.676280872511721, "manhattan": {"accuracy": 0.8400786791440663, "accuracy_threshold": 201.89576721191406, "ap": 0.6729303796504298, "f1": 0.6346015793251975, "f1_threshold": 223.19154357910156, "precision": 0.5805604203152365, "recall": 0.6997361477572559}, "max": {"accuracy": 0.8403767062049234, "ap": 0.676280872511721, "f1": 0.6346015793251975}, "similarity": {"accuracy": 0.8403767062049234, "accuracy_threshold": 0.821123480796814, "ap": 0.676280872511721, "f1": 0.6302290836653387, "f1_threshold": 0.7898015975952148, "precision": 0.5966525223950967, "recall": 0.6678100263852242}}]}, "task_name": "TwitterSemEval2015", "avg_layerwise_entropy": 0.5224183973918919, "num_samples": 64}