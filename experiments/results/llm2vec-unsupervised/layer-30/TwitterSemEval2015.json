{"dataset_revision": "70970daeab8776df92f5ea462b6173c0b46fd2d1", "evaluation_time": 190.71024537086487, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"cosine": {"accuracy": 0.8347141920486381, "accuracy_threshold": 0.8556991815567017, "ap": 0.6390698371385974, "f1": 0.6024064524659527, "f1_threshold": 0.8281393647193909, "precision": 0.6037635833554201, "recall": 0.6010554089709762}, "dot": {"accuracy": 0.7834535375812124, "accuracy_threshold": 552.3474731445312, "ap": 0.37966415325871233, "f1": 0.39810906635151105, "f1_threshold": 454.66717529296875, "precision": 0.2927010923535253, "recall": 0.6221635883905013}, "euclidean": {"accuracy": 0.8312570781426953, "accuracy_threshold": 12.982813835144043, "ap": 0.6326090211449171, "f1": 0.5982318271119842, "f1_threshold": 14.432478904724121, "precision": 0.5594855305466238, "recall": 0.6427440633245383}, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.6390698371385974, "manhattan": {"accuracy": 0.8323895809739524, "accuracy_threshold": 648.6703491210938, "ap": 0.6357673377011182, "f1": 0.6020420420420421, "f1_threshold": 733.7678833007812, "precision": 0.5525909592061742, "recall": 0.6612137203166227}, "max": {"accuracy": 0.8347141920486381, "ap": 0.6390698371385974, "f1": 0.6024064524659527}, "similarity": {"accuracy": 0.8347141920486381, "accuracy_threshold": 0.8556991815567017, "ap": 0.6390698371385974, "f1": 0.6024064524659527, "f1_threshold": 0.8281393647193909, "precision": 0.6037635833554201, "recall": 0.6010554089709762}}]}, "task_name": "TwitterSemEval2015", "avg_layerwise_entropy": 0.8658723722293261, "num_samples": 64}