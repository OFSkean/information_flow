{"dataset_revision": "0fd18e25b25c072e09e0d92ab615fda904d66300", "evaluation_time": 170.52028226852417, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"accuracy": 0.8465584415584415, "f1": 0.8456722454932974, "f1_weighted": 0.8456722454932974, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.8465584415584415, "scores_per_experiment": [{"accuracy": 0.8522727272727273, "f1": 0.8522398940335676, "f1_weighted": 0.8522398940335677}, {"accuracy": 0.8503246753246754, "f1": 0.849413781430468, "f1_weighted": 0.849413781430468}, {"accuracy": 0.8357142857142857, "f1": 0.8332701831880995, "f1_weighted": 0.8332701831880994}, {"accuracy": 0.8350649350649351, "f1": 0.8337634960487388, "f1_weighted": 0.8337634960487388}, {"accuracy": 0.8551948051948052, "f1": 0.8534784177066063, "f1_weighted": 0.8534784177066063}, {"accuracy": 0.8512987012987013, "f1": 0.849761244144888, "f1_weighted": 0.8497612441448879}, {"accuracy": 0.8509740259740259, "f1": 0.8507898197816466, "f1_weighted": 0.8507898197816465}, {"accuracy": 0.8353896103896103, "f1": 0.83474564367859, "f1_weighted": 0.8347456436785902}, {"accuracy": 0.85, "f1": 0.8497683785648719, "f1_weighted": 0.849768378564872}, {"accuracy": 0.8493506493506493, "f1": 0.8494915963554981, "f1_weighted": 0.8494915963554983}]}]}, "task_name": "Banking77Classification", "avg_layerwise_entropy": 0.7700532393466779, "num_samples": 64}