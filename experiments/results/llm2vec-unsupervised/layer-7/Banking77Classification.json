{"dataset_revision": "0fd18e25b25c072e09e0d92ab615fda904d66300", "evaluation_time": 150.55091857910156, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"accuracy": 0.6763636363636365, "f1": 0.6724303925216197, "f1_weighted": 0.6724303925216197, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.6763636363636365, "scores_per_experiment": [{"accuracy": 0.6675324675324675, "f1": 0.6621801833034948, "f1_weighted": 0.6621801833034949}, {"accuracy": 0.6834415584415584, "f1": 0.6810327237264086, "f1_weighted": 0.6810327237264088}, {"accuracy": 0.6636363636363637, "f1": 0.6618447066590715, "f1_weighted": 0.6618447066590714}, {"accuracy": 0.6831168831168831, "f1": 0.6793073670091982, "f1_weighted": 0.6793073670091982}, {"accuracy": 0.6847402597402598, "f1": 0.6793196772900727, "f1_weighted": 0.6793196772900726}, {"accuracy": 0.6922077922077922, "f1": 0.6887247363824206, "f1_weighted": 0.6887247363824205}, {"accuracy": 0.6892857142857143, "f1": 0.6842992236162212, "f1_weighted": 0.6842992236162212}, {"accuracy": 0.6766233766233766, "f1": 0.6736711465179205, "f1_weighted": 0.6736711465179206}, {"accuracy": 0.6571428571428571, "f1": 0.6531822063618102, "f1_weighted": 0.6531822063618103}, {"accuracy": 0.6659090909090909, "f1": 0.6607419543495785, "f1_weighted": 0.6607419543495784}]}]}, "task_name": "Banking77Classification", "avg_layerwise_entropy": 0.09594853800988894, "num_samples": 64}