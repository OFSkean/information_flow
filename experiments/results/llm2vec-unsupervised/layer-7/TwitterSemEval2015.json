{"dataset_revision": "70970daeab8776df92f5ea462b6173c0b46fd2d1", "evaluation_time": 201.5778510570526, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"cosine": {"accuracy": 0.8218990284317816, "accuracy_threshold": 0.8083483576774597, "ap": 0.6079321992770842, "f1": 0.5755834408349948, "f1_threshold": 0.7661723494529724, "precision": 0.5180493983533883, "recall": 0.6474934036939314}, "dot": {"accuracy": 0.7851224891220123, "accuracy_threshold": 6.715691566467285, "ap": 0.44485751316236916, "f1": 0.4786872123273964, "f1_threshold": 5.644221305847168, "precision": 0.38555770470664086, "recall": 0.6311345646437995}, "euclidean": {"accuracy": 0.8217798176074388, "accuracy_threshold": 1.696019172668457, "ap": 0.6027268921632108, "f1": 0.5750321149129978, "f1_threshold": 1.8806464672088623, "precision": 0.5158181437251205, "recall": 0.6496042216358839}, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.6079321992770842, "manhattan": {"accuracy": 0.8232699529117243, "accuracy_threshold": 84.87578582763672, "ap": 0.6038413600590783, "f1": 0.5784594254320705, "f1_threshold": 94.18108367919922, "precision": 0.5108146351324034, "recall": 0.666754617414248}, "max": {"accuracy": 0.8232699529117243, "ap": 0.6079321992770842, "f1": 0.5784594254320705}, "similarity": {"accuracy": 0.8218990284317816, "accuracy_threshold": 0.8083483576774597, "ap": 0.6079321992770842, "f1": 0.5755834408349948, "f1_threshold": 0.7661723494529724, "precision": 0.5180493983533883, "recall": 0.6474934036939314}}]}, "task_name": "TwitterSemEval2015", "avg_layerwise_entropy": 0.17816179961800105, "num_samples": 64}