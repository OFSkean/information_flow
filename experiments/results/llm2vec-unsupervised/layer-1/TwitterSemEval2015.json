{"dataset_revision": "70970daeab8776df92f5ea462b6173c0b46fd2d1", "evaluation_time": 193.53590393066406, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"cosine": {"accuracy": 0.8075341240984681, "accuracy_threshold": 0.8858017921447754, "ap": 0.5313945673281903, "f1": 0.5228426395939086, "f1_threshold": 0.8456774353981018, "precision": 0.48238180196253344, "recall": 0.570712401055409}, "dot": {"accuracy": 0.7794599749657269, "accuracy_threshold": 0.2669520378112793, "ap": 0.35927694635897406, "f1": 0.4048050024683232, "f1_threshold": 0.1994406282901764, "precision": 0.29411764705882354, "recall": 0.6490765171503958}, "euclidean": {"accuracy": 0.7995469988674971, "accuracy_threshold": 0.23496437072753906, "ap": 0.503166990150081, "f1": 0.5013241220495106, "f1_threshold": 0.2826213240623474, "precision": 0.44473953013278855, "recall": 0.5744063324538259}, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.5313945673281903, "manhattan": {"accuracy": 0.7999642367526971, "accuracy_threshold": 11.528396606445312, "ap": 0.5049992748702978, "f1": 0.5025079799361605, "f1_threshold": 13.778868675231934, "precision": 0.4423926134082698, "recall": 0.5815303430079156}, "max": {"accuracy": 0.8075341240984681, "ap": 0.5313945673281903, "f1": 0.5228426395939086}, "similarity": {"accuracy": 0.8075341240984681, "accuracy_threshold": 0.8858017921447754, "ap": 0.5313945673281903, "f1": 0.5228426395939086, "f1_threshold": 0.8456774353981018, "precision": 0.48238180196253344, "recall": 0.570712401055409}}]}, "task_name": "TwitterSemEval2015", "avg_layerwise_entropy": 0.9073850884402831, "num_samples": 64}