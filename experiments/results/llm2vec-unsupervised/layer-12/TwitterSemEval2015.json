{"dataset_revision": "70970daeab8776df92f5ea462b6173c0b46fd2d1", "evaluation_time": 187.9263482093811, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"cosine": {"accuracy": 0.8291708887166955, "accuracy_threshold": 0.8584344387054443, "ap": 0.6256545309160234, "f1": 0.5933072789447972, "f1_threshold": 0.8316044807434082, "precision": 0.5522964984083675, "recall": 0.6408970976253299}, "dot": {"accuracy": 0.7812481373308696, "accuracy_threshold": 16.263378143310547, "ap": 0.3967056354413968, "f1": 0.4386610735382245, "f1_threshold": 13.812065124511719, "precision": 0.32467367887466736, "recall": 0.6759894459102902}, "euclidean": {"accuracy": 0.827144304702867, "accuracy_threshold": 2.164693593978882, "ap": 0.6210760931862749, "f1": 0.5886830753615833, "f1_threshold": 2.3846561908721924, "precision": 0.5669599217986315, "recall": 0.6121372031662269}, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.6256545309160234, "manhattan": {"accuracy": 0.8279191750610956, "accuracy_threshold": 109.75255584716797, "ap": 0.6228050872463872, "f1": 0.5907866033619916, "f1_threshold": 117.50273132324219, "precision": 0.575068698476143, "recall": 0.6073878627968338}, "max": {"accuracy": 0.8291708887166955, "ap": 0.6256545309160234, "f1": 0.5933072789447972}, "similarity": {"accuracy": 0.8291708887166955, "accuracy_threshold": 0.8584344387054443, "ap": 0.6256545309160234, "f1": 0.5933072789447972, "f1_threshold": 0.8316044807434082, "precision": 0.5522964984083675, "recall": 0.6408970976253299}}]}, "task_name": "TwitterSemEval2015", "avg_layerwise_entropy": 0.2794327248315168, "num_samples": 64}