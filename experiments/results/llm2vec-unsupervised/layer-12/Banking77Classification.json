{"dataset_revision": "0fd18e25b25c072e09e0d92ab615fda904d66300", "evaluation_time": 161.4174783229828, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"accuracy": 0.6850324675324675, "f1": 0.6822576177092119, "f1_weighted": 0.682257617709212, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.6850324675324675, "scores_per_experiment": [{"accuracy": 0.6808441558441558, "f1": 0.6793441817218578, "f1_weighted": 0.6793441817218578}, {"accuracy": 0.6840909090909091, "f1": 0.6828220872758558, "f1_weighted": 0.6828220872758558}, {"accuracy": 0.7071428571428572, "f1": 0.7056391278830224, "f1_weighted": 0.7056391278830226}, {"accuracy": 0.6785714285714286, "f1": 0.6782669087834757, "f1_weighted": 0.6782669087834757}, {"accuracy": 0.6655844155844156, "f1": 0.6589881981205452, "f1_weighted": 0.6589881981205453}, {"accuracy": 0.6912337662337662, "f1": 0.685757666831535, "f1_weighted": 0.685757666831535}, {"accuracy": 0.6775974025974026, "f1": 0.6718711339151243, "f1_weighted": 0.6718711339151244}, {"accuracy": 0.6811688311688312, "f1": 0.6815766885926325, "f1_weighted": 0.6815766885926325}, {"accuracy": 0.6756493506493506, "f1": 0.6720207854943933, "f1_weighted": 0.6720207854943934}, {"accuracy": 0.7084415584415584, "f1": 0.7062893984736772, "f1_weighted": 0.7062893984736772}]}]}, "task_name": "Banking77Classification", "avg_layerwise_entropy": 0.1555365814532364, "num_samples": 64}