{"dataset_revision": "70970daeab8776df92f5ea462b6173c0b46fd2d1", "evaluation_time": 182.5189778804779, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"cosine": {"accuracy": 0.8406747332657806, "accuracy_threshold": 0.8167229890823364, "ap": 0.6735964553080139, "f1": 0.6296764408493427, "f1_threshold": 0.7825570702552795, "precision": 0.6043182920912179, "recall": 0.6572559366754618}, "dot": {"accuracy": 0.7923347439947547, "accuracy_threshold": 59.33807373046875, "ap": 0.4858132178231242, "f1": 0.4988215127490893, "f1_threshold": 53.12691879272461, "precision": 0.4199134199134199, "recall": 0.6142480211081794}, "euclidean": {"accuracy": 0.838350122191095, "accuracy_threshold": 4.998623847961426, "ap": 0.6699213955828365, "f1": 0.6343634745008266, "f1_threshold": 5.440822124481201, "precision": 0.6123250675178001, "recall": 0.658047493403694}, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.6735964553080139, "manhattan": {"accuracy": 0.8408535495022949, "accuracy_threshold": 245.91822814941406, "ap": 0.6734484375478731, "f1": 0.6393719806763285, "f1_threshold": 275.8876037597656, "precision": 0.589532293986637, "recall": 0.6984168865435356}, "max": {"accuracy": 0.8408535495022949, "ap": 0.6735964553080139, "f1": 0.6393719806763285}, "similarity": {"accuracy": 0.8406747332657806, "accuracy_threshold": 0.8167229890823364, "ap": 0.6735964553080139, "f1": 0.6296764408493427, "f1_threshold": 0.7825570702552795, "precision": 0.6043182920912179, "recall": 0.6572559366754618}}]}, "task_name": "TwitterSemEval2015", "avg_layerwise_entropy": 0.6181624806994018, "num_samples": 64}