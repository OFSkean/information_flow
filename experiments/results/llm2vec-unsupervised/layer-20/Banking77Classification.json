{"dataset_revision": "0fd18e25b25c072e09e0d92ab615fda904d66300", "evaluation_time": 192.83292198181152, "kg_co2_emissions": null, "mteb_version": "1.12.49", "scores": {"test": [{"accuracy": 0.8002272727272727, "f1": 0.7994405189809755, "f1_weighted": 0.7994405189809757, "hf_subset": "default", "languages": ["eng-Latn"], "main_score": 0.8002272727272727, "scores_per_experiment": [{"accuracy": 0.8094155844155844, "f1": 0.8083357480430564, "f1_weighted": 0.8083357480430564}, {"accuracy": 0.7987012987012987, "f1": 0.7964402848156125, "f1_weighted": 0.7964402848156124}, {"accuracy": 0.8035714285714286, "f1": 0.8037770079592005, "f1_weighted": 0.8037770079592007}, {"accuracy": 0.7993506493506494, "f1": 0.7985498086324321, "f1_weighted": 0.7985498086324323}, {"accuracy": 0.8006493506493506, "f1": 0.7990870262252883, "f1_weighted": 0.7990870262252882}, {"accuracy": 0.7896103896103897, "f1": 0.7889082021063211, "f1_weighted": 0.7889082021063208}, {"accuracy": 0.7857142857142857, "f1": 0.7852848774875272, "f1_weighted": 0.7852848774875273}, {"accuracy": 0.8061688311688312, "f1": 0.805265589894659, "f1_weighted": 0.8052655898946592}, {"accuracy": 0.7964285714285714, "f1": 0.7963399331220455, "f1_weighted": 0.7963399331220455}, {"accuracy": 0.8126623376623376, "f1": 0.8124167115236129, "f1_weighted": 0.8124167115236128}]}]}, "task_name": "Banking77Classification", "avg_layerwise_entropy": 0.4163311132816612, "num_samples": 64}