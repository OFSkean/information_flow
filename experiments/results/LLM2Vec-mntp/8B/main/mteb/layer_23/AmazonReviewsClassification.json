{
  "dataset_revision": "1399c76144fd37290681b995c656ef9b2e06e26d",
  "evaluation_time": 600.278434753418,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.21",
  "scores": {
    "test": [
      {
        "accuracy": 0.38992000000000004,
        "f1": 0.3846262837362612,
        "f1_weighted": 0.38462628373626123,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.38992000000000004,
        "scores_per_experiment": [
          {
            "accuracy": 0.414,
            "f1": 0.405891640870209,
            "f1_weighted": 0.405891640870209
          },
          {
            "accuracy": 0.403,
            "f1": 0.406120391295351,
            "f1_weighted": 0.406120391295351
          },
          {
            "accuracy": 0.3756,
            "f1": 0.37328440096982524,
            "f1_weighted": 0.37328440096982524
          },
          {
            "accuracy": 0.3834,
            "f1": 0.38632586427067966,
            "f1_weighted": 0.3863258642706797
          },
          {
            "accuracy": 0.4108,
            "f1": 0.39939304801234127,
            "f1_weighted": 0.3993930480123413
          },
          {
            "accuracy": 0.3816,
            "f1": 0.36842432182083973,
            "f1_weighted": 0.3684243218208398
          },
          {
            "accuracy": 0.3822,
            "f1": 0.38153195614774155,
            "f1_weighted": 0.38153195614774155
          },
          {
            "accuracy": 0.4012,
            "f1": 0.39272357287909976,
            "f1_weighted": 0.39272357287909976
          },
          {
            "accuracy": 0.3666,
            "f1": 0.3573623480687182,
            "f1_weighted": 0.35736234806871814
          },
          {
            "accuracy": 0.3808,
            "f1": 0.3752052930278066,
            "f1_weighted": 0.37520529302780653
          }
        ]
      }
    ]
  },
  "task_name": "AmazonReviewsClassification"
}