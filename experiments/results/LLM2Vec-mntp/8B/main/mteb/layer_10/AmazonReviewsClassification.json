{
  "dataset_revision": "1399c76144fd37290681b995c656ef9b2e06e26d",
  "evaluation_time": 274.3095886707306,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.21",
  "scores": {
    "test": [
      {
        "accuracy": 0.417,
        "f1": 0.41050603712845646,
        "f1_weighted": 0.41050603712845646,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.417,
        "scores_per_experiment": [
          {
            "accuracy": 0.4212,
            "f1": 0.4190752803217886,
            "f1_weighted": 0.4190752803217886
          },
          {
            "accuracy": 0.4326,
            "f1": 0.42508101596696085,
            "f1_weighted": 0.42508101596696085
          },
          {
            "accuracy": 0.398,
            "f1": 0.39264314326182725,
            "f1_weighted": 0.3926431432618272
          },
          {
            "accuracy": 0.4242,
            "f1": 0.42389672016575125,
            "f1_weighted": 0.4238967201657513
          },
          {
            "accuracy": 0.4428,
            "f1": 0.4215499748993678,
            "f1_weighted": 0.42154997489936785
          },
          {
            "accuracy": 0.4088,
            "f1": 0.38925008366384123,
            "f1_weighted": 0.3892500836638413
          },
          {
            "accuracy": 0.4026,
            "f1": 0.40406372427846654,
            "f1_weighted": 0.4040637242784666
          },
          {
            "accuracy": 0.4142,
            "f1": 0.41506143673938123,
            "f1_weighted": 0.41506143673938123
          },
          {
            "accuracy": 0.4154,
            "f1": 0.4136715995771306,
            "f1_weighted": 0.41367159957713057
          },
          {
            "accuracy": 0.4102,
            "f1": 0.4007673924100496,
            "f1_weighted": 0.40076739241004966
          }
        ]
      }
    ]
  },
  "task_name": "AmazonReviewsClassification"
}