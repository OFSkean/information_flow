{
  "dataset_revision": "1399c76144fd37290681b995c656ef9b2e06e26d",
  "evaluation_time": 492.50685930252075,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.21",
  "scores": {
    "test": [
      {
        "accuracy": 0.40828,
        "f1": 0.4017087983907217,
        "f1_weighted": 0.4017087983907217,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.40828,
        "scores_per_experiment": [
          {
            "accuracy": 0.425,
            "f1": 0.4169701033581167,
            "f1_weighted": 0.41697010335811674
          },
          {
            "accuracy": 0.4282,
            "f1": 0.42980717263669393,
            "f1_weighted": 0.42980717263669393
          },
          {
            "accuracy": 0.391,
            "f1": 0.38680966266605166,
            "f1_weighted": 0.3868096626660516
          },
          {
            "accuracy": 0.4032,
            "f1": 0.4046254448258895,
            "f1_weighted": 0.40462544482588947
          },
          {
            "accuracy": 0.431,
            "f1": 0.4149067679546875,
            "f1_weighted": 0.4149067679546875
          },
          {
            "accuracy": 0.4062,
            "f1": 0.38715998489408293,
            "f1_weighted": 0.38715998489408304
          },
          {
            "accuracy": 0.396,
            "f1": 0.3953160342168073,
            "f1_weighted": 0.3953160342168072
          },
          {
            "accuracy": 0.4098,
            "f1": 0.40452128508525265,
            "f1_weighted": 0.40452128508525265
          },
          {
            "accuracy": 0.3926,
            "f1": 0.38594712410739057,
            "f1_weighted": 0.38594712410739057
          },
          {
            "accuracy": 0.3998,
            "f1": 0.39102440416224443,
            "f1_weighted": 0.39102440416224443
          }
        ]
      }
    ]
  },
  "task_name": "AmazonReviewsClassification"
}