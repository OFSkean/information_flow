{
  "dataset_revision": "1399c76144fd37290681b995c656ef9b2e06e26d",
  "evaluation_time": 150.4957184791565,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.21",
  "scores": {
    "test": [
      {
        "accuracy": 0.3755,
        "f1": 0.3720211305893156,
        "f1_weighted": 0.3720211305893156,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.3755,
        "scores_per_experiment": [
          {
            "accuracy": 0.3944,
            "f1": 0.3906416843487167,
            "f1_weighted": 0.39064168434871677
          },
          {
            "accuracy": 0.3838,
            "f1": 0.3830370913961328,
            "f1_weighted": 0.38303709139613273
          },
          {
            "accuracy": 0.3722,
            "f1": 0.3713347721693057,
            "f1_weighted": 0.3713347721693057
          },
          {
            "accuracy": 0.3882,
            "f1": 0.3868143203979171,
            "f1_weighted": 0.3868143203979171
          },
          {
            "accuracy": 0.4114,
            "f1": 0.40007002734194747,
            "f1_weighted": 0.40007002734194747
          },
          {
            "accuracy": 0.3446,
            "f1": 0.336376372100265,
            "f1_weighted": 0.33637637210026505
          },
          {
            "accuracy": 0.3558,
            "f1": 0.3576053166887457,
            "f1_weighted": 0.3576053166887456
          },
          {
            "accuracy": 0.3812,
            "f1": 0.3791710313200837,
            "f1_weighted": 0.3791710313200837
          },
          {
            "accuracy": 0.36,
            "f1": 0.35594794403554947,
            "f1_weighted": 0.3559479440355494
          },
          {
            "accuracy": 0.3634,
            "f1": 0.3592127460944924,
            "f1_weighted": 0.3592127460944924
          }
        ]
      }
    ]
  },
  "task_name": "AmazonReviewsClassification"
}