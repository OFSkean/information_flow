{
  "dataset_revision": "1399c76144fd37290681b995c656ef9b2e06e26d",
  "evaluation_time": 589.7873556613922,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.21",
  "scores": {
    "test": [
      {
        "accuracy": 0.38783999999999996,
        "f1": 0.38252706813818055,
        "f1_weighted": 0.38252706813818044,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.38783999999999996,
        "scores_per_experiment": [
          {
            "accuracy": 0.4088,
            "f1": 0.39993192819153034,
            "f1_weighted": 0.39993192819153034
          },
          {
            "accuracy": 0.4012,
            "f1": 0.4046857851149996,
            "f1_weighted": 0.4046857851149997
          },
          {
            "accuracy": 0.3748,
            "f1": 0.37228508517117376,
            "f1_weighted": 0.3722850851711737
          },
          {
            "accuracy": 0.3772,
            "f1": 0.3803483385418079,
            "f1_weighted": 0.38034833854180783
          },
          {
            "accuracy": 0.413,
            "f1": 0.40283313476009913,
            "f1_weighted": 0.4028331347600992
          },
          {
            "accuracy": 0.3792,
            "f1": 0.36775178695194155,
            "f1_weighted": 0.36775178695194155
          },
          {
            "accuracy": 0.3772,
            "f1": 0.37714103822146244,
            "f1_weighted": 0.3771410382214624
          },
          {
            "accuracy": 0.3998,
            "f1": 0.39067470431410245,
            "f1_weighted": 0.39067470431410245
          },
          {
            "accuracy": 0.3674,
            "f1": 0.3574199499993889,
            "f1_weighted": 0.35741994999938886
          },
          {
            "accuracy": 0.3798,
            "f1": 0.37219893011529903,
            "f1_weighted": 0.3721989301152991
          }
        ]
      }
    ]
  },
  "task_name": "AmazonReviewsClassification"
}