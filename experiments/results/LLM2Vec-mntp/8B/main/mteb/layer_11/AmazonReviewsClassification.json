{
  "dataset_revision": "1399c76144fd37290681b995c656ef9b2e06e26d",
  "evaluation_time": 312.38111305236816,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.21",
  "scores": {
    "test": [
      {
        "accuracy": 0.42244000000000004,
        "f1": 0.416560517204112,
        "f1_weighted": 0.416560517204112,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.42244000000000004,
        "scores_per_experiment": [
          {
            "accuracy": 0.4304,
            "f1": 0.4300480416043236,
            "f1_weighted": 0.4300480416043236
          },
          {
            "accuracy": 0.4422,
            "f1": 0.4380542598176563,
            "f1_weighted": 0.4380542598176563
          },
          {
            "accuracy": 0.396,
            "f1": 0.39226593882677785,
            "f1_weighted": 0.3922659388267778
          },
          {
            "accuracy": 0.4244,
            "f1": 0.4235004200624043,
            "f1_weighted": 0.4235004200624043
          },
          {
            "accuracy": 0.449,
            "f1": 0.428608906649491,
            "f1_weighted": 0.42860890664949103
          },
          {
            "accuracy": 0.419,
            "f1": 0.4010754436025591,
            "f1_weighted": 0.40107544360255915
          },
          {
            "accuracy": 0.3992,
            "f1": 0.40095411162099237,
            "f1_weighted": 0.4009541116209924
          },
          {
            "accuracy": 0.4168,
            "f1": 0.4168827071119253,
            "f1_weighted": 0.41688270711192527
          },
          {
            "accuracy": 0.4196,
            "f1": 0.41872115818095973,
            "f1_weighted": 0.4187211581809596
          },
          {
            "accuracy": 0.4278,
            "f1": 0.415494184564031,
            "f1_weighted": 0.415494184564031
          }
        ]
      }
    ]
  },
  "task_name": "AmazonReviewsClassification"
}