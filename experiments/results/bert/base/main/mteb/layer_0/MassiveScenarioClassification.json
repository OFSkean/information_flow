{
  "dataset_revision": "fad2c6e8459f9e1c45d9315f4953d921437d70f8",
  "evaluation_time": 31.305375576019287,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.5152320107599192,
        "f1": 0.5333934194627046,
        "f1_weighted": 0.5352894370561072,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5152320107599192,
        "scores_per_experiment": [
          {
            "accuracy": 0.6059179556153329,
            "f1": 0.6147517510919898,
            "f1_weighted": 0.6207553408337713
          },
          {
            "accuracy": 0.5965030262273033,
            "f1": 0.6023027141644199,
            "f1_weighted": 0.5974857614987816
          },
          {
            "accuracy": 0.5060524546065904,
            "f1": 0.5265484314003599,
            "f1_weighted": 0.5264777508292943
          },
          {
            "accuracy": 0.4996637525218561,
            "f1": 0.5187673651042597,
            "f1_weighted": 0.5169879069814084
          },
          {
            "accuracy": 0.4932750504371217,
            "f1": 0.5158376421559407,
            "f1_weighted": 0.48301098538674037
          },
          {
            "accuracy": 0.4377942165433759,
            "f1": 0.471530542053088,
            "f1_weighted": 0.505500061481831
          },
          {
            "accuracy": 0.527236045729657,
            "f1": 0.5400592861707888,
            "f1_weighted": 0.5389810659173672
          },
          {
            "accuracy": 0.4899125756556826,
            "f1": 0.5121032864485549,
            "f1_weighted": 0.5213476211182977
          },
          {
            "accuracy": 0.531607262945528,
            "f1": 0.5166358793354765,
            "f1_weighted": 0.5236709200365398
          },
          {
            "accuracy": 0.4643577673167451,
            "f1": 0.5153972967021682,
            "f1_weighted": 0.5186769564770405
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.609394982784063,
        "f1": 0.6056184642748178,
        "f1_weighted": 0.6099664886291294,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.609394982784063,
        "scores_per_experiment": [
          {
            "accuracy": 0.6527299557304476,
            "f1": 0.6412436964321941,
            "f1_weighted": 0.65984527788619
          },
          {
            "accuracy": 0.6502705361534677,
            "f1": 0.6449922585312596,
            "f1_weighted": 0.648225513669206
          },
          {
            "accuracy": 0.5848499754058042,
            "f1": 0.5875658723849126,
            "f1_weighted": 0.5865802831233431
          },
          {
            "accuracy": 0.6104279390063945,
            "f1": 0.5946761690356338,
            "f1_weighted": 0.6070422071637677
          },
          {
            "accuracy": 0.5828824397442204,
            "f1": 0.5826033676813868,
            "f1_weighted": 0.5831235552151834
          },
          {
            "accuracy": 0.5966551893753075,
            "f1": 0.5986691070620388,
            "f1_weighted": 0.602831874142256
          },
          {
            "accuracy": 0.5937038858829317,
            "f1": 0.5919203837390143,
            "f1_weighted": 0.5927235259646538
          },
          {
            "accuracy": 0.632562715199213,
            "f1": 0.6304751961356382,
            "f1_weighted": 0.6320236177105502
          },
          {
            "accuracy": 0.6000983767830792,
            "f1": 0.6065246929584822,
            "f1_weighted": 0.5920241078875857
          },
          {
            "accuracy": 0.5897688145597639,
            "f1": 0.5775138987876183,
            "f1_weighted": 0.5952449235285574
          }
        ]
      }
    ]
  },
  "task_name": "MassiveScenarioClassification"
}