{
  "dataset_revision": "4672e20407010da34463acc759c162ca9734bca6",
  "evaluation_time": 39.8754403591156,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.5170477471418964,
        "f1": 0.531554134971211,
        "f1_weighted": 0.5312224075507668,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5170477471418964,
        "scores_per_experiment": [
          {
            "accuracy": 0.5453934095494284,
            "f1": 0.5505320094625927,
            "f1_weighted": 0.5536741061130794
          },
          {
            "accuracy": 0.5988567585743106,
            "f1": 0.5655919259379557,
            "f1_weighted": 0.601601433794262
          },
          {
            "accuracy": 0.5131136516476127,
            "f1": 0.5274718409228796,
            "f1_weighted": 0.5166229747911533
          },
          {
            "accuracy": 0.5295897780766644,
            "f1": 0.5421370919823162,
            "f1_weighted": 0.5503109312628043
          },
          {
            "accuracy": 0.5124411566913248,
            "f1": 0.5222608716482424,
            "f1_weighted": 0.5319728438028827
          },
          {
            "accuracy": 0.48386012104909215,
            "f1": 0.5073322734971206,
            "f1_weighted": 0.5038269274331938
          },
          {
            "accuracy": 0.5110961667787491,
            "f1": 0.5388025010277434,
            "f1_weighted": 0.5129551073675386
          },
          {
            "accuracy": 0.48755884330867516,
            "f1": 0.5129505846719858,
            "f1_weighted": 0.5095760640728628
          },
          {
            "accuracy": 0.484196368527236,
            "f1": 0.501294753288416,
            "f1_weighted": 0.49256937716462174
          },
          {
            "accuracy": 0.5043712172158709,
            "f1": 0.5471674972728565,
            "f1_weighted": 0.539114309705268
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.5867191342843089,
        "f1": 0.5770065242212109,
        "f1_weighted": 0.5904224737294058,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5867191342843089,
        "scores_per_experiment": [
          {
            "accuracy": 0.5828824397442204,
            "f1": 0.5796560579999152,
            "f1_weighted": 0.5811852581756641
          },
          {
            "accuracy": 0.5159862272503689,
            "f1": 0.5143238511606413,
            "f1_weighted": 0.5314408239621995
          },
          {
            "accuracy": 0.6055090998524348,
            "f1": 0.6001904623259366,
            "f1_weighted": 0.6118161859463445
          },
          {
            "accuracy": 0.5966551893753075,
            "f1": 0.5778996029984437,
            "f1_weighted": 0.6032687693635435
          },
          {
            "accuracy": 0.6163305459911461,
            "f1": 0.5978431705910413,
            "f1_weighted": 0.6143681453145474
          },
          {
            "accuracy": 0.5705853418593212,
            "f1": 0.5655801413737164,
            "f1_weighted": 0.577591028319388
          },
          {
            "accuracy": 0.5814067879980325,
            "f1": 0.5787752916652346,
            "f1_weighted": 0.5775857190959486
          },
          {
            "accuracy": 0.5907525823905558,
            "f1": 0.5719954636066984,
            "f1_weighted": 0.5936930747164875
          },
          {
            "accuracy": 0.5823905558288244,
            "f1": 0.5706746188187398,
            "f1_weighted": 0.5829026973937856
          },
          {
            "accuracy": 0.6246925725528775,
            "f1": 0.6131265816717423,
            "f1_weighted": 0.6303730350061494
          }
        ]
      }
    ]
  },
  "task_name": "MassiveIntentClassification"
}