{
  "dataset_revision": "ae001d0e6b1228650b7bd1c2c65fb50ad11a8aba",
  "evaluation_time": 52.23872184753418,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.6756497948016416,
        "f1": 0.49019280515015307,
        "f1_weighted": 0.7160572399748183,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.6756497948016416,
        "scores_per_experiment": [
          {
            "accuracy": 0.6833105335157319,
            "f1": 0.49795742952353833,
            "f1_weighted": 0.722460969289132
          },
          {
            "accuracy": 0.6735066119471044,
            "f1": 0.48462264503424307,
            "f1_weighted": 0.7110761041780479
          },
          {
            "accuracy": 0.6826265389876881,
            "f1": 0.4832727784742248,
            "f1_weighted": 0.7260212354237777
          },
          {
            "accuracy": 0.6746466028271774,
            "f1": 0.4882264326802742,
            "f1_weighted": 0.7149828764934832
          },
          {
            "accuracy": 0.6794345645234838,
            "f1": 0.5084930277566221,
            "f1_weighted": 0.7233271505463886
          },
          {
            "accuracy": 0.6782945736434108,
            "f1": 0.4993485826797922,
            "f1_weighted": 0.7189173763318234
          },
          {
            "accuracy": 0.6502507979936161,
            "f1": 0.46812533451582816,
            "f1_weighted": 0.6910093112628231
          },
          {
            "accuracy": 0.6917464660282717,
            "f1": 0.5112309409799806,
            "f1_weighted": 0.7306684964745415
          },
          {
            "accuracy": 0.6707706338349293,
            "f1": 0.48428217982441424,
            "f1_weighted": 0.7101691250146667
          },
          {
            "accuracy": 0.6719106247150023,
            "f1": 0.47636870003261284,
            "f1_weighted": 0.7119397547334994
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.6743624161073826,
        "f1": 0.47584616627028264,
        "f1_weighted": 0.7169590213696668,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.6743624161073826,
        "scores_per_experiment": [
          {
            "accuracy": 0.6644295302013423,
            "f1": 0.46089947962021016,
            "f1_weighted": 0.7108140565029081
          },
          {
            "accuracy": 0.661744966442953,
            "f1": 0.46455552640954956,
            "f1_weighted": 0.7054709666690072
          },
          {
            "accuracy": 0.6774049217002237,
            "f1": 0.4639505333945213,
            "f1_weighted": 0.7161730302783411
          },
          {
            "accuracy": 0.6751677852348993,
            "f1": 0.48387242415470516,
            "f1_weighted": 0.7169479120736577
          },
          {
            "accuracy": 0.6988814317673379,
            "f1": 0.4892552188955917,
            "f1_weighted": 0.7402503549687598
          },
          {
            "accuracy": 0.6769574944071588,
            "f1": 0.4816186465226383,
            "f1_weighted": 0.7180222199750537
          },
          {
            "accuracy": 0.6536912751677852,
            "f1": 0.45492721386573176,
            "f1_weighted": 0.6981396170045268
          },
          {
            "accuracy": 0.6917225950782998,
            "f1": 0.49393258987833116,
            "f1_weighted": 0.7352077953672417
          },
          {
            "accuracy": 0.6644295302013423,
            "f1": 0.4683088703518313,
            "f1_weighted": 0.7119400613912092
          },
          {
            "accuracy": 0.6791946308724832,
            "f1": 0.4971411596097153,
            "f1_weighted": 0.7166241994659621
          }
        ]
      }
    ]
  },
  "task_name": "MTOPIntentClassification"
}