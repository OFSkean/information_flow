{
  "dataset_revision": "ae001d0e6b1228650b7bd1c2c65fb50ad11a8aba",
  "evaluation_time": 63.068912506103516,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.6782717738258094,
        "f1": 0.4941191397545962,
        "f1_weighted": 0.7177501571958882,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.6782717738258094,
        "scores_per_experiment": [
          {
            "accuracy": 0.6826265389876881,
            "f1": 0.492256724183497,
            "f1_weighted": 0.7227066500687548
          },
          {
            "accuracy": 0.6789785681714546,
            "f1": 0.4954303546403541,
            "f1_weighted": 0.7150977754826334
          },
          {
            "accuracy": 0.6880984952120383,
            "f1": 0.4906027720712627,
            "f1_weighted": 0.7258397011448151
          },
          {
            "accuracy": 0.6785225718194254,
            "f1": 0.4958529650651801,
            "f1_weighted": 0.7174090124363267
          },
          {
            "accuracy": 0.6732786137710899,
            "f1": 0.497350092606334,
            "f1_weighted": 0.7203042942559706
          },
          {
            "accuracy": 0.6732786137710899,
            "f1": 0.49181092623685524,
            "f1_weighted": 0.7116341474697921
          },
          {
            "accuracy": 0.6580027359781122,
            "f1": 0.4835892971541479,
            "f1_weighted": 0.6963997700051774
          },
          {
            "accuracy": 0.7099863201094391,
            "f1": 0.5198989376103853,
            "f1_weighted": 0.7471512660472621
          },
          {
            "accuracy": 0.6719106247150023,
            "f1": 0.5037998940350537,
            "f1_weighted": 0.7121870522126074
          },
          {
            "accuracy": 0.6680346557227542,
            "f1": 0.470599433942892,
            "f1_weighted": 0.7087719028355415
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.6761521252796421,
        "f1": 0.4841462257888292,
        "f1_weighted": 0.7170114186291452,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.6761521252796421,
        "scores_per_experiment": [
          {
            "accuracy": 0.6791946308724832,
            "f1": 0.47161804503165683,
            "f1_weighted": 0.7231433652754852
          },
          {
            "accuracy": 0.6635346756152125,
            "f1": 0.47550877261154745,
            "f1_weighted": 0.7045919065671539
          },
          {
            "accuracy": 0.6747203579418345,
            "f1": 0.4738835945913382,
            "f1_weighted": 0.7112514539811392
          },
          {
            "accuracy": 0.6899328859060403,
            "f1": 0.4980372857952277,
            "f1_weighted": 0.7296709618840187
          },
          {
            "accuracy": 0.7154362416107383,
            "f1": 0.519194540537758,
            "f1_weighted": 0.7533846544415249
          },
          {
            "accuracy": 0.6604026845637584,
            "f1": 0.48251221141048806,
            "f1_weighted": 0.703977097091817
          },
          {
            "accuracy": 0.6492170022371365,
            "f1": 0.45064949334831533,
            "f1_weighted": 0.6939026685136847
          },
          {
            "accuracy": 0.7002237136465325,
            "f1": 0.5113079742633793,
            "f1_weighted": 0.7409023959742125
          },
          {
            "accuracy": 0.6487695749440716,
            "f1": 0.4642432001259794,
            "f1_weighted": 0.6941258292118532
          },
          {
            "accuracy": 0.680089485458613,
            "f1": 0.49450714017260083,
            "f1_weighted": 0.7151638533505615
          }
        ]
      }
    ]
  },
  "task_name": "MTOPIntentClassification"
}