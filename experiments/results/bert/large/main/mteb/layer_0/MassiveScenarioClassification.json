{
  "dataset_revision": "fad2c6e8459f9e1c45d9315f4953d921437d70f8",
  "evaluation_time": 26.26516366004944,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.5040349697377269,
        "f1": 0.5202488150831197,
        "f1_weighted": 0.5231108653982612,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5040349697377269,
        "scores_per_experiment": [
          {
            "accuracy": 0.6533288500336247,
            "f1": 0.6496918403006062,
            "f1_weighted": 0.6600788309239438
          },
          {
            "accuracy": 0.5232010759919301,
            "f1": 0.5507437961624533,
            "f1_weighted": 0.545781436465478
          },
          {
            "accuracy": 0.5447209145931405,
            "f1": 0.5528384590971887,
            "f1_weighted": 0.5581512749340994
          },
          {
            "accuracy": 0.36314727639542704,
            "f1": 0.42790644159316205,
            "f1_weighted": 0.40588104493899824
          },
          {
            "accuracy": 0.5386684599865501,
            "f1": 0.545638228819882,
            "f1_weighted": 0.534402635987487
          },
          {
            "accuracy": 0.5124411566913248,
            "f1": 0.5143316311168752,
            "f1_weighted": 0.5418001821554769
          },
          {
            "accuracy": 0.5184936112979153,
            "f1": 0.5257763165626748,
            "f1_weighted": 0.5329539376747804
          },
          {
            "accuracy": 0.5305985205110961,
            "f1": 0.528084400394202,
            "f1_weighted": 0.5505920923650622
          },
          {
            "accuracy": 0.5642232683254875,
            "f1": 0.5499546477887833,
            "f1_weighted": 0.5553076586010625
          },
          {
            "accuracy": 0.2915265635507734,
            "f1": 0.3575223889953696,
            "f1_weighted": 0.3461595599362231
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.5972946384653222,
        "f1": 0.5970899115949205,
        "f1_weighted": 0.5998654506085253,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5972946384653222,
        "scores_per_experiment": [
          {
            "accuracy": 0.6330545991146089,
            "f1": 0.6314159826821402,
            "f1_weighted": 0.6430387290944325
          },
          {
            "accuracy": 0.6507624200688638,
            "f1": 0.6439828990345668,
            "f1_weighted": 0.6532442802007314
          },
          {
            "accuracy": 0.5750122970978849,
            "f1": 0.5759206575254248,
            "f1_weighted": 0.5766172849922053
          },
          {
            "accuracy": 0.6123954746679784,
            "f1": 0.5951385342659792,
            "f1_weighted": 0.6091237493362124
          },
          {
            "accuracy": 0.5558288243974422,
            "f1": 0.5595239596236837,
            "f1_weighted": 0.5546557504723194
          },
          {
            "accuracy": 0.5848499754058042,
            "f1": 0.5901560422504626,
            "f1_weighted": 0.597600838224836
          },
          {
            "accuracy": 0.5750122970978849,
            "f1": 0.577451735535713,
            "f1_weighted": 0.5762282494697466
          },
          {
            "accuracy": 0.5991146089522873,
            "f1": 0.6022389711276863,
            "f1_weighted": 0.6051179087961555
          },
          {
            "accuracy": 0.5887850467289719,
            "f1": 0.6040908742111519,
            "f1_weighted": 0.5823110480910529
          },
          {
            "accuracy": 0.5981308411214953,
            "f1": 0.5909794596923965,
            "f1_weighted": 0.6007166674075609
          }
        ]
      }
    ]
  },
  "task_name": "MassiveScenarioClassification"
}