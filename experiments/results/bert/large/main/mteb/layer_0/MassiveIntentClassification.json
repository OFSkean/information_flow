{
  "dataset_revision": "4672e20407010da34463acc759c162ca9734bca6",
  "evaluation_time": 37.23690342903137,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.19",
  "scores": {
    "test": [
      {
        "accuracy": 0.4918291862811028,
        "f1": 0.5118653287062347,
        "f1_weighted": 0.506129286529384,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.4918291862811028,
        "scores_per_experiment": [
          {
            "accuracy": 0.5107599193006053,
            "f1": 0.5231809413046273,
            "f1_weighted": 0.5222961216778773
          },
          {
            "accuracy": 0.5998655010087425,
            "f1": 0.5838330066755869,
            "f1_weighted": 0.6131287336665602
          },
          {
            "accuracy": 0.49663752521856086,
            "f1": 0.506441667246422,
            "f1_weighted": 0.4960620134620077
          },
          {
            "accuracy": 0.4704102219233356,
            "f1": 0.4970712226996469,
            "f1_weighted": 0.49232941088694426
          },
          {
            "accuracy": 0.5716207128446537,
            "f1": 0.5474204512185438,
            "f1_weighted": 0.5767264502621022
          },
          {
            "accuracy": 0.39172831203765973,
            "f1": 0.4437880691770035,
            "f1_weighted": 0.4153234502337688
          },
          {
            "accuracy": 0.47377269670477473,
            "f1": 0.5188403180427648,
            "f1_weighted": 0.4875826494405696
          },
          {
            "accuracy": 0.5033624747814391,
            "f1": 0.527101016550314,
            "f1_weighted": 0.5332589269318277
          },
          {
            "accuracy": 0.5026899798251513,
            "f1": 0.5015133602417842,
            "f1_weighted": 0.4983309660953861
          },
          {
            "accuracy": 0.39744451916610624,
            "f1": 0.46946323390565287,
            "f1_weighted": 0.42625414263679684
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.5787506148548942,
        "f1": 0.5696271253732809,
        "f1_weighted": 0.585089500171488,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5787506148548942,
        "scores_per_experiment": [
          {
            "accuracy": 0.5858337432365962,
            "f1": 0.5701774255754898,
            "f1_weighted": 0.5853041581391702
          },
          {
            "accuracy": 0.49188391539596654,
            "f1": 0.49564285566755484,
            "f1_weighted": 0.5083760481223906
          },
          {
            "accuracy": 0.602065912444663,
            "f1": 0.6002636232241451,
            "f1_weighted": 0.6074850364751958
          },
          {
            "accuracy": 0.6084604033448107,
            "f1": 0.577813629264156,
            "f1_weighted": 0.6175268380579745
          },
          {
            "accuracy": 0.602557796360059,
            "f1": 0.5944026649369876,
            "f1_weighted": 0.6024795638144497
          },
          {
            "accuracy": 0.5691096901131333,
            "f1": 0.556426595698378,
            "f1_weighted": 0.5755071380082986
          },
          {
            "accuracy": 0.5661583866207575,
            "f1": 0.5764652343709542,
            "f1_weighted": 0.5682519747780341
          },
          {
            "accuracy": 0.5666502705361535,
            "f1": 0.5513551137889143,
            "f1_weighted": 0.5741581325060481
          },
          {
            "accuracy": 0.5725528775209051,
            "f1": 0.5672296213853267,
            "f1_weighted": 0.5822098949611318
          },
          {
            "accuracy": 0.6222331529758977,
            "f1": 0.6064944898209015,
            "f1_weighted": 0.6295962168521864
          }
        ]
      }
    ]
  },
  "task_name": "MassiveIntentClassification"
}