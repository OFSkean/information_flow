{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook uses matrix-based entropy [1, 2, 3] to look at layer-wise entropies of pretrained LLMs. For an introduction to this, check out the sentence_entropies.ipynb notebook.\n",
    "\n",
    "Authors: Oscar Skean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GPT2LMHeadModel, GPT2Model, AutoModel\n",
    "import torch\n",
    "import numpy as np\n",
    "import tqdm\n",
    "from utils import get_model_path, get_dataloader, normalize\n",
    "import repitl.matrix_itl as itl\n",
    "import math\n",
    "\n",
    "device = \"cuda:1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layerwise Entropies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The information plane is a probe on the model to analyze the mutual information between a pair of variables (input/output, input/layer representation, output/layer representation, etc.) as some quantity is changed (layer depth, context length, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import compute_LDA_matrix\n",
    "\n",
    "def entropy_normalization(entropy, normalization, N, D):\n",
    "    assert normalization in ['maxEntropy', 'logN', 'logD', 'logNlogD', 'raw']\n",
    "\n",
    "    if normalization == 'maxEntropy':\n",
    "        entropy /= min(math.log(N), math.log(D))\n",
    "    elif normalization == 'logN':\n",
    "        entropy /= math.log(N)\n",
    "    elif normalization == 'logD':\n",
    "        entropy /= math.log(D)\n",
    "    elif normalization == 'logNlogD':\n",
    "        entropy /= (math.log(N) * math.log(D))\n",
    "    elif normalization == 'raw':\n",
    "        pass\n",
    "\n",
    "    return entropy\n",
    "\n",
    "def compute_sentence_entropies_over_layers(model, dataloader, alpha=1, normalization='maxEntropy'):\n",
    "    batched_layerwise_entropy = []\n",
    "    \n",
    "    counter = 0\n",
    "    max_samples = 1000\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm.tqdm(dataloader, total=max_samples):\n",
    "            counter += 1\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            N, D = outputs.hidden_states[0].shape[1:]\n",
    "\n",
    "            hidden_states_list = [normalize(x.squeeze()) for x in outputs.hidden_states]\n",
    "\n",
    "            # get the covariance matrices for each layer outputs\n",
    "            layer_cov_list = []\n",
    "            for layer_states in hidden_states_list:\n",
    "                layer_states = layer_states.squeeze()\n",
    "                if N > D:\n",
    "                   layer_cov = layer_states.T @ layer_states\n",
    "                else:\n",
    "                    layer_cov = layer_states @ layer_states.T\n",
    "                layer_cov /= torch.trace(layer_cov)\n",
    "                layer_cov = torch.clamp(layer_cov, min=0)\n",
    "                layer_cov_list.append(layer_cov)\n",
    "\n",
    "            # compute entropy for each covariance\n",
    "            layerwise_entropies = [itl.matrixAlphaEntropy(X.double(), alpha=alpha).item() for X in layer_cov_list]\n",
    "            layerwise_entropies = [entropy_normalization(x, normalization, N, D) for x in layerwise_entropies]\n",
    "            batched_layerwise_entropy.append(layerwise_entropies)\n",
    "\n",
    "            if counter > max_samples:\n",
    "                break\n",
    "    \n",
    "    avg_layerwise_entropy = np.array(batched_layerwise_entropy).mean(axis=0)\n",
    "    return avg_layerwise_entropy\n",
    "\n",
    "def compute_dataset_entropies_over_layers(model, dataloader, alpha=1, normalization='maxEntropy'):\n",
    "    counter = 0\n",
    "    max_samples = 1000\n",
    "\n",
    "    with torch.no_grad():\n",
    "        layerwise_samples = []\n",
    "        for batch in tqdm.tqdm(dataloader, total=max_samples):\n",
    "            counter += 1\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "\n",
    "            hidden_states_list = [normalize(x.squeeze()) for x in outputs.hidden_states]\n",
    "\n",
    "            # get mean hidden state for the sample in each layer\n",
    "            layer_means = [torch.mean(x, dim=0) for x in hidden_states_list]\n",
    "            layer_means = torch.stack(layer_means)\n",
    "            layerwise_samples.append(layer_means)\n",
    "\n",
    "            if counter >= max_samples:\n",
    "                break\n",
    "\n",
    "    Z = torch.stack(layerwise_samples)\n",
    "    Z = Z.transpose(0, 1)\n",
    "    L, NUM_SAMPLES, D = Z.shape\n",
    "\n",
    "    if NUM_SAMPLES > D:\n",
    "        cov = torch.matmul(Z.transpose(1, 2), Z)  # L x D x D\n",
    "    else:\n",
    "        cov = torch.matmul(Z, Z.transpose(1, 2))  # L x NUM_SAMPLES x NUM_SAMPLES\n",
    "\n",
    "    layerwise_entropies = [itl.matrixAlphaEntropy(LAYER_COV.double(), alpha=alpha).item() for LAYER_COV in cov]\n",
    "    layerwise_entropies = [entropy_normalization(x, normalization, NUM_SAMPLES, D) for x in layerwise_entropies]\n",
    "\n",
    "    return layerwise_entropies\n",
    "\n",
    "\n",
    "def compute_dataset_lidar_over_layers(model, dataloader, alpha=1, normalization='maxEntropy'):        \n",
    "    counter = 0\n",
    "    max_samples = 1000\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        batched_augmented_sample_vectors = []\n",
    "        for batch in tqdm.tqdm(dataloader, total=max_samples):\n",
    "            counter += 1\n",
    "\n",
    "            augmented_sample_vectors = []\n",
    "\n",
    "            for augmented_sample in batch:\n",
    "                augmented_sample = {k: v.unsqueeze(0).to(device) for k, v in augmented_sample.items()}\n",
    "                outputs = model(**augmented_sample)\n",
    "\n",
    "                hidden_states_list = [normalize(x.squeeze()) for x in outputs.hidden_states] # L x N x D\n",
    "\n",
    "                # get mean hidden state for the sample in each layer\n",
    "                layer_means = [torch.mean(x, dim=0) for x in hidden_states_list] # L x D\n",
    "                layer_means = torch.stack(layer_means)\n",
    "                augmented_sample_vectors.append(layer_means)\n",
    "\n",
    "            augmented_sample_vectors = torch.stack(augmented_sample_vectors) # NUM_AUGMENTATIONS x L x D\n",
    "            batched_augmented_sample_vectors.append(augmented_sample_vectors)\n",
    "\n",
    "            if counter >= max_samples:\n",
    "                break\n",
    "\n",
    "\n",
    "    batched_augmented_sample_vectors = torch.stack(batched_augmented_sample_vectors) # NUM_SAMPLES x NUM_AUGMENTATIONS x L x D\n",
    "    batched_augmented_sample_vectors = batched_augmented_sample_vectors.transpose(0,1).transpose(0,2) # L x NUM_SAMPLES x NUM_AUGMENTATIONS x D \n",
    "    L, NUM_SAMPLES, NUM_AUGMENTATIONS, D = batched_augmented_sample_vectors.shape\n",
    "    \n",
    "    layerwise_entropies = [\n",
    "        itl.matrixAlphaEntropy(\n",
    "            compute_LDA_matrix(LAYER_EMBEDDINGS.double()),\n",
    "            alpha=alpha\n",
    "        ).item() \n",
    "        for LAYER_EMBEDDINGS in batched_augmented_sample_vectors\n",
    "    ]\n",
    "    layerwise_entropies = [entropy_normalization(x, normalization, NUM_SAMPLES, D) for x in layerwise_entropies]\n",
    "\n",
    "    return layerwise_entropies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import model_name_to_sizes, get_augmentation_collated_dataloader\n",
    "import pickle\n",
    "\n",
    "def calculate_and_save_layerwise_entropies(model_name, experiment_name, granularity='sentence', alpha=1, normalization='maxEntropy'):\n",
    "    assert experiment_name in ['alpha1', 'lidar']\n",
    "    assert granularity in ['sentence', 'dataset']\n",
    "\n",
    "    if experiment_name == 'lidar':\n",
    "        assert granularity == 'dataset'\n",
    "\n",
    "    layerwise_entropies_per_model = {}\n",
    "    for model_size in model_name_to_sizes[model_name]:\n",
    "        model_path = get_model_path(model_name, model_size)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "        model = AutoModel.from_pretrained(model_path, output_hidden_states=True, torch_dtype=torch.bfloat16).to(device)\n",
    "\n",
    "        if granularity == 'sentence' and experiment_name == 'alpha1':\n",
    "            dataloader = get_dataloader(tokenizer, \"wikitext\", split=\"train\")\n",
    "            layerwise_entropies_per_model[model_size] = compute_sentence_entropies_over_layers(model, dataloader, alpha=alpha, normalization=normalization)\n",
    "        elif granularity == 'dataset' and experiment_name == 'alpha1':\n",
    "            dataloader = get_dataloader(tokenizer, \"wikitext\", split=\"train\")\n",
    "            layerwise_entropies_per_model[model_size] = compute_dataset_entropies_over_layers(model, dataloader, alpha=alpha, normalization=normalization)\n",
    "        elif granularity == 'dataset' and experiment_name == 'lidar':\n",
    "            dataloader = get_augmentation_collated_dataloader(tokenizer, \"wikitext\", split=\"train\", num_augmentations_per_sample=16)\n",
    "            layerwise_entropies_per_model[model_size] = compute_dataset_lidar_over_layers(model, dataloader, alpha=alpha, normalization=normalization)\n",
    "\n",
    "        del model\n",
    "    \n",
    "    with open(f\"entropy_results/entropy={experiment_name}_model={model_name}_granularity={granularity}_normalization={normalization}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(layerwise_entropies_per_model, f)\n",
    "\n",
    "    return layerwise_entropies_per_model\n",
    "\n",
    "def load_results_layerwise_entropies(model_name, experiment_name, granularity, normalization):\n",
    "    with open(f\"entropy_results/entropy={experiment_name}_model={model_name}_granularity={granularity}_normalization={normalization}.pkl\", \"rb\") as f:\n",
    "        layerwise_entropies_per_model = pickle.load(f)\n",
    "    return layerwise_entropies_per_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for model in ['EleutherAI', 'cerebras', 'mamba', 'mamba2']:\n",
    "#for model in ['mamba', 'mamba2']:\n",
    "for model in ['EleutherAI', 'cerebras']:\n",
    "    for experiment in ['lidar', 'alpha1']:\n",
    "        for granularity in ['sentence', 'dataset']:\n",
    "            for normalization in ['maxEntropy', 'logN', 'logNlogD', 'raw']:\n",
    "                try:\n",
    "                    # check if already computed\n",
    "                    try:\n",
    "                        load_results_layerwise_entropies(model, experiment, granularity, normalization)\n",
    "                        print(f\"Already computed {model} {experiment} {granularity} {normalization}\")\n",
    "                        continue\n",
    "                    except:\n",
    "                        calculate_and_save_layerwise_entropies(model, experiment, granularity, alpha=1, normalization=normalization)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error for {model} {experiment} {granularity} {normalization}\")\n",
    "                    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "layerwise_entropies = load_results_layerwise_entropies(\"mamba\", 'alpha1', granularity='sentence', normalization='raw')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "for model_name, entropies in layerwise_entropies.items():\n",
    "    ax.plot(entropies, marker='o', label=model_name)\n",
    "\n",
    "ax.set_title('Entropies for Mamba Models of different numbers of parameters')\n",
    "ax.set_xlabel('Layer Index')\n",
    "ax.set_ylabel('Entropy')\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "layerwise_entropies = load_results_layerwise_entropies(\"mamba\", 'alpha1', granularity='sentence', normalization='raw')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "for model_name, entropies in layerwise_entropies.items():\n",
    "    ax.plot(entropies, marker='o', label=model_name)\n",
    "\n",
    "ax.set_title('Entropies for Mamba Models of different numbers of parameters')\n",
    "ax.set_xlabel('Layer Index')\n",
    "ax.set_ylabel('Entropy')\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "layerwise_entropies = load_results_layerwise_entropies(\"EleutherAI\", 'alpha1', granularity='dataset', normalization='maxEntropy')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "for model_name, entropies in layerwise_entropies.items():\n",
    "    ax.plot(entropies, marker='o', label=model_name)\n",
    "\n",
    "ax.set_title('Entropies for Mamba Models of different numbers of parameters')\n",
    "ax.set_xlabel('Layer Index')\n",
    "ax.set_ylabel('Entropy')\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a figure with a subplot for each model\n",
    "num_models = len(layerwise_entropies_per_model)\n",
    "fig, axs = plt.subplots(num_models, 1, figsize=(8, 3 * num_models), sharex=True)\n",
    "\n",
    "all_entropies = [entropy for entropies in layerwise_entropies_per_model.values() for entropy in entropies]\n",
    "y_min, y_max = min(all_entropies), max(all_entropies)\n",
    "\n",
    "# Flatten axs if there is only one subplot\n",
    "if num_models == 1:\n",
    "    axs = [axs]\n",
    "\n",
    "# Plot each model's data\n",
    "for ax, (model_name, entropies) in zip(axs, layerwise_entropies_per_model.items()):\n",
    "    ax.plot(entropies, marker='o')\n",
    "    ax.set_title(f'Entropies for {model_name}')\n",
    "    ax.set_ylabel('Entropy')\n",
    "    ax.set_ylim(y_min*0.95, y_max*1.05)\n",
    "    ax.grid(True)\n",
    "\n",
    "# Set common x-axis label\n",
    "axs[-1].set_xlabel('Layer Index')\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import text_augmentation\n",
    "\n",
    "input_text = [\"The quick brown fox jumps over the lazy dog.\"]\n",
    "\n",
    "augmented_text = text_augmentation(input_text, num_augmentations_per_sample=10)\n",
    "print(f\"Original text: {input_text}\")\n",
    "for i, t in enumerate(augmented_text[0].split(',')):\n",
    "    print(f\"Augmented text {i}: {t}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "information_plane",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
