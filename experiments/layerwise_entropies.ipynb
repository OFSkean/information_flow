{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "This notebook does layerwise evaluations of large language models using a variety of evaluation metrics.\n",
    "Authors: Oscar Skean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GPT2LMHeadModel, GPT2Model, AutoModel\n",
    "from utils.misc.metric_utils import (\n",
    "    compute_per_forward_pass,\n",
    "    compute_on_concatenated_passes,\n",
    "    metric_name_to_function\n",
    ")\n",
    "from utils.misc.model_dataloader_utils import model_name_to_sizes, get_model_path, get_dataloader, get_augmentation_collated_dataloader\n",
    "\n",
    "device = \"cuda:1\"\n",
    "DISABLE_TQDM = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layerwise Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelSpecifications:\n",
    "    def __init__(self, model_family, model_size, revision):\n",
    "        self.model_family = model_family\n",
    "        self.model_size = model_size\n",
    "        self.revision = revision\n",
    "\n",
    "        self.do_checks()\n",
    "    \n",
    "    def do_checks(self):\n",
    "        if self.revision != \"main\":\n",
    "            # currently only supporting 14m and 410m Pythia models for checkpoints\n",
    "            assert self.model_family == \"Pythia\"\n",
    "            assert self.model_size in [\"14m\", \"410m\"]\n",
    "\n",
    "        assert self.model_family in model_name_to_sizes.keys()\n",
    "        assert self.model_size in model_name_to_sizes[self.model_family]\n",
    "\n",
    "class EvaluationMetricSpecifications:\n",
    "    def __init__(\n",
    "        self, \n",
    "        evaluation_metric, \n",
    "        num_samples = 1000, \n",
    "        alpha = 1, \n",
    "        normalizations = ['maxEntropy', 'raw', 'logN', 'logNlogD', 'logD'],\n",
    "        curvature_k = 1\n",
    "    ):\n",
    "        self.evaluation_metric = evaluation_metric\n",
    "        self.num_samples = num_samples\n",
    "\n",
    "        \n",
    "        if self.evaluation_metric == 'sentence-entropy':\n",
    "            self.granularity = 'sentence'\n",
    "            self.evaluation_metric = 'entropy'\n",
    "        elif self.evaluation_metric == 'dataset-entropy':\n",
    "            self.granularity = 'dataset'\n",
    "            self.evaluation_metric = 'entropy'\n",
    "        else:\n",
    "            self.granularity = None\n",
    "\n",
    "        # for matrix-based metrics (LIDAR, DIME, entropy)\n",
    "        self.normalizations = normalizations\n",
    "        self.alpha = alpha\n",
    "\n",
    "        # for curvature\n",
    "        self.curvature_k = curvature_k\n",
    "        \n",
    "        self.do_checks()\n",
    "\n",
    "    def do_checks(self):\n",
    "        assert self.evaluation_metric in metric_name_to_function.keys()\n",
    "        assert self.granularity in ['sentence', 'dataset', None]\n",
    "\n",
    "        assert self.alpha > 0\n",
    "        assert self.num_samples > 0\n",
    "        assert self.curvature_k > 0 and isinstance(self.curvature_k, int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(results, model_specs: ModelSpecifications, evaluation_metric_specs: EvaluationMetricSpecifications, dataloader_kwargs):\n",
    "    model_family = model_specs.model_family\n",
    "    model_size = model_specs.model_size\n",
    "    revision = model_specs.revision\n",
    "    evaluation_metric = evaluation_metric_specs.evaluation_metric\n",
    "    granularity = evaluation_metric_specs.granularity\n",
    "    dataset = dataloader_kwargs['dataset_name']\n",
    "\n",
    "    if evaluation_metric == 'entropy':\n",
    "        evaluation_metric = f\"{evaluation_metric}_{granularity}\"\n",
    "\n",
    "    save_dir = f\"results/{model_family}/{model_size}/{revision}/metrics/{dataset}/\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    with open(f\"{save_dir}/{evaluation_metric}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(results, f)\n",
    "        \n",
    "def load_results(model_specs: ModelSpecifications, evaluation_metric_specs: EvaluationMetricSpecifications, dataloader_kwargs):\n",
    "    model_family = model_specs.model_family\n",
    "    model_size = model_specs.model_size\n",
    "    revision = model_specs.revision\n",
    "    evaluation_metric = evaluation_metric_specs.evaluation_metric\n",
    "    granularity = evaluation_metric_specs.granularity\n",
    "    dataset = dataloader_kwargs['dataset_name']\n",
    "\n",
    "    if evaluation_metric == 'entropy':\n",
    "        evaluation_metric = f\"{evaluation_metric}_{granularity}\"\n",
    "\n",
    "    load_dir = f\"results/{model_family}/{model_size}/{revision}/metrics/{dataset}/\"\n",
    "    file_path = f\"{load_dir}/{evaluation_metric}.pkl\"\n",
    "\n",
    "    try:\n",
    "        with open(file_path, \"rb\") as f:\n",
    "            results = pickle.load(f)\n",
    "        return results\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "        return None\n",
    "\n",
    "def load_results_for_model_and_revisions(model_family, model_size, revisions, evaluation_metrics):\n",
    "    results = {}\n",
    "    for revision in revisions:\n",
    "        model_specs = ModelSpecifications(model_family, model_size, revision)\n",
    "        for evaluation_metric in evaluation_metrics:\n",
    "            evaluation_metric_specs = EvaluationMetricSpecifications(evaluation_metric)\n",
    "            dataloader_kwargs = {'dataset_name': 'wikitext'}\n",
    "            results[(revision, evaluation_metric)] = load_results(model_specs, evaluation_metric_specs, dataloader_kwargs)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_and_save_layerwise_metrics(\n",
    "    model_specs: ModelSpecifications,\n",
    "    evaluation_metric_specs: EvaluationMetricSpecifications,\n",
    "):\n",
    "    dataloader_kwargs = {\n",
    "        \"dataset_name\": \"wikitext\",\n",
    "        \"split\": \"train\",\n",
    "        \"num_samples\": 10000,\n",
    "        \"max_sample_length\": 512 if model_specs.model_family == \"bert\" else 2048,\n",
    "    }\n",
    "\n",
    "    model_path = get_model_path(model_specs.model_family, model_specs.model_size)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model = AutoModel.from_pretrained(model_path, \n",
    "                                      output_hidden_states=True, \n",
    "                                      torch_dtype=torch.bfloat16,\n",
    "                                      revision=model_specs.revision).to(device)\n",
    "    \n",
    "\n",
    "\n",
    "    if evaluation_metric_specs.evaluation_metric == 'entropy':\n",
    "        dataloader = get_dataloader(tokenizer, **dataloader_kwargs)\n",
    "        compute_func_kwargs = {\n",
    "            'alpha': evaluation_metric_specs.alpha,\n",
    "            'normalizations': evaluation_metric_specs.normalizations\n",
    "        }\n",
    "        forward_pass_func = compute_per_forward_pass if evaluation_metric_specs.granularity == 'sentence' else compute_on_concatenated_passes\n",
    "  \n",
    "\n",
    "    elif evaluation_metric_specs.evaluation_metric == 'curvature':\n",
    "        dataloader = get_dataloader(tokenizer, **dataloader_kwargs)\n",
    "        compute_func_kwargs = {\n",
    "            'k': evaluation_metric_specs.curvature_k\n",
    "        }\n",
    "        forward_pass_func = compute_per_forward_pass\n",
    "\n",
    "    elif evaluation_metric_specs.evaluation_metric == 'lidar':\n",
    "        dataloader_kwargs['num_augmentations_per_sample'] = 16\n",
    "        dataloader = get_augmentation_collated_dataloader(tokenizer, **dataloader_kwargs)\n",
    "        compute_func_kwargs = {\n",
    "            'alpha': evaluation_metric_specs.alpha,\n",
    "            'normalizations': evaluation_metric_specs.normalizations,\n",
    "        }\n",
    "        forward_pass_func = compute_on_concatenated_passes\n",
    "\n",
    "    elif evaluation_metric_specs.evaluation_metric == 'dime':\n",
    "        dataloader_kwargs['num_augmentations_per_sample'] = 2\n",
    "        dataloader = get_augmentation_collated_dataloader(tokenizer, **dataloader_kwargs)\n",
    "        compute_func_kwargs = {\n",
    "            'alpha': evaluation_metric_specs.alpha,\n",
    "            'normalizations': evaluation_metric_specs.normalizations,\n",
    "        }\n",
    "        forward_pass_func = compute_on_concatenated_passes\n",
    "\n",
    "    elif evaluation_metric_specs.evaluation_metric == 'infonce':\n",
    "        dataloader_kwargs['num_augmentations_per_sample'] = 2\n",
    "        dataloader = get_augmentation_collated_dataloader(tokenizer, **dataloader_kwargs)\n",
    "        compute_func_kwargs = {\n",
    "            'temperature': 0.1,\n",
    "        }\n",
    "        forward_pass_func = compute_on_concatenated_passes\n",
    "\n",
    "    compute_func = metric_name_to_function[evaluation_metric_specs.evaluation_metric]\n",
    "    results = forward_pass_func(model, dataloader, compute_func, **compute_func_kwargs)\n",
    "    save_results(results, model_specs, evaluation_metric_specs, dataloader_kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already computed: Pythia, infonce, main, 14m\n",
      "Already computed: Pythia, infonce, main, 70m\n",
      "Already computed: Pythia, infonce, main, 160m\n",
      "Already computed: Pythia, infonce, main, 410m\n",
      "Already computed: Pythia, infonce, main, 1b\n",
      "Already computed: Pythia, infonce, main, 1.4b\n",
      "Already computed: Pythia, infonce, main, 2.8b\n",
      "Already computed: Pythia, dime, main, 14m\n",
      "Already computed: Pythia, dime, main, 70m\n",
      "Already computed: Pythia, dime, main, 160m\n",
      "Already computed: Pythia, dime, main, 410m\n",
      "Already computed: Pythia, dime, main, 1b\n",
      "Already computed: Pythia, dime, main, 1.4b\n",
      "Already computed: Pythia, dime, main, 2.8b\n",
      "Already computed: Pythia, lidar, main, 14m\n",
      "Already computed: Pythia, lidar, main, 70m\n",
      "Already computed: Pythia, lidar, main, 160m\n",
      "Already computed: Pythia, lidar, main, 410m\n",
      "Already computed: Pythia, lidar, main, 1b\n",
      "Already computed: Pythia, lidar, main, 1.4b\n",
      "Already computed: Pythia, lidar, main, 2.8b\n",
      "Already computed: Pythia, sentence-entropy, main, 14m\n",
      "Already computed: Pythia, sentence-entropy, main, 70m\n",
      "Already computed: Pythia, sentence-entropy, main, 160m\n",
      "Already computed: Pythia, sentence-entropy, main, 410m\n",
      "Already computed: Pythia, sentence-entropy, main, 1b\n",
      "Already computed: Pythia, sentence-entropy, main, 1.4b\n",
      "Already computed: Pythia, sentence-entropy, main, 2.8b\n",
      "Already computed: Pythia, dataset-entropy, main, 14m\n",
      "Already computed: Pythia, dataset-entropy, main, 70m\n",
      "Already computed: Pythia, dataset-entropy, main, 160m\n",
      "Already computed: Pythia, dataset-entropy, main, 410m\n",
      "Already computed: Pythia, dataset-entropy, main, 1b\n",
      "Already computed: Pythia, dataset-entropy, main, 1.4b\n",
      "Already computed: Pythia, dataset-entropy, main, 2.8b\n",
      "Already computed: Pythia, curvature, main, 14m\n",
      "Already computed: Pythia, curvature, main, 70m\n",
      "Already computed: Pythia, curvature, main, 160m\n",
      "Already computed: Pythia, curvature, main, 410m\n",
      "Already computed: Pythia, curvature, main, 1b\n",
      "Already computed: Pythia, curvature, main, 1.4b\n",
      "Already computed: Pythia, curvature, main, 2.8b\n",
      "Already computed: bert, infonce, main, base\n",
      "Already computed: bert, infonce, main, large\n",
      "Already computed: bert, dime, main, base\n",
      "Already computed: bert, dime, main, large\n",
      "Already computed: bert, lidar, main, base\n",
      "Already computed: bert, lidar, main, large\n",
      "Already computed: bert, sentence-entropy, main, base\n",
      "Already computed: bert, sentence-entropy, main, large\n",
      "Already computed: bert, dataset-entropy, main, base\n",
      "Already computed: bert, dataset-entropy, main, large\n",
      "Already computed: bert, curvature, main, base\n",
      "Already computed: bert, curvature, main, large\n",
      "Already computed: mamba, infonce, main, 130m\n",
      "Already computed: mamba, infonce, main, 370m\n",
      "Already computed: mamba, infonce, main, 790m\n",
      "Already computed: mamba, infonce, main, 1.4b\n",
      "Already computed: mamba, infonce, main, 2.8b\n",
      "Already computed: mamba, dime, main, 130m\n",
      "Already computed: mamba, dime, main, 370m\n",
      "Already computed: mamba, dime, main, 790m\n",
      "Already computed: mamba, dime, main, 1.4b\n",
      "Already computed: mamba, dime, main, 2.8b\n",
      "Already computed: mamba, lidar, main, 130m\n",
      "Already computed: mamba, lidar, main, 370m\n",
      "Already computed: mamba, lidar, main, 790m\n",
      "Already computed: mamba, lidar, main, 1.4b\n",
      "File not found: results/mamba/2.8b/main/metrics/wikitext//lidar.pkl\n",
      "Computing: mamba, lidar, main, 2.8b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/AD/ofsk222/miniconda3/envs/information_plane/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a03293e769cb4310b25efabae8e62b00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 999/1000 [16:17<00:00,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tError for mamba, lidar, main: CUDA out of memory. Tried to allocate 9.92 GiB. GPU \u0001 has a total capacity of 23.68 GiB of which 4.83 GiB is free. Including non-PyTorch memory, this process has 18.83 GiB memory in use. Of the allocated memory 15.49 GiB is allocated by PyTorch, and 3.09 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Already computed: mamba, sentence-entropy, main, 130m\n",
      "Already computed: mamba, sentence-entropy, main, 370m\n",
      "Already computed: mamba, sentence-entropy, main, 790m\n",
      "Already computed: mamba, sentence-entropy, main, 1.4b\n",
      "Already computed: mamba, sentence-entropy, main, 2.8b\n",
      "Already computed: mamba, dataset-entropy, main, 130m\n",
      "Already computed: mamba, dataset-entropy, main, 370m\n",
      "Already computed: mamba, dataset-entropy, main, 790m\n",
      "Already computed: mamba, dataset-entropy, main, 1.4b\n",
      "Already computed: mamba, dataset-entropy, main, 2.8b\n",
      "Already computed: mamba, curvature, main, 130m\n",
      "Already computed: mamba, curvature, main, 370m\n",
      "Already computed: mamba, curvature, main, 790m\n",
      "Already computed: mamba, curvature, main, 1.4b\n",
      "Already computed: mamba, curvature, main, 2.8b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "\n",
    "def is_already_computed(model_specs, evaluation_metric_specs):\n",
    "    try:\n",
    "        results = load_results(model_specs, evaluation_metric_specs, {'dataset_name': 'wikitext'})\n",
    "        return results is not None\n",
    "    except:\n",
    "        return False\n",
    "    \n",
    "\n",
    "model_names = ['Pythia', 'bert', 'mamba', ]\n",
    "evaluation_metrics = ['infonce', 'dime', 'lidar', 'sentence-entropy', 'dataset-entropy', 'curvature']\n",
    "\n",
    "#pythia_revision_steps = [1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1000, 2000, 4000, 8000, 16000, 32000, 64000, 128000]\n",
    "#revisions = ['main'] + [f\"step{step}\" for step in pythia_revision_steps]\n",
    "revisions = ['main']\n",
    "num_samples = 1000\n",
    "\n",
    "for model_name, revision, evaluation_metric in product(model_names, revisions, evaluation_metrics):\n",
    "    for model_size in model_name_to_sizes[model_name]:\n",
    "        try:\n",
    "        \n",
    "            model_spec = ModelSpecifications(model_family=model_name, model_size=model_size, revision=revision)\n",
    "            eval_spec = EvaluationMetricSpecifications(\n",
    "                evaluation_metric=evaluation_metric,\n",
    "                alpha=2,\n",
    "                num_samples=num_samples\n",
    "            )\n",
    "            \n",
    "            if not is_already_computed(model_spec, eval_spec):\n",
    "                print(f\"Computing: {model_name}, {evaluation_metric}, {revision}, {model_size}\")\n",
    "                calculate_and_save_layerwise_metrics(model_spec, eval_spec)\n",
    "            else:\n",
    "                print(f\"Already computed: {model_name}, {evaluation_metric}, {revision}, {model_size}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\tError for {model_name}, {evaluation_metric}, {revision}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'text_augmentation' from 'utils' (unknown location)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m text_augmentation\n\u001b[1;32m      3\u001b[0m input_text \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe quick brown fox jumps over the lazy dog.\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      5\u001b[0m augmented_text \u001b[38;5;241m=\u001b[39m text_augmentation(input_text, num_augmentations_per_sample\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'text_augmentation' from 'utils' (unknown location)"
     ]
    }
   ],
   "source": [
    "from utils import text_augmentation\n",
    "\n",
    "input_text = [\"The quick brown fox jumps over the lazy dog.\"]\n",
    "\n",
    "augmented_text = text_augmentation(input_text, num_augmentations_per_sample=10)\n",
    "print(f\"Original text: {input_text}\")\n",
    "for i, t in enumerate(augmented_text[0].split(',')):\n",
    "    print(f\"Augmented text {i}: {t}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "def load_results_layerwise_entropies(model_name, experiment_name, granularity, normalization):\n",
    "    with open(f\"entropy_results/entropy={experiment_name}_model={model_name}_granularity={granularity}_normalization={normalization}.pkl\", \"rb\") as f:\n",
    "        layerwise_entropies_per_model = pickle.load(f)\n",
    "    return layerwise_entropies_per_model\n",
    "\n",
    "SHOULD_USE_DEPTH_PERCENTAGE =True \n",
    "\n",
    "\n",
    "models = [\"EleutherAI\", \"mamba\", \"bert\"] \n",
    "experiments = [\"curvature\", \"entropy\", \"entropy\",\"lidar\", \"dime\", 'infonce']\n",
    "\n",
    "A = len(models)\n",
    "B = len(experiments)\n",
    "fig, axs = plt.subplots(A, B, figsize=(5 * B, 5 * A))\n",
    "\n",
    "row_labels = ['Pythia', 'Mamba', 'BERT']\n",
    "col_labels = ['Curvature', 'Von Neumann Entropy (Prompt)', 'Von Neumann Entropy (Dataset)', 'LIDAR', 'DiME']\n",
    "\n",
    "# Iterate over models and experiments\n",
    "for i, model_family in enumerate(models):\n",
    "    for j, evaluation_metric in enumerate(experiments):\n",
    "        # Load entropies for each model and experiment\n",
    "        granularity = 'sentence' if 'entropy' in evaluation_metric and j==1 else 'dataset'\n",
    "        granularity = 'sentence' if 'curvature' in evaluation_metric else granularity\n",
    "\n",
    "        normalization = 'maxEntropy'\n",
    "        normalization = 'raw' if 'curvature' in evaluation_metric else normalization\n",
    "\n",
    "        try:\n",
    "            layerwise_entropies = load_results_layerwise_entropies(model_family, evaluation_metric, granularity=granularity, normalization=normalization)\n",
    "        except FileNotFoundError as e:\n",
    "            print(e)\n",
    "            layerwise_entropies=None\n",
    "\n",
    "        # Access the corresponding subplot\n",
    "        ax = axs[i, j] if A > 1 and B > 1 else axs[j] if A == 1 else axs[i]\n",
    "\n",
    "        if layerwise_entropies is not None:\n",
    "            # for pythia, only plot 1B and 1.4B models\n",
    "            if model_family == \"EleutherAI\":\n",
    "                layerwise_entropies = {k: v for k, v in layerwise_entropies.items() if k not in ['1.4b', '2.8b', '6.9b']}\n",
    "            for model_size_idx, (model_variant, entropies) in enumerate(layerwise_entropies.items()):\n",
    "                offset = 4\n",
    "                color = plt.cm.BuGn( (model_size_idx+offset) / (len(layerwise_entropies) + offset) )\n",
    "\n",
    "                if SHOULD_USE_DEPTH_PERCENTAGE:\n",
    "                    depth_percentage = np.linspace(0, 100, len(entropies))\n",
    "                    ax.plot(depth_percentage, entropies, marker='o', label=model_variant, color=color)\n",
    "                else:\n",
    "                    ax.plot(entropies, marker='o', label=model_variant, color=color)\n",
    "        else:\n",
    "            ax.text(0.5, 0.5, 'Missing data', ha='center', va='center', fontsize=12, color='red')\n",
    "\n",
    "        if SHOULD_USE_DEPTH_PERCENTAGE:\n",
    "            ax.set_xlabel('Depth Percentage')\n",
    "        else:   \n",
    "            ax.set_xlabel('Layer')\n",
    "        ax.set_ylabel(f'{col_labels[j]}')\n",
    "        if i==0:\n",
    "            ax.set_title(f'{col_labels[j]}')\n",
    "        \n",
    "        if j == 0:\n",
    "            ax.legend()\n",
    "        ax.grid(True)\n",
    "# Set row labels using ax.text\n",
    "for i, row in enumerate(row_labels):\n",
    "    y_position = (A-i-0.5)/A\n",
    "    if i == len(models)-1:\n",
    "        y_position += 0.02\n",
    "    fig.text(-0.001, y_position, row, rotation=90, va='center', ha='right', fontsize='x-large')\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Normalizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def load_results_for_model_and_revisions(model_family, model_sizes, revisions, evaluation_metrics):\n",
    "    results = {}\n",
    "    for revision in revisions:\n",
    "        for model_size in model_sizes:\n",
    "            model_specs = ModelSpecifications(model_family, model_size, revision)\n",
    "            for evaluation_metric in evaluation_metrics:\n",
    "                evaluation_metric_specs = EvaluationMetricSpecifications(evaluation_metric)\n",
    "            dataloader_kwargs = {'dataset_name': 'wikitext'}\n",
    "            results[(revision, evaluation_metric, model_size)] = load_results(model_specs, evaluation_metric_specs, dataloader_kwargs)\n",
    "    return results\n",
    "\n",
    "model_family = \"Pythia\"\n",
    "revisions = ['main']\n",
    "evaluation_metric = \"sentence-entropy\"\n",
    "model_sizes = model_name_to_sizes[model_family]\n",
    "\n",
    "results = load_results_for_model_and_revisions(model_family, model_sizes, revisions, [evaluation_metric])\n",
    "# Create a figure with subplots for each normalization\n",
    "normalizations = list(next(iter(results.values())).keys())\n",
    "fig, axs = plt.subplots(1, len(normalizations), figsize=(18, 6), squeeze=False)\n",
    "axs = axs.flatten()  # Flatten axs to handle both single and multiple subplots\n",
    "fig.suptitle(f'{evaluation_metric.capitalize()} for {model_family.capitalize()} Models', fontsize=16)\n",
    "\n",
    "for i, normalization in enumerate(normalizations):\n",
    "    for model_size_idx, model_size in enumerate(model_sizes):\n",
    "        entropies = results[(revisions[0], evaluation_metric, model_size)][normalization]\n",
    "        offset = 4\n",
    "        color = plt.cm.BuGn((model_size_idx + offset) / (len(model_sizes) + offset))\n",
    "        \n",
    "        if True:\n",
    "            depth_percentage = np.linspace(0, 100, len(entropies))\n",
    "            axs[i].plot(depth_percentage, entropies, marker='o', label=f'{model_size}', color=color)\n",
    "        else:\n",
    "            axs[i].plot(entropies, marker='o', label=f'{model_size}', color=color)\n",
    "    \n",
    "    axs[i].set_title(f'Normalization: {normalization.capitalize()}')\n",
    "    axs[i].set_xlabel('Depth Percentage')\n",
    "    axs[i].set_ylabel(evaluation_metric.capitalize())\n",
    "    axs[i].legend()\n",
    "    axs[i].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Model Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib.colors as colors\n",
    "\n",
    "models = [\"Pythia\"]\n",
    "metrics = [\"dime\", \"sentence-entropy\", \"dataset-entropy\", \"infonce\", \"lidar\"]\n",
    "#pythia_revision_steps = [1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1000, 2000, 4000, 8000, 16000, 32000, 64000, 128000]\n",
    "pythia_revision_steps = [1, 1000, 2000, 4000, 8000, 16000, 32000, 64000, 128000]\n",
    "revisions = ['main'] + [f\"step{step}\" for step in pythia_revision_steps]\n",
    "\n",
    "results = load_results_for_model_and_revisions(\"Pythia\", \"410m\", revisions, metrics)\n",
    "\n",
    "model_name_to_label = {\n",
    "    \"Pythia\": \"Pythia\",\n",
    "}\n",
    "metric_name_to_label = {\n",
    "    \"dime\": \"DiME\",\n",
    "    \"sentence-entropy\": \"Entropy (Sentence)\",\n",
    "    \"dataset-entropy\": \"Entropy (Dataset)\",\n",
    "    \"infonce\": \"InfoNCE\",\n",
    "    \"lidar\": \"LiDAR\"\n",
    "}\n",
    "\n",
    "metric_name_to_normalization = {\n",
    "    \"dime\": \"raw\",\n",
    "    \"sentence-entropy\": \"maxEntropy\",\n",
    "    \"dataset-entropy\": \"maxEntropy\",\n",
    "    \"infonce\": \"raw\",\n",
    "    \"lidar\": \"raw\"\n",
    "}\n",
    "\n",
    "# Add a new metric for DiME normalized by dataset entropy\n",
    "metrics.append(\"dime-normalized\")\n",
    "metric_name_to_label[\"dime-normalized\"] = \"DiME / Dataset Entropy\"\n",
    "metric_name_to_normalization[\"dime-normalized\"] = \"raw\"\n",
    "for revision in revisions:\n",
    "    dime_results = results[(revision, \"dime\")][\"raw\"]\n",
    "    dataset_entropy_results = results[(revision, \"dataset-entropy\")][\"maxEntropy\"]\n",
    "    if len(dime_results) != len(dataset_entropy_results):\n",
    "        raise ValueError(f\"Length mismatch for revision {revision}: DiME ({len(dime_results)}) vs Dataset Entropy ({len(dataset_entropy_results)})\")\n",
    "    normalized_results = [dime / entropy if entropy != 0 else 0 for dime, entropy in zip(dime_results, dataset_entropy_results)]\n",
    "    results[(revision, \"dime-normalized\")] = {\"raw\": normalized_results}\n",
    "\n",
    "\n",
    "A = len(models)\n",
    "B = len(metrics)\n",
    "fig, axs = plt.subplots(A, B, figsize=(5 * B, 5 * A))\n",
    "if B == 1:\n",
    "    axs = [axs]\n",
    "\n",
    "\n",
    "\n",
    "for i, model_family in enumerate(models):\n",
    "    for j, evaluation_metric in enumerate(metrics):\n",
    "        ax = axs[i, j] if A > 1 and B > 1 else axs[j] if A == 1 else axs[i]\n",
    "        \n",
    "        if results:\n",
    "            cmap = plt.cm.Greens\n",
    "            norm = colors.LogNorm(vmin=min(pythia_revision_steps), vmax=max(pythia_revision_steps))\n",
    "\n",
    "            for revision in revisions:\n",
    "                metric_results = results[(revision, evaluation_metric)]\n",
    "                metric_results = metric_results[metric_name_to_normalization[evaluation_metric]]\n",
    "                \n",
    "                if revision == 'main':\n",
    "                    step = 143000\n",
    "                else:\n",
    "                    step = int(revision.split('step')[1])\n",
    "                color = cmap(norm(step))\n",
    "\n",
    "                ax.plot(range(len(metric_results)), metric_results, marker='o', color=color, label=f'Step {step}')\n",
    "        else:\n",
    "            ax.text(0.5, 0.5, 'Missing data', ha='center', va='center', fontsize=12, color='red')\n",
    "\n",
    "        ax.set_xlabel('Layer')\n",
    "        ax.set_ylabel(metric_name_to_label[evaluation_metric])\n",
    "        if i == 0:\n",
    "            ax.set_title(metric_name_to_label[evaluation_metric])\n",
    "        \n",
    "        ax.grid(True)\n",
    "\n",
    "        sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "        sm.set_array([])\n",
    "        cbar = plt.colorbar(sm, ax=ax)\n",
    "        cbar.set_label('Training Step')\n",
    "\n",
    "# Set row labels\n",
    "for i, model in enumerate(models):\n",
    "    y_position = (A-i-0.5)/A\n",
    "    if i == len(models)-1:\n",
    "        y_position += 0.02\n",
    "    fig.text(-0.001, y_position, model_name_to_label[model], rotation=90, va='center', ha='right', fontsize='x-large')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Todo: for dime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib.colors as colors\n",
    "\n",
    "SHOULD_USE_DEPTH_PERCENTAGE = False\n",
    "\n",
    "models = [\"EleutherAI\"]\n",
    "experiments = [\"dime\"]\n",
    "\n",
    "A = len(models)\n",
    "B = len(experiments)\n",
    "fig, axs = plt.subplots(A, B, figsize=(5 * B, 5 * A))\n",
    "if A == 1:\n",
    "    axs = [axs]\n",
    "\n",
    "\n",
    "dime_results = load_results_layerwise_entropies(\"EleutherAI\", \"dime\", granularity=\"sentence\", normalization=\"raw\")\n",
    "\n",
    "row_labels = ['Pythia']\n",
    "col_labels = ['DiME']\n",
    "\n",
    "# Iterate over models and experiments\n",
    "for i, model_family in enumerate(models):\n",
    "    for j, evaluation_metric in enumerate(experiments):\n",
    "        # Load entropies for each model and experiment\n",
    "        granularity = 'dataset'\n",
    "\n",
    "        normalization = 'raw'\n",
    "        normalization = 'raw' if 'curvature' in evaluation_metric else normalization\n",
    "        normalization = 'raw' if 'infonce' in evaluation_metric else normalization\n",
    "        normalization = 'maxEntropy' if 'lidar' in evaluation_metric else normalization\n",
    "\n",
    "        layerwise_entropies = load_results_layerwise_entropies(model_family, evaluation_metric, granularity=granularity, normalization=normalization, revisioned=True)\n",
    "\n",
    "        # Access the corresponding subplot\n",
    "        ax = axs[i, j] if A > 1 and B > 1 else axs[j] if A == 1 else axs[i]\n",
    "        if layerwise_entropies is not None:\n",
    "            for model_size_idx, (model_variant, entropies) in enumerate(layerwise_entropies.items()):\n",
    "                step = int(model_variant.split('_step')[1])\n",
    "                color = plt.cm.Greens((30000+step) / 150000)  # Normalize step to [0, 1]\n",
    "\n",
    "\n",
    "                entropies /= (2*entropy_results)**1\n",
    "\n",
    "                if SHOULD_USE_DEPTH_PERCENTAGE:\n",
    "                    depth_percentage = np.linspace(0, 100, len(entropies))\n",
    "                    ax.plot(depth_percentage, entropies, marker='o', color=color)\n",
    "                else:\n",
    "                    ax.plot(entropies, marker='o', color=color)\n",
    "        else:\n",
    "            ax.text(0.5, 0.5, 'Missing data', ha='center', va='center', fontsize=12, color='red')\n",
    "\n",
    "        if SHOULD_USE_DEPTH_PERCENTAGE:\n",
    "            ax.set_xlabel('Depth Percentage')\n",
    "        else:   \n",
    "            ax.set_xlabel('Layer')\n",
    "        ax.set_ylabel(f'{col_labels[j]}')\n",
    "        if i==0:\n",
    "            ax.set_title(f'{col_labels[j]}')\n",
    "        \n",
    "        ax.grid(True)\n",
    "\n",
    "        # Add colorbar as legend\n",
    "        cmap = plt.cm.get_cmap('Greens').copy()\n",
    "        sm = plt.cm.ScalarMappable(cmap=cmap, norm=plt.Normalize(vmin=0, vmax=140000))\n",
    "        sm.set_array([])\n",
    "        cbar = plt.colorbar(sm, ax=ax)\n",
    "        cbar.set_label('Training Step')\n",
    "\n",
    "# Set row labels using ax.text\n",
    "for i, row in enumerate(row_labels):\n",
    "    y_position = (A-i-0.5)/A\n",
    "    if i == len(models)-1:\n",
    "        y_position += 0.02\n",
    "    fig.text(-0.001, y_position, row, rotation=90, va='center', ha='right', fontsize='x-large')\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "information_plane",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
